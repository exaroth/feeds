<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Reddit</title><link>https://konrad.website/feeds/</link><description></description><item><title>Simple Made Inevitable: The Economics of Language Choice in the LLM Era</title><link>https://felixbarbalet.com/simple-made-inevitable-the-economics-of-language-choice-in-the-llm-era/</link><author>/u/alexdmiller</author><category>reddit</category><pubDate>Sun, 1 Mar 2026 03:45:18 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Two years ago, I wrote about managing twenty microservices at Qantas with a small team. The problem was keeping services in sync, coordinating changes across system boundaries, fighting the  of a codebase that grew faster than our ability to reason about it. Many years before my time, someone had chosen Clojure to build these systems. I suggested we add Polylith - this was a powerful combination because it enabled us to attack that "entropy" directly. Composition over coordination. Data over ceremony. Simplicity over familiarity.I described it at the time as a "fight against accidental complexity" - the stuff that isn't the problem itself, but the overhead imposed by our tools and processes. The stuff that accretes.Fast forward to today - I've been watching LLM coding agents struggle with the exact same fight, and I think the choice of language matters far more than most people realise. I've used Clojure for a decade, and I'm biased. But I think the  have shifted in ways that make my bias look less like preference and more like - well, let's call it a "fortunate capital allocation".The distinction that mattersFred Brooks drew the line in 1986. In "No Silver Bullet," he separated the difficulty of software into two categories:  - fundamental to the problem, irreducible - and  - imposed by our tools, languages, and processes. Brooks argued that no tool would deliver an order-of-magnitude improvement because most of programming's difficulty is essential. But he also argued that accidental complexity was the  part amenable to radical improvement.Rich Hickey picked up that thread and built a programming language around it (Clojure).In his 2011 talk "Simple Made Easy," Hickey drew a distinction that the industry has spent fifteen years : the difference between  (objectively unentangled, not braided together) and  (familiar, near to hand, comfortable). The industry systematically confuses the two. We choose languages because they're easy - because the syntax looks familiar, because we can find developers on LinkedIn, because there are ten thousand Stack Overflow answers for every error message. Not many people choose languages because they're simple.Hickey's word for accidental complexity is "incidental." As he puts it: "Incidental is Latin for ."He catalogued the sources with uncomfortable precision. State complects everything it touches. Objects complect state, identity, and value. Methods complect function and state. Syntax complects meaning and order. Inheritance complects types. Every one of these entanglements is a source of accidental complexity that has nothing to do with the problem you're trying to solve.Clojure was designed to avoid these entanglements. Immutable data by default. Plain maps instead of class hierarchies. Functions instead of methods. Composition instead of inheritance. It was, and is, a language that optimises for simplicity over ease.For fifteen years, the response has been: "Sure, but the learning curve.", or "Sure, but we can't hire Clojure developers, it's too niche."And there it is. The objections that no longer hold.The learning curve is deadNathan Marz recently described building a complex distributed system with Claude Code using Rama, a Clojure framework. Claude absorbed the framework's patterns through a few corrections and some documentation, and then wrote load modules, transaction handlers, and query topologies fluently. Marz's conclusion is worth reading carefully:"If AI can absorb a framework's semantics quickly, then the right framework to choose is the one with the best actual abstractions - the one that eliminates the most accidental complexity - regardless of how 'easy to learn' it is for a human picking it up on a weekend. Developer familiarity stops being the dominant selection criterion."Read that again. Developer familiarity stops being the dominant selection criterion.Wes McKinney - the creator of pandas, a developer who knows something about language ecosystems - demonstrates this from the other direction. He writes in his recent essay "The Mythical Agent-Month" that he "basically does not write code anymore, and now writes tons of code in a language (Go) I have never written by hand."The barrier to entry for all languages has collapsed. An LLM doesn't look at Clojure's parentheses and feel intimidated. It doesn't need a weekend tutorial. It doesn't care whether the syntax resembles what it learned in university. The "easy" axis - familiarity, comfort, prior experience - has been zeroed out.What remains is the "simple" axis. The intrinsic quality of the abstractions.Thinking like an economist: the learning curve was always a switching cost, not a measure of the language's value. It's easy to confuse the price of entry with the value of the asset. Now, LLMs have driven that switching cost toward zero. What's left is the underlying return on investment - and that's where Clojure was built to compete.McKinney's essay contains what I think is the most important observation about LLM-assisted development written so far:"I am already dealing with this problem as I begin to reach the 100 KLOC mark and watch the agents begin to chase their own tails and contextually choke on the bloated codebases they have generated."He calls this "technical debt on an unprecedented scale, accrued at machine speed."Stop me if you've heard this one before. Systems grow and age, they accrete, they accumulate stuff. The accidental complexity compounds until the codebase becomes too large and too tangled for anyone (human or machine) to navigate effectively. I described this at Qantas as a problem of coordination overhead and context-switching costs. McKinney is describing the same phenomenon, accelerated by an order of magnitude.The mechanism is straightforward. LLMs are, as McKinney puts it, "probably the most powerful tool ever created to tackle accidental complexity." They can refactor, write tests, clean up messes. But they also  new accidental complexity as a byproduct: "large amounts of defensive boilerplate that is rarely needed in real-world use," "overwrought solutions to problems when a simple solution would do just fine."Brooks predicted this. His "No Silver Bullet" argument is that agents are brilliant at accidental complexity but struggle with essential design problems - and worse, they can't reliably tell the difference. They attack the accidental complexity with extraordinary capability while simultaneously producing more of it.This is where language choice becomes a capital allocation decision with compounding returns. The brownfield barrier isn't about whether an LLM  write Python or Go or JavaScript - of course it can. It's about what happens at scale. The cost of a language choice isn't visible in the first ten thousand lines. It's visible at a hundred thousand, when the compounding effects of accidental complexity become the dominant cost. Classic economics where marginal cost curves that look flat early and then inflect sharply.Why Clojure pushes the barrier furtherClojure attacks this "brownfield barrier" from multiple directions simultaneously, and the effects compound.Martin Alderson's analysis of Rosetta Code tasks across nineteen popular languages found Clojure to be the most token-efficient. Not by a trivial margin:These aren't obscure comparisons. Python, JavaScript, and Java are the three most used languages in the world. Clojure expresses the same logic in roughly a fifth fewer tokens than Python and a third fewer than JavaScript or Java.Why does this matter? Because context windows are a hard constraint, and they degrade non-linearly. Research from Stanford and Berkeley shows that LLM performance drops by more than 30% when relevant information falls in the middle of the context window. Factory.ai found that models claiming 200,000 tokens of context become unreliable around 130,000 - a sharp cliff, not a gentle slope. Anthropic describes context engineering as a first-class discipline, noting that "structured data like code consumes disproportionately more tokens."If 80% of a coding agent's context window is code - reads, edits, diffs - then Clojure's 19% token advantage over Python translates to roughly 15% more room for actual problem context. Against JavaScript or Java, it's nearly 30% more room. Over a long session with multiple file reads and iterative edits, this compounds. The agent that runs out of useful context first loses.And these are just the token-level numbers. At the program level, the difference is starker. Anthony Marcar at WalmartLabs reported that "Clojure shrinks our code base to about one-fifth the size it would be if we had written in Java." A fifth. McKinney's 100 KLOC brownfield barrier in Go could be structurally unreachable in Clojure - not because the agent is smarter, but because there's less accidental complexity for it to choke on.Immutability eliminates defensive boilerplateMcKinney specifically identifies that agents "tend to introduce unnecessary complexity, generating large amounts of defensive boilerplate." Null checks. Defensive copies. Synchronisation guards. Clojure's immutable data structures eliminate entire categories of this bloat. The agent literally cannot generate certain kinds of accidental complexity because the language makes it unnecessary.As Hickey puts it: "Values support reproducible results. If you define a function in terms of values, every time you call that function with the same values, will you get the same answer? Yes."An LLM reasoning about immutable code doesn't need to track  a variable was modified or . It can reason algebraically: this function takes X and returns Y. Full stop. No temporal reasoning required. That's fewer balls to juggle - and as Hickey reminds us, even the best juggler in the world maxes out at about twelve.Stuart Halloway made this point devastatingly in his talk "Running With Scissors." When you use typed structs or classes, "all of your data manipulation scissors are gone. You do not have generic data any more. Each one of these structs requires its own custom special scissors to manipulate it."With Clojure's maps, the LLM learns one toolkit - , , , ,  - that works on  data. With an object-oriented language, the LLM must learn a different API for every class. That's the difference between O(n) and O(n^2) in what the agent must hold in context. As the codebase grows, this gap widens.The REPL closes the feedback loopHalloway's formulation is the best I've seen: "REPL + Functional = faster bricks. Things that you understand how they are going to work. They always work the same way, and you can compose them to build your system." And the dark corollary: "REPL + imperative = faster spaghetti. If you are a net negative producing developer and we speed you up... we have just made things worse."An LLM agent at a Clojure REPL can evaluate any expression in the running system, inspect the result, and adjust. No compilation step. No build system. No waiting. The feedback loop is as tight as it gets.I should note that the major coding agents today - Claude Code, Codex, Cursor - don't use REPLs. They use file-edit, compile-or-test, read-errors, iterate loops. The industry has implicitly chosen compiler-style feedback. This is worth engaging with honestly.But the evidence is more nuanced than it appears. Research on CodePatchLLM (KDD 2024) found that compiler feedback improves Java and Kotlin code generation by 45% - but provides  improvement for Python, because there's no compiler feedback to give. Dynamic languages get nothing from the compile loop. Replit Agent, notably,  use a REPL-based verification system and reports results three times faster and ten times cheaper than previous approaches.And Halloway's distinction cuts precisely here. A Python or JavaScript REPL creates exactly the temporal coupling problem that critics identify - mutable state accumulating in the session, order-dependent evaluation, "faster spaghetti." Clojure's REPL evaluates expressions that return immutable values. Data in, data out. No temporal coupling. The REPL provides richer feedback than a compiler - actual return values, not just "compiled" or "didn't" - while Clojure's immutability means it doesn't create the stateful mess that imperative REPLs do. Clojure-MCP bridges the remaining gap: the agent writes to files and validates in the REPL. Bille reported tasks completing in hours instead of days.There's a revealing irony buried in the data. McKinney chose Go for his new projects - a language famous for its simplicity. He writes it via LLM agents and hits the brownfield barrier at 100 KLOC.But Go's simplicity is an  simplicity in Hickey's sense. It's familiar. It's readable. You can hire for it. It achieves this through verbosity: explicit error handling on every function call, no generics until recently, no macros, no metaprogramming. For human programmers, this verbosity is a feature - it makes code predictable and reviewable.For LLM agents, it's a tax.Alderson's data shows Go as one of the more token-inefficient popular languages. Every if err != nil { return err } consumes tokens that could be used for problem context. The language chosen for  simplicity creates  problems. Go is optimised for human-readable code; Clojure is optimised for expressing ideas with minimal ceremony. The LLM era rewards the latter.There's a seductive counter argument here: that Go's verbosity actually  the model reason. Verbose output as chain-of-thought scaffolding - the same mechanism that helps LLMs solve maths problems. More tokens, more thinking.It's wrong, and the architecture tells you why.Modern reasoning models - o1, o3, Claude with extended thinking - do their reasoning in hidden tokens that are discarded after generation. The thinking has already happened before the model outputs a single character of code. Go's if err != nil { return err } is output tokens, not reasoning tokens. It doesn't expand the model's thinking budget. It spends the context budget.The empirical evidence is decisive. Research presented at ICML 2025 found that generating code , then reasoning, yielded a 9.86% improvement over the traditional reason-then-code order. If verbose output were serving as reasoning scaffolding, the opposite should be true. The Token Sugar paper (ICSE 2025) systematically compressed high-frequency verbose patterns - exactly the kind Go generates - and achieved up to 15.1% token reduction with near-identical correctness scores. If the boilerplate were contributing to correctness, removing it would degrade performance. It didn't.Worse, context dilution research shows that repetitive, low-information tokens actively harm performance by diluting the model's finite attention budget - accuracy drops of 13.9 - 85%. Every  repeated fifty times across a codebase isn't scaffolding. It's noise competing for the model's attention with the actual problem.Let's assess some of the arguments against my thesis above - some of which are genuinely strong.LLMs are measurably worse at ClojureThis is the big one. The FPEval benchmark found that GPT-5 generates code with 94% imperative patterns in Scala, 88% in Haskell, and 80% in OCaml. LLMs don't just write worse functional code - they write imperative code  as functional code, and the prevalence of non-idiomatic patterns actually  alongside gains in functional correctness. Jack Palvich's Gemini experiments across twenty-four languages found that "the Lisps suffer from paren mis-matches and mistakes using standard library functions." The MultiPL-E benchmark shows performance correlating with language popularity. And the "LLMs Love Python" paper found that models default to Python in 93-97% of language-agnostic problems.This is real. I'm not going to pretend it isn't.But notice what's actually being measured. These benchmarks measure whether the LLM can generate a  in language X. They don't measure whether the resulting  - the codebase at 50 or 100 KLOC - is maintainable, navigable, or tractable for future agent sessions. "Better at generating Python" and "Python generates better systems" are different claims.And the FPEval result is, if you squint, actually evidence  the thesis. If LLMs default to imperative patterns even when writing in functional languages, then the language's  matter more, not less. Clojure's immutability isn't a suggestion - it's a default. The language itself acts as a guardrail. An LLM generating Clojure has fewer ways to produce the kind of stateful, tangled code that compounds into the brownfield barrier. You can't mutate what the language won't let you mutate.The parenthesis problem is real but solvable. Julien Bille documented his experience with Clojure-MCP: initially "simple things took way too long" and the AI was "unable to get parentheses right." But after integrating s-expression-aware tooling, "the agent experience got much better" and "it goes a LOT faster to write good code solutions." The parenthesis issue is a tooling gap, not a fundamental limitation.And the training data argument is about the , not the . Models are improving rapidly. The accidental complexity argument is about permanent properties of the language. One is a snapshot; the other is a trajectory.And the snapshot is less damning than it looks. Cassano et al.'s MultiPL-E study (IEEE TSE, 2023) found that model perplexity - how uncertain the model is when predicting the next token - is not strongly correlated with the correctness of generated code. Codex's perplexity (uncertainty) was highest for JavaScript and TypeScript, yet it performed best on those languages. Some niche languages performed as well as popular ones. Training data volume is not the determinant the gravity well argument assumes.MultiPL-T (OOPSLA, 2024) went further: fine-tuning on automatically translated data closed the gap entirely. Lua exceeded base Python performance after targeted fine-tuning. Julia saw 67% relative improvement. The gap isn't a permanent feature of the landscape - it's bridgeable engineering.There's also the question of cross-lingual transfer. Research on scaling laws for code found that training on one language improves performance on related languages. Clojure sits on the JVM. The massive Java training corpus isn't irrelevant - it's a shared ecosystem, shared libraries, shared concepts. Static type systems provide a feedback loop Clojure lacksAlso strong. Research from ETH Zurich (PLDI 2025) shows that type-constrained decoding reduces compilation errors by more than half and increases functional correctness by 3.5-5.5%. TypeScript advocates report 90% reductions in certain bug categories. Rust's strict compiler creates tight generate-compile-fix loops.I'll grant it: types help LLMs get individual functions right. The evidence is clear.But types also create coupling. As Hickey argues: "Statically typed languages yield much more heavily coupled systems. Flowing type information is a major source of coupling in programs." Types help the LLM write correct function A. But they also create structural dependencies between A, B, C, and D that make the  harder to reason about as it grows. The question is which effect dominates at scale - and McKinney's brownfield barrier suggests that system-level coupling is the bigger problem.Clojure offers a middle path. Spec and Malli provide optional schema validation - type-like constraints when you want them, without the token overhead and coupling when you don't. And the REPL provides a runtime feedback loop that is arguably faster than a compilation cycle: the agent evaluates an expression, sees the result or the error, and corrects immediately.This is how I'm leveraging Clojure (and Polylith) while I'm building AXONLORE - components with Malli function schema on every interface, enforced at testing and development time.It's also worth noting Alderson's data: Haskell and F#, typed languages with strong inference, are nearly as token-efficient as Clojure. If the type system feedback loop is your priority, those are better choices than TypeScript or Rust, both of which are significantly more token-heavy. But Haskell and F# have their own ecosystem and adoption challenges. There's no free lunch.The ecosystem is small and hiring is hardThis is the objection I've spent a decade fielding, and it cuts differently now. If developers aren't writing code by hand, "knowing Clojure" matters less than having good design taste - which McKinney identifies as the scarce resource: "Design talent and good taste are the most scarce resources, and now with agents doing all of the coding labor, I argue that these skills matter more now than ever."The hiring bottleneck shifts from language fluency to architectural judgement. Clojure developers tend to be more senior and more experienced. That's exactly the profile McKinney says will thrive.And on ecosystem: Clojure has access to the entire JVM ecosystem through Java interop. The "small ecosystem" argument was always about discoverability for humans - and LLMs don't need Stack Overflow. There's one more structural advantage worth noting. Hickey argued in his talk "Spec-ulation" that "dependency hell is not a different thing than mutability hell. It IS mutability hell. It is just at this scale."LLMs are trained on vast codebases. Breaking changes in a language ecosystem mean that the training data contains conflicting information about the same names.  has meant the same thing for seventeen years. Compare that with Python 2 versus 3, React class components versus hooks versus server components, Angular.js versus Angular, or JavaScript's shifting parade of module systems.Stability means consistent training signal. Consistent signal means more reliable output. This isn't a flashy advantage, but it's a durable one. When an LLM generates Clojure, it's drawing on seventeen years of consistent semantics. When it generates React, it's navigating a minefield of deprecated patterns, version-specific APIs, and conflicting idioms from different eras of the framework.Erik Bernhardsson built a tool called Git of Theseus - after the philosophical paradoxabout the ship whose planks are replaced one by one until nothing original remains. Run itagainst a Git repository and it shows you what percentage of each year's code survives intothe present. The half-life of a line of code in Angular is 0.32 years. In Rails, 2.43years. In Linux, 6.6 years. Linux's longevity, Bernhardsson notes, comes from itsmodularity - drivers and architecture support scale linearly because they have well-definedinterfaces. Each marginal feature takes roughly the same amount of code. Bad projects, on the other hand, scale superlinearly - every marginal feature takes more and more code.Rich Hickey published code retention charts for Clojure in his ACM paper "AHistory of Clojure." The Clojure chart is nearly flat - almost all code from every releasesurvives into the current version. For an LLM, this is the difference between signal and noise. Every breaking change in alanguage's history creates conflicting training data - the same function name meaningdifferent things in different eras. Every renamed API, every deprecated pattern, everyframework migration is a source of confusion that the model must navigateprobabilistically. Clojure's stability means the probability mass is concentrated. There'sone way to use map, one way to use assoc, and that's been true since 2007. The modeldoesn't have to guess which era of the language it's generating for.I'm not arguing that Clojure is perfect. I'm arguing that the selection criteria have changed, and we haven't updated our decision-making frameworks to match.The industry has - until now - selected languages for human convenience: familiar syntax, large hiring pools, abundant tutorials, massive ecosystems of libraries with thousands of GitHub stars. These were rational criteria when humans wrote the code. They optimised for the dominant constraint.But the dominant constraint has shifted. Humans increasingly don't write the code. Machines do. And machines have different constraints: context windows, token efficiency, the ability to reason about entangled state, the compounding cost of accidental complexity at scale.The question you should ask is: what's the time horizon?If you're building a prototype that needs to work next week, use Python. The LLM is better at it today, the ecosystem is massive, and the brownfield barrier is someone else's problem (perhaps future you?). This is the savings account - safe, familiar, reliable returns.If you're building something you plan to maintain for five years, the calculation changes. The language that generates the most maintainable codebase - the one that produces the least accidental complexity per unit of work, that fits more meaning into fewer tokens, that constrains the agent away from its worst impulses - that's the language with the higher compounding return. Even if the individual function quality is lower today.There's also an uncomfortable possibility lurking here: that the best language for LLMs might not be any existing language at all. Perhaps we'll see languages designed from scratch for machine cognition - token-efficient, structurally regular, with built-in verification. But if we're choosing among what exists today, the properties Hickey optimised for seventeen years ago - simplicity, immutability, data orientation, homoiconicity, stability - happen to be exactly what machines need.There's an obvious outcome though, at least while humans still choose the tools. Developer preference, hiring committees, LinkedIn keyword searches - these are powerful forces, and they don't evaporate just because the code is being written by a machine. The industryhas spent decades optimising for human convenience, and switching costs are real. It's entirely possible we stick with the popular languages for another decade, not because they're the most efficient allocation of capital, but because the humans holding the cheque books are comfortable with them.My bet is on the other outcome. An industry that chose languages for humans will eventually notice that the humans have left the keyboard. And when the constraint you optimised for no longer binds, the economics eventually catch up. They always do.]]></content:encoded></item><item><title>Use Fly.io to power Kubernetes LoadBalancer services</title><link>https://github.com/zhming0/fly-tunnel-operator</link><author>/u/zhming0</author><category>reddit</category><pubDate>Sun, 1 Mar 2026 02:32:40 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why does everything gets removed here?</title><link>https://www.reddit.com/r/golang/comments/1rhk451/why_does_everything_gets_removed_here/</link><author>/u/o82</author><category>golang</category><category>reddit</category><pubDate>Sun, 1 Mar 2026 01:28:22 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Sorry, this post has been removed by moderators of r/golang.Seriously, what is wrong with the mods of this community?I keep finding interesting posts, leaving them open to read later, and when I come back - gone. No explanation. No discussion. Just removed.Anything that mentions another language alongside Go? Removed. Any criticism - even constructive, technical criticism? Removed. Comparisons? Tradeoffs? Real-world frustrations? Also removed.What's the point of a discussion forum where discussion itself is unwelcome?I'm not talking about spam or low-effort posts - obviously that should be moderated. But when normal conversations disappear just because they're not pure praise, it stops feeling like a community and starts feeling like a curated promo page.People learn by comparing tools. People improve things by criticizing them. That's how engineering works. Pretending a language has no downsides doesn't make it better - it just makes the conversation worse.Threads are vanishing faster than anyone can actually participate in them. It's exhausting.I want to enjoy reading and participating here, but what's the point if everything remotely interesting gets wiped?Anyone else noticing this, or is it just me?]]></content:encoded></item><item><title>I built go-date-fns: 140+ date utility functions for Go, inspired by date-fns</title><link>https://www.reddit.com/r/golang/comments/1rhianu/i_built_godatefns_140_date_utility_functions_for/</link><author>/u/LazyDog80</author><category>golang</category><category>reddit</category><pubDate>Sun, 1 Mar 2026 00:06:06 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I got tired of writing boilerplate date logic in every Go project, so I builtgo-date-fns â€” a comprehensive date utility library inspired by the popularJavaScript date-fns library.- 140+ pure, immutable functions- Business days, ISO weeks, interval utilities- FormatDistance ("about 2 hours ago")- Timezone-aware operations- Zero external dependenciesimport "github.com/chmenegatti/go-date-fns/dateutils"// Add 5 business days (skips weekends)deadline := dateutils.AddBusinessDays(time.Now(), 5)// Human-readable relative timefmt.Println(dateutils.FormatDistanceToNow(deadline, &dateutils.FormatDistanceOptions{AddSuffix: true}))Coming from JavaScript? The API will feel very familiar.]]></content:encoded></item><item><title>Data Range intersection Lib</title><link>https://www.reddit.com/r/golang/comments/1rhhv7m/data_range_intersection_lib/</link><author>/u/Obvious-Image-9688</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 23:47:58 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Implements the universal span intersection algorithm. The algorithm represents a unified way to find intersections and overlaps of "one dimensional spans" of any data type. The package is built around the SpanUtil[E any] struct, and the manipulation of the SpanBoundry[E any] interface.The SpanUtils[E any] struct requires 2 methods be passed to the constructor in order to implement the algorithm:A "Next" function, takes a given value and returns next value. The next value must be greater than the input valueThe algorithm is primarily implemented by 3 methods of the SpanUtil[E] struct:FirstSpan, finds the initial data span intersection.NextSpan, finds all subsequent data span intersections.CreateOverlapSpan, finds the most common intersection of all overlapping spans.Other features of this package:Provide ways to consolidate overlaps.Iterate through intersections of multiple data sets.In this example we will find the intersections of 3 sets of integers. The full example can be found: here.Setup the package and imports:We will need to import our "st" package along with the "fmt" and "cmp" packages in order to process the example data sets.import ( "github.com/akalinux/span-tools" "fmt" "cmp" ) Create our SpanUtil[E] instance:We will use the factory interface NewSpanUtil to generate our SpanUtil[int] instance for these examples. This ensures that the Validate and Sort options are by set to true for all base examples.var u=st.NewSpanUtil( // use the standard Compare function cmp.Compare, // Define our Next function func(e int) int { return e+1}, ) Find our the initial SpanBoundry intersection:We need to find the initial intersection, before we can iterate through of these data sets. The initial SpanBoundry is found by making a call to u.FirstSapn(list).// Create our initial span var span,ok=u.FirstSpan(list) // Denote our overlap set position var count=0 Iterate through all of our SpanBoundry intersections:We can now step through each data intersection point and output the results. Each subsequent intersection is found by making a call to u.NextSpan(span,list).for ok { // Get the indexes of the columns this overlap relates to var sources=u.GetOverlapIndexes(span,list) // output our intersection data fmt.Printf("Overlap Set: %d, Span: %v, Columns: %v\n",count,span,sources) // update our overlap set count++ // get our next set span,ok=u.NextSpan(span,list) } Overlap Set: 0, Span: &{1 1}, Columns: &[0] Overlap Set: 1, Span: &{2 2}, Columns: &[0 1] Overlap Set: 2, Span: &{3 5}, Columns: &[1 2] Overlap Set: 3, Span: &{6 7}, Columns: &[1 2] Overlap Set: 4, Span: &{8 11}, Columns: &[2] ]]></content:encoded></item><item><title>pending: minimal pure-Go deferred task scheduler (ID debounce + cancel + graceful shutdown)</title><link>https://www.reddit.com/r/golang/comments/1rhdj9c/pending_minimal_purego_deferred_task_scheduler_id/</link><author>/u/Hungry-Plantain-1008</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 20:46:11 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I released pending, a tiny scheduler for in-memory deferred work in Go:ID-based scheduling and debouncing (reschedule same ID)concurrency limits: StrategyBlock / StrategyDropzero dependencies (stdlib only)Itâ€™s deliberately not cron syntax or persistent job storage. Target use case is process-local deferred actions.Would love feedback on API design and edge cases I should harden.   submitted by    /u/Hungry-Plantain-1008 ]]></content:encoded></item><item><title>How suitable is Golang for building an eCommerce website?</title><link>https://www.reddit.com/r/golang/comments/1rhdfon/how_suitable_is_golang_for_building_an_ecommerce/</link><author>/u/Worth-Leader3219</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 20:42:16 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[How suitable is Golang for building an eCommerce website?Iâ€™ve been searching online but havenâ€™t found any ready-to-use frameworks or boilerplates specifically for building eCommerce websites with Golang.Do you have any experience building eCommerce sites with Golang?Iâ€™m also interested to know whether itâ€™s possible to build both the backend and frontend using pure Golang and Go libraries only, instead of separating the frontend into another language or framework?]]></content:encoded></item><item><title>Segment Anything with One mouse click</title><link>https://eranfeit.net/one-click-segment-anything-in-python-sam-vit-h/</link><author>/u/Feitgemel</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 20:07:44 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Last Updated on 30/01/2026 by Eran FeitSegment Anything in Python lets you segment any object with a single click using SAM ViT-H, delivering three high-quality masks instantly.In this tutorial, youâ€™ll set up the environment, load the checkpoint, click a point, and export overlaysâ€”clean, practical code included.Whether youâ€™re labeling datasets or prototyping, this one-click workflow is quick, reliable, and easy to reuse.Segment Anything in Python builds on a powerful promptable segmentation pipeline: a ViT-H image encoder extracts features once, a lightweight prompt encoder turns your click into guidance, and a mask decoder returns multiple high-quality candidates. This tutorial shows the exact flowâ€”load the checkpoint, set the image, provide a single positive point, and review three masks with scoresâ€”so you can pick the cleanest boundary without manual tracing.Segment Anything in Python is also practical beyond demos: youâ€™ll learn how to avoid OpenCV headless conflicts, run on CPU/GPU/MPS, and export overlays for quick sharing. We also cover adding negative points to suppress spillover, saving binary masks for downstream tasks, and keeping your run reproducible with clear paths and model_type matching. Use it to bootstrap datasets, refine labels, or prototype segmentations in seconds.



For a deeper dive into automatic mask creation from detections, see my post on YOLOv8 object detection with Jetson Nano and OpenCV.



ðŸš€ Want to get started with Computer Vision or take your skills to the next level ?Create a conda environment, install PyTorch (CUDA optional), and add the key libraries: , , and .These steps make your runtime stable and reproducible.Youâ€™re creating an isolated Python 3.9 environment, ensuring compatible PyTorch/CUDA, installing OpenCV + Matplotlib, and pulling SAM directly from the official repo. after this step, your machine is ready to run SAM and display interactive windows.Import NumPy, PyTorch, Matplotlib, OpenCV, then add three tiny helpers to draw masks, points, and boxes.These functions make SAMâ€™s results easy to see.Youâ€™ll visualize the clicked point (green star), optional negatives (red), and overlay semi-transparent masks on the image. your visual overlays are readyâ€”clicks and masks will be easy to inspect.



If you prefer a full framework, check out Detectron2 panoptic segmentation made easy for beginners for training-ready pipelines.



Load an image, open an OpenCV window, and  the object once.Press  to confirm and capture the coordinates.Youâ€™ll build a tiny helper function that returns the (x, y) coordinates of your clickâ€”SAMâ€™s only required input in this flow. you now have a single (x, y) pointing to the objectâ€”SAM will do the rest.



Want point-based interaction in videos? See Segment Anything in Python â€” no training, instant masks for more live demos.



Load the SAM checkpoint (ViT-H), move it to GPU if available, and attach a .Then set the current image so SAM can compute features.This step binds the model + image together and readies the predictor for your single click. SAM is loaded, on the right device, and primed with your image.



If youâ€™re exploring medical or structured masks, compare with U-Net medical segmentation with TensorFlow & Keras.



Turn your (x, y) into SAM inputs, get , show them, and save each result.Youâ€™ll see mask scores to help you pick your favorite.Youâ€™ll get three high-quality segmentations and PNGs saved to disk for later use. you now have three crisp segmentations savedâ€”choose the best and keep creating.



Next, try improving mask quality with post-processing or super-resolution: upscale your images and videos using super-resolution.



SAM is a general-purpose segmentation model that returns object masks from simple prompts like a single click. Itâ€™s ideal for fast labeling and prototyping.Use ViT-H for best quality. Use ViT-L/B for lower memory. Match model_type to your checkpoint name.No, but GPU or Apple MPS speeds up inference significantly. CPU works, just slower.Compare the three candidates by score and visual quality. Choose the one that cleanly captures your object.Yes. Label 0 for background to suppress unwanted regions. Mix positives and negatives for precision.Use opencv-python (GUI) instead of the headless build. The post includes a cleanup step.Anywhere. Update the codeâ€™s path_for_sam_model to match your file location.Yes. The code saves overlay images. You can also save binary masks by converting to 0/255 and writing with OpenCV.Yes. SAM accepts points and bounding boxes. Boxes help guide segmentation when objects are crowded.Absolutely. One-click masks are a quick way to bootstrap datasets or refine labels with minimal effort.Youâ€™ve just built a complete  tool around  in Python.The workflow is intentionally lightweight: create an environment, install SAM, click a point, and export masks.Because SAM generalizes broadly, itâ€™s excellent for new domains where you donâ€™t have labeled data yet.From here, you can add negative clicks for refinement, use bounding boxes, or integrate with super-resolution and post-processing to lift mask quality even further.If you plan to use this in production, consider wrapping the flow in a small GUI, storing your clicks/masks, and adding batch processing for entire image sets.For research, this pipeline is a fantastic way to prototype and compare segmentations across different scenes quickly.]]></content:encoded></item><item><title>How do you handle all these AI subscribtions?</title><link>https://www.reddit.com/r/artificial/comments/1rhbyyd/how_do_you_handle_all_these_ai_subscribtions/</link><author>/u/tdjordash</author><category>ai</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 19:44:16 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[how do you guys handle all these AI subscriptions? CLAUDE, ChatGpt, Gemini, Grok, Perplexity,Poe... they're all like $20/mo each do you just pick one? Or pay for 2 or more? Or use something that combines them.?...is it even worth paing for any of these? What's your setup?]]></content:encoded></item><item><title>A Rabbit Hole Called WebGL (8-part series on the technical background of a WebGL application w/ functional demo)</title><link>https://www.hendrik-erz.de/post/a-rabbit-hole-called-webgl</link><author>/u/nathan_lesage</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 19:14:49 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[I have this tradition. At least, it appears like a tradition, because it happens with frightening regularity. Every one to two years, as Christmas draws close, I get this urge to do something new. In 2017, I released a tiny tool that has turned into one of the go-to solutions for hundreds of thousands of people to write, Zettlr. In 2019, I wrote my first Rust program. In 2021, I did a large-scale analysis of the coalition agreement of the German â€œTraffic lightâ€ government. During the pandemic, I built a bunch of mechanical keyboards (because  I did). In 2023, I didnâ€™t really do much, but in 2024, I wrote a local LLM application. So okay, itâ€™s not necessarily every year, but if you search this website, youâ€™ll find many tiny projects that I used to distract myself from especially dire stretches in my PhD education.Now, is it a good use of my time to spend it on some weird technical topics instead of doing political sociology? I emphatically say yes. If you are a knowledge-worker, you need to keep your muscles moving. Even as a researcher, if you do too much of the same thing, you become less of a knowledge-worker, and more of a secretary. Call it an artistic outlet, that just so happens to make my research job . The last time I had to think about wrong data structures in my analytical code or when running some linear regression was â€¦ letâ€™s say a long time ago. The more I know about software and hardware, the more I can actually focus on my research questions when I turn to the next corpus of text data.But alright, you didnâ€™t click on this article because you wanted to hear me rationalize my questionable life choices, you want to read up on the next rabbit hole I fell into: OpenGL and WebGL. In the following, I want to walk you through the core aspects of what WebGL is and what you can do with it, what I actually did with it, and what the end result was. If youâ€™re not into technical topics (which, given the history of articles here, I actually have to start to doubt at this point), click here to see the full glory of my recent escapade.Note: In the following, I will skip over a lot of basics, and merely explain some interesting bits of the source code (which you can find here), central decisions I took, and things I learned. I donâ€™t verbatim copy the entire code that you can find in the repository. The entire thing is still insanely long and will span multiple articles, even though I try to leave out a lot which you can learn via, e.g., WebGLFundamentals, which I recommend you read to learn more.First, some context. At the end of 2024, someone complained that project exports in my app, Zettlr, were lacking any visual indication of their progress. As a quick primer: Zettlr uses Pandoc to convert Markdown to whichever format you choose. However, especially for long projects, exporting may take quite some time, during which the app looks as if itâ€™s doing nothing. You can still work with the app, and do things, but itâ€™s hard to know when Zettlr is actually done performing the project export. The biggest issue was less finding a way to just  users which background tasks are currently running, and more how to adequately visualize this to them. For quite a bit of time, my brain kept churning idea after idea in the search for a cool way to visualize â€œsomething is happening in the background.â€ You can read up on many discussions that Iâ€™ve had with Artem in the corresponding issue on the issue tracker.Indeed, the task was quite massive, because the requirements were so odd:The indication should convey a sense of â€œsomething is happeningâ€ without actually knowing the precise progress of the task being performed.It should quickly and easily convey how many tasks are currently running in the background, and what their status is.It should be so compact that it fits into a toolbar icon.It should absolutely avoid giving people the impression that something might be stuck.At some point, I had my  moment: Why not produce an iris-like visualization? Intuitively, it ticked all the boxes: One can animate the picture to convey a sense of movement without looking like a run-of-the-mill loading spinner that we have collectively come to dread; by coloring its segments, one can include several â€œthingsâ€ with different status; and by toggling between an â€œonâ€- and â€œoffâ€-state, one could indicate whether something is running, or not.I currently suspect that my brain simple mangled together the circular appearance of a loading spinner and the logo of 3Blue1Brown into a contraption that would prove to be insanely difficult to create.Because I wanted to convey a lot of subtle movement, I opted to choose WebGL to implement it, using all the fanciness of graphics processing. My thinking was as follows: I could combine something Iâ€™d have to do at some point anyway with something new to learn. I thought: â€œHow hard can it be to learn some shader programming on the side?â€â€¦ well, if youâ€™ve read until here, you know that I was  so wrong with my estimate of how long it would take as this time. What started as a â€œlet me hack something together in two Christmas afternoonsâ€ ended up being an almost two-week intensive endeavor that has had my partner get  mad at me for spending so much time in front of my computer.But now, it is done, and I have succeeded in achieving exactly what I had imagined weeks ago. To salvage what I can, I am writing these lines to let you partake in my experience, and maybe you find understanding the guts of GPU-accelerated rendering on the web even intriguing!On the page, there are four sections: Some settings, configuration for the segments, a frame counter, and the actual animation below that.Let me guide you through the settings first:: This setting sets how long it takes for the indicator to rotate once around. By default it is set to 120 seconds, so two minutes, but you can turn it down to increase its speed. The minimum setting is 10 seconds which is quite fast.: This setting determines how fast the individual rays will increase and shrink in size. It is pre-set to five seconds for one full movement, but you can turn it down to increase their speed. The minimum is 100ms, which is stupidly fast.: This enables or disabled multi-sample antialiasing. If disabled, the animation can look very rugged and pixelated.: This setting enables or disables the bloom effect which makes the entire indicator â€œglow.â€ This can actually reduce the performance of the animation quite a bit, but it also has a great visual impact.: This effectively allows you to determine how much blurring will be applied to the image. It is preset to 2Ã—, which is a good default. You can reduce it to 1Ã— which will make the effect more subtle. A setting of 8Ã— may be a bit much, but I decided to leave it in since I feel it is instructive.: This setting determines how detailed the resolution is. It is preset with whatever device pixel ratio your display has. If youâ€™re opening the website on a modern phone or on a MacBook, it will probably be preset to 2Ã—, but on other displays, it will be 1Ã—. It has a moderate performance impact.Segment adjustment step duration: This setting determines how fast the segment colors adjust when you change the segment counts in the next section.The next section allows you to determine the segments that will be displayed. As a reminder: The whole intention of this project was to visualize the status of running tasks, which might be successful, unsuccessful, or still en route. You have four segments available, and can determine how many tasks are in each segment, alongside their color. The colors are hard-coded because this way I can ensure that they all fit and blend together well.By default, the demonstration page will auto-simulate changes to the segments so that you donâ€™t have to click around. When the simulation is active it will, each second, determine what to do. There is a 30% chance each that one of the first three segments will be incremented by one. Further, there is a 10% chance that the simulation will reset everything to zero and start again.The last section includes settings for the frame rate. The frame rate simply means how often the entire animation will be re-drawn (hence, frames-per-second). At the top, it displays the current frame rate. The frame rate is bound to your display, so on a MacBook (which has a refresh rate of 120 Hz), the frame rate will be at most 120 frames per second. On my secondary display, the frame rate is 75 Hz.By default, I have implemented a frame limit of at most 30 frames per second. This ensures that the animation is still smooth without being too demanding on your computer or phone. However, you can change the frame rate to, e.g., 60 fps. This will render the animation twice as frequently. Especially if you turn the rotation speed to the max, you actually want to increase the frame limit, because on 30 frames per second, it can indeed look very stuttery.Feel free to play around with the settings to see how they change the animation. Again, you can also go through the source code of the animation to learn how it works.About This Article SeriesOver the next three months, I will publish one part per week on how I finally managed to achieve this feat. The logic behind it is very complex, and it takes a lot of research to understand how to achieve the various effects. The articles will be as follows:In the next article, I will introduce you to WebGL, OpenGL, and how to set everything up to actually start doing things with WebGL. I will talk about the basic architectural decisions I took, and how code can be properly organized. I will also introduce you to OpenGLâ€™s rendering pipeline, and how it works.In article three, I will guide you to drawing the rays that make up the iris. You will learn about how to provide data to OpenGL, and how the drawing actually works.In the fourth installment, I will talk through how to add two of the three animations that make up the iris: rotation, and the movement of the rays. This article almost exclusively focuses on JavaScript, and contains minimal changes to the shaders, because movement is mostly a thing of JavaScript.In article five, I will introduce you to the algorithm I designed to both color the segments of the iris according to the number of running tasks, i.e., the main goal of the entire endeavor. I will also explain the final, third animation that the indicator includes: animating the colors of the iris.This article will be more in-depth and explain another big part of OpenGLâ€™s rendering pipeline. It explains how to enable a renderer to perform post-processing. It also adds one post-processing step: tone-mapping.Article seven focuses on the centerpiece of the animation, the one big part that would not have been possible using other techniques such as SVG. I explain how to add a bloom post-processing step in between the ray rendering and the output, and how bloom actually works. (Itâ€™s surprisingly simple!)Adding Multi-Sample AntialiasingIn the eight and final practical article in this series, I explain MSAA a bit more in detail, why it sometimes works, and sometimes doesnâ€™t, and how to actually add it to the animation. I also explain the final piece of the OpenGL Rendering pipeline that you probably need to know to understand what is happening.When I set out to create this animation, I imagined it would take me maybe two days â€” nothing to write home about (literally). However, I was wrong, and, to the contrary, we are now looking towards an astonishing nine (!) articles just to explain what has happened here.I found the journey extremely rewarding, even though it ate up my winter holidays. I want to let you partake in what I learned, and I hope you stick along for the ride.So, please, come back next Friday for part two: Setting everything up!Jump directly to an article that piques your interest.]]></content:encoded></item><item><title>MQTT: The Protocol Behind Every Smart Device (Golang)</title><link>https://youtu.be/S64crfW9tQU</link><author>/u/huseyinbabal</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 19:03:40 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>MQTT: The Protocol Behind Every Smart Device (Golang)</title><link>https://youtu.be/S64crfW9tQU</link><author>/u/huseyinbabal</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 19:02:54 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I built a tool to automate your workflow after recording yourself doing the task once (Open Source)</title><link>https://v.redd.it/6q1swgl96amg1</link><author>/u/bullmeza</author><category>ai</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 18:44:42 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Exclusive interview: Anthropic CEO Dario Amodei on Pentagon feud</title><link>https://youtu.be/MPTNHrq_4LU</link><author>/u/CBSnews</author><category>ai</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 18:36:06 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I built a 1 GiB/s file encryption CLI using io_uring, O_DIRECT, and a lock-free triple buffer</title><link>https://www.reddit.com/r/rust/comments/1rh9tj5/i_built_a_1_gibs_file_encryption_cli_using_io/</link><author>/u/supergari</author><category>rust</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 18:20:16 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I got frustrated with how slow standard encryption tools (like GPG or age) get when you throw a massive 50GB database backup or disk image at them. They are incredibly secure, but their core ciphers are largely single-threaded, usually topping out around 200-400 MiB/s.I wanted to see if I could saturate a Gen4 NVMe drive while encrypting, so I built .I started out just mapping files into memory, but to hit multi-gigabyte/s throughput without locking up the CPU or thrashing the kernel page cache, the architecture evolved into something pretty crazy:Lock-Free Triple-Buffering: Instead of using async MPSC channels (which introduced severe lock contention on small chunks), I built a 3-stage rotating state machine. While io_uring writes batch N-2 to disk, Rayon encrypts batch N-1 across all 12 CPU cores, and io_uring reads batch N. I wrote a custom 4096-byte aligned memory allocator using std::alloc. This pads the header and chunk slots so the Linux kernel can bypass the page cache entirely and DMA straight to the drive. It uses ring for assembly-optimized AES-256-GCM and ChaCha20-Poly1305. To prevent chunk-reordering attacks, it uses a TLS 1.3-style nonce derivation (base_nonce XOR chunk_index). The full serialized file header (which contains the Argon2id parameters, salt, and base nonce) plus an is_final flag are bound into every single chunk's AAD. This mathematically prevents truncation and append attacks.It reliably pushes  entirely CPU-bound, and scales beautifully with cores.The README has a massive deep-dive into the binary file format, the memory alignment math, and the threat model. I'd love for the community to tear into the architecture or the code and tell me what I missed.Let me know what you think!]]></content:encoded></item><item><title>Yes, and...</title><link>https://htmx.org/essays/yes-and/</link><author>/u/BinaryIgor</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 18:01:29 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[I teach computer science at Montana State University.  I am the father of three sons who
all know I am a computer programmer and one of whom, at least, has expressed interest in the field.  I love computer
programming and try to communicate that love to my sons, the students in my classes and anyone else who will listen.A question I am increasingly getting from relatives, friends and students is:Given AI, should I still consider becoming a computer programmer?My response to this is: â€œYes, andâ€¦â€Computer programming is, fundamentally, about two things:Problem-solving using computersLearning to control complexity while solving these problemsI have a hard time imagining a future where knowing how to solve problems with computers and how to control the complexity
of those solutions is  valuable than it is today, so I think it will continue to be a viable career even with the
advent of AI tools.That being said, I view AI as very dangerous for junior programmers because it  able to effectively generate code for
many problems.  If a junior programmer does not learn to write code and simply generates it, they are robbing
themselves of the opportunity to develop the visceral understanding of code that comes with being down in the trenches.Because of this, I warn my students:â€œYes, AI can generate the code for this assignment. Donâ€™t let it. You  to write the code.â€I explain that, if they donâ€™t write the code, they will not be able to effectively  the code.  The ability to
read code is certainly going to be valuable, maybe  valuable, in an AI-based coding future.I do not agree with this simile.Compilers are, for the most part, deterministic in a way that current AI tools are not.  Given a high-level programming
language construct such as a for loop or if statement, you can, with reasonable certainty, say what the generated
assembly will look like for a given computer architecture (at least pre-optimization).The same cannot be said for an LLM-based solution to a particular prompt.High level programming languages are a  way to create highly specified solutions to problems
using computers with a minimum of text in a way that assembly was not.  They eliminated a lot of
accidental complexity, leaving (assuming the code was written
reasonably well) mostly necessary complexity.LLM generated code, on the other hand, often does not eliminate accidental complexity and, in fact, can add
significant accidental complexity by choosing inappropriate approaches to problems, taking shortcuts, etc.If you canâ€™t read the code, how can you tell?And if you want to read the code you must write the code.Another thing that I tell my students is that AI, used properly, is a tremendously effective TA.  If you donâ€™t use it
as a code-generator but rather as a partner to help you understand concepts and techniques, it can provide a huge boost
to your intellectual development.One of the most difficult things when learning computer programming is getting â€œstuckâ€.  You just donâ€™t see the trick
or know where to even start well enough to make progress.Even worse is when you get stuck due to accidental complexity: you donâ€™t know how to work with a particular tool chain
or even what a tool chain is.This isnâ€™t a problem with , this is a problem with your environment.  Getting stuck pointlessly robs you of time to
actually be learning and often knocks people out of computer science.(I got stuck trying to learn Unix on my own at Berkeley, which is one reason I dropped out of the computer science
program there.)AI can help you get past these roadblocks, and can be a great TA if used correctly.  I have posted an
AGENTS.md file that I provide to my students to configure
coding agents to behave like a great TA, rather than a code generator, and I encourage them to use AI in this role.AI doesnâ€™t  to be a detriment to your ability to grow as a computer programmer, so long as it is used
appropriately.I do think AI is going to change computer programming.  Not as dramatically as some people think, but in some
fundamental ways.It may be that the  of coding will lose  value.I regard this as too bad: I usually like the act of coding, it is fun to make something do something with your
(metaphorical) bare hands.  There is an art and satisfaction to writing code well, and lots of aesthetic decisions to be
made doing it.However, it does appear that raw code writing prowess may be less important in the future.As this becomes relatively less important, it seems to me that other skills will become more important.For example, the ability to write, think and communicate clearly, both with LLMs and humans seems likely to be much more
important in the future.  Many computer programmers have a literary bent anyway, and this is a skill that will likely
increase in value over time and is worth working on.Reading books and writing essays/blog posts seem like activities likely to help in this regard.Another thing you can work on is turning some of your mental energy towards understanding a business (or government
role, etc) better.Computer programming is about solving problems with computers and businesses have plenty of both of these.Some business folks look at AI and say â€œGreat, we donâ€™t need programmers!â€, but it seems just as plausible to me that
a programmer might say â€œGreat, we donâ€™t need business people!â€I think both of these views are short-sighted, but I do think that AI can give programmers the ability to continue
fundamentally working as a programmer while  investing more time in understanding the real-world problems (business or
otherwise) that they are solving.This dovetails well with improving communication skills.Like many computer programmers, I am ambivalent towards the term â€œsoftware architect.â€  I have seen
architect astronauts inflict
a lot of pain on the world.For lack of a better term, however, I think software architecture will become a more important skill over time: the
ability to organize large software systems effectively and, crucially, to control the complexity of those systems.A tough part of this for juniors is that traditionally the ability to architect larger solutions well has come from
experience building smaller parts of systems, first poorly then, over time, more effectively.Most bad architects I have met were either bad coders or simply didnâ€™t have much coding experience at all.If you let AI take over as a code generator for the â€œsimpleâ€ stuff, how are you going to develop the intuitions necessary
to be an effective architect?This is why, again, you must write the code.Another skill that seems likely to increase in value (obviously) is knowing how to use LLMs effectively.  I think that
currently we are still in the process of figuring out what that means.I also think that what this means varies by experience level.Senior programmers who already have a lot of experience from the pre-AI era are in a good spot to use LLMs effectively:
they know what â€œgoodâ€ code looks like, they have experience with building larger systems and know what matters and
what doesnâ€™t.  The danger with senior programmers is that they stop programming entirely and start suffering from
brain rot.Particularly dangerous is firing off prompts and then getting sucked into
The Eternal Scroll while waiting.I typically try to use LLMs in the following way:To analyze existing code to better understand it and find issues and inconsistencies in itTo help organize my thoughts for larger projects I want to take onTo generate relatively small bits of code for systems I am working onTo generate code that I donâ€™t enjoy writing (e.g. regular expressions & CSS)To generate demos/exploratory code that I am willing to throw away or donâ€™t intend to maintain deeplyTo suggest tests for a particular feature I am working onI try not to use LLMs to generate full solutions that I am going to need to support.  I will sometimes use LLMs alongside
my manual coding as I build out a solution to help me understand APIs and my options while coding.I never let LLMs design the APIs to the systems I am building.Juniors are in a tougher spot.  I will say it again: you must write the code.The temptation to vibe your way through problems is very, very high, but you will need to fight against that temptation.Peers  be vibing their way through things and that will be annoying: you will need to work harder than they do,
and you may be criticized for being slow.  The work dynamics here are important to understand: if your company
prioritizes speed over understanding (as many are currently) you need to accept that and not get fired.However, I think that this is a temporary situation and that soon companies are going to realize that vibe coding at
speed suffers from worse complexity explosion issues than well understood, deliberate coding does.At that point I expect slower, more deliberate coding with AI assistance will be understood as the best way to utilize
this new technology.Where AI  help juniors is in accelerating the road to senior developer by eliminating accidental complexity that often
trips juniors up.  As I said above, viewing AI as a useful although sometimes overly-eager helper rather than a servant
can be very effective in understanding the shape of code bases, what the APIs and techniques available for a particular
problem are, how a given build system or programming language works, etc.But you must write the code.And companies: you must let juniors write the code.The questions I get around AI and programming fundamentally revolve around getting a decent job.It is no secret that the programmer job market is bad right now, and I am seeing good CS students struggle to find
positions programming.While I do not have a crystal ball, I believe this is a temporary rather than permanent situation.  The computer
programmer job market tends to be cyclical with booms and busts, and I believe we will recover from the current bust
at some point.Thatâ€™s cold comfort to someone looking for a job now, however, so I want to offer the specific job-seeking advice that
I give to my students.I view the online job sites as mostly pointless, especially for juniors.  They are a lottery and the chances of finding
a good job through them are low.  Since they are free they are probably still worth using, but they are not worth
investing a lot of time in.A better approach is the four Fâ€™s: Family, Friends & Family of Friends.  Use your personal connections to find positions
at companies in which you have a competitive advantage of knowing people in the company.  Family is the strongest
possibility.  Friends are often good too.  Family of friends is weaker, but also worth asking about.  If you know or
are only a few degrees separated from someone at a company you have a much stronger chance of getting a job at that
company.I stress to many students that this doesnâ€™t mean your family has to work for Google or some other big tech company. companies of any significant size have problems that need to be solved using computers.  Almost every company over 100
people has some sort of development group, even if they donâ€™t call it that.As an example, I had a student who was struggling to find a job.  I asked what their parent did, and they said they worked
for Costco corporate.I told them that they were in fact extremely lucky and that this was their ticket into a great company.Maybe they donâ€™t start as a â€œcomputer programmerâ€ there, maybe they start as an analyst or some other role.  But the
ability to program on top of that role will be very valuable and likely set up a great career.So I still think pursuing computer programming as a career is a good idea.  The current job market is bad, no doubt, but
I think this is temporary.I do think how computer programming is done is changing, and programmers should look at building up skills beyond
â€œpureâ€ code-writing.  This has always been a good idea.I donâ€™t think programming is changing as dramatically as some people claim and I think the fundamentals of programming,
particularly writing good code and controlling complexity, will be perennially important.I hope this essay is useful in answering that question, especially for junior programmers, and helps people feel
more confident entering a career that I have found very rewarding and expect to continue to do for a long time.And companies: let the juniors write at least some of the code.  It is in your interest.]]></content:encoded></item><item><title>Servo v0.0.5 released</title><link>https://github.com/servo/servo/releases/tag/v0.0.5</link><author>/u/Right-Grapefruit-507</author><category>rust</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 17:44:48 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Alliance of Open Media is working on Open Audio Codec, based on libopus &amp; meant to succeed Opus</title><link>https://github.com/AOMediaCodec/oac</link><author>/u/TheTwelveYearOld</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 17:29:05 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AMD Prepares Linux For Instruction-Based Sampling Improvements With Zen 6</title><link>https://www.phoronix.com/news/Linux-Perf-AMD-IBS-Zen-6</link><author>/u/somerandomxander</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 17:21:25 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com.]]></content:encoded></item><item><title>ssh honeypot</title><link>https://www.reddit.com/r/golang/comments/1rh856c/ssh_honeypot/</link><author>/u/KitchenBlackberry332</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 17:15:37 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/KitchenBlackberry332 ]]></content:encoded></item><item><title>[R] Tiny transformers (&lt;100 params) can add two 10-digit numbers to 100% accuracy</title><link>https://github.com/anadim/AdderBoard</link><author>/u/LetsTacoooo</author><category>ai</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 17:15:05 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Really interesting project. Crazy you can get such good performance. A key component is that they are digit tokens. Floating math will be way tricker. ]]></content:encoded></item><item><title>Linux 6.19.4 regression may cause failure to suspend properly on certain AMD hardware</title><link>https://lore.kernel.org/all/aW3d4B3xMwe-pyzJwFnM7q4q5WjOjAajU2c6gk65arrBx5-soWv9AAZPzZHxAiX1XOxILELauRQdnxGxMectmmW76xfxyQyErVEH8nR_iyw=@protonmail.com/T/#u</link><author>/u/anh0516</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 16:53:21 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Servo v0.0.5 released</title><link>https://github.com/servo/servo/releases/tag/v0.0.5</link><author>/u/Right-Grapefruit-507</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 16:43:57 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Coming from Python - How do experienced Go developers navigate codebases with distributed method definitions?</title><link>https://www.reddit.com/r/golang/comments/1rh6yea/coming_from_python_how_do_experienced_go/</link><author>/u/SevenIsMyTherapist</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 16:29:10 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I'm a senior developer who's built full-stack projects with Python backends (FastAPI, Pydantic, mypy) and TypeScript frontends. Python is my backend language of choice, but I'm frustrated by its loose typing, even with strict mypy enforcement, it's not quite the same as true static typing.Go appeals to me because it handles natively what I have to work hard to enforce in Python. However, I'm struggling with code navigation patterns that feel counterintuitive coming from Python.In Python, when I jump to a function or class definition, it takes me to a single location where I can see all methods and understand the structure. In Go, "go to definition" often takes me to an interface, and methods can be defined anywhere by adding a receiver. This distribution of code makes it harder to get a complete picture of a type's capabilities.This is especially painful with third-party libraries. The only way I know to discover all methods on a type is to type a dot and wait for autocomplete suggestions, which feels like I'm missing something fundamental.How do experienced Go developers navigate codebases efficiently?Is there a better way to see all methods attached to a particular type without relying on autocomplete?Are there IDE features, tools, or mental models I should be using to work more effectively with Go's approach to organizing code?I suspect I'm approaching this with Python patterns when I should be thinking differently. Any guidance would be appreciated.]]></content:encoded></item><item><title>[D] AI/ML PhD Committee</title><link>https://www.reddit.com/r/MachineLearning/comments/1rh6v3s/d_aiml_phd_committee/</link><author>/u/dead_CS</author><category>ai</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 16:25:29 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Hey all â€” quick question for senior PhD folks.Iâ€™m finalizing my Plan of Study and trying to decide on my committee composition. Thereâ€™s a professor in our department whose work is aligned with mine and who has strong industry ties (split appointment). Iâ€™ve always admired their work and initially wanted them on my committee.The challenge is availability â€” theyâ€™re very hard to reach and not very present on campus. I also havenâ€™t worked directly with them, so they wouldnâ€™t be in a position to write a strong letter. For those further along: how much does committee composition actually matter for jobs (industry RS roles or academia)? Does having a recognizable name help meaningfully, or is it better to prioritize accessibility and engagement i.e. I look for a more accessible professor?Would really appreciate any honest thoughts.]]></content:encoded></item><item><title>Servo Browser Engine Starts 2026 With Many Notable Improvements</title><link>https://www.phoronix.com/news/Servo-January-2026</link><author>/u/anh0516</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 15:52:29 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[
The Servo project has issued their January 2026 development report that highlights all the interesting changes they made to this open-source browser layout engine last month. With Servo 0.0.5 they have landed many improvements to this engine and also continuing to enhance its ability to embed Servo inside other applications.
Some of the recent improvements to Servo include:
- Support for playing Ogg audio files via the audio HTML tag.
- Support for cursor-color, content: image, and other CSS features.
- Improved support for mixed content protections.
- Servo now leads other browsers in supporting new Web Cryptography algorithms in now supporting ML-KEM, ML-DSA, and improved AES-GCM support.
- Improved support for JavaScript module loading.
- Improved support for IndexedDB.
- A lot of work on text input fields support.
- Support for cross-compiling Servo using Microsoft Windows as the host.
- Various performance and stability enhancements.
More details on the recent Servo improvements via the Servo.org blog.]]></content:encoded></item><item><title>One-Click EKS Upgrades? The Reality Behind the Button</title><link>https://www.reddit.com/r/golang/comments/1rh5s1a/oneclick_eks_upgrades_the_reality_behind_the/</link><author>/u/Downtown-Warning6818</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 15:42:18 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Managing multiple AWS EKS clusters lifecycle without proper EOL dashboard is very difficult, here blog post I shared my experience,EKS has auto upgrades feature, but it will only update AWS control plane and AWs addons. To solve this problem, we build our own Golang Prometheus exporter for custom metrices]]></content:encoded></item><item><title>gobench.dev - Comparisons of different stdlib features</title><link>https://www.reddit.com/r/golang/comments/1rh5ry4/gobenchdev_comparisons_of_different_stdlib/</link><author>/u/MarvinJWendt</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 15:42:12 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[The purpose of the site is to compare different functions in the standard library that achieve the same result. It helps you see how they perform in terms of speed, memory usage, and allocations, across different amounts of CPU cores used. I hope this helps someone!Please let me know if the charts are easy to understand and if you have ideas for improvements!]]></content:encoded></item><item><title>Pumba v1.0 â€” chaos testing for containerd nodes (no Docker daemon needed)</title><link>https://www.reddit.com/r/kubernetes/comments/1rh4ojy/pumba_v10_chaos_testing_for_containerd_nodes_no/</link><author>/u/alexei_led</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 14:56:44 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I've maintained Pumba since 2016. It's a chaos testing CLI that kills containers, injects network delays, drops packets, and stress-tests resources at the container runtime level â€” not the Kubernetes API level. Think Chaos Monkey, but for individual containers. Named after the Lion King warthog because a tool that intentionally breaks things should at least have a sense of humor. Kubernetes dropped dockershim in v1.24. Containerd is now the dominant CRI (53% of clusters, up from 23% the year before). Pumba only spoke Docker. On containerd-only nodes, it was a paperweight. I've been watching issues about this pile up for two years. Direct gRPC to . Three flags:bash pumba --runtime containerd --containerd-namespace k8s.io \ netem --duration 5m delay --time 3000 my-service Everything works on containerd: kill, stop, pause, restart, remove, netem (delay/loss/duplicate/corrupt/rate), iptables filtering with IP/port targeting, stress testing, exec.The interesting part under the hood: Docker gives you  for network namespace sharing. One flag. Containerd has no such abstraction â€” you build OCI-spec sidecar containers, bind them to , manage the full task lifecycle, and make sure cleanup happens even when your parent context gets cancelled. If your containers don't have  installed (most don't),  spawns a nettools sidecar:bash pumba --runtime containerd netem \ --tc-image ghcr.io/alexei-led/pumba-alpine-nettools:latest \ --duration 5m delay --time 3000 my-minimal-container cgroups v2 stress testing â€” no privileged containers, no SYS_ADMINReal OOM kill testing â€”  shares the target's cgroup, triggers actual kernel OOM via  (not simulated SIGKILL â€” different container state, different K8s events, different recovery paths) ships inside the ghcr.io/alexei-led/stress-ng scratch image â€” minimal, no shellK8s container name resolution from labels ( format, no SHA256 hunting)40 advanced Go integration tests â€” crash recovery under OOM, sidecar lifecycle, network verification, concurrent chaos80+ bats integration tests for containerdWhy container-level instead of pod-level? Chaos Mesh and Litmus are great for pod-level chaos through K8s CRDs. Pumba does something different: if you need to delay one specific container in a multi-container pod, run chaos outside K8s entirely, or trigger a real OOM kill â€” you need runtime access.Happy to answer questions about containerd's API, the OCI sidecar pattern, or the cgroup injection approach.]]></content:encoded></item><item><title>Low-Latency Python: Separating Signal from Noise</title><link>https://open.substack.com/pub/lucisqr/p/low-latency-python-separating-signal?utm_campaign=post-expanded-share&amp;amp;utm_medium=web</link><author>/u/OkSadMathematician</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 14:29:09 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Understanding RabbitMQ in simple terms</title><link>https://sushantdhiman.dev/understanding-rabbitmq/</link><author>/u/Sushant098123</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 14:15:25 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Hi, I hope you all are doing well. Recently I was exploring RabbitMQ, and I found it fascinating. Previously I've used Kafka. RabbitMQ is very different from Kafka. This article is mostly useful for beginners or people who haven't used RabbitMQ. If you are an experienced developer, you might not find anything new in this post.RabbitMQ (Rabbit Message Queueing) is an Open Source message broker. It is used by applications to interact asynchronously. Simplest use case of RabbitMQ can be establishing a communication between multiple micro-services.Those who are new to message brokers or message queue read this to understand them. Experienced persons can skip.Let's say you need an e-commerce solution. When any user places an order, 3 things happen: checkout handling, email sending and inventory update. You have a monolithic system where all 3 things happen sequentially, and on average it takes 5 seconds. Your user needs to hang on for 5 seconds. Later you decide to address this problem and break your system into 3 microservices.Now each service will handle a particular thing. But the user still needs 5+ seconds because things will still happen sequentially. This is where the message broker/message queue comes into play. You will modify your application in this way:When the checkout service confirms payment, just return success to the user.Publish a message with order details in a message queue.Email & Inventory service will continuously wait for messages in the queue.Both microservices will do their task in the background without forcing the user to wait.Message Queues provide a reliable way for micro-services to communicate with each other.These are the services that send messages to RabbitMQ. In our example, the is the producer.A  is just a packet of data sent by the producer. It has 2 parts: & . Properties let us define delivery mode, content type, priority, expiration and much more functionality of a message.These are the applications that receive message and process them.Consumers can't directly receive messages from producers; instead, they look for messages in a queue. A queue is a place where messages are stored so that they can be consumed by consumers.RabbitMQ is not a message queue; it is a message broker. Unlike other message queues that push messages to a particular queue, RabbitMQ sends messages to an exchange. This is the most important component of RabbitMQ.We need to understand exchanges deeply. One thing to remember is that producers never produce messages directly to the queue. Instead, they send messages to an exchange, and an exchange decides which queue a message should go to.You might doubt why RabbitMQ puts messages in exchanges and why not directly in queues. This is because RabbitMQ provides many more features to route messages based on specific conditions. Let's understand it.At the end, messages will go to the queue. With Exchange you can decide which queue you want the message to go into. RabbitMQ will act as a router.Binding connects to a. It is basically a . Binding tells RabbitMQ that a queue is interesting in receiving messages from a particular exchange.A queue basically tells RabbitMQ:â€œIf a message matching this rule comes to the exchange, send a copy to me.â€Without binding, an exchange has no idea where to send the message.So the actual routing logic of RabbitMQ lives inside bindings.Binding key (routing rule)You have an  exchange. You create a queue : send messages with routing key  to this queueNow whenever a producer publishes a message with routing key , the email service queue will receive it.Important thing:One exchange can send the same message to multiple queues.So a single event can trigger:Without the checkout service even knowing those services exist. That is actually the real power of message queues.RabbitMQ provides multiple exchange types because not every system routes messages the same way.This is the simplest one. It matches messages using an .routing key = order.createdIf key matches â†’ message goes to that queue. If the key doesnâ€™t match, the queue will not receive the message.You can think of it as send this job to a specific worker typeFanout exchange ignores routing keys completely. It simply broadcasts messages to . So if 5 queues are bound to the exchange â†’ all 5 get the message. It is used for implementing broadcast mechanism.This is where RabbitMQ becomes very powerful. Topic exchange routes messages using patterns.order.created.india
order.created.us
order.cancelled.indiaNow queues can subscribe using patterns:order.created.*
*.india
order.#*  = one word#  = zero or more wordsI only care about Indian orders.This is very useful in real systems:event-driven architectureInstead of routing key, RabbitMQ uses message headers.x-tenant: premiumx-region: asiaQueues receive messages based on header matching. This is not used very commonly but useful in special cases like SaaS platforms.This is one of the most important reliability features. When a consumer receives a message, RabbitMQ does NOT immediately delete it. RabbitMQ waits.Did you actually process the message?After processing, the consumer sends . If ACK is received message is removed. If consumer crashes before ACK than message goes back to queue. This prevents data loss.Email service crashes while sending mail.Email lost forever.RabbitMQ gives the same message to another worker.This is why message brokers are used in payment systems and email systems.By default RabbitMQ can push many messages to a consumer.message processing takes 10 seconds?worker receives 100 messages?The worker becomes overloaded. Prefetch fixes this. Prefetch tells RabbitMQ:Donâ€™t send me more than N unprocessed messages.Worker receives only one message at a time. This ensures:This is also called .TTL means message expiration.If message is not consumed within X time, discard it.People implement  using TTL + dead letter queues. RabbitMQ does not have built-in delay queues, so this becomes a common production trick.RabbitMQ uses a . You can run multiple workers consuming from the same queue. Example: You have 1 queue . You start 5 worker instances.RabbitMQ distributes messages between them:This is called .Each message goes to only , not all. This gives you horizontal scaling without changing code. If traffic increases than just start more workers.This is a concept beginners often ignore but it is very important in real systems.A TCP connection between your application and RabbitMQ server. Connections are expensive. You should NOT open a new connection per request. This will crash your server under load.A lightweight virtual connection inside a connection.Your entire service usually shares one RabbitMQ connection and creates channels for publishing/consuming. Channels are cheap, connections are not.At this point you should understand something important:RabbitMQ is not just a queue.It is a reliable message routing system that allows services to communicate asynchronously, scale independently and recover from failures.Once you start using it in real systems (emails, notifications, retries, background jobs), youâ€™ll realize many backend problems become much easier to solve.If you made it this far, I hope RabbitMQ feels less intimidating now.I usually write about backend engineering, distributed systems, and things I learn while working on real problems. Not theory â€” mostly practical stuff that I wish someone had explained to me earlier.I run a free newsletter where I share these kinds of write-ups. No spam. Just occasional backend engineering notes.]]></content:encoded></item><item><title>[D] Works on flow matching where source distribution comes from dataset instead of Gaussian noise?</title><link>https://www.reddit.com/r/MachineLearning/comments/1rh3k0f/d_works_on_flow_matching_where_source/</link><author>/u/fliiiiiiip</author><category>ai</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 14:08:25 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Flow matching is often discussed in the context of image generation from Gaussian noise.In principle, we could model the flow from a complicated image distribution into another complicated image distribution (image to image).Is that possible / well-understood in theoretical sense? Or are limited to the case where the source distribution is simple e.g. Gaussian?]]></content:encoded></item><item><title>Learning Go as a backend dev - what actually matters?</title><link>https://www.reddit.com/r/golang/comments/1rh1v79/learning_go_as_a_backend_dev_what_actually_matters/</link><author>/u/eurz</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 12:51:24 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Coming from Python/Java and trying to pick up Go. There's so many tutorials out there but a lot of them feel like they just rehash the tour and call it a day.For those who've made the switch, what actually helped you  Go beyond the syntax? Not just writing code that works, but writing code that feels like Go.Also curious about what projects made things click. I've done a couple small APIs but feel like I'm just writing Python in Go syntax.Any resources or approaches that actually worked?]]></content:encoded></item><item><title>gitcredits â€” movie-style end credits for your git repo, built with Bubble Tea</title><link>https://www.reddit.com/r/golang/comments/1rh1j3s/gitcredits_moviestyle_end_credits_for_your_git/</link><author>/u/Ts-ssh</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 12:34:19 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Small weekend project. Reads git log and GitHub metadata, then scrolls them like movie end credits with a starfield background in your terminal.Just cd into any repo and run it. Single file, no config.Built with Bubble Tea + Lip Gloss + x/term.]]></content:encoded></item><item><title>json-canon: Implementing Burger-Dybvig (IEEE 754 â†’ shortest decimal) in Go for RFC 8785</title><link>https://www.reddit.com/r/golang/comments/1rh0q0y/jsoncanon_implementing_burgerdybvig_ieee_754/</link><author>/u/UsrnameNotFound-404</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 11:51:41 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[This is Part 1 of a four-part series on building an RFC 8785 JSON Canonicalization library in Go ([github.com/lattice-substrate/json-canon](https://github.com/lattice-substrate/json-canon)). Parts 2â€“4 cover the strict RFC 8259 parser, infrastructure-grade design decisions, and evidence-based release engineering. This article covers the hardest part: number formatting. RFC 8785 requires ECMA-262â€“compatible output, which means you need the shortest decimal that round-trips to the original IEEE 754 bits, with even-digit tie-breaking. Go's `strconv.FormatFloat` is high quality but doesn't expose an ECMA-262 conformance contract, so I implemented Burger-Dybvig from scratch in 490 lines with `math/big`. Validated against 286K oracle vectors with SHA-256 pinned test data. Pure Go, zero deps. Happy to discuss the algorithm, the testing approach, or the design tradeoffs.]]></content:encoded></item><item><title>Searching 1GB JSON on a phone: 44s to 1.8s, a journey through every wrong approach</title><link>https://www.reddit.com/r/rust/comments/1rgzhhl/searching_1gb_json_on_a_phone_44s_to_18s_a/</link><author>/u/kotysoft</author><category>rust</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 10:40:32 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[After further investigation with the  author burntsushi :The results were specific to running inside an Android app (shared library). When I compiled the same benchmark as a standalone binary and ran it directly on the same device, Finder was actually  than FinderRev â€” consistent with expected behavior.Standalone binary on S23 Ultra (1GB real JSON, mmap'd): Finder::find 28.3ms FinderRev::rfind 96.4ms (3.4x slower) The difference between my app and the standalone binary might be related to how Rust compiles shared libraries (cdylib with PIC) vs standalone executables â€” possibly affecting SIMD inlining or dispatch. But we haven't confirmed the exact root cause yet.--------------------------------------------------I found the root cause of the 150x slowdown. And I am an absolute idiot. ðŸ¤¦â€â™‚ï¸I spent the entire day benchmarking CPU frequencies, checking memory maps, and building a standalone JNI benchmark app to prove that Android was killing SIMD performance.The actual reason?My standalone binary was compiled in. My Android JNI library was secretly compiling inmode without optimizations.Once I fixed the compiler profile,  dropped from 4.2 seconds to ~30ms on the phone. The SIMD degradation doesn't exist. It was just me experiencing the sheer, unoptimized horror of Debug-mode Rust on a 1GB JSON file. for raising an issue and questioning his crate when the problem was entirely my own build config!Leaving this post up as a monument to my own stupidity and a reminder to always check your . Thank you all for the upvotes on my absolute hallucination of a bug! --------------------------------------------------Before the roasting starts, yes I know, gigabyte JSON files shouldnt exist. People should fix their pipelines, use a database, normalize things. You're right. But this whole thing started as a "can I even do this on a phone?" challenge, and somewhere along the way I fell into the rabbit hole and just kept going. First app, solo dev, having way too much fun to stop.So I was working on a search position indicator, a small status bar at the top that shows where the scan is in the file, kind of like a timeline. While testing it on a 1GB JSON I noticed the forward search took . Fourty four. On a flagship phone. Meanwhile the backward search, which I already had using , was done in about 2 seconds. Same file, same query, same everything. That drove me absolutely crazy.First thing I tried was switching to , same thing I was already using for the COUNT feature. That brought it down to about 9 seconds, big improvement, but I still couldnt understand why backward was 5 times faster on the exact same data. That gap kept bugging me.Here's the full journey from there.The original, memchr on the first byte, 44 secondsThis was the code that started everything.  anchored on the first byte of the query, whatever that byte happend to be. No frequency analysis, nothing smart. In a 1GB JSON with millions of repeated keys and values, common bytes show up literally everywhere. The scanner was stopping billions of times at false positives, checking each one, moving on, stopping again.memmem::Finder with SIMD Two-Way, 9.4 secondsSwitched to the proper algorithm. Good improvement over 44s but still nowhere close to the 1.9 seconds that  was doing backward. The prefilter uses byte frequency heuristics to find candidate positions, but on repetitive structured data like JSON it generates tons of false positives and keeps hitting the slow path.memmem::Finder with prefilter disabled, 9.2 secondsI thought the prefilter must be the problem. Disabled it via FinderBuilder::new().prefilter(Prefilter::None). Same speed. Also lost cancellation support because  just blocks on the entire data slice until its done. No progress bar, no cancel button. Great.Rarest byte memchr, 6.3 secondsWent back to the memchr approach but smarter this time. Wrote a byte frequency table tuned for JSON (structural chars like  scored high, rare letters scored low) and picked the least common byte in the query as anchor. This actually beat memmem::Finder, which surprised me. But still 3x slower than backward.Two byte pair anchor, 6.2 secondsInstead of anchoring on one rare byte, pick the rarest two consecutive bytes from the needle. Use memchr on the first one, immediately check if the next byte matches before doing the full comparison. Barely any improvement. The problem wasnt the verification cost, it was that memchr itself was stopping about 2 million times at the anchor byte.Why is FinderRev so fast?After some digging, turns out  deliberately does not use the SIMD prefilter, "because it wasn't clear it was worth the extra code". On structured data full of repetitive delimiters, the "dumber" algorithm just plows straight through without the overhead. The thing that was supposed to make forward search faster was actually making it slower on this kind of data.FinderRev powered forward search, 1.8 secondsAt this point it was still annoying me. So I thought, if reverse is fast and forward is slow, why not just use reverse for forward? I process the file in 5MB chunks from the beginning to the end. For each chunk I call  as a quick existence check, is there any match in this chunk at all? If no, skip it, move to the next one. That rejection happens at about 533 MB/s. When rfind returns a hit, I know there is a match somewhere in that 5MB chunk, so I do a small  on just that chunk to locate the first occurrence.In practice 99.9% of chunks have no match and get skipped at FinderRev speed. The one chunk that actually contains the result takes about 0.03 seconds for the forward scan. Total: 1.8 seconds for the entire 1GB file.All benchmarks on Samsung Galaxy S23 Ultra, ARM64, 1GB JSON with about 50 million lines, case sensitive forward search for a unique 24 byte string.Since last time the app also picked up a full API Client (Postman collection import, OAuth 2.0, AWS Sig V4), a HAR network analyzer, highlight keywords with color picker and pinch to zoom. Still one person, still Rust powered, still occasionally surprised when things actually work on a phone.Has anyone else hit this Finder vs FinderRev gap on non natural language data?Curious if this is a known thing or if I just got lucky with my data pattern.]]></content:encoded></item><item><title>I really like Goâ€¦ but Iâ€™ve never had a real reason to use it</title><link>https://www.reddit.com/r/golang/comments/1rgygvj/i_really_like_go_but_ive_never_had_a_real_reason/</link><author>/u/AggravatingHome4193</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 09:37:02 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I come from a Node.js/TypeScript background, and Iâ€™ve been learning Go on and off for a while now. And honestlyâ€¦ I really like the language.Thereâ€™s something about its simplicity, the standard library, the tooling, the compilation speed, it just feels clean and pragmatic. Itâ€™s refreshing compared to the heavy ecosystem and abstraction layers Iâ€™m used to.But hereâ€™s the thing: Iâ€™ve never actually had a real-world project where Go was the obvious choice. Most of the things I build (APIs, SaaS backends, internal tools, etc.) are already comfortably handled with Node + TypeScript. So I havenâ€™t yet had that â€œthis must be written in Goâ€ moment. So Iâ€™m curious:For those who also came from Node/TS, what made you switch (or adopt Go seriously)?Would love to hear your experiences]]></content:encoded></item><item><title>How I Taught a Dragonfly to Fuzz Itself</title><link>https://medium.com/@v.yavdoshenko/how-i-taught-a-dragonfly-to-fuzz-itself-879734578250</link><author>/u/yavdoshenko</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 09:20:44 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>sudo-rs shows password asterisks by default â€“ break with Unix tradition</title><link>https://www.heise.de/en/news/sudo-rs-shows-password-asterisks-by-default-break-with-Unix-tradition-11193037.html</link><author>/u/FryBoyter</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 08:40:06 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[The Rust implementation sudo-rs breaks with a decades-old Unix convention: by default, asterisks now appear on the screen when typing passwords. As can be seen from a commit in the GitHub repository, the software has been activating the â€œpwfeedbackâ€ option by default since mid-February 2026. Traditionally, for 46 years, sudo has provided no feedback when typing passwords â€“ a conscious design decision for security reasons.The developers justify the change with usability improvements for new users. The commit message states that security is theoretically worse because password lengths would be visible to observers in the user's immediate vicinity. However, this minimal disadvantage is outweighed by significantly improved usability. In fact, sudo is thus one of the last Unix tools that provides no visual feedback at all when entering passwords; other applications have long shown placeholder characters.The change affects Ubuntu users with all versions that use sudo-rs by default. In a bug report, at least one traditionally-minded user vehemently complained about the innovation: displaying asterisks violates decades of practice and reveals the password length to â€œshoulder surfersâ€ â€“ people looking over the user's shoulder. However, Ubuntu marked the bug report as â€œWon't Fix.â€ A rollback of the change is not planned.Simple deactivation possibleAdministrators who prefer the old behavior can deactivate the asterisk display. To achieve this, the line  must be inserted into the sudoers configuration file. For server environments, the change is likely less relevant, as SSH keys are typically used instead of passwords there.sudo-rs is a complete reimplementation of the sudo command in the Rust programming language. The project aims to avoid the security issues that can arise from the original's 30-year-old C codebase. Rust, through its borrow checker, prevents entire classes of memory management errors such as buffer overflows. sudo-rs can now be used instead of the conventional sudo in many other distributions, although a transition comparable to Ubuntu has not yet occurred in other mainstream systems.The Trifecta Tech Foundation, which develops sudo-rs, has had the project externally audited twice. The last audit in August 2025 found no security vulnerabilities. During the first audit in 2023, the auditors discovered a path traversal vulnerability, which also affected the original sudo. Ubuntu users can switch back to the classic sudo via  up to version 26.04.This article was originally published in
      
        German.
      
      It was translated with technical assistance and editorially reviewed before publication.]]></content:encoded></item><item><title>I built a CLI tool with Go to visualize file trees with line counts</title><link>https://www.reddit.com/r/golang/comments/1rgwv8j/i_built_a_cli_tool_with_go_to_visualize_file/</link><author>/u/Suitable_Jump_6465</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 07:59:43 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/Suitable_Jump_6465 ]]></content:encoded></item><item><title>Washington Gaming Forum - Ultra Fast Open source Discussion Plataform</title><link>https://github.com/Quirson/washington-forum</link><author>/u/Quirson_Ngale</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 07:12:27 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Washington Forum - Ultra-Fast Open Source Discussion PlatformBuilt with Go + React | Real-time | Production-ReadyBlazing Fast Performance While Node.js struggles with 400MB+ RAM usage on small projects, our Go backend sips resources at just 8MB even with millions of routes! That's 50x better memory efficiency Â· Backend: Go (Golang) - Built for maximum performance Â· Frontend: React + Vite - Lightning-fast UI Â· Real-time: Instant updates, smooth user experience Â· Live Demo: forum.washingtongaming.techOur forum is already running in production! Experience the speed yourself:Â· Sub-second response times Â· Real-time discussions Â· Mobile-responsive design Â· Production-tested performanceWe're building something amazing and we need YOU! Whether you're:Â· A Go enthusiast wanting to learn Â· A React developer looking for a cool project Â· A performance geek interested in optimization Â· Just love open source!Give us a star and let's build the fastest forum together!I'm super open to collaboration! Found a bug? Want to add a feature? Have performance tips? Open an issue or PR! Let's make this project better together.Open source with respect for contributors. Feel free to fork and improve, but please maintain proper attribution.Ready to experience forum software done right?]]></content:encoded></item><item><title>A Social Filesystem</title><link>https://overreacted.io/a-social-filesystem/</link><author>/u/fagnerbrack</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 07:11:31 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[You write a document, hit save, and the file is on your computer. Itâ€™s yours. You can inspect it, you can send it to a friend, and you can open it with other apps.Files come from the paradigm of .This post, however, isnâ€™t about personal computing. What I want to talk about is â€”apps like Instagram, Reddit, Tumblr, GitHub, and TikTok.What do files have to do with social computing?Historically, not a lotâ€”But first, a shoutout to files.Files, as originally invented, were not meant to live  the apps.Since files represent  creations, they should live somewhere that  control. Apps create and read your files on your behalf, but files donâ€™t belong  the apps.Files belong to youâ€”the person using those apps.Apps (and their developers) may not own your files, but they do need to be able to  them. To do that reliably, apps need your files to be structured. This is why app developers, as part of creating apps, may invent and evolve .A file format is like a language. An app might â€œspeakâ€ several formats. A single format can be understood by many apps. Apps and formats are many-to-many. File formats let different apps work together without knowing about each other.SVG is an open specification. This means that different developers agree on how to read and write SVG. I created this SVG file in Excalidraw, but I could have used Adobe Illustrator or Inkscape instead. Your browser already knew how to display this SVG. It didnâ€™t need to hit any Excalidraw APIs or to ask permissions from Excalidraw to display this SVG. It doesnâ€™t matter which app has created this SVG.The file format is the API.Of course, not all file formats are open or documented.Some file formats are application-specific or even proprietary like . And yet, although  was undocumented, it didnâ€™t stop motivated developers from reverse-engineering it and creating more software that reads and writes :Another win for the files paradigm.The files paradigm captures a real-world intuition about tools: what we make  a tool does not belong  the tool. A manuscript doesnâ€™t stay inside the typewriter, a photo doesnâ€™t stay inside the camera, and a song doesnâ€™t stay in the microphone.Our memories, our thoughts, our designs  outlive the software we used to create them. An app-agnostic storage (the filesystem) enforces this separation.You may create a file in one app, but someone else can read it using another app. You may switch the apps you use, or use them together. You may convert a file from one format to another. As long as two apps correctly â€œspeakâ€ the same file format, they can work in tandem even if their developers hate each othersâ€™ guts.Someone could always create â€œthe next appâ€ for the files you already have:Apps may come and go, but files stayâ€”at least, as long as our apps think in files.When you think of social appsâ€”Instagram, Reddit, Tumblr, GitHub, TikTokâ€”you probably donâ€™t think about files. Files are for  computing only, right?A Tumblr post isnâ€™t a file.An Instagram follow isnâ€™t a file.A Hacker News upvote isnâ€™t a file.But what if they  as filesâ€”at least, in all the important ways? Suppose you had a folder that contained all of the things ever ed by your online persona:It would include everything youâ€™ve created across different social appsâ€”your posts, likes, scrobbles, recipes, etc. Maybe we can call it your â€œeverything folderâ€.Of course, closed apps like Instagram arenâ€™t built this way. But imagine they were. In that world, a â€œTumblr postâ€ or an â€œInstagram followâ€ are social file formats:You posting on Tumblr would create a  file in your folder.You following on Instagram would put an  file into your folder.You upvoting on Hacker News would add an  file to your folder.Note this folder is not some kind of an archive. Itâ€™s where your data actually lives:Files are the source of truthâ€”the apps would reflect whateverâ€™s in your folder.Any writes to your folder would be synced to the interested apps. For example, deleting an  file would work just as well as unfollowing through the app. Crossposting to three Tumblr communities could be done by creating three  files. Under the hood, each app manages files in your folder.In this paradigm, apps are  to files. Every appâ€™s database mostly becomes derived dataâ€”an app-specific cached materialized view of everybodyâ€™s folders.This might sound very hypothetical, but itâ€™s not. What Iâ€™ve described so far is the premise behind the AT protocol. It works in production at scale. Bluesky, Leaflet, Tangled, Semble, and Wisp are some of the new open social apps built this way.It doesnâ€™t  different to use those apps. But by lifting user data out of the apps, we force the same separation as weâ€™ve had in personal computing: apps donâ€™t trap what you make with them. Someone can always make a new app for old data:Like before, app developers evolve their file formats. However, they canâ€™t gatekeep who reads and writes files in those formats. Which apps to use is up to you.Together, everyoneâ€™s folders form something like a distributed :Iâ€™ve previously written about the AT protocol in Open Social, looking at its model from a web-centric perspective. But I think that looking at it from the filesystem perspective is just as intriguing, so I invite you to take a tour of how it works.A personal filesystem starts with a file.What does a social filesystem start with?Here is a typical social media post:How would you represent it as a file?Itâ€™s natural to consider JSON as a format. After all, thatâ€™s what youâ€™d return if you were building an API. So letâ€™s fully describe this post as a piece of JSON:However, if we want to store this post , it doesnâ€™t make sense to embed the author information there. After all, if the author later changes their display name or avatar, we wouldnâ€™t want to go through their every post and change them there.So letâ€™s assume their avatar and name live somewhere elseâ€”perhaps, in another file. We could leave  in the JSON but this is unnecessary too. Since this file lives inside the creatorâ€™s folderâ€”itâ€™s  post, after allâ€”we can always figure out the author based on  folder weâ€™re currently looking at.Letâ€™s remove the  field completely:This seems like a good way to describe this post:But wait, no, this is still wrong.You see, , , and  are not really something that the postâ€™s author has . These values are derived from the data created by other peopleâ€” replies,  reposts,  likes. The app that displays this post will have to keep track of those somehow, but they arenâ€™t  userâ€™s data.So really, weâ€™re left with just this:Thatâ€™s our post as a file!Notice how it took some trimming to identify which parts of the data actually belong in this file. This is something that you have to be intentional about when creating apps with the AT protocol. My mental model for this is to think about the  request. When the user created this thing,  Thatâ€™s likely close to what weâ€™ll want to store. Thatâ€™s the stuff the user has just created.Our social filesystem will be structured more rigidly than a traditional filesystem. For example, it will  consist of JSON files. To make this more explicit, weâ€™ll start introducing our new terminology. Weâ€™ll call this kind of file a .Now we need to give our record a name. There are no natural names for posts. Could we use sequential numbers? Our names need only be unique within a folder:One downside is that weâ€™d have to keep track of the latest one so thereâ€™s a risk of collisions when creating many files from different devices at the same time.Instead, letâ€™s use timestamps with some per-clock randomness mixed in:This is nicer because these can be generated locally and will almost never collide.Weâ€™ll use these names in URLs so letâ€™s encode them more compactly. Weâ€™ll pick our encoding carefully so that sorting alphabetically goes in the chronological order:Now  gives us a reverse chronological timeline of posts! Thatâ€™s neat. Also, since weâ€™re sticking with JSON as our lingua franca, we donâ€™t need file extensions.Not all records accumulate over time. For example, you can write many posts, but you only have one copy of profile informationâ€”your avatar and display name. For â€œsingletonâ€ records, it makes sense to use a predefined name, like  or :By the way, letâ€™s save this profile record to :Note how, taken together,  and  let us reconstruct more of the UI we started with, although some parts are still missing:Before we fill them in, though, we need to make our system sturdier.This was the shape of our post record:And this was the shape of our profile record:Since these are stored as files, itâ€™s important for the format not to drift.Letâ€™s write some type definitions:TypeScript seems convenient for this but it isnâ€™t sufficient. For example, we canâ€™t express constraints like â€œthe  string should have at most 300 Unicode graphemesâ€, or â€œthe  string should be formatted as datetimeâ€.We need a richer way to define social file formats.We might shop around for existing options (RDF? JSON Schema?) but if nothing quite fits, we might as well design our own schema language explicitly geared towards the needs of our social filesystem. This is what our  looks like:Weâ€™ll call this the Post  because itâ€™s like a language our app wants to speak.My first reaction was also â€œouchâ€ but it helped to think that conceptually itâ€™s this:I used to yearn for a bettersyntax but Iâ€™ve actually come around to hesitantly appreciate the JSON. It being trivial to parse makes it super easy to build tooling around it (more on that in the end). And of course, we can makebindings turning these into type definitions and validation code for any programming language.Our social filesystem looks like this so far:The  folder has records that satisfy the Post lexicon, and the  folder contains records (a single record, really) that satisfy the Profile lexicon.This can be made to work well for a single app. But hereâ€™s a problem. What if thereâ€™s another app with its own notion of â€œpostsâ€ and â€œprofilesâ€?Recall, each user has an â€œeverything folderâ€ with data from every app:Different apps will likely disagree on what the format of a â€œpostâ€ is! For example, a microblog post might have a 300 character limit, but a proper blog post might not.Can we get the apps to agree with each other?We could try to put every app developer in the same room until they all agree on a perfect lexicon for a post. That would be an interesting use of everyoneâ€™s time.For some use cases, like cross-site syndication, a standard-ish jointly governed lexicon makes sense. For other cases, you really want the app to be in charge. Itâ€™s actually  that different products can disagree about what a post is! Different products, different vibes. Weâ€™d want to support that, not to fight it.Really, weâ€™ve been asking the wrong question. We donâ€™t need every app developer to agree on what a  is; we just need to  anyone â€œdefineâ€ their own .We could try namespacing types of records by the app name:But then, app names can also clash. Luckily, we already have a way to avoid conflictsâ€”domain names. A domain name is unique and implies ownership.Why donâ€™t we take some inspiration from Java?This gives us A collection is a folder with records of a certain lexicon type. Twitterâ€™s lexicon for posts might differ from Tumblrâ€™s, and thatâ€™s fineâ€”theyâ€™re in separate collections. The collection is always named like <whoever.designs.the.lexicon>.<name>.For example, you could imagine these collection names: for Instagram follows for Last.fm scrobbles for Letterboxd reviewsYou could also imagine these slightly whackier collection names:com.ycombinator.news.vote (subdomains are ok) (personal domains work too) (a shared standard someday?) (breaking changes = new lexicon, just like file formats)Itâ€™s like having a dedicated folder for every file extension.There Is No Lexicon PoliceIf youâ€™re an application author, you might be thinking:Who enforces that the records match their lexicons? If any app can (with the userâ€™s explicit consent) write into any other appâ€™s collection, how do we not end up with a lot of invalid data? What if some other app puts junk into â€œmyâ€ collection?The answer is that records could be junk, but it still works out anyway.It helps to draw a parallel to file extensions. Nothing stops someone from renaming  to . A PDF reader would just refuse to open it.Lexicon validation works the same way. The  in  signals who  the lexicon, but the records themselves could have been created by  This is why apps always treat records as untrusted input, similar to  request bodies. When you generate type definitions from a lexicon, you also get a function that will do the validation for you. If some record passes the check, greatâ€”you get a typed object. If not, fine, ignore that record.So, validate on read, just like files.Some care is required when evolving lexicons. From the moment some lexicon is used in the wild, you should never change which records it would consider valid. For example, you can add new optional fields, but you canâ€™t change  some field is optional. This ensures that the new code can still read old records  that the old code will be able to read any new records. Thereâ€™s a linter to check for this. (For breaking changes, make a new lexicon, as you would do with a file format.)Although this is not required, you can publish your lexicons for documentation and distribution. Itâ€™s like publishing type definitions. Thereâ€™s no separate registry for those; you just put them into a com.atproto.lexicon.schema collection of some account, and then prove the lexiconâ€™s domain is owned by you. For example, if I wanted to publish an  lexicon, I could place it here:Letâ€™s circle back to our post.Weâ€™ve already decided that the profile should live in the  collection, and the post itself should live in the  collection:But what about the likes?A like is something that the user , so it makes sense for each like to be a record. A like record doesnâ€™t convey any data other than which post is being liked:So, a Like is a record that refers to its Post.But how do we express this in JSON?How do we refer from one JSON file to another JSON file?We could try to refer to the Post record by its path in our â€œeverything folderâ€:But this only uniquely identifies it  â€œeverything folderâ€. Recall that each user has their own, completely isolated folders with all of their stuff:We need to find some way to refer to the This is a difficult problem.So far, weâ€™ve been building up a kind of a filesystem for social apps. But the â€œsocialâ€ part requires linking  users. We need a reliable way to refer to some user. The challenge is that weâ€™re building a  filesystem where the â€œeverything foldersâ€ of different users may be hosted on different computers, by different companies, communities or organizations, or be self-hosted.Whatâ€™s more, we donâ€™t want anyone to be  their current hosting. The user should be able to change who hosts their â€œeverything folderâ€ at any point, and without breaking any existing links to their files. The main tension is that we want to preserve usersâ€™ ability to change their hosting, but we donâ€™t want that to break any links. Additionally, we want to make sure that, although the system is distributed, weâ€™re confident that each piece of data has not been tampered with.For now, you can forget all about records, collections, and folders. Weâ€™ll focus on a single problem: links. More concretely, we need a design for permanent links that allow swappable hosting. If we donâ€™t make this work, everything else falls apart.Attempt 1: Host as IdentitySuppose drilâ€™s content is hosted by some-cool-free-hosting.com. The most intuitive way to link to his content is to use a normal HTTP link to his hosting:This works, but then if dril wants to change his hosting, heâ€™d break every link. So this is not a solutionâ€”itâ€™s the exact  that weâ€™re trying to solve. We want the links to point at â€œwherever drilâ€™s stuff will beâ€, not â€œwhere drilâ€™s stuff is right nowâ€.We need some kind of an indirection.Attempt 2: Handle as IdentityWe could give dril some persistent identifier like  and use that in links:We could then run a registry that stores a JSON document like this for each user:The idea is that this document tells us how to find â€™s actual hosting.Weâ€™d also need to provide some way for dril to update this document.Some version of this could work but it seems unfortunate to invent our own global namespace when one already exists on the internet. Letâ€™s try a twist on this idea.Attempt 3: Domain as IdentityThereâ€™s already a global namespace anyone can participate in: DNS. If dril owns , maybe we could let him use  as his persistent identity:This doesnâ€™t mean that the actual content is hosted at ; it just means that  hosts the JSON document that says where the content currently is. For example, maybe the convention is to serve that document as . Again, the document points us at the hosting. Obviously, dril can update his doc.This is somewhat elegant but in practice the tradeoff isnâ€™t great. Losing domains is pretty common, and most people wouldnâ€™t want that to brick their accounts.Attempt 4: Hash as IdentityThe last two attempts share a flaw: they tie you to the same handle forever.Whether itâ€™s a handle like  or a domain handle like , we want people to be able to change their handles at any time without breaking links.Sounds familiar? We also want the same for hosting. So letâ€™s keep the â€œdomain handlesâ€ idea but store the current handle in JSON alongside the current hosting:This JSON is turning into sort of a calling card for your identity. â€œCall me , my stuff is at https://some-cool-free-hosting.com.â€Now we need somewhere to host this document, and some way for you to edit it.Letâ€™s revisit the â€œcentralized registryâ€ from approach #2. One problem with it was using handles as permanent identifiers. Also, centralized is bad, but why is it bad? Itâ€™s bad for many reasons, but usually itâ€™s the risk of abuse of power or a single point of failure. Maybe we can, if not remove, then reduce some of those risks. For example, it would be nice if could make the registryâ€™s output self-verifiable.Letâ€™s see if we can use mathematics to help with this.When you create an account, weâ€™ll generate a private and a public key. We then create a piece of JSON with your initial handle, hosting, and public key. We sign this â€œcreate accountâ€ operation with your private key. Then we hash the signed operation. That gives us a string of gibberish like .The registry will store your operation under that hash. That hash becomes the permanent identifier for your account. Weâ€™ll use it in links to refer to you:To resolve a link like this, we ask the registry for the document belonging to . It returns current your hosting, handle, and public key. Then we fetch com.twitter.post/34qye3wows2c5 from your hosting.Okay, but how do you update your handle or your hosting in this registry?To update, you create a new operation with a  field set to the hash of your previous operation. You sign it and send it to the registry. The registry validates the signature, appends the operation to your log, and updates the document.To prove that it doesnâ€™t forge the served documents, the registry exposes an endpoint that lists past operations for an identifier. To verify an operation, you check that its signature is valid and that its  field matches the hash of the operation before it. This lets you verify the entire chain of updates down to the first operation. The hash of the first operation  the identifier, so you can verify that too. At that point, you know that every change was signed with the userâ€™s key.With this approach, the registry is still centralized but it canâ€™t forge anyoneâ€™s documents without the risk of that being detected. To further reduce the need to trust the registry, we make its entire operation log auditable. The registry would hold no private data and be entirely open source. Ideally, it would eventually be spun it out into an independent legal entity so that long-term it can be like ICANN.Since most people wouldnâ€™t want to do key management, itâ€™s assumed the hosting would hold the keys on behalf of the user. The registry includes a way to register an overriding rotational key, which is helpful in case the hosting itself goes rogue. (I wish for a way to set this up with a good UX; most people donâ€™t have this on.)Finally, since the handle is now determined by the document held in the registry, weâ€™ll need to add some way for a domain to signal that it  with being some identifierâ€™s handle. This could be done via DNS, HTTPS, or a mix of both.Phew! This is not perfect but it gets us surprisingly far.Attempt 5: DID as IdentityFrom the end user perspective, attempt #4 (hash as identity) is the most friendly. It doesnâ€™t use domains for identity (only as handles), so losing a domain is fine.However, some find relying on a third-party registry, no matter how transparent, untenable. So it would be nice to support approach #3 (domain as identity) too.Weâ€™ll use a flexible identifier standard called DID (decentralized identifier) which is essentially a way to namespace multiple unrelated identification methods: and such â€” domain-based (attempt #3)did:plc:6wpkkitfdkgthatfvspcfmjo and such â€” registry-based (attempt #4)This also leaves us a room to add other methods in the future, like This makes our Like record look like this:This is going to be its final form. We write  here to remind ourselves that this isnâ€™t an HTTP link, and that you need to follow the resolution procedure (get the document, get the hosting, then get the record) to actually get the result.Now you can forget everything we just discussed and remember four things:A DID is a string identifier that represents an account.An accountâ€™s DID never changes.Every DID points at a document with the current hosting, handle, and public key.A handle needs to be verified in the other direction (the domain must agree).The mental model is that thereâ€™s a function like this:You give it a DID, and it returns where to find their stuff, their bidirectionally verified current handle, and their public key. Youâ€™ll want a  on it.Letâ€™s now finish our social filesystem.With a DID, we can finally construct a path that identifies every particular record:An  URI is a link to a record that survives hosting and handle changes.The mental model here is that you can always resolve it to a record:If the hosting is down, it would temporarily not resolve, but if the user puts it up anywhere and points their DID there, it will start resolving again. The user can also delete the record, which would remove it from the userâ€™s â€œeverything folderâ€.Another way to think about  URI is that it is as a unique identifier of every record in our filesystem, so it can serve as a key in a database or a cache.With links, we can finally represent relationships between records.Letâ€™s look at drilâ€™s post again:Where do the 125 thousand likes come from?These are just 125 thousand  records in different peopleâ€™s â€œeverything foldersâ€ that each  to drilâ€™s  record:Where do the 56K reposts come from? Similarly, this means that there are 56K  records across our social filesystem linking to this post:A reply is just a post that has a parent post. In TypeScript, weâ€™d write it like this:In lexicon, weâ€™d write it like this:This says: the  field is an  link to another record.Every reply to drilâ€™s post will have drilâ€™s post as their :So, to get the reply count, we just need to count every such post:Weâ€™ve now explained how every piece of the original UI can be derived from files:The display name and avi come from drilâ€™s .The tweet text and date come from drilâ€™s com.twitter.post/34qye3wows2c5.The like count is aggregated from everyoneâ€™s s.The repost count is aggregated from everyoneâ€™s s.The reply count is aggregated from everyoneâ€™s s.The last finishing touch is the handle. Unfortunately,  can no longer work as a handle since weâ€™ve chosen to use domains as handles. As a consolation, dril would be able to use  across every future social app if he would like to.Itâ€™s time to give our â€œeverything folderâ€ a proper name. Weâ€™ll call it a . A repository is identified by a DID. It contains collections, which contain records:Each repository is a userâ€™s little piece of the social filesystem. A repository can be hosted anywhereâ€”a free provider, a paid service, or your own server. You can move your repository as many times as youâ€™d like without breaking links.One challenge with building a social filesystem in practice is that apps need to be able to compute derived data (e.g. like counts) with no extra overhead. Of course, it would be completely impractical to look for every  record in every repo referencing a specific post when trying to serve the UI for that post.This is why, in addition to treating a repository as a filesystemâ€”you can  and  stuffâ€”you can treat it as a stream,  to it by a WebSocket. This lets anyone build a local app-specific cache with just the derived data that app needs. Over the stream, you receive each commit as an event, along with the tree delta.For example, a Hacker News backend could listen to creates/updates/deletes of  records in every known repository and save those records locally for fast querying. It could also track derived data like .Subscribing to every known repository from every app is inconvenient. It is nicer to use dedicated services called  which retransmit all events. However, this raises the issue of trust: how do you know whether someone elseâ€™s relay is lying?To solve this, letâ€™s make the repository data self-certifying. We can structure the repository as a hash tree. Each write is a signed  containing the new root hash. This makes it possible to verify records as they come in against their original authorsâ€™ public keys. As long as you subscribe to a relay that retransmits its proofs, you can check every proof to know the records are authentic.Verifying authenticity of records does not require storing their content, which means that relays can act as simple retransmitters and are affordable to run.If you want to explore the Atmosphere (-mosphere, get it?), pdsls is the best starting point. Given a DID or a handle, it shows a list of collections and their records. Itâ€™s really like an old school file manager, except for the social stuff.Go to  if you want some random place to start. Notice that you understand 80% of whatâ€™s going on thereâ€”Collections, Identity, Records, etc.Feel free to branch out. Records link to other records. There is no app-specific aggregation there so it feels a little â€œungroundedâ€ (e.g. there is no thread view like in Bluesky) but there are some interesting navigational features like Backlinks.Watch me walk around the Atmosphere for a bit:(Yeah, what  that lexicon?! I didnâ€™t expect to run into this while recording.)My favorite demo is this.Watch me create a Bluesky post by creating a record via pdsls:The app â€œreactsâ€ to the change. Files are the source of truth!To make the filesystem metaphor more visceral, I can mount my (or anyone elseâ€™s) repository as a FUSE drive with . Now every change shows up there as well:What are files good for? For one, agents really like files. Here Iâ€™m asking Claude to find what my friends have recently made  in the Atmosphere:No API calls, no MCP servers. This may not be the most efficient way to analyze social data, but if you squint, you might see a glimpse of a post-app future. Apps curate data into experiences, but the web we create floats above every app.Thereâ€™s nothing specific to Bluesky here.Data always flows down in the Atmosphereâ€”from our repos to apps.A month ago, Iâ€™ve made a little app called Sidetrail (itâ€™s open source) to practice full-stack development. It lets you create step-by-step walkthroughs and â€œwalkâ€ those. Here you can see Iâ€™m deleting an  record in pdsls, and the corresponding walk disappears from my Sidetrail â€œwalkingâ€ tab:I know exactly  it works, itâ€™s not supposed to  me, but it does! My repo really  the source of truth. My data lives in the Atmosphere, and apps â€œreactâ€ to it.This syncs everyoneâ€™s repo changes to my database so I have a snapshot thatâ€™s easy to query. Iâ€™m sure I could write this more clearly, but conceptually, itâ€™s like Iâ€™m re-rendering my database. Itâ€™s like I called a  â€œaboveâ€ the internet, and now the new props flow down from files into apps, and my DB reacts to them.I could delete those tables in production, and then use Tap to backfill my database . Iâ€™m just caching a slice of the global data. And everyone building AT apps also needs to cache some slices. Maybe different slices, but they overlap. So pooling resources becomes more useful. More of our tooling can be shared too.Hereâ€™s another example that I really like.Now, you can see it says â€œ678,850 scrobblesâ€ at the top of the screen. You might think people have been scrobbling their plays to the teal.fm API for a while.The teal.fm API doesnâ€™t actually exist. Itâ€™s not a thing. Moreover, the teal.fm product doesnâ€™t actually exist either. I mean, I  itâ€™s in development (this is a hobby project!), but at the time of writing, https://teal.fm/ is only a landing page.All you need to start scrobbling is to put records of the  lexicon into your repo.Letâ€™s see if anyone is doing this right now:The lexicon isnâ€™t published as a record (yet?) but itâ€™s easy to find on GitHub. So anyone can build a scrobbler that writes these. Iâ€™m using one of those scrobblers.Hereâ€™s my scrobble showing up:(Itâ€™s a bit slow but the delay is on the Spotify/scrobbler integration side.)To be clear, the person who made this demo doesnâ€™t work on teal.fm either. Itâ€™s not an â€œofficialâ€ demo or anything, and itâ€™s also not using the â€œteal.fm databaseâ€ or â€œteal.fm APIâ€ or anything like it. It just indexes s.The demoâ€™s data layer is using the new  package, which is another of â€™s experiments. You give it some lexicons, and it lets you run GraphQL on your backfilled snapshot of the relevant parts of the social filesystem.Every app can blend cross-product information like this. For example, here is an AT app called Blento that lets you display your teal.fm plays on your homepage:(Again, it doesnâ€™t talk to teal.fmâ€”which doesnâ€™t exist yet!â€”it just reads your files.)Blento is an AT replacement for Bento, which is shutting down. If Blento  itself ever shuts down, any motivated developer can  with the existing content.Thereâ€™s one last example that I wanted to share.For months, Iâ€™ve been complaining about the Blueskyâ€™s default Discover feed which, frankly, doesnâ€™t work all that great for me. Then I heard people saying good things about @spacecowboy17.bsky.socialâ€™s For You algorithm.Iâ€™ve been giving it a try, and I really like it!I ended up switching to it completely. It reminds me of the Twitter algo in 2017â€”the swings are a bit hard but it finds the stuff I wouldnâ€™t want to miss. Itâ€™s also much more responsive to â€œShow Lessâ€. Its core principle seems pretty simple.How does a custom feed like this work? Well, a Bluesky feed is just an endpoint that returns a list of  URIs. Thatâ€™s the contract. You know how this works.Could there be feeds of things other than posts? Sure.There was a tweet a few weeks ago clowning on Bluesky for being so bad at algorithms that users have to install a third-party feed to get a good experience.I agree with  that this shows something important: Bluesky is a place where that  Why? In the Atmosphere, third party is first party. Weâ€™re all building projections of the same data. Itâ€™s a  that someone can do it better.An everything app tries to do everything.An everything ecosystem lets everything get done.]]></content:encoded></item><item><title>Beware of 6.19.4 nftables regression - can render systems unbootable. Hold back on updating if you&apos;re using nftables.</title><link>https://lore.kernel.org/all/bb9ab61c-3bed-4c3d-baf0-0bce4e142292@moonlit-rail.com/</link><author>/u/i-hate-birch-trees</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 06:46:40 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Paper: The framing of a system prompt changes how a transformer generates tokens â€” measured across 3,830 runs with effect sizes up to d&gt;1.0</title><link>https://www.reddit.com/r/artificial/comments/1rgv1kl/paper_the_framing_of_a_system_prompt_changes_how/</link><author>/u/TheTempleofTwo</author><category>ai</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 06:13:46 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Quick summary of an independent preprint I just published: Does the relational framing of a system prompt â€” not its instructions, not its topic â€” change the generative dynamics of an LLM? Two framing variables (relational presence + epistemic openness), crossed into 4 conditions, measured against token-level Shannon entropy across 3 experimental phases, 5 model architectures, 3,830 total inference runs.Yes, framing changes entropy regimes â€” significantly at 7B+ scale (d>1.0 on Mistral-7B)Small models (sub-1B) are largely unaffectedSSMs (Mamba) show no effect â€” this is transformer-specificThe effect is mediated through attention mechanisms (confirmed via ablation study)RÃ—E interaction is superadditive: collaborative + epistemically open framing produces more than either factor alone If you're using ChatGPT, Claude, Mistral, or any 7B+ transformer, the way you frame your system prompt is measurably changing the model's generation dynamics â€” not just steering the output topic. The prompt isn't just instructions. It's a distributional parameter.]]></content:encoded></item><item><title>New Sorted map for go</title><link>https://www.reddit.com/r/golang/comments/1rguj4o/new_sorted_map_for_go/</link><author>/u/Obvious-Image-9688</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 05:45:24 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[It is in general faster than go's internal map for strings, and keeps pace go's internal map with ints. It was created as scheduler and cache invalidator for another project, but has so many features that is very useful on its own. Its optimized for pre-pending and appending elements.Please have a look and provide some feedback and insight.Example showing the fuzzy logic:kv:=omap.NewTs[string,string](cmp.Compare) // Save a value kv.Put("Hello"," ") kv.Put("World","!\n") // Itertor for k,v :=range kv.All { fmt.Printf("%s%s",k,v) The resulting output will be:We can now make things a bit smaller by removing things by a range.// Note, both "Sell" and "Universe", were never added to the instance, // but the between operation works on these keys any ways. kv.RemoveBetween("Sell","Zoo") // Itertor for k,v :=range kv.All() { fmt.Printf("%s%s\n",k,v) } The resulting output will now be:The string "Sell" comes before the string "World"The string "Zoo" comes after the string "World"The index lookup creates 2 values for each potential key:Array position, example: 0Offset can be any of the following: -1,0,1Since lookups create both an index position and offset, it becomes possible to look for the following:Elements before the arrayPositions between elements of the array]]></content:encoded></item><item><title>Workaround for Sunshine access at Wayland greeter after reboot (Plasma Login Manager)</title><link>https://www.reddit.com/r/linux/comments/1rguekb/workaround_for_sunshine_access_at_wayland_greeter/</link><author>/u/withlovefromspace</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 05:38:13 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[So I recently switched to Arch from opensuse and switched to Plasma Login Manager from SDDM as well. On opensuse I had SDDM running on Wayland with enable linger for user services. Now I don't know why but sunshine (KMS) used to work even at the login screen with SDDM Wayland. Now on Arch with PLM, Sunshine (also KMS) doesn't run until after login even with linger active and even if i restart the service so that it isn't inactive (from ssh) it still says it can't find a display when connecting from moonlight.Now every LLM was just telling me to enable auto login but I didn't want to accept defeat. I remembered that I was using ydotool to wake the monitor (before I knew another method with kscreen-doctor, I can share that too if anyone is curious) and I used it to enter my password and fully login without ever seeing the gui. Then I created a script (generated by chatgpt) and I thought it was too cool not to share.The script checks if plasma login manager owns seat0 and tries to start ydotoold. Then uses the bash read command to silently read in your password, clear the field for 1.5 seconds (holds backspace key), then passes what you type into read and hits enter then terminates ydotoold. So far this is working flawlessly. You also need to have uinput module active and access to /dev/uinput (I added my user to input group).I wanted to share the script in case anyone finds it useful for this specific use case and also to ask if anyone has any insight to why sunshine/moonlight connections ran just fine with sddm/wayland on opensuse but not PLM on Arch both with linger enabled. Anyway, this is a pretty specific use case, but I fucking love Linux.#!/usr/bin/env bash set -uo pipefail # â† remove -e to avoid premature exits wait_for_greeter() { echo "[*] Waiting for Plasma Login Manager on seat0..." while true; do if loginctl list-sessions --no-legend | grep -q 'seat0.*greeter'; then echo "[âœ“] Greeter detected on seat0" return fi sleep 0.5 done } wait_for_socket() { echo "[*] Waiting for ydotoold socket..." for _ in {1..100}; do if ydotool key 57:1 57:0 >/dev/null 2>&1; then echo "[âœ“] ydotoold ready" return fi sleep 0.1 done echo "[!] ydotoold did not become ready" exit 1 } ######################################## wait_for_greeter echo "[*] Starting temporary ydotoold (user mode)..." ydotoold >/dev/null 2>&1 & YD_PID=$! cleanup() { echo "[*] Stopping ydotoold..." kill "$YD_PID" 2>/dev/null || true } trap cleanup EXIT wait_for_socket echo "[*] Enter your login password:" read -rsp "Password: " PW echo echo "[*] Clearing field..." ydotool key 14:1 sleep 1.5 ydotool key 14:0 echo "[*] Typing password..." ydotool type "$PW" unset PW echo "[*] Pressing Enter..." ydotool key 28:1 28:0 echo "[âœ“] Done." ]]></content:encoded></item><item><title>Intel releases updated CPU microcode for Xeon 6 Granite Rapids D SoCs</title><link>https://www.phoronix.com/news/Intel-Microcode-20260227</link><author>/u/somerandomxander</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 05:25:07 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com.]]></content:encoded></item><item><title>OpenAI strikes deal with Pentagon after Trump orders government to stop using Anthropic</title><link>https://www.nbcnews.com/tech/tech-news/trump-bans-anthropic-government-use-rcna261055</link><author>/u/Fcking_Chuck</author><category>ai</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 04:52:06 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Do you actually check the error for crypto/rand.Read?</title><link>https://www.reddit.com/r/golang/comments/1rgt8ps/do_you_actually_check_the_error_for_cryptorandread/</link><author>/u/Existing-Search3853</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 04:36:19 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Advice Needed: What AI/ML Topic Would Be Most Useful for a Tech Talk to a Non-ML Tech Team? [D]</title><link>https://www.reddit.com/r/MachineLearning/comments/1rgswtj/advice_needed_what_aiml_topic_would_be_most/</link><author>/u/Same_Half3758</author><category>ai</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 04:19:23 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Iâ€™m a foreign PhD student currently studying in China, and Iâ€™ve recently connected with a mid-sized technology/manufacturing company based in China. Theyâ€™re traditionally focused on audio, communications, and public-address electronic systems that are widely used in education, transportation, and enterprise infrastructureOver the past few weeks, weâ€™ve had a couple of positive interactions:Their team invited me to visit their manufacturing facility and showed me around.More recently, they shared that theyâ€™ve been working on or exploring smart solutions involving AI â€” including some computer vision elements in sports/EdTech contexts.Theyâ€™ve now invited me to give a talk about AI and left it open for me to choose the topic.Since their core isnâ€™t pure machine learning research, Iâ€™m trying to figure out what would be most engaging and useful for them â€” something that comes out of my academic experience as a PhD student but that still applies to their practical interests. I also get the sense this could be an early step toward potential collaboration or even future work with them, so Iâ€™d like to make a strong impression.Questions for the community:What AI/ML topics would you highlight if you were presenting to a mixed technical audience like this?What insights from academic research are most surprising and immediately useful for teams building real systems?Any specific talk structures, demos, or example case studies that keep non-ML specialists engaged?]]></content:encoded></item><item><title>Anthropic should move to Europe</title><link>https://www.reddit.com/r/artificial/comments/1rgsnhn/anthropic_should_move_to_europe/</link><author>/u/Late-Masterpiece-452</author><category>ai</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 04:06:40 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Wouldnâ€˜t it be a great opportunity to offer Anthropic a â€žsafe havenâ€œ from US government bullying? Letâ€˜s try to move them over to Europe. ]]></content:encoded></item><item><title>[P] Micro Diffusion â€” Discrete text diffusion in ~150 lines of pure Python</title><link>https://www.reddit.com/r/MachineLearning/comments/1rgsgt6/p_micro_diffusion_discrete_text_diffusion_in_150/</link><author>/u/Impossible-Pay-4885</author><category>ai</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 03:57:34 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Inspired by Karpathy's MicroGPT, I wanted to build the equivalent for text diffusion â€” a minimal implementation that shows the core algorithm without the complexity.Autoregressive models generate left to right. Diffusion generates all tokens at once by iteratively unmasking from noise:_ _ _ _ _ _ â†’ _ o r _ a â†’ n o r i aThree implementations included:- train_minimal.py (143 lines, pure NumPy) â€” bare minimum- train_pure.py (292 lines, pure NumPy) â€” with comments and visualization- train .py (413 lines, PyTorch) â€” bidirectional Transformer denoiserAll three share the same diffusion loop. Only the denoiser differs â€” because the denoiser is a pluggable component.Trains on 32K SSA names, runs on CPU in a few minutes. No GPU needed.(I am not good at English, so I would like to inform you that I wrote this with the help of AI.)]]></content:encoded></item><item><title>Anthropic says it will challenge Pentagon&apos;s supply chain risk designation in court</title><link>https://www.reuters.com/world/us/anthropic-says-it-will-challenge-pentagons-supply-chain-risk-designation-court-2026-02-28/</link><author>/u/Gloomy_Nebula_5138</author><category>ai</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 03:27:25 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Letter from Google and OpenAI employees against the use of AI for mass surveillance and fully autonomous weapons</title><link>https://notdivided.org/</link><author>/u/an-com-42</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 03:02:26 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Frequently Asked Questions]]></content:encoded></item><item><title>AI Added &apos;Basically Zero&apos; to US Economic Growth Last Year, Goldman Sachs Says. Imported chips and hardware mean the AI investments are translating into US GDP growth.</title><link>https://gizmodo.com/ai-added-basically-zero-to-us-economic-growth-last-year-goldman-sachs-says-2000725380</link><author>/u/esporx</author><category>ai</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 01:45:16 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Meta, Amazon, Google, OpenAI, and other tech companies spent billions last year investing in AI. Theyâ€™re expected to spend even more, roughly $700 billion, this year on dozens of new data centers to train and run their advanced models.This spending frenzy has kept Wall Street buzzing and fueled a narrative that all this investment is helping prop up and even grow the U.S. economy.President Donald Trump has cited that argument as a reason the industry should not face state-level regulations.â€œInvestment in AI is helping to make the U.S. Economy the â€˜HOTTESTâ€™ in the World â€” But overregulation by the States is threatening to undermine this Growth Engine,â€ Trump wrote in a post on Truth Social in November. â€œWe MUST have one Federal Standard instead of a patchwork of 50 State Regulatory Regimes.â€Some prominent economists have also given credibility to this story with their analysis. Jason Furman, a Harvard economics professor, said in a post on X that investments in information processing equipment and software accounted for 92% of GDP growth in the first half of the year. Meanwhile, economists at the Federal Reserve Bank of St. Louis similarly estimated that AI-related investments made up 39% of GDP growth in the third quarter of 2025.But now some Wall Street analysts are starting to rethink this narrative.â€œIt was a very intuitive story,â€ Joseph Briggs, a Goldman Sachs analyst, told The Washington Post on Monday. â€œThat maybe prevented or limited the need to actually dig deeper into what was happening.â€Briggsâ€™ colleague, Goldman Sachs Chief Economist Jan Hatzius, said in an interview with the Atlantic Council that AI investment spending has had â€œbasically zeroâ€ contribution to the U.S. GDP growth in 2025.â€œWe donâ€™t actually view AI investment as strongly growth positive,â€ said Hatzius. â€œI think thereâ€™s a lot of misreporting, actually, of the impact AI investment had on U.S. GDP growth in 2025, and itâ€™s much smaller than is often perceived.â€Hatzius said one major reason is that much of the equipment powering AI is imported. While U.S. companies are spending billions, importing chips and hardware offsets those investments in GDP calculations.â€œA lot of the AI investment that weâ€™re seeing in the U.S. adds to Taiwanese GDP, and it adds to Korean GDP but not really that much to U.S. GDP,â€ he said.On top of that, there is currently no reliable way to accurately measure how AI use among businesses and consumers contributes to economic growth.So far, many business leaders say AI hasnâ€™t significantly improved productivity.A recent survey of nearly 6,000 executives in the U.S., Europe, and Australia found that despite 70% of firms actively using AI, about 80% reported no impact on employment or productivity.]]></content:encoded></item><item><title>First Go Project - Theia</title><link>https://www.reddit.com/r/golang/comments/1rgoi5s/first_go_project_theia/</link><author>/u/DaddyDio3008</author><category>golang</category><category>reddit</category><pubDate>Sat, 28 Feb 2026 00:52:20 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I started learning Go less than a week ago. I thought a fun first program would be a TUI file explorer that lets you change directories, and copy paths to your clipboard. I'm still working on it, but now it is at least usable. Drop a star if you think it's cool, but I'm just looking for some feedback.]]></content:encoded></item><item><title>Distributed Systems for Fun and Profit</title><link>https://book.mixu.net/distsys/single-page.html</link><author>/u/ketralnis</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 00:40:20 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[I wanted a text that would bring together the ideas behind many of the more recent distributed systems - systems such as Amazon's Dynamo, Google's BigTable and MapReduce, Apache's Hadoop and so on.In this text I've tried to provide a more accessible introduction to distributed systems. To me, that means two things: introducing the key concepts that you will need in order to have a good time reading more serious texts, and providing a narrative that covers things in enough detail that you get a gist of what's going on without getting stuck on details. It's 2013, you've got the Internet, and you can selectively read more about the topics you find most interesting.In my view, much of distributed programming is about dealing with the implications of two consequences of distribution:that information travels at the speed of lightthat independent things fail independently*In other words, that the core of distributed programming is dealing with distance (duh!) and having more than one thing (duh!). These constraints define a space of possible system designs, and my hope is that after reading this you'll have a better sense of how distance, time and consistency models interact.This text is focused on distributed programming and systems concepts you'll need to understand commercial systems in the data center. It would be madness to attempt to cover everything. You'll learn many key protocols and algorithms (covering, for example, many of the most cited papers in the discipline), including some new exciting ways to look at eventual consistency that haven't still made it into college textbooks - such as CRDTs and the CALM theorem.The first chapter covers distributed systems at a high level by introducing a number of important terms and concepts. It covers high level goals, such as scalability, availability, performance, latency and fault tolerance; how those are hard to achieve, and how abstractions and models as well as partitioning and replication come into play.The second chapter dives deeper into abstractions and impossibility results. It starts with a Nietzsche quote, and then introduces system models and the many assumptions that are made in a typical system model. It then discusses the CAP theorem and summarizes the FLP impossibility result. It then turns to the implications of the CAP theorem, one of which is that one ought to explore other consistency models. A number of consistency models are then discussed.A big part of understanding distributed systems is about understanding time and order.  To the extent that we fail to understand and model time, our systems will fail. The third chapter discusses time and order, and clocks as well as the various uses of time, order and clocks (such as vector clocks and failure detectors).The fourth chapter introduces the replication problem, and the two basic ways in which it can be performed. It turns out that most of the relevant characteristics can be discussed with just this simple characterization. Then, replication methods for maintaining single-copy consistency are discussed from the least fault tolerant (2PC) to Paxos.The fifth chapter discussed replication with weak consistency guarantees. It introduces a basic reconciliation scenario, where partitioned replicas attempt to reach agreement. It then discusses Amazon's Dynamo as an example of a system design with weak consistency guarantees. Finally, two perspectives on disorderly programming are discussed: CRDTs and the CALM theorem.Distributed programming is the art of solving the same problem that you can solve on a single computer using multiple computers.There are two basic tasks that any computer system needs to accomplish:Distributed programming is the art of solving the same problem that you can solve on a single computer using multiple computers - usually, because the problem no longer fits on a single computer.Nothing really demands that you use distributed systems. Given infinite money and infinite R&D time, we wouldn't need distributed systems. All computation and storage could be done on a magic box - a single, incredibly fast and incredibly reliable system that you pay someone else to design for you.However, few people have infinite resources. Hence, they have to find the right place on some real-world cost-benefit curve. At a small scale, upgrading hardware is a viable strategy. However, as problem sizes increase you will reach a point where either the hardware upgrade that allows you to solve the problem on a single node does not exist, or becomes cost-prohibitive. At that point, I welcome you to the world of distributed systems.It is a current reality that the best value is in mid-range, commodity hardware - as long as the maintenance costs can be kept down through fault-tolerant software.Computations primarily benefit from high-end hardware to the extent to which they can replace slow network accesses with internal memory accesses. The performance advantage of high-end hardware is limited in tasks that require large amounts of communication between nodes.As the figure above from Barroso, Clidaras & HÃ¶lzle shows, the performance gap between high-end and commodity hardware decreases with cluster size assuming a uniform memory access pattern across all nodes.Ideally, adding a new machine would increase the performance and capacity of the system linearly. But of course this is not possible, because there is some overhead that arises due to having separate computers. Data needs to be copied around, computation tasks have to be coordinated and so on. This is why it's worthwhile to study distributed algorithms - they provide efficient solutions to specific problems, as well as guidance about what is possible, what the minimum cost of a correct implementation is, and what is impossible.The focus of this text is on distributed programming and systems in a mundane, but commercially relevant setting: the data center. For example, I will not discuss specialized problems that arise from having an exotic network configuration, or that arise in a shared-memory setting. Additionally, the focus is on exploring the system design space rather than on optimizing any specific design - the latter is a topic for a much more specialized text.What we want to achieve: Scalability and other good thingsThe way I see it, everything starts with the need to deal with size.Most things are trivial at a small scale - and the same problem becomes much harder once you surpass a certain size, volume or other physically constrained thing. It's easy to lift a piece of chocolate, it's hard to lift a mountain. It's easy to count how many people are in a room, and hard to count how many people are in a country.So everything starts with size - scalability. Informally speaking, in a scalable system as we move from small to large, things should not get incrementally worse. Here's another definition:is the ability of a system, network, or process, to handle a growing amount of work in a capable manner or its ability to be enlarged to accommodate that growth.What is it that is growing? Well, you can measure growth in almost any terms (number of people, electricity usage etc.). But there are three particularly interesting things to look at:Size scalability: adding more nodes should make the system linearly faster; growing the dataset should not increase latencyGeographic scalability: it should be possible to use multiple data centers to reduce the time it takes to respond to user queries, while dealing with cross-data center latency in some sensible manner.Administrative scalability: adding more nodes should not increase the administrative costs of the system (e.g. the administrators-to-machines ratio).Of course, in a real system growth occurs on multiple different axes simultaneously; each metric captures just some aspect of growth.A scalable system is one that continues to meet the needs of its users as scale increases. There are two particularly relevant aspects - performance and availability - which can be measured in various ways.is characterized by the amount of useful work accomplished by a computer system compared to the time and resources used.Depending on the context, this may involve achieving one or more of the following:Short response time/low latency for a given piece of workHigh throughput (rate of processing work)Low utilization of computing resource(s)There are tradeoffs involved in optimizing for any of these outcomes. For example, a system may achieve a higher throughput by processing larger batches of work thereby reducing operation overhead. The tradeoff would be longer response times for individual pieces of work due to batching.I find that low latency - achieving a short response time - is the most interesting aspect of performance, because it has a strong connection with physical (rather than financial) limitations. It is harder to address latency using financial resources than the other aspects of performance.There are a lot of really specific definitions for latency, but I really like the idea that the etymology of the word evokes:The state of being latent; delay, a period between the initiation of something and the occurrence.And what does it mean to be "latent"?From Latin latens, latentis, present participle of lateo ("lie hidden"). Existing or present but concealed or inactive.This definition is pretty cool, because it highlights how latency is really the time between when something happened and the time it has an impact or becomes visible.For example, imagine that you are infected with an airborne virus that turns people into zombies. The latent period is the time between when you became infected, and when you turn into a zombie. That's latency: the time during which something that has already happened is concealed from view.Let's assume for a moment that our distributed system does just one high-level task: given a query, it takes all of the data in the system and calculates a single result. In other words, think of a distributed system as a data store with the ability to run a single deterministic computation (function) over its current content:result = query(all data in the system)Then, what matters for latency is not the amount of old data, but rather the speed at which new data "takes effect" in the system. For example, latency could be measured in terms of how long it takes for a write to become visible to readers.The other key point based on this definition is that if nothing happens, there is no "latent period". A system in which data doesn't change doesn't (or shouldn't) have a latency problem.In a distributed system, there is a minimum latency that cannot be overcome: the speed of light limits how fast information can travel, and hardware components have a minimum latency cost incurred per operation (think RAM and hard drives but also CPUs).How much that minimum latency impacts your queries depends on the nature of those queries and the physical distance the information needs to travel.Availability (and fault tolerance)The second aspect of a scalable system is availability.the proportion of time a system is in a functioning condition. If a user cannot access the system, it is said to be unavailable. Distributed systems allow us to achieve desirable characteristics that would be hard to accomplish on a single system. For example, a single machine cannot tolerate any failures since it either fails or doesn't.Distributed systems can take a bunch of unreliable components, and build a reliable system on top of them.Systems that have no redundancy can only be as available as their underlying components. Systems built with redundancy can be tolerant of partial failures and thus be more available. It is worth noting that "redundant" can mean different things depending on what you look at - components, servers, datacenters and so on.Formulaically, availability is: Availability = uptime / (uptime + downtime).Availability from a technical perspective is mostly about being fault tolerant. Because the probability of a failure occurring increases with the number of components, the system should be able to compensate so as to not become less reliable as the number of components increases.How much downtime is allowed per year?Availability is in some sense a much wider concept than uptime, since the availability of a service can also be affected by, say, a network outage or the company owning the service going out of business (which would be a factor which is not really relevant to fault tolerance but would still influence the availability of the system). But without knowing every single specific aspect of the system, the best we can do is design for fault tolerance.What does it mean to be fault tolerant?ability of a system to behave in a well-defined manner once faults occurFault tolerance boils down to this: define what faults you expect and then design a system or an algorithm that is tolerant of them. You can't tolerate faults you haven't considered.What prevents us from achieving good things?Distributed systems are constrained by two physical factors:the number of nodes (which increases with the required storage and computation capacity)the distance between nodes (information travels, at best, at the speed of light)Working within those constraints:an increase in the number of independent nodes increases the probability of failure in a system (reducing availability and increasing administrative costs)an increase in the number of independent nodes may increase the need for communication between nodes (reducing performance as scale increases)an increase in geographic distance increases the minimum latency for communication between distant nodes (reducing performance for certain operations)Beyond these tendencies - which are a result of the physical constraints - is the world of system design options.Both performance and availability are defined by the external guarantees the system makes. On a high level, you can think of the guarantees as the SLA (service level agreement) for the system: if I write data, how quickly can I access it elsewhere? After the data is written, what guarantees do I have of durability? If I ask the system to run a computation, how quickly will it return results? When components fail, or are taken out of operation, what impact will this have on the system?There is another criterion, which is not explicitly mentioned but implied: intelligibility. How understandable are the guarantees that are made? Of course, there are no simple metrics for what is intelligible.I was kind of tempted to put "intelligibility" under physical limitations. After all, it is a hardware limitation in people that we have a hard time understanding anything that involves more moving things than we have fingers. That's the difference between an error and an anomaly - an error is incorrect behavior, while an anomaly is unexpected behavior. If you were smarter, you'd expect the anomalies to occur.This is where abstractions and models come into play. Abstractions make things more manageable by removing real-world aspects that are not relevant to solving a problem. Models describe the key properties of a distributed system in a precise manner. I'll discuss many kinds of models in the next chapter, such as:System model (asynchronous / synchronous)Failure model (crash-fail, partitions, Byzantine)Consistency model (strong, eventual)A good abstraction makes working with a system easier to understand, while capturing the factors that are relevant for a particular purpose.There is a tension between the reality that there are many nodes and with our desire for systems that "work like a single system". Often, the most familiar model (for example, implementing a shared memory abstraction on a distributed system) is too expensive.A system that makes weaker guarantees has more freedom of action, and hence potentially greater performance - but it is also potentially hard to reason about. People are better at reasoning about systems that work like a single system, rather than a collection of nodes.One can often gain performance by exposing more details about the internals of the system. For example, in columnar storage, the user can (to some extent) reason about the locality of the key-value pairs within the system and hence make decisions that influence the performance of typical queries. Systems which hide these kinds of details are easier to understand (since they act more like single unit, with fewer details to think about), while systems that expose more real-world details may be more performant (because they correspond more closely to reality).Several types of failures make writing distributed systems that act like a single system difficult. Network latency and network partitions (e.g. total network failure between some nodes) mean that a system needs to sometimes make hard choices about whether it is better to stay available but lose some crucial guarantees that cannot be enforced, or to play it safe and refuse clients when these types of failures occur.The CAP theorem - which I will discuss in the next chapter - captures some of these tensions. In the end, the ideal system meets both programmer needs (clean semantics) and business needs (availability/consistency/latency).Design techniques: partition and replicateThe manner in which a data set is distributed between multiple nodes is very important. In order for any computation to happen, we need to locate the data and then act on it.There are two basic techniques that can be applied to a data set. It can be split over multiple nodes (partitioning) to allow for more parallel processing. It can also be copied or cached on different nodes to reduce the distance between the client and the server and for greater fault tolerance (replication).Divide and conquer - I mean, partition and replicate.The picture below illustrates the difference between these two: partitioned data (A and B below) is divided into independent sets, while replicated data (C below) is copied to multiple locations.This is the one-two punch for solving any problem where distributed computing plays a role. Of course, the trick is in picking the right technique for your concrete implementation; there are many algorithms that implement replication and partitioning, each with different limitations and advantages which need to be assessed against your design objectives.Partitioning is dividing the dataset into smaller distinct independent sets; this is used to reduce the impact of dataset growth since each partition is a subset of the data.Partitioning improves performance by limiting the amount of data to be examined and by locating related data in the same partitionPartitioning improves availability by allowing partitions to fail independently, increasing the number of nodes that need to fail before availability is sacrificedPartitioning is also very much application-specific, so it is hard to say much about it without knowing the specifics. That's why the focus is on replication in most texts, including this one.Partitioning is mostly about defining your partitions based on what you think the primary access pattern will be, and dealing with the limitations that come from having independent partitions (e.g. inefficient access across partitions, different rate of growth etc.).Replication is making copies of the same data on multiple machines; this allows more servers to take part in the computation.To replication! The cause of, and solution to all of life's problems.Replication - copying or reproducing something - is the primary way in which we can fight latency.Replication improves performance by making additional computing power and bandwidth applicable to a new copy of the dataReplication improves availability by creating additional copies of the data, increasing the number of nodes that need to fail before availability is sacrificedReplication is about providing extra bandwidth, and caching where it counts. It is also about maintaining consistency in some way according to some consistency model.Replication allows us to achieve scalability, performance and fault tolerance. Afraid of loss of availability or reduced performance? Replicate the data to avoid a bottleneck or single point of failure. Slow computation? Replicate the computation on multiple systems. Slow I/O? Replicate the data to a local cache to reduce latency or onto multiple machines to increase throughput.Replication is also the source of many of the problems, since there are now independent copies of the data that has to be kept in sync on multiple machines - this means ensuring that the replication follows a consistency model.The choice of a consistency model is crucial: a good consistency model provides clean semantics for programmers (in other words, the properties it guarantees are easy to reason about) and meets business/design goals such as high availability or strong consistency.Only one consistency model for replication - strong consistency - allows you to program as-if the underlying data was not replicated. Other consistency models expose some internals of the replication to the programmer. However, weaker consistency models can provide lower latency and higher availability - and are not necessarily harder to understand, just different.In this chapter, we'll travel up and down the level of abstraction, look at some impossibility results (CAP and FLP), and then travel back down for the sake of performance.If you've done any programming, the idea of levels of abstraction is probably familiar to you. You'll always work at some level of abstraction, interface with a lower level layer through some API, and probably provide some higher-level API or user interface to your users. The seven-layer OSI model of computer networking is a good example of this.Distributed programming is, I'd assert, in large part dealing with consequences of distribution (duh!). That is, there is a tension between the reality that there are many nodes and with our desire for systems that "work like a single system". That means finding a good abstraction that balances what is possible with what is understandable and performant.What do we mean when say X is more abstract than Y? First, that X does not introduce anything new or fundamentally different from Y. In fact, X may remove some aspects of Y or present them in a way that makes them more manageable.
Second, that X is in some sense easier to grasp than Y, assuming that the things that X removed from Y are not important to the matter at hand.Every concept originates through our equating what is unequal. No leaf ever wholly equals another, and the concept "leaf" is formed through an arbitrary abstraction from these individual differences, through forgetting the distinctions; and now it gives rise to the idea that in nature there might be something besides the leaves which would be "leaf" - some kind of original form after which all leaves have been woven, marked, copied, colored, curled, and painted, but by unskilled hands, so that no copy turned out to be a correct, reliable, and faithful image of the original form.Abstractions, fundamentally, are fake. Every situation is unique, as is every node. But abstractions make the world manageable: simpler problem statements - free of reality - are much more analytically tractable and provided that we did not ignore anything essential, the solutions are widely applicable.Indeed, if the things that we kept around are essential, then the results we can derive will be widely applicable. This is why impossibility results are so important: they take the simplest possible formulation of a problem, and demonstrate that it is impossible to solve within some set of constraints or assumptions.All abstractions ignore something in favor of equating things that are in reality unique. The trick is to get rid of everything that is not essential. How do you know what is essential? Well, you probably won't know a priori.Every time we exclude some aspect of a system from our specification of the system, we risk introducing a source of error and/or a performance issue. That's why sometimes we need to go in the other direction, and selectively introduce some aspects of real hardware and the real-world problem back. It may be sufficient to reintroduce some specific hardware characteristics (e.g. physical sequentiality) or other physical characteristics to get a system that performs well enough.With this in mind, what is the least amount of reality we can keep around while still working with something that is still recognizable as a distributed system? A system model is a specification of the characteristics we consider important; having specified one, we can then take a look at some impossibility results and challenges.A key property of distributed systems is distribution. More specifically, programs in a distributed system:run concurrently on independent nodes ...are connected by a network that may introduce nondeterminism and message loss ...and have no shared memory or shared clock.There are many implications:each node executes a program concurrentlyknowledge is local: nodes have fast access only to their local state, and any information about global state is potentially out of datenodes can fail and recover from failure independentlymessages can be delayed or lost (independent of node failure; it is not easy to distinguish network failure and node failure)and clocks are not synchronized across nodes (local timestamps do not correspond to the global real time order, which cannot be easily observed)A system model enumerates the many assumptions associated with a particular system design.a set of assumptions about the environment and facilities on which a distributed system is implementedSystem models vary in their assumptions about the environment and facilities. These assumptions include:what capabilities the nodes have and how they may failhow communication links operate and how they may fail andproperties of the overall system, such as assumptions about time and orderA robust system model is one that makes the weakest assumptions: any algorithm written for such a system is very tolerant of different environments, since it makes very few and very weak assumptions.On the other hand, we can create a system model that is easy to reason about by making strong assumptions. For example, assuming that nodes do not fail means that our algorithm does not need to handle node failures. However, such a system model is unrealistic and hence hard to apply into practice.Let's look at the properties of nodes, links and time and order in more detail.Nodes in our system modelNodes serve as hosts for computation and storage. They have:the ability to execute a programthe ability to store data into volatile memory (which can be lost upon failure) and into stable state (which can be read after a failure)a clock (which may or may not be assumed to be accurate)Nodes execute deterministic algorithms: the local computation, the local state after the computation, and the messages sent are determined uniquely by the message received and local state when the message was received.There are many possible failure models which describe the ways in which nodes can fail. In practice, most systems assume a crash-recovery failure model: that is, nodes can only fail by crashing, and can (possibly) recover after crashing at some later point.Another alternative is to assume that nodes can fail by misbehaving in any arbitrary way. This is known as Byzantine fault tolerance. Byzantine faults are rarely handled in real world commercial systems, because algorithms resilient to arbitrary faults are more expensive to run and more complex to implement. I will not discuss them here.Communication links connect individual nodes to each other, and allow messages to be sent in either direction. Many books that discuss distributed algorithms assume that there are individual links between each pair of nodes, that the links provide FIFO (first in, first out) order for messages, that they can only deliver messages that were sent, and that sent messages can be lost.Some algorithms assume that the network is reliable: that messages are never lost and never delayed indefinitely. This may be a reasonable assumption for some real-world settings, but in general it is preferable to consider the network to be unreliable and subject to message loss and delays.A network partition occurs when the network fails while the nodes themselves remain operational. When this occurs, messages may be lost or delayed until the network partition is repaired. Partitioned nodes may be accessible by some clients, and so must be treated differently from crashed nodes. The diagram below illustrates a node failure vs. a network partition:It is rare to make further assumptions about communication links. We could assume that links only work in one direction, or we could introduce different communication costs (e.g. latency due to physical distance) for different links. However, these are rarely concerns in commercial environments except for long-distance links (WAN latency) and so I will not discuss them here; a more detailed model of costs and topology allows for better optimization at the cost of complexity.Timing / ordering assumptionsOne of the consequences of physical distribution is that each node experiences the world in a unique manner. This is inescapable, because information can only travel at the speed of light. If nodes are at different distances from each other, then any messages sent from one node to the others will arrive at a different time and potentially in a different order at the other nodes.Timing assumptions are a convenient shorthand for capturing assumptions about the extent to which we take this reality into account. The two main alternatives are:Processes execute in lock-step; there is a known upper bound on message transmission delay; each process has an accurate clockNo timing assumptions - e.g. processes execute at independent rates; there is no bound on message transmission delay; useful clocks do not existThe synchronous system model imposes many constraints on time and order. It essentially assumes that the nodes have the same experience: that messages that are sent are always received within a particular maximum transmission delay, and that processes execute in lock-step. This is convenient, because it allows you as the system designer to make assumptions about time and order, while the asynchronous system model doesn't.Asynchronicity is a non-assumption: it just assumes that you can't rely on timing (or a "time sensor").It is easier to solve problems in the synchronous system model, because assumptions about execution speeds, maximum message transmission delays and clock accuracy all help in solving problems since you can make inferences based on those assumptions and rule out inconvenient failure scenarios by assuming they never occur.Of course, assuming the synchronous system model is not particularly realistic. Real-world networks are subject to failures and there are no hard bounds on message delay. Real world systems are at best partially synchronous: they may occasionally work correctly and provide some upper bounds, but there will be times where messages are delayed indefinitely and clocks are out of sync. I won't really discuss algorithms for synchronous systems here, but you will probably run into them in many other introductory books because they are analytically easier (but unrealistic).During the rest of this text, we'll vary the parameters of the system model. Next, we'll look at how varying two system properties:whether or not network partitions are included in the failure model, andsynchronous vs. asynchronous timing assumptionsinfluence the system design choices by discussing two impossibility results (FLP and CAP).Of course, in order to have a discussion, we also need to introduce a problem to solve. The problem I'm going to discuss is the consensus problem.Several computers (or nodes) achieve consensus if they all agree on some value. More formally:Agreement: Every correct process must agree on the same value.Integrity: Every correct process decides at most one value, and if it decides some value, then it must have been proposed by some process.Termination: All processes eventually reach a decision.Validity: If all correct processes propose the same value V, then all correct processes decide V.The consensus problem is at the core of many commercial distributed systems. After all, we want the reliability and performance of a distributed system without having to deal with the consequences of distribution (e.g. disagreements / divergence between nodes), and solving the consensus problem makes it possible to solve several related, more advanced problems such as atomic broadcast and atomic commit.Two impossibility resultsThe first impossibility result, known as the FLP impossibility result, is an impossibility result that is particularly relevant to people who design distributed algorithms. The second - the CAP theorem - is a related result that is more relevant to practitioners; people who need to choose between different system designs but who are not directly concerned with the design of algorithms.The FLP impossibility resultI will only briefly summarize the FLP impossibility result, though it is considered to be more important in academic circles. The FLP impossibility result (named after the authors, Fischer, Lynch and Patterson) examines the consensus problem under the asynchronous system model (technically, the agreement problem, which is a very weak form of the consensus problem). It is assumed that nodes can only fail by crashing; that the network is reliable, and that the typical timing assumptions of the asynchronous system model hold: e.g. there are no bounds on message delay.Under these assumptions, the FLP result states that "there does not exist a (deterministic) algorithm for the consensus problem in an asynchronous system subject to failures, even if messages can never be lost, at most one process may fail, and it can only fail by crashing (stopping executing)".This result means that there is no way to solve the consensus problem under a very minimal system model in a way that cannot be delayed forever.  The argument is that if such an algorithm existed, then one could devise an execution of that algorithm in which it would remain undecided ("bivalent") for an arbitrary amount of time by delaying message delivery - which is allowed in the asynchronous system model. Thus, such an algorithm cannot exist.This impossibility result is important because it highlights that assuming the asynchronous system model leads to a tradeoff: algorithms that solve the consensus problem must either give up safety or liveness when the guarantees regarding bounds on message delivery do not hold.This insight is particularly relevant to people who design algorithms, because it imposes a hard constraint on the problems that we know are solvable in the asynchronous system model. The CAP theorem is a related theorem that is more relevant to practitioners: it makes slightly different assumptions (network failures rather than node failures), and has more clear implications for practitioners choosing between system designs.The CAP theorem was initially a conjecture made by computer scientist Eric Brewer. It's a popular and fairly useful way to think about tradeoffs in the guarantees that a system design makes. It even has a formal proof by Gilbert and Lynch and no, Nathan Marz didn't debunk it, in spite of what a particular discussion site thinks.The theorem states that of these three properties:Consistency: all nodes see the same data at the same time.Availability: node failures do not prevent survivors from continuing to operate.Partition tolerance: the system continues to operate despite message loss due to network and/or node failureonly two can be satisfied simultaneously. We can even draw this as a pretty diagram, picking two properties out of three gives us three types of systems that correspond to different intersections:Note that the theorem states that the middle piece (having all three properties) is not achievable. Then we get three different system types:CA (consistency + availability). Examples include full strict quorum protocols, such as two-phase commit.CP (consistency + partition tolerance). Examples include majority quorum protocols in which minority partitions are unavailable such as Paxos.AP (availability + partition tolerance). Examples include protocols using conflict resolution, such as Dynamo.The CA and CP system designs both offer the same consistency model: strong consistency. The only difference is that a CA system cannot tolerate any node failures; a CP system can tolerate up to  faults given  nodes in a non-Byzantine failure model (in other words, it can tolerate the failure of a minority  of the nodes as long as majority  stays up). The reason is simple:A CA system does not distinguish between node failures and network failures, and hence must stop accepting writes everywhere to avoid introducing divergence (multiple copies). It cannot tell whether a remote node is down, or whether just the network connection is down: so the only safe thing is to stop accepting writes.A CP system prevents divergence (e.g. maintains single-copy consistency) by forcing asymmetric behavior on the two sides of the partition. It only keeps the majority partition around, and requires the minority partition to become unavailable (e.g. stop accepting writes), which retains a degree of availability (the majority partition) and still ensures single-copy consistency.I'll discuss this in more detail in the chapter on replication when I discuss Paxos. The important thing is that CP systems incorporate network partitions into their failure model and distinguish between a majority partition and a minority partition using an algorithm like Paxos, Raft or viewstamped replication. CA systems are not partition-aware, and are historically more common: they often use the two-phase commit algorithm and are common in traditional distributed relational databases.Assuming that a partition occurs, the theorem reduces to a binary choice between availability and consistency.I think there are four conclusions that should be drawn from the CAP theorem:First, that many system designs used in early distributed relational database systems did not take into account partition tolerance (e.g. they were CA designs). Partition tolerance is an important property for modern systems, since network partitions become much more likely if the system is geographically distributed (as many large systems are).Second, that there is a tension between strong consistency and high availability during network partitions. The CAP theorem is an illustration of the tradeoffs that occur between strong guarantees and distributed computation.In some sense, it is quite crazy to promise that a distributed system consisting of independent nodes connected by an unpredictable network "behaves in a way that is indistinguishable from a non-distributed system".Strong consistency guarantees require us to give up availability during a partition. This is because one cannot prevent divergence between two replicas that cannot communicate with each other while continuing to accept writes on both sides of the partition.How can we work around this? By strengthening the assumptions (assume no partitions) or by weakening the guarantees. Consistency can be traded off against availability (and the related capabilities of offline accessibility and low latency). If "consistency" is defined as something less than "all nodes see the same data at the same time" then we can have both availability and some (weaker) consistency guarantee.Third, that there is a tension between strong consistency and performance in normal operation.Strong consistency / single-copy consistency requires that nodes communicate and agree on every operation. This results in high latency during normal operation.If you can live with a consistency model other than the classic one, a consistency model that allows replicas to lag or to diverge, then you can reduce latency during normal operation and maintain availability in the presence of partitions.When fewer messages and fewer nodes are involved, an operation can complete faster. But the only way to accomplish that is to relax the guarantees: let some of the nodes be contacted less frequently, which means that nodes can contain old data.This also makes it possible for anomalies to occur. You are no longer guaranteed to get the most recent value. Depending on what kinds of guarantees are made, you might read a value that is older than expected, or even lose some updates.Fourth - and somewhat indirectly - that if we do not want to give up availability during a network partition, then we need to explore whether consistency models other than strong consistency are workable for our purposes.For example, even if user data is georeplicated to multiple datacenters, and the link between those two datacenters is temporarily out of order, in many cases we'll still want to allow the user to use the website / service. This means reconciling two divergent sets of data later on, which is both a technical challenge and a business risk. But often both the technical challenge and the business risk are manageable, and so it is preferable to provide high availability.Consistency and availability are not really binary choices, unless you limit yourself to strong consistency. But strong consistency is just one consistency model: the one where you, by necessity, need to give up availability in order to prevent more than a single copy of the data from being active. As Brewer himself points out, the "2 out of 3" interpretation is misleading.If you take away just one idea from this discussion, let it be this: "consistency" is not a singular, unambiguous property. Remember:Instead, a consistency model is a guarantee - any guarantee - that a data store gives to programs that use it.a contract between programmer and system, wherein the system guarantees that if the programmer follows some specific rules, the results of operations on the data store will be predictableThe "C" in CAP is "strong consistency", but "consistency" is not a synonym for "strong consistency".Let's take a look at some alternative consistency models.Strong consistency vs. other consistency modelsConsistency models can be categorized into two types: strong and weak consistency models:Strong consistency models (capable of maintaining a single copy)Weak consistency models (not strong)Client-centric consistency modelsCausal consistency: strongest model availableEventual consistency modelsStrong consistency models guarantee that the apparent order and visibility of updates is equivalent to a non-replicated system. Weak consistency models, on the other hand, do not make such guarantees.Note that this is by no means an exhaustive list. Again, consistency models are just arbitrary contracts between the programmer and system, so they can be almost anything.Strong consistency modelsStrong consistency models can further be divided into two similar, but slightly different consistency models:: Under linearizable consistency, all operations  to have executed atomically in an order that is consistent with the global real-time ordering of operations. (Herlihy & Wing, 1991): Under sequential consistency, all operations  to have executed atomically in some order that is consistent with the order seen at individual nodes and that is equal at all nodes. (Lamport, 1979)The key difference is that linearizable consistency requires that the order in which operations take effect is equal to the actual real-time ordering of operations. Sequential consistency allows for operations to be reordered as long as the order observed on each node remains consistent. The only way someone can distinguish between the two is if they can observe all the inputs and timings going into the system; from the perspective of a client interacting with a node, the two are equivalent.The difference seems immaterial, but it is worth noting that sequential consistency does not compose.Strong consistency models allow you as a programmer to replace a single server with a cluster of distributed nodes and not run into any problems.All the other consistency models have anomalies (compared to a system that guarantees strong consistency), because they behave in a way that is distinguishable from a non-replicated system. But often these anomalies are acceptable, either because we don't care about occasional issues or because we've written code that deals with inconsistencies after they have occurred in some way.Note that there really aren't any universal typologies for weak consistency models, because "not a strong consistency model" (e.g. "is distinguishable from a non-replicated system in some way") can be almost anything.Client-centric consistency modelsClient-centric consistency models are consistency models that involve the notion of a client or session in some way. For example, a client-centric consistency model might guarantee that a client will never see older versions of a data item. This is often implemented by building additional caching into the client library, so that if a client moves to a replica node that contains old data, then the client library returns its cached value rather than the old value from the replica.Clients may still see older versions of the data, if the replica node they are on does not contain the latest version, but they will never see anomalies where an older version of a value resurfaces (e.g. because they connected to a different replica). Note that there are many kinds of consistency models that are client-centric.The  model says that if you stop changing values, then after some undefined amount of time all replicas will agree on the same value. It is implied that before that time results between replicas are inconsistent in some undefined manner. Since it is trivially satisfiable (liveness property only), it is useless without supplemental information.Saying something is merely eventually consistent is like saying "people are eventually dead". It's a very weak constraint, and we'd probably want to have at least some more specific characterization of two things:First, how long is "eventually"? It would be useful to have a strict lower bound, or at least some idea of how long it typically takes for the system to converge to the same value.Second, how do the replicas agree on a value? A system that always returns "42" is eventually consistent: all replicas agree on the same value. It just doesn't converge to a useful value since it just keeps returning the same fixed value. Instead, we'd like to have a better idea of the method. For example, one way to decide is to have the value with the largest timestamp always win.So when vendors say "eventual consistency", what they mean is some more precise term, such as "eventually last-writer-wins, and read-the-latest-observed-value in the meantime" consistency. The "how?" matters, because a bad method can lead to writes being lost - for example, if the clock on one node is set incorrectly and timestamps are used.I will look into these two questions in more detail in the chapter on replication methods for weak consistency models.What is order and why is it important?What do you mean "what is order"?I mean, why are we so obsessed with order in the first place? Why do we care whether A happened before B? Why don't we care about some other property, like "color"?Well, my crazy friend, let's go back to the definition of distributed systems to answer that.As you may remember, I described distributed programming as the art of solving the same problem that you can solve on a single computer using multiple computers.This is, in fact, at the core of the obsession with order. Any system that can only do one thing at a time will create a total order of operations. Like people passing through a single door, every operation will have a well-defined predecessor and successor. That's basically the programming model that we've worked very hard to preserve.The traditional model is: a single program, one process, one memory space running on one CPU. The operating system abstracts away the fact that there might be multiple CPUs and multiple programs, and that the memory on the computer is actually shared among many programs. I'm not saying that threaded programming and event-oriented programming don't exist; it's just that they are special abstractions on top of the "one/one/one" model. Programs are written to be executed in an ordered fashion: you start from the top, and then go down towards the bottom.Order as a property has received so much attention because the easiest way to define "correctness" is to say "it works like it would on a single machine". And that usually means that a) we run the same operations and b) that we run them in the same order - even if there are multiple machines.The nice thing about distributed systems that preserve order (as defined for a single system) is that they are generic. You don't need to care about what the operations are, because they will be executed exactly like on a single machine. This is great because you know that you can use the same system no matter what the operations are.In reality, a distributed program runs on multiple nodes; with multiple CPUs and multiple streams of operations coming in. You can still assign a total order, but it requires either accurate clocks or some form of communication. You could timestamp each operation using a completely accurate clock then use that to figure out the total order. Or you might have some kind of communication system that makes it possible to assign sequential numbers as in a total order.The natural state in a distributed system is partial order. Neither the network nor independent nodes make any guarantees about relative order; but at each node, you can observe a local order.A total order is a binary relation that defines an order for every element in some set.Two distinct elements are  when one of them is greater than the other. In a partially ordered set, some pairs of elements are not comparable and hence a partial order doesn't specify the exact order of every item.Both total order and partial order are transitive and antisymmetric. The following statements hold in both a total order and a partial order for all a, b and c in X:If a â‰¤ b and b â‰¤ a then a = b (antisymmetry);
If a â‰¤ b and b â‰¤ c then a â‰¤ c (transitivity);However, a total order is total:a â‰¤ b or b â‰¤ a (totality) for all a, b in Xa â‰¤ a (reflexivity) for all a in XNote that totality implies reflexivity; so a partial order is a weaker variant of total order.
For some elements in a partial order, the totality property does not hold - in other words, some of the elements are not comparable.Git branches are an example of a partial order. As you probably know, the git revision control system allows you to create multiple branches from a single base branch - e.g. from a master branch. Each branch represents a history of source code changes derived based on a common ancestor:[ branch A (1,2,0)]  [ master (3,0,0) ]  [ branch B (1,0,2) ]
[ branch A (1,1,0)]  [ master (2,0,0) ]  [ branch B (1,0,1) ]
                  \  [ master (1,0,0) ]  /The branches A and B were derived from a common ancestor, but there is no definite order between them: they represent different histories and cannot be reduced to a single linear history without additional work (merging). You could, of course, put all the commits in some arbitrary order (say, sorting them first by ancestry and then breaking ties by sorting A before B or B before A) - but that would lose information by forcing a total order where none existed.In a system consisting of one node, a total order emerges by necessity: instructions are executed and messages are processed in a specific, observable order in a single program. We've come to rely on this total order - it makes executions of programs predictable. This order can be maintained on a distributed system, but at a cost: communication is expensive, and time synchronization is difficult and fragile.Time is a source of order - it allows us to define the order of operations - which coincidentally also has an interpretation that people can understand (a second, a minute, a day and so on).In some sense, time is just like any other integer counter. It just happens to be important enough that most computers have a dedicated time sensor, also known as a clock. It's so important that we've figured out how to synthesize an approximation of the same counter using some imperfect physical system (from wax candles to cesium atoms). By "synthesize", I mean that we can approximate the value of the integer counter in physically distant places via some physical property without communicating it directly.Timestamps really are a shorthand value for representing the state of the world from the start of the universe to the current moment - if something occurred at a particular timestamp, then it was potentially influenced by everything that happened before it. This idea can be generalized into a causal clock that explicitly tracks causes (dependencies) rather than simply assuming that everything that preceded a timestamp was relevant. Of course, the usual assumption is that we should only worry about the state of the specific system rather than the whole world.Assuming that time progresses at the same rate everywhere - and that is a big assumption which I'll return to in a moment - time and timestamps have several useful interpretations when used in a program. The three interpretations are:. When I say that time is a source of order, what I mean is that:we can attach timestamps to unordered events to order themwe can use timestamps to enforce a specific ordering of operations or the delivery of messages (for example, by delaying an operation if it arrives out of order)we can use the value of a timestamp to determine whether something happened chronologically before something else - time as a universally comparable value. The absolute value of a timestamp can be interpreted as a date, which is useful for people. Given a timestamp of when a downtime started from a log file, you can tell that it was last Saturday, when there was a thunderstorm. - durations measured in time have some relation to the real world. Algorithms generally don't care about the absolute value of a clock or its interpretation as a date, but they might use durations to make some judgment calls. In particular, the amount of time spent waiting can provide clues about whether a system is partitioned or merely experiencing high latency.By their nature, the components of distributed systems do not behave in a predictable manner. They do not guarantee any specific order, rate of advance, or lack of delay. Each node does have some local order - as execution is (roughly) sequential - but these local orders are independent of each other.Imposing (or assuming) order is one way to reduce the space of possible executions and possible occurrences. Humans have a hard time reasoning about things when things can happen in any order - there just are too many permutations to consider.Does time progress at the same rate everywhere?We all have an intuitive concept of time based on our own experience as individuals. Unfortunately, that intuitive notion of time makes it easier to picture total order rather than partial order. It's easier to picture a sequence in which things happen one after another, rather than concurrently. It is easier to reason about a single order of messages than to reason about messages arriving in different orders and with different delays.However, when implementing distributing systems we want to avoid making strong assumptions about time and order, because the stronger the assumptions, the more fragile a system is to issues with the "time sensor" - or the onboard clock. Furthermore, imposing an order carries a cost. The more temporal nondeterminism that we can tolerate, the more we can take advantage of distributed computation.There are three common answers to the question "does time progress at the same rate everywhere?". These are:These correspond roughly to the three timing assumptions that I mentioned in the second chapter: the synchronous system model has a global clock, the partially synchronous model has a local clock, and in the asynchronous system model one cannot use clocks at all. Let's look at these in more detail.Time with a "global-clock" assumptionThe global clock assumption is that there is a global clock of perfect accuracy, and that everyone has access to that clock. This is the way we tend to think about time, because in human interactions small differences in time don't really matter.The global clock is basically a source of total order (exact order of every operation on all nodes even if those nodes have never communicated).However, this is an idealized view of the world: in reality, clock synchronization is only possible to a limited degree of accuracy. This is limited by the lack of accuracy of clocks in commodity computers, by latency if a clock synchronization protocol such as NTP is used and fundamentally by the nature of spacetime.Assuming that clocks on distributed nodes are perfectly synchronized means assuming that clocks start at the same value and never drift apart. It's a nice assumption because you can use timestamps freely to determine a global total order - bound by clock drift rather than latency - but this is a nontrivial operational challenge and a potential source of anomalies. There are many different scenarios where a simple failure - such as a user accidentally changing the local time on a machine, or an out-of-date machine joining a cluster, or synchronized clocks drifting at slightly different rates and so on that can cause hard-to-trace anomalies.Nevertheless, there are some real-world systems that make this assumption. Facebook's Cassandra is an example of a system that assumes clocks are synchronized. It uses timestamps to resolve conflicts between writes - the write with the newer timestamp wins. This means that if clocks drift, new data may be ignored or overwritten by old data; again, this is an operational challenge (and from what I've heard, one that people are acutely aware of). Another interesting example is Google's Spanner: the paper describes their TrueTime API, which synchronizes time but also estimates worst-case clock drift.Time with a "Local-clock" assumptionThe second, and perhaps more plausible assumption is that each machine has its own clock, but there is no global clock. It means that you cannot use the local clock in order to determine whether a remote timestamp occurred before or after a local timestamp; in other words, you cannot meaningfully compare timestamps from two different machines.The local clock assumption corresponds more closely to the real world. It assigns a partial order: events on each system are ordered but events cannot be ordered across systems by only using a clock.However, you can use timestamps to order events on a single machine; and you can use timeouts on a single machine as long as you are careful not to allow the clock to jump around. Of course, on a machine controlled by an end-user this is probably assuming too much: for example, a user might accidentally change their date to a different value while looking up a date using the operating system's date control.Time with a "No-clock" assumptionFinally, there is the notion of logical time. Here, we don't use clocks at all and instead track causality in some other way. Remember, a timestamp is simply a shorthand for the state of the world up to that point - so we can use counters and communication to determine whether something happened before, after or concurrently with something else.This way, we can determine the order of events between different machines, but cannot say anything about intervals and cannot use timeouts (since we assume that there is no "time sensor"). This is a partial order: events can be ordered on a single system using a counter and no communication, but ordering events across systems requires a message exchange.One of the most cited papers in distributed systems is Lamport's paper on time, clocks and the ordering of events. Vector clocks, a generalization of that concept (which I will cover in more detail), are a way to track causality without using clocks. Cassandra's cousins Riak (Basho) and Voldemort (Linkedin) use vector clocks rather than assuming that nodes have access to a global clock of perfect accuracy. This allows those systems to avoid the clock accuracy issues mentioned earlier.When clocks are not used, the maximum precision at which events can be ordered across distant machines is bound by communication latency.How is time used in a distributed system?What is the benefit of time?Time can define order across a system (without communication)Time can define boundary conditions for algorithmsThe order of events is important in distributed systems, because many properties of distributed systems are defined in terms of the order of operations/events:where correctness depends on (agreement on) correct event ordering, for example serializability in a distributed databaseorder can be used as a tie breaker when resource contention occurs, for example if there are two orders for a widget, fulfill the first and cancel the second oneA global clock would allow operations on two different machines to be ordered without the two machines communicating directly. Without a global clock, we need to communicate in order to determine order.Time can also be used to define boundary conditions for algorithms - specifically, to distinguish between "high latency" and "server or network link is down". This is a very important use case; in most real-world systems timeouts are used to determine whether a remote machine has failed, or whether it is simply experiencing high network latency. Algorithms that make this determination are called failure detectors; and I will discuss them fairly soon.Vector clocks (time for causal order)Earlier, we discussed the different assumptions about the rate of progress of time across a distributed system. Assuming that we cannot achieve accurate clock synchronization - or starting with the goal that our system should not be sensitive to issues with time synchronization, how can we order things?Lamport clocks and vector clocks are replacements for physical clocks which rely on counters and communication to determine the order of events across a distributed system. These clocks provide a counter that is comparable across different nodes. is simple. Each process maintains a counter using the following rules:Whenever a process does work, increment the counterWhenever a process sends a message, include the counterWhen a message is received, set the counter to max(local_counter, received_counter) + 1function LamportClock() {
  this.value = 1;
}

LamportClock.prototype.get = function() {
  return this.value;
}

LamportClock.prototype.increment = function() {
  this.value++;
}

LamportClock.prototype.merge = function(other) {
  this.value = Math.max(this.value, other.value) + 1;
}A Lamport clock allows counters to be compared across systems, with a caveat: Lamport clocks define a partial order. If timestamp(a) < timestamp(b): may have happened before  or may be incomparable with This is known as clock consistency condition: if one event comes before another, then that event's logical clock comes before the others. If  and  are from the same causal history, e.g. either both timestamp values were produced on the same process; or  is a response to the message sent in  then we know that  happened before .Intuitively, this is because a Lamport clock can only carry information about one timeline / history; hence, comparing Lamport timestamps from systems that never communicate with each other may cause concurrent events to appear to be ordered when they are not.Imagine a system that after an initial period divides into two independent subsystems which never communicate with each other.For all events in each independent system, if a happened before b, then ; but if you take two events from the different independent systems (e.g. events that are not causally related) then you cannot say anything meaningful about their relative order.  While each part of the system has assigned timestamps to events, those timestamps have no relation to each other. Two events may appear to be ordered even though they are unrelated.However - and this is still a useful property - from the perspective of a single machine, any message sent with  will receive a response with  which is . is an extension of Lamport clock, which maintains an array  of N logical clocks - one per each node. Rather than incrementing a common counter, each node increments its own logical clock in the vector by one on each internal event. Hence the update rules are:Whenever a process does work, increment the logical clock value of the node in the vectorWhenever a process sends a message, include the full vector of logical clocksWhen a message is received:update each element in the vector to be increment the logical clock value representing the current node in the vectorAgain, expressed as code:function VectorClock(value) {
  // expressed as a hash keyed by node id: e.g. { node1: 1, node2: 3 }
  this.value = value || {};
}

VectorClock.prototype.get = function() {
  return this.value;
};

VectorClock.prototype.increment = function(nodeId) {
  if(typeof this.value[nodeId] == 'undefined') {
    this.value[nodeId] = 1;
  } else {
    this.value[nodeId]++;
  }
};

VectorClock.prototype.merge = function(other) {
  var result = {}, last,
      a = this.value,
      b = other.value;
  // This filters out duplicate keys in the hash
  (Object.keys(a)
    .concat(b))
    .sort()
    .filter(function(key) {
      var isDuplicate = (key == last);
      last = key;
      return !isDuplicate;
    }).forEach(function(key) {
      result[key] = Math.max(a[key] || 0, b[key] || 0);
    });
  this.value = result;
};This illustration (source) shows a vector clock:Each of the three nodes (A, B, C) keeps track of the vector clock. As events occur, they are timestamped with the current value of the vector clock. Examining a vector clock such as  lets us accurately identify the messages that (potentially) influenced that event.The issue with vector clocks is mainly that they require one entry per node, which means that they can potentially become very large for large systems. A variety of techniques have been applied to reduce the size of vector clocks (either by performing periodic garbage collection, or by reducing accuracy by limiting the size).We've looked at how order and causality can be tracked without physical clocks. Now, let's look at how time durations can be used for cutoff.Failure detectors (time for cutoff)As I stated earlier, the amount of time spent waiting can provide clues about whether a system is partitioned or merely experiencing high latency. In this case, we don't need to assume a global clock of perfect accuracy - it is simply enough that there is a reliable-enough local clock.Given a program running on one node, how can it tell that a remote node has failed? In the absence of accurate information, we can infer that an unresponsive remote node has failed after some reasonable amount of time has passed.But what is a "reasonable amount"? This depends on the latency between the local and remote nodes. Rather than explicitly specifying algorithms with specific values (which would inevitably be wrong in some cases), it would be nicer to deal with a suitable abstraction.A failure detector is a way to abstract away the exact timing assumptions. Failure detectors are implemented using heartbeat messages and timers. Processes exchange heartbeat messages. If a message response is not received before the timeout occurs, then the process suspects the other process.A failure detector based on a timeout will carry the risk of being either overly aggressive (declaring a node to have failed) or being overly conservative (taking a long time to detect a crash). How accurate do failure detectors need to be for them to be usable?Chandra et al. (1996) discuss failure detectors in the context of solving consensus - a problem that is particularly relevant since it underlies most replication problems where the replicas need to agree in environments with latency and network partitions.They characterize failure detectors using two properties, completeness and accuracy:Every crashed process is eventually suspected by every correct process.Every crashed process is eventually suspected by some correct process.No correct process is suspected ever.Some correct process is never suspected.Completeness is easier to achieve than accuracy; indeed, all failure detectors of importance achieve it - all you need to do is not to wait forever to suspect someone. Chandra et al. note that a failure detector with weak completeness can be transformed to one with strong completeness (by broadcasting information about suspected processes), allowing us to concentrate on the spectrum of accuracy properties.Avoiding incorrectly suspecting non-faulty processes is hard unless you are able to assume that there is a hard maximum on the message delay. That assumption can be made in a synchronous system model - and hence failure detectors can be strongly accurate in such a system. Under system models that do not impose hard bounds on message delay, failure detection can at best be eventually accurate.Chandra et al. show that even a very weak failure detector - the eventually weak failure detector â‹„W (eventually weak accuracy + weak completeness) - can be used to solve the consensus problem. The diagram below (from the paper) illustrates the relationship between system models and problem solvability:As you can see above, certain problems are not solvable without a failure detector in asynchronous systems. This is because without a failure detector (or strong assumptions about time bounds e.g. the synchronous system model), it is not possible to tell whether a remote node has crashed, or is simply experiencing high latency. That distinction is important for any system that aims for single-copy consistency: failed nodes can be ignored because they cannot cause divergence, but partitioned nodes cannot be safely ignored.How can one implement a failure detector? Conceptually, there isn't much to a simple failure detector, which simply detects failure when a timeout expires. The most interesting part relates to how the judgments are made about whether a remote node has failed.Ideally, we'd prefer the failure detector to be able to adjust to changing network conditions and to avoid hardcoding timeout values into it. For example, Cassandra uses an accrual failure detector, which is a failure detector that outputs a suspicion level (a value between 0 and 1) rather than a binary "up" or "down" judgment. This allows the application using the failure detector to make its own decisions about the tradeoff between accurate detection and early detection.Earlier, I alluded to having to pay the cost for order. What did I mean?If you're writing a distributed system, you presumably own more than one computer. The natural (and realistic) view of the world is a partial order, not a total order. You can transform a partial order into a total order, but this requires communication, waiting and imposes restrictions that limit how many computers can do work at any particular point in time.All clocks are mere approximations bound by either network latency (logical time) or by physics. Even keeping a simple integer counter in sync across multiple nodes is a challenge.While time and order are often discussed together, time itself is not such a useful property. Algorithms don't really care about time as much as they care about more abstract properties:the causal ordering of eventsfailure detection (e.g. approximations of upper bounds on message delivery)consistent snapshots (e.g. the ability to examine the state of a system at some point in time; not discussed here)Imposing a total order is possible, but expensive. It requires you to proceed at the common (lowest) speed. Often the easiest way to ensure that events are delivered in some defined order is to nominate a single (bottleneck) node through which all operations are passed.Is time / order / synchronicity really necessary? It depends. In some use cases, we want each intermediate operation to move the system from one consistent state to another. For example, in many cases we want the responses from a database to represent all of the available information, and we want to avoid dealing with the issues that might occur if the system could return an inconsistent result.But in other cases, we might not need that much time / order / synchronization. For example, if you are running a long running computation, and don't really care about what the system does until the very end - then you don't really need much synchronization as long as you can guarantee that the answer is correct.Synchronization is often applied as a blunt tool across all operations, when only a subset of cases actually matter for the final outcome. When is order needed to guarantee correctness? The CALM theorem - which I will discuss in the last chapter - provides one answer.In other cases, it is acceptable to give an answer that only represents the best known estimate - that is, is based on only a subset of the total information contained in the system. In particular, during a network partition one may need to answer queries with only a part of the system being accessible. In other use cases, the end user cannot really distinguish between a relatively recent answer that can be obtained cheaply and one that is guaranteed to be correct and is expensive to calculate. For example, is the Twitter follower count for some user X, or X+1? Or are movies A, B and C the absolutely best answers for some query? Doing a cheaper, mostly correct "best effort" can be acceptable.In the next two chapters we'll examine replication for fault-tolerant strongly consistent systems - systems which provide strong guarantees while being increasingly resilient to failures. These systems provide solutions for the first case: when you need to guarantee correctness and are willing to pay for it. Then, we'll discuss systems with weak consistency guarantees, which can remain available in the face of partitions, but that can only give you a "best effort" answer.Lamport clocks, vector clocksThe replication problem is one of many problems in distributed systems. I've chosen to focus on it over other problems such as leader election, failure detection, mutual exclusion, consensus and global snapshots because it is often the part that people are most interested in. One way in which parallel databases are differentiated is in terms of their replication features, for example. Furthermore, replication provides a context for many subproblems, such as leader election, failure detection, consensus and atomic broadcast.Replication is a group communication problem. What arrangement and communication pattern gives us the performance and availability characteristics we desire? How can we ensure fault tolerance, durability and non-divergence in the face of network partitions and simultaneous node failure?Again, there are many ways to approach replication. The approach I'll take here just looks at high level patterns that are possible for a system with replication. Looking at this visually helps keep the discussion focused on the overall pattern rather than the specific messaging involved. My goal here is to explore the design space rather than to explain the specifics of each algorithm.Let's first define what replication looks like. We assume that we have some initial database, and that clients make requests which change the state of the database.The arrangement and communication pattern can then be divided into several stages:(Request) The client sends a request to a server(Sync) The synchronous portion of the replication takes place(Response) A response is returned to the client(Async) The asynchronous portion of the replication takes placeThis model is loosely based on this article. Note that the pattern of messages exchanged in each portion of the task depends on the specific algorithm: I am intentionally trying to get by without discussing the specific algorithm.Given these stages, what kind of communication patterns can we create? And what are the performance and availability implications of the patterns we choose?The first pattern is synchronous replication (also known as active, or eager, or push, or pessimistic replication). Let's draw what that looks like:Here, we can see three distinct stages: first, the client sends the request. Next, what we called the synchronous portion of replication takes place. The term refers to the fact that the client is blocked - waiting for a reply from the system.During the synchronous phase, the first server contacts the two other servers and waits until it has received replies from all the other servers. Finally, it sends a response to the client informing it of the result (e.g. success or failure).All this seems straightforward. What can we say of this specific arrangement of communication patterns, without discussing the details of the algorithm during the synchronous phase? First, observe that this is a write N - of - N approach: before a response is returned, it has to be seen and acknowledged by every server in the system.From a performance perspective, this means that the system will be as fast as the slowest server in it. The system will also be very sensitive to changes in network latency, since it requires every server to reply before proceeding.Given the N-of-N approach, the system cannot tolerate the loss of any servers. When a server is lost, the system can no longer write to all the nodes, and so it cannot proceed. It might be able to provide read-only access to the data, but modifications are not allowed after a node has failed in this design.This arrangement can provide very strong durability guarantees: the client can be certain that all N servers have received, stored and acknowledged the request when the response is returned. In order to lose an accepted update, all N copies would need to be lost, which is about as good a guarantee as you can make.Let's contrast this with the second pattern - asynchronous replication (a.k.a. passive replication, or pull replication, or lazy replication). As you may have guessed, this is the opposite of synchronous replication:Here, the master (/leader / coordinator) immediately sends back a response to the client. It might at best store the update locally, but it will not do any significant work synchronously and the client is not forced to wait for more rounds of communication to occur between the servers.At some later stage, the asynchronous portion of the replication task takes place. Here, the master contacts the other servers using some communication pattern, and the other servers update their copies of the data. The specifics depend on the algorithm in use.What can we say of this specific arrangement without getting into the details of the algorithm? Well, this is a write 1 - of - N approach: a response is returned immediately and update propagation occurs sometime later.From a performance perspective, this means that the system is fast: the client does not need to spend any additional time waiting for the internals of the system to do their work. The system is also more tolerant of network latency, since fluctuations in internal latency do not cause additional waiting on the client side.This arrangement can only provide weak, or probabilistic durability guarantees. If nothing goes wrong, the data is eventually replicated to all N machines. However, if the only server containing the data is lost before this can take place, the data is permanently lost.Given the 1-of-N approach, the system can remain available as long as at least one node is up (at least in theory, though in practice the load will probably be too high). A purely lazy approach like this provides no durability or consistency guarantees; you may be allowed to write to the system, but there are no guarantees that you can read back what you wrote if any faults occur.Finally, it's worth noting that passive replication cannot ensure that all nodes in the system always contain the same state. If you accept writes at multiple locations and do not require that those nodes synchronously agree, then you will run the risk of divergence: reads may return different results from different locations (particularly after nodes fail and recover), and global constraints (which require communicating with everyone) cannot be enforced.I haven't really mentioned the communication patterns during a read (rather than a write), because the pattern of reads really follows from the pattern of writes: during a read, you want to contact as few nodes as possible. We'll discuss this a bit more in the context of quorums.We've only discussed two basic arrangements and none of the specific algorithms. Yet we've been able to figure out quite a bit of about the possible communication patterns as well as their performance, durability guarantees and availability characteristics.An overview of major replication approachesHaving discussed the two basic replication approaches: synchronous and asynchronous replication, let's have a look at the major replication algorithms.There are many, many different ways to categorize replication techniques. The second distinction (after sync vs. async) I'd like to introduce is between:Replication methods that prevent divergence (single copy systems) andReplication methods that risk divergence (multi-master systems)The first group of methods has the property that they "behave like a single system". In particular, when partial failures occur, the system ensures that only a single copy of the system is active. Furthermore, the system ensures that the replicas are always in agreement. This is known as the consensus problem.Several processes (or computers) achieve consensus if they all agree on some value. More formally:Agreement: Every correct process must agree on the same value.Integrity: Every correct process decides at most one value, and if it decides some value, then it must have been proposed by some process.Termination: All processes eventually reach a decision.Validity: If all correct processes propose the same value V, then all correct processes decide V.Mutual exclusion, leader election, multicast and atomic broadcast are all instances of the more general problem of consensus. Replicated systems that maintain single copy consistency need to solve the consensus problem in some way.The replication algorithms that maintain single-copy consistency include:1n messages (asynchronous primary/backup)2n messages (synchronous primary/backup)4n messages (2-phase commit, Multi-Paxos)6n messages (3-phase commit, Paxos with repeated leader election)These algorithms vary in their fault tolerance (e.g. the types of faults they can tolerate). I've classified these simply by the number of messages exchanged during an execution of the algorithm, because I think it is interesting to try to find an answer to the question "what are we buying with the added message exchanges?"The diagram below, adapted from Ryan Barret at Google, describes some of the aspects of the different options:The consistency, latency, throughput, data loss and failover characteristics in the diagram above can really be traced back to the two different replication methods: synchronous replication (e.g. waiting before responding) and asynchronous replication. When you wait, you get worse performance but stronger guarantees. The throughput difference between 2PC and quorum systems will become apparent when we discuss partition (and latency) tolerance.In that diagram, algorithms enforcing weak (/eventual) consistency are lumped up into one category ("gossip"). However, I will discuss replication methods for weak consistency - gossip and (partial) quorum systems - in more detail. The "transactions" row really refers more to global predicate evaluation, which is not supported in systems with weak consistency (though local predicate evaluation can be supported).It is worth noting that systems enforcing weak consistency requirements have fewer generic algorithms, and more techniques that can be selectively applied. Since systems that do not enforce single-copy consistency are free to act like distributed systems consisting of multiple nodes, there are fewer obvious objectives to fix and the focus is more on giving people a way to reason about the characteristics of the system that they have.Client-centric consistency models attempt to provide more intelligible consistency guarantees while allowing for divergence.CRDTs (convergent and commutative replicated datatypes) exploit semilattice properties (associativity, commutativity, idempotency) of certain state and operation-based data types.Confluence analysis (as in the Bloom language) uses information regarding the monotonicity of computations to maximally exploit disorder.PBS (probabilistically bounded staleness) uses simulation and information collected from a real world system to characterize the expected behavior of partial quorum systems.I'll talk about all of these a bit  further on, first; let's look at the replication algorithms that maintain single-copy consistency.Primary/backup replicationPrimary/backup replication (also known as primary copy replication master-slave replication or log shipping) is perhaps the most commonly used replication method, and the most basic algorithm. All updated are performed on the primary, and a log of operations (or alternatively, changes) is shipped across the network to the backup replicas. There are two variants:asynchronous primary/backup replication andsynchronous primary/backup replicationThe synchronous version requires two messages ("update" + "acknowledge receipt") while the asynchronous version could run with just one ("update").P/B is very common. For example, by default MySQL replication uses the asynchronous variant. MongoDB also uses P/B (with some additional procedures for failover). All operations are performed on one master server, which serializes them to a local log, which is then replicated asynchronously to the backup servers.As we discussed earlier in the context of asynchronous replication, any asynchronous replication algorithm can only provide weak durability guarantees. In MySQL replication this manifests as replication lag: the asynchronous backups are always at least one operation behind the primary. If the primary fails, then the updates that have not yet been sent to the backups are lost.The synchronous variant of primary/backup replication ensures that writes have been stored on other nodes before returning back to the client - at the cost of waiting for responses from other replicas. However, it is worth noting that even this variant can only offer weak guarantees. Consider the following simple failure scenario:the primary receives a write and sends it to the backupthe backup persists and ACKs the writeand then primary fails before sending ACK to the clientThe client now assumes that the commit failed, but the backup committed it; if the backup is promoted to primary, it will be incorrect. Manual cleanup may be needed to reconcile the failed primary or divergent backups.I am simplifying here of course. While all primary/backup replication algorithms follow the same general messaging pattern, they differ in their handling of failover, replicas being offline for extended periods and so on. However, it is not possible to be resilient to inopportune failures of the primary in this scheme.What is key in the log-shipping / primary/backup based schemes is that they can only offer a best-effort guarantee (e.g. they are susceptible to lost updates or incorrect updates if nodes fail at inopportune times). Furthermore, P/B schemes are susceptible to split-brain, where the failover to a backup kicks in due to a temporary network issue and causes both the primary and backup to be active at the same time.To prevent inopportune failures from causing consistency guarantees to be violated; we need to add another round of messaging, which gets us the two phase commit protocol (2PC).Two phase commit (2PC) is a protocol used in many classic relational databases. For example, MySQL Cluster (not to be confused with the regular MySQL) provides synchronous replication using 2PC. The diagram below illustrates the message flow:[ Coordinator ] -> OK to commit?     [ Peers ]
                <- Yes / No

[ Coordinator ] -> Commit / Rollback [ Peers ]
                <- ACKIn the first phase (voting), the coordinator sends the update to all the participants. Each participant processes the update and votes whether to commit or abort. When voting to commit, the participants store the update onto a temporary area (the write-ahead log). Until the second phase completes, the update is considered temporary.In the second phase (decision), the coordinator decides the outcome and informs every participant about it. If all participants voted to commit, then the update is taken from the temporary area and made permanent.Having a second phase in place before the commit is considered permanent is useful, because it allows the system to roll back an update when a node fails. In contrast, in primary/backup ("1PC"), there is no step for rolling back an operation that has failed on some nodes and succeeded on others, and hence the replicas could diverge.2PC is prone to blocking, since a single node failure (participant or coordinator) blocks progress until the node has recovered. Recovery is often possible thanks to the second phase, during which other nodes are informed about the system state. Note that 2PC assumes that the data in stable storage at each node is never lost and that no node crashes forever. Data loss is still possible if the data in the stable storage is corrupted in a crash.The details of the recovery procedures during node failures are quite complicated so I won't get into the specifics. The major tasks are ensuring that writes to disk are durable (e.g. flushed to disk rather than cached) and making sure that the right recovery decisions are made (e.g. learning the outcome of the round and then redoing or undoing an update locally).As we learned in the chapter regarding CAP, 2PC is a CA - it is not partition tolerant. The failure model that 2PC addresses does not include network partitions; the prescribed way to recover from a node failure is to wait until the network partition heals. There is no safe way to promote a new coordinator if one fails; rather a manual intervention is required. 2PC is also fairly latency-sensitive, since it is a write N-of-N approach in which writes cannot proceed until the slowest node acknowledges them.2PC strikes a decent balance between performance and fault tolerance, which is why it has been popular in relational databases. However, newer systems often use a partition tolerant consensus algorithm, since such an algorithm can provide automatic recovery from temporary network partitions as well as more graceful handling of increased between-node latency.Let's look at partition tolerant consensus algorithms next.Partition tolerant consensus algorithmsPartition tolerant consensus algorithms are as far as we're going to go in terms of fault-tolerant algorithms that maintain single-copy consistency. There is a further class of fault tolerant algorithms: algorithms that tolerate arbitrary (Byzantine) faults; these include nodes that fail by acting maliciously. Such algorithms are rarely used in commercial systems, because they are more expensive to run and more complicated to implement - and hence I will leave them out.When it comes to partition tolerant consensus algorithms, the most well-known algorithm is the Paxos algorithm. It is, however, notoriously difficult to implement and explain, so I will focus on Raft, a recent (~early 2013) algorithm designed to be easier to teach and implement. Let's first take a look at network partitions and the general characteristics of partition tolerant consensus algorithms.What is a network partition?A network partition is the failure of a network link to one or several nodes. The nodes themselves continue to stay active, and they may even be able to receive requests from clients on their side of the network partition. As we learned earlier - during the discussion of the CAP theorem - network partitions do occur and not all systems handle them gracefully.Network partitions are tricky because during a network partition, it is not possible to distinguish between a failed remote node and the node being unreachable. If a network partition occurs but no nodes fail, then the system is divided into two partitions which are simultaneously active. The two diagrams below illustrate how a network partition can look similar to a node failure.A system of 2 nodes, with a failure vs. a network partition:A system of 3 nodes, with a failure vs. a network partition:A system that enforces single-copy consistency must have some method to break symmetry: otherwise, it will split into two separate systems, which can diverge from each other and can no longer maintain the illusion of a single copy.Network partition tolerance for systems that enforce single-copy consistency requires that during a network partition, only one partition of the system remains active since during a network partition it is not possible to prevent divergence (e.g. CAP theorem).This is why partition tolerant consensus algorithms rely on a majority vote. Requiring a majority of nodes - rather than all of the nodes (as in 2PC) - to agree on updates allows a minority of the nodes to be down, or slow, or unreachable due to a network partition. As long as  nodes are up and accessible, the system can continue to operate.Partition tolerant consensus algorithms use an odd number of nodes (e.g. 3, 5 or 7). With just two nodes, it is not possible to have a clear majority after a failure. For example, if the number of nodes is three, then the system is resilient to one node failure; with five nodes the system is resilient to two node failures.When a network partition occurs, the partitions behave asymmetrically. One partition will contain the majority of the nodes. Minority partitions will stop processing operations to prevent divergence during a network partition, but the majority partition can remain active. This ensures that only a single copy of the system state remains active.Majorities are also useful because they can tolerate disagreement: if there is a perturbation or failure, the nodes may vote differently. However, since there can be only one majority decision, a temporary disagreement can at most block the protocol from proceeding (giving up liveness) but it cannot violate the single-copy consistency criterion (safety property).There are two ways one might structure a system: all nodes may have the same responsibilities, or nodes may have separate, distinct roles.Consensus algorithms for replication generally opt for having distinct roles for each node. Having a single fixed leader or master server is an optimization that makes the system more efficient, since we know that all updates must pass through that server. Nodes that are not the leader just need to forward their requests to the leader.Note that having distinct roles does not preclude the system from recovering from the failure of the leader (or any other role). Just because roles are fixed during normal operation doesn't mean that one cannot recover from failure by reassigning the roles after a failure (e.g. via a leader election phase). Nodes can reuse the result of a leader election until node failures and/or network partitions occur.Both Paxos and Raft make use of distinct node roles. In particular, they have a leader node ("proposer" in Paxos) that is responsible for coordination during normal operation. During normal operation, the rest of the nodes are followers ("acceptors" or "voters" in Paxos).Each period of normal operation in both Paxos and Raft is called an epoch ("term" in Raft). During each epoch only one node is the designated leader (a similar system is used in Japan where era names change upon imperial succession).After a successful election, the same leader coordinates until the end of the epoch. As shown in the diagram above (from the Raft paper), some elections may fail, causing the epoch to end immediately.Epochs act as a logical clock, allowing other nodes to identify when an outdated node starts communicating - nodes that were partitioned or out of operation will have a smaller epoch number than the current one, and their commands are ignored.During normal operation, a partition-tolerant consensus algorithm is rather simple. As we've seen earlier, if we didn't care about fault tolerance, we could just use 2PC. Most of the complexity really arises from ensuring that once a consensus decision has been made, it will not be lost and the protocol can handle leader changes as a result of a network or node failure.All nodes start as followers; one node is elected to be a leader at the start. During normal operation, the leader maintains a heartbeat which allows the followers to detect if the leader fails or becomes partitioned.When a node detects that a leader has become non-responsive (or, in the initial case, that no leader exists), it switches to an intermediate state (called "candidate" in Raft) where it increments the term/epoch value by one, initiates a leader election and competes to become the new leader.In order to be elected a leader, a node must receive a majority of the votes. One way to assign votes is to simply assign them on a first-come-first-served basis; this way, a leader will eventually be elected. Adding a random amount of waiting time between attempts at getting elected will reduce the number of nodes that are simultaneously attempting to get elected.Numbered proposals within an epochDuring each epoch, the leader proposes one value at a time to be voted upon. Within each epoch, each proposal is numbered with a unique strictly increasing number. The followers (voters / acceptors) accept the first proposal they receive for a particular proposal number.During normal operation, all proposals go through the leader node. When a client submits a proposal (e.g. an update operation), the leader contacts all nodes in the quorum. If no competing proposals exist (based on the responses from the followers), the leader proposes the value. If a majority of the followers accept the value, then the value is considered to be accepted.Since it is possible that another node is also attempting to act as a leader, we need to ensure that once a single proposal has been accepted, its value can never change. Otherwise a proposal that has already been accepted might for example be reverted by a competing leader. Lamport states this as:P2: If a proposal with value  is chosen, then every higher-numbered proposal that is chosen has value .Ensuring that this property holds requires that both followers and proposers are constrained by the algorithm from ever changing a value that has been accepted by a majority. Note that "the value can never change" refers to the value of a single execution (or run / instance / decision) of the protocol. A typical replication algorithm will run multiple executions of the algorithm, but most discussions of the algorithm focus on a single run to keep things simple. We want to prevent the decision history from being altered or overwritten.In order to enforce this property, the proposers must first ask the followers for their (highest numbered) accepted proposal and value. If the proposer finds out that a proposal already exists, then it must simply complete this execution of the protocol, rather than making its own proposal. Lamport states this as:P2b. If a proposal with value  is chosen, then every higher-numbered proposal issued by any proposer has value .P2c. For any  and , if a proposal with value  and number  is issued [by a leader], then there is a set  consisting of a majority of acceptors [followers] such that either (a) no acceptor in  has accepted any proposal numbered less than , or (b)  is the value of the highest-numbered proposal among all proposals numbered less than  accepted by the followers in .This is the core of the Paxos algorithm, as well as algorithms derived from it. The value to be proposed is not chosen until the second phase of the protocol. Proposers must sometimes simply retransmit a previously made decision to ensure safety (e.g. clause b in P2c) until they reach a point where they know that they are free to impose their own proposal value (e.g. clause a).If multiple previous proposals exist, then the highest-numbered proposal value is proposed. Proposers may only attempt to impose their own value if there are no competing proposals at all.To ensure that no competing proposals emerge between the time the proposer asks each acceptor about its most recent value, the proposer asks the followers not to accept proposals with lower proposal numbers than the current one.Putting the pieces together, reaching a decision using Paxos requires two rounds of communication:[ Proposer ] -> Prepare(n)                                [ Followers ]
             <- Promise(n; previous proposal number
                and previous value if accepted a
                proposal in the past)

[ Proposer ] -> AcceptRequest(n, own value or the value   [ Followers ]
                associated with the highest proposal number
                reported by the followers)
                <- Accepted(n, value)The prepare stage allows the proposer to learn of any competing or previous proposals. The second phase is where either a new value or a previously accepted value is proposed. In some cases - such as if two proposers are active at the same time (dueling); if messages are lost; or if a majority of the nodes have failed - then no proposal is accepted by a majority. But this is acceptable, since the decision rule for what value to propose converges towards a single value (the one with the highest proposal number in the previous attempt).Indeed, according to the FLP impossibility result, this is the best we can do: algorithms that solve the consensus problem must either give up safety or liveness when the guarantees regarding bounds on message delivery do not hold. Paxos gives up liveness: it may have to delay decisions indefinitely until a point in time where there are no competing leaders, and a majority of nodes accept a proposal. This is preferable to violating the safety guarantees.Of course, implementing this algorithm is much harder than it sounds. There are many small concerns which add up to a fairly significant amount of code even in the hands of experts. These are issues such as:practical optimizations:avoiding repeated leader election via leadership leases (rather than heartbeats)avoiding repeated propose messages when in a stable state where the leader identity does not changeensuring that followers and proposers do not lose items in stable storage and that results stored in stable storage are not subtly corrupted (e.g. disk corruption)enabling cluster membership to change in a safe manner (e.g. base Paxos depends on the fact that majorities always intersect in one node, which does not hold if the membership can change arbitrarily)procedures for bringing a new replica up to date in a safe and efficient manner after a crash, disk loss or when a new node is provisionedprocedures for snapshotting and garbage collecting the data required to guarantee safety after some reasonable period (e.g. balancing storage requirements and fault tolerance requirements)Partition-tolerant consensus algorithms: Paxos, Raft, ZABHopefully, this has given you a sense of how a partition-tolerant consensus algorithm works. I encourage you to read one of the papers in the further reading section to get a grasp of the specifics of the different algorithms.. Paxos is one of the most important algorithms when writing strongly consistent partition tolerant replicated systems. It is used in many of Google's systems, including the Chubby lock manager used by BigTable/Megastore, the Google File System as well as Spanner.Paxos is named after the Greek island of Paxos, and was originally presented by Leslie Lamport in a paper called "The Part-Time Parliament" in 1998. It is often considered to be difficult to implement, and there have been a series of papers from companies with considerable distributed systems expertise explaining further practical details (see the further reading). You might want to read Lamport's commentary on this issue here and here.The issues mostly relate to the fact that Paxos is described in terms of a single round of consensus decision making, but an actual working implementation usually wants to run multiple rounds of consensus efficiently. This has led to the development of many extensions on the core protocol that anyone interested in building a Paxos-based system still needs to digest. Furthermore, there are additional practical challenges such as how to facilitate cluster membership change.. ZAB - the Zookeeper Atomic Broadcast protocol is used in Apache Zookeeper. Zookeeper is a system which provides coordination primitives for distributed systems, and is used by many Hadoop-centric distributed systems for coordination (e.g. HBase, Storm, Kafka). Zookeeper is basically the open source community's version of Chubby. Technically speaking atomic broadcast is a problem different from pure consensus, but it still falls under the category of partition tolerant algorithms that ensure strong consistency.. Raft is a recent (2013) addition to this family of algorithms. It is designed to be easier to teach than Paxos, while providing the same guarantees. In particular, the different parts of the algorithm are more clearly separated and the paper also describes a mechanism for cluster membership change. It has recently seen adoption in etcd inspired by ZooKeeper.Replication methods with strong consistencyIn this chapter, we took a look at replication methods that enforce strong consistency. Starting with a contrast between synchronous work and asynchronous work, we worked our way up to algorithms that are tolerant of increasingly complex failures. Here are some of the key characteristics of each of the algorithms:Replicated log, slaves are not involved in executing operationsNo bounds on replication delayManual/ad-hoc failover, not fault tolerant, "hot backup"Unanimous vote: commit or abort2PC cannot survive simultaneous failure of the coordinator and a node during a commitNot partition tolerant, tail latency sensitiveRobust to n/2-1 simultaneous failures as part of protocolLess sensitive to tail latencyNow that we've taken a look at protocols that can enforce single-copy consistency under an increasingly realistic set of supported failure cases, let's turn our attention at the world of options that opens up once we let go of the requirement of single-copy consistency.By and large, it is hard to come up with a single dimension that defines or characterizes the protocols that allow for replicas to diverge. Most such protocols are highly available, and the key issue is more whether or not the end users find the guarantees, abstractions and APIs useful for their purpose in spite of the fact that the replicas may diverge when node and/or network failures occur.Why haven't weakly consistent systems been more popular?As I stated in the introduction, I think that much of distributed programming is about dealing with the implications of two consequences of distribution:that information travels at the speed of lightthat independent things fail independentlyThe implication that follows from the limitation on the speed at which information travels is that nodes experience the world in different, unique ways. Computation on a single node is easy, because everything happens in a predictable global total order. Computation on a distributed system is difficult, because there is no global total order.For the longest while (e.g. decades of research), we've solved this problem by introducing a global total order. I've discussed the many methods for achieving strong consistency by creating order (in a fault-tolerant manner) where there is no naturally occurring total order.Of course, the problem is that enforcing order is expensive. This breaks down in particular with large scale internet systems, where a system needs to remain available. A system enforcing strong consistency doesn't behave like a distributed system: it behaves like a single system, which is bad for availability during a partition.Furthermore, for each operation, often a majority of the nodes must be contacted - and often not just once, but twice (as you saw in the discussion on 2PC). This is particularly painful in systems that need to be geographically distributed to provide adequate performance for a global user base.So behaving like a single system by default is perhaps not desirable.Perhaps what we want is a system where we can write code that doesn't use expensive coordination, and yet returns a "usable" value. Instead of having a single truth, we will allow different replicas to diverge from each other - both to keep things efficient but also to tolerate partitions - and then try to find a way to deal with the divergence in some manner.Eventual consistency expresses this idea: that nodes can for some time diverge from each other, but that eventually they will agree on the value.Within the set of systems providing eventual consistency, there are two types of system designs:Eventual consistency with probabilistic guarantees. This type of system can detect conflicting writes at some later point, but does not guarantee that the results are equivalent to some correct sequential execution. In other words, conflicting updates will sometimes result in overwriting a newer value with an older one and some anomalies can be expected to occur during normal operation (or during partitions).In recent years, the most influential system design offering single-copy consistency is Amazon's Dynamo, which I will discuss as an example of a system that offers eventual consistency with probabilistic guarantees.Eventual consistency with strong guarantees. This type of system guarantees that the results converge to a common value equivalent to some correct sequential execution. In other words, such systems do not produce any anomalous results; without any coordination you can build replicas of the same service, and those replicas can communicate in any pattern and receive the updates in any order, and they will eventually agree on the end result as long as they all see the same information.CRDT's (convergent replicated data types) are data types that guarantee convergence to the same value in spite of network delays, partitions and message reordering. They are provably convergent, but the data types that can be implemented as CRDT's are limited.The CALM (consistency as logical monotonicity) conjecture is an alternative expression of the same principle: it equates logical monotonicity with convergence. If we can conclude that something is logically monotonic, then it is also safe to run without coordination. Confluence analysis - in particular, as applied for the Bloom programming language - can be used to guide programmer decisions about when and where to use the coordination techniques from strongly consistent systems and when it is safe to execute without coordination.Reconciling different operation ordersWhat does a system that does not enforce single-copy consistency look like?  Let's try to make this more concrete by looking at a few examples.Perhaps the most obvious characteristic of systems that do not enforce single-copy consistency is that they allow replicas to diverge from each other. This means that there is no strictly defined pattern of communication: replicas can be separated from each other and yet continue to be available and accept writes.Let's imagine a system of three replicas, each of which is partitioned from the others. For example, the replicas might be in different datacenters and for some reason unable to communicate. Each replica remains available during the partition, accepting both reads and writes from some set of clients:[Clients]   - > [A]

--- Partition ---

[Clients]   - > [B]

--- Partition ---

[Clients]   - > [C]After some time, the partitions heal and the replica servers exchange information. They have received different updates from different clients and have diverged each other, so some sort of reconciliation needs to take place. What we would like to happen is that all of the replicas converge to the same result.[A] \
    --> [merge]
[B] /     |
          |
[C] ----[merge]---> resultAnother way to think about systems with weak consistency guarantees is to imagine a set of clients sending messages to two replicas in some order. Because there is no coordination protocol that enforces a single total order, the messages can get delivered in different orders at the two replicas:[Clients]  --> [A]  1, 2, 3
[Clients]  --> [B]  2, 3, 1This is, in essence, the reason why we need coordination protocols. For example, assume that we are trying to concatenate a string and the operations in messages 1, 2 and 3 are:1: { operation: concat('Hello ') }
2: { operation: concat('World') }
3: { operation: concat('!') }Then, without coordination, A will produce "Hello World!", and B will produce "World!Hello ".A: concat(concat(concat('', 'Hello '), 'World'), '!') = 'Hello World!'
B: concat(concat(concat('', 'World'), '!'), 'Hello ') = 'World!Hello 'This is, of course, incorrect. Again, what we'd like to happen is that the replicas converge to the same result.Keeping these two examples in mind, let's look at Amazon's Dynamo first to establish a baseline, and then discuss a number of novel approaches to building systems with weak consistency guarantees, such as CRDT's and the CALM theorem.Amazon's Dynamo system design (2007) is probably the best-known system that offers weak consistency guarantees but high availability. It is the basis for many other real world systems, including LinkedIn's Voldemort, Facebook's Cassandra and Basho's Riak.Dynamo is an eventually consistent, highly available key-value store. A key value store is like a large hash table: a client can set values via  and retrieve them by key using . A Dynamo cluster consists of N peer nodes; each node has a set of keys which is it responsible for storing.Dynamo prioritizes availability over consistency; it does not guarantee single-copy consistency. Instead, replicas may diverge from each other when values are written; when a key is read, there is a read reconciliation phase that attempts to reconcile differences between replicas before returning the value back to the client.For many features on Amazon, it is more important to avoid outages than it is to ensure that data is perfectly consistent, as an outage can lead to lost business and a loss of credibility. Furthermore, if the data is not particularly important, then a weakly consistent system can provide better performance and higher availability at a lower cost than a traditional RDBMS.Since Dynamo is a complete system design, there are many different parts to look at beyond the core replication task. The diagram below illustrates some of the tasks; notably, how a write is routed to a node and written to multiple replicas.[ Client ]
    |
( Mapping keys to nodes )
    |
    V
[ Node A ]
    |     \
( Synchronous replication task: minimum durability )
    |        \
[ Node B]  [ Node C ]
    A
    |
( Conflict detection; asynchronous replication task:
  ensuring that partitioned / recovered nodes recover )
    |
    V
[ Node D]After looking at how a write is initially accepted, we'll look at how conflicts are detected, as well as the asynchronous replica synchronization task. This task is needed because of the high availability design, in which nodes may be temporarily unavailable (down or partitioned). The replica synchronization task ensures that nodes can catch up fairly rapidly even after a failure.Whether we are reading or writing, the first thing that needs to happen is that we need to locate where the data should live on the system. This requires some type of key-to-node mapping.In Dynamo, keys are mapped to nodes using a hashing technique known as consistent hashing (which I will not discuss in detail). The main idea is that a key can be mapped to a set of nodes responsible for it by a simple calculation on the client. This means that a client can locate keys without having to query the system for the location of each key; this saves system resources as hashing is generally faster than performing a remote procedure call.Once we know where a key should be stored, we need to do some work to persist the value. This is a synchronous task; the reason why we will immediately write the value onto multiple nodes is to provide a higher level of durability (e.g. protection from the immediate failure of a node).Just like Paxos or Raft, Dynamo uses quorums for replication. However, Dynamo's quorums are sloppy (partial) quorums rather than strict (majority) quorums.Informally, a strict quorum system is a quorum system with the property that any two quorums (sets) in the quorum system overlap. Requiring a majority to vote for an update before accepting it guarantees that only a single history is admitted since each majority quorum must overlap in at least one node. This was the property that Paxos, for example, relied on.Partial quorums do not have that property; what this means is that a majority is not required and that different subsets of the quorum may contain different versions of the same data. The user can choose the number of nodes to write to and read from:the user can choose some number W-of-N nodes required for a write to succeed; andthe user can specify the number of nodes (R-of-N) to be contacted during a read. and  specify the number of nodes that need to be involved to a write or a read. Writing to more nodes makes writes slightly slower but increases the probability that the value is not lost; reading from more nodes increases the probability that the value read is up to date.The usual recommendation is that , because this means that the read and write quorums overlap in one node - making it less likely that a stale value is returned. A typical configuration is  (e.g. a total of three replicas for each value); this means that the user can choose between: R = 1, W = 3;
 R = 2, W = 2 or
 R = 3, W = 1More generally, again assuming :, : fast reads, slow writes, : fast writes, slow reads and : favorable to bothN is rarely more than 3, because keeping that many copies of large amounts of data around gets expensive!As I mentioned earlier, the Dynamo paper has inspired many other similar designs. They all use the same partial quorum based replication approach, but with different defaults for N, W and R:Basho's Riak (N = 3, R = 2, W = 2 default)Linkedin's Voldemort (N = 2 or 3, R = 1, W = 1 default)Apache's Cassandra (N = 3, R = 1, W = 1 default)There is another detail: when sending a read or write request, are all N nodes asked to respond (Riak), or only a number of nodes that meets the minimum (e.g. R or W; Voldemort). The "send-to-all" approach is faster and less sensitive to latency (since it only waits for the fastest R or W nodes of N) but also less efficient, while the "send-to-minimum" approach is more sensitive to latency (since latency communicating with a single node will delay the operation) but also more efficient (fewer messages / connections overall).What happens when the read and write quorums overlap, e.g. ()? Specifically, it is often claimed that this results in "strong consistency".Is R + W > N the same as "strong consistency"?It's not completely off base: a system where  can detect read/write conflicts, since any read quorum and any write quorum share a member. E.g. at least one node is in both quorums:   1     2   N/2+1     N/2+2    N
  [...] [R]  [R + W]   [W]    [...]This guarantees that a previous write will be seen by a subsequent read. However, this only holds if the nodes in N never change. Hence, Dynamo doesn't qualify, because in Dynamo the cluster membership can change if nodes fail.Dynamo is designed to be always writable. It has a mechanism which handles node failures by adding a different, unrelated server into the set of nodes responsible for certain keys when the original server is down. This means that the quorums are no longer guaranteed to always overlap. Even  would not qualify, since while the quorum sizes are equal to N, the nodes in those quorums can change during a failure. Concretely, during a partition, if a sufficient number of nodes cannot be reached, Dynamo will add new nodes to the quorum from unrelated but accessible nodes.Furthermore, Dynamo doesn't handle partitions in the manner that a system enforcing a strong consistency model would: namely, writes are allowed on both sides of a partition, which means that for at least some time the system does not act as a single copy. So calling  "strongly consistent" is misleading; the guarantee is merely probabilistic - which is not what strong consistency refers to.Conflict detection and read repairSystems that allow replicas to diverge must have a way to eventually reconcile two different values. As briefly mentioned during the partial quorum approach, one way to do this is to detect conflicts at read time, and then apply some conflict resolution method. But how is this done?In general, this is done by tracking the causal history of a piece of data by supplementing it with some metadata. Clients must keep the metadata information when they read data from the system, and must return back the metadata value when writing to the database.We've already encountered a method for doing this: vector clocks can be used to represent the history of a value. Indeed, this is what the original Dynamo design uses for detecting conflicts.However, using vector clocks is not the only alternative. If you look at many practical system designs, you can deduce quite a bit about how they work by looking at the metadata that they track.. When a system does not track metadata, and only returns the value (e.g. via a client API), it cannot really do anything special about concurrent writes. A common rule is that the last writer wins: in other words, if two writers are writing at the same time, only the value from the slowest writer is kept around.. Nominally, the value with the higher timestamp value wins. However, if time is not carefully synchronized, many odd things can happen where old data from a system with a faulty or fast clock overwrites newer values. Facebook's Cassandra is a Dynamo variant that uses timestamps instead of vector clocks.. Version numbers may avoid some of the issues related with using timestamps. Note that the smallest mechanism that can accurately track causality when multiple histories are possible are vector clocks, not version numbers.. Using vector clocks, concurrent and out of date updates can be detected. Performing read repair then becomes possible, though in some cases (concurrent changes) we need to ask the client to pick a value. This is because if the changes are concurrent and we know nothing more about the data (as is the case with a simple key-value store), then it is better to ask than to discard data arbitrarily.When reading a value, the client contacts  of  nodes and asks them for the latest value for a key. It takes all the responses, discards the values that are strictly older (using the vector clock value to detect this). If there is only one unique vector clock + value pair, it returns that. If there are multiple vector clock + value pairs that have been edited concurrently (e.g. are not comparable), then all of those values are returned.As is obvious from the above, read repair may return multiple values. This means that the client / application developer must occasionally handle these cases by picking a value based on some use-case specific criterion.In addition, a key component of a practical vector clock system is that the clocks cannot be allowed to grow forever - so there needs to be a procedure for occasionally garbage collecting the clocks in a safe manner to balance fault tolerance with storage requirements.Replica synchronization: gossip and Merkle treesGiven that the Dynamo system design is tolerant of node failures and network partitions, it needs a way to deal with nodes rejoining the cluster after being partitioned, or when a failed node is replaced or partially recovered.Replica synchronization is used to bring nodes up to date after a failure, and for periodically synchronizing replicas with each other.Gossip is a probabilistic technique for synchronizing replicas. The pattern of communication (e.g. which node contacts which node) is not determined in advance. Instead, nodes have some probability  of attempting to synchronize with each other. Every  seconds, each node picks a node to communicate with. This provides an additional mechanism beyond the synchronous task (e.g. the partial quorum writes) which brings the replicas up to date.Gossip is scalable, and has no single point of failure, but can only provide probabilistic guarantees.In order to make the information exchange during replica synchronization efficient, Dynamo uses a technique called Merkle trees, which I will not cover in detail. The key idea is that a data store can be hashed at multiple different levels of granularity: a hash representing the whole content, half the keys, a quarter of the keys and so on.By maintaining this fairly granular hashing, nodes can compare their data store content much more efficiently than a naive technique. Once the nodes have identified which keys have different values, they exchange the necessary information to bring the replicas up to date.Dynamo in practice: probabilistically bounded staleness (PBS)And that pretty much covers the Dynamo system design:consistent hashing to determine key placementpartial quorums for reading and writingconflict detection and read repair via vector clocks andgossip for replica synchronizationHow might we characterize the behavior of such a system? A fairly recent paper from Bailis et al. (2012) describes an approach called PBS (probabilistically bounded staleness) uses simulation and data collected from a real world system to characterize the expected behavior of such a system.PBS estimates the degree of inconsistency by using information about the anti-entropy (gossip) rate, the network latency and local processing delay to estimate the expected level of consistency of reads. It has been implemented in Cassandra, where timing information is piggybacked on other messages and an estimate is calculated based on a sample of this information in a Monte Carlo simulation.Based on the paper, during normal operation eventually consistent data stores are often faster and can read a consistent state within tens or hundreds of milliseconds. The table below illustrates amount of time required from a 99.9% probability of consistent reads given different  and  settings on empirical timing data from LinkedIn (SSD and 15k RPM disks) and Yammer:For example, going from ,  to ,  in the Yammer case reduces the inconsistency window from 1352 ms to 202 ms - while keeping the read latencies lower (32.6 ms) than the fastest strict quorum (, ; 219.27 ms).For more details, have a look at the PBS website  and the associated paper.Let's look back at the examples of the kinds of situations that we'd like to resolve. The first scenario consisted of three different servers behind partitions; after the partitions healed, we wanted the servers to converge to the same value. Amazon's Dynamo made this possible by reading from  out of  nodes and then performing read reconciliation.In the second example, we considered a more specific operation: string concatenation. It turns out that there is no known technique for making string concatenation resolve to the same value without imposing an order on the operations (e.g. without expensive coordination). However, there are operations which can be applied safely in any order, where a simple register would not be able to do so. As Pat Helland wrote:... operation-centric work can be made commutative (with the right operations and the right semantics) where a simple READ/WRITE semantic does not lend itself to commutativity.For example, consider a system that implements a simple accounting system with the  and  operations in two different ways:using a register with  and  operations, andusing a integer data type with native  and  operationsThe latter implementation knows more about the internals of the data type, and so it can preserve the intent of the operations in spite of the operations being reordered. Debiting or crediting can be applied in any order, and the end result is the same:100 + credit(10) + credit(20) = 130 and
100 + credit(20) + credit(10) = 130 However, writing a fixed value cannot be done in any order: if writes are reordered, the one of the writes will overwrite the other:100 + write(110) + write(130) = 130 but
100 + write(130) + write(110) = 110Let's take the example from the beginning of this chapter, but use a different operation. In this scenario, clients are sending messages to two nodes, which see the operations in different orders:[Clients]  --> [A]  1, 2, 3
[Clients]  --> [B]  2, 3, 1Instead of string concatenation, assume that we are looking to find the largest value (e.g. MAX()) for a set of integers. The messages 1, 2 and 3 are:1: { operation: max(previous, 3) }
2: { operation: max(previous, 5) }
3: { operation: max(previous, 7) }Then, without coordination, both A and B will converge to 7, e.g.:A: max(max(max(0, 3), 5), 7) = 7
B: max(max(max(0, 5), 7), 3) = 7In both cases, two replicas see updates in different order, but we are able to merge the results in a way that has the same result in spite of what the order is. The result converges to the same answer in both cases because of the merge procedure () we used.It is likely not possible to write a merge procedure that works for all data types. In Dynamo, a value is a binary blob, so the best that can be done is to expose it and ask the application to handle each conflict.However, if we know that the data is of a more specific type, handling these kinds of conflicts becomes possible. CRDT's are data structures designed to provide data types that will always converge, as long as they see the same set of operations (in any order).CRDTs: Convergent replicated data typesCRDTs (convergent replicated datatypes) exploit knowledge regarding the commutativity and associativity of specific operations on specific datatypes.In order for a set of operations to converge on the same value in an environment where replicas only communicate occasionally, the operations need to be order-independent and insensitive to (message) duplication/redelivery. Thus, their operations need to be:Associative (), so that grouping doesn't matterCommutative (), so that order of application doesn't matterIdempotent (), so that duplication does not matterIt turns out that these structures are already known in mathematics; they are known as join or meet semilattices.A lattice is a partially ordered set with a distinct top (least upper bound) and a distinct bottom (greatest lower bound). A semilattice is like a lattice, but one that only has a distinct top or bottom. A join semilattice is one with a distinct top (least upper bound) and a meet semilattice is one with a distinct bottom (greatest lower bound).Any data type that be expressed as a semilattice can be implemented as a data structure which guarantees convergence. For example, calculating the  of a set of values will always return the same result regardless of the order in which the values were received, as long as all values are eventually received, because the  operation is associative, commutative and idempotent.For example, here are two lattices: one drawn for a set, where the merge operator is  and one drawn for a strictly increasing integer counter, where the merge operator is :   { a, b, c }              7
  /      |    \            /  \
{a, b} {b,c} {a,c}        5    7
  |  \  /  | /           /   |  \
  {a} {b} {c}            3   5   7With data types that can be expressed as semilattices, you can have replicas communicate in any pattern and receive the updates in any order, and they will eventually agree on the end result as long as they all see the same information. That is a powerful property that can be guaranteed as long as the prerequisites hold.However, expressing a data type as a semilattice often requires some level of interpretation. Many data types have operations which are not in fact order-independent. For example, adding items to a set is associative, commutative and idempotent. However, if we also allow items to be removed from a set, then we need some way to resolve conflicting operations, such as  and . What does it mean to remove an element if the local replica never added it? This resolution has to be specified in a manner that is order-independent, and there are several different choices with different tradeoffs.This means that several familiar data types have more specialized implementations as CRDT's which make a different tradeoff in order to resolve conflicts in an order-independent manner. Unlike a key-value store which simply deals with registers (e.g. values that are opaque blobs from the perspective of the system), someone using CRDTs must use the right data type to avoid anomalies.Some examples of the different data types specified as CRDT's include:CountersGrow-only counter (merge = max(values); payload = single integer)Positive-negative counter (consists of two grow counters, one for increments and another for decrements)RegistersLast Write Wins -register (timestamps or version numbers; merge = max(ts); payload = blob)Multi-valued -register (vector clocks; merge = take both)SetsGrow-only set (merge = union(items); payload = set; no removal)Two-phase set (consists of two sets, one for adding, and another for removing; elements can be added once and removed once)Unique set (an optimized version of the two-phase set)Last write wins set (merge = max(ts); payload = set)Positive-negative set (consists of one PN-counter per set item)Graphs and text sequences (see the paper)To ensure anomaly-free operation, you need to find the right data type for your specific application - for example, if you know that you will only remove an item once, then a two-phase set works; if you will only ever add items to a set and never remove them, then a grow-only set works.Not all data structures have known implementations as CRDTs, but there are CRDT implementations for booleans, counters, sets, registers and graphs in the recent (2011) survey paper from Shapiro et al.Interestingly, the register implementations correspond directly with the implementations that key value stores use: a last-write-wins register uses timestamps or some equivalent and simply converges to the largest timestamp value; a multi-valued register corresponds to the Dynamo strategy of retaining, exposing and reconciling concurrent changes. For the details, I recommend that you take a look at the papers in the further reading section of this chapter.The CRDT data structures were based on the recognition that data structures expressible as semilattices are convergent. But programming is about more than just evolving state, unless you are just implementing a data store.Clearly, order-independence is an important property of any computation that converges: if the order in which data items are received influences the result of the computation, then there is no way to execute a computation without guaranteeing order.However, there are many programming models in which the order of statements does not play a significant role. For example, in the MapReduce model, both the Map and the Reduce tasks are specified as stateless tuple-processing tasks that need to be run on a dataset. Concrete decisions about how and in what order data is routed to the tasks is not specified explicitly, instead, the batch job scheduler is responsible for scheduling the tasks to run on the cluster.Similarly, in SQL one specifies the query, but not how the query is executed. The query is simply a declarative description of the task, and it is the job of the query optimizer to figure out an efficient way to execute the query (across multiple machines, databases and tables).Of course, these programming models are not as permissive as a general purpose programming language. MapReduce tasks need to be expressible as stateless tasks in an acyclic dataflow program; SQL statements can execute fairly sophisticated computations but many things are hard to express in it.However, it should be clear from these two examples that there are many kinds of data processing tasks which are amenable to being expressed in a declarative language where the order of execution is not explicitly specified. Programming models which express a desired result while leaving the exact order of statements up to an optimizer to decide often have semantics that are order-independent. This means that such programs may be possible to execute without coordination, since they depend on the inputs they receive but not necessarily the specific order in which the inputs are received.The key point is that such programs  safe to execute without coordination. Without a clear rule that characterizes what is safe to execute without coordination, and what is not, we cannot implement a program while remaining certain that the result is correct.This is what the CALM theorem is about. The CALM theorem is based on a recognition of the link between logical monotonicity and useful forms of eventual consistency (e.g. confluence / convergence). It states that logically monotonic programs are guaranteed to be eventually consistent.Then, if we know that some computation is logically monotonic, then we know that it is also safe to execute without coordination.To better understand this, we need to contrast monotonic logic (or monotonic computations) with non-monotonic logic (or non-monotonic computations).if sentence  is a consequence of a set of premises , then it can also be inferred from any set  of premises extending Most standard logical frameworks are monotonic: any inferences made within a framework such as first-order logic, once deductively valid, cannot be invalidated by new information. A non-monotonic logic is a system in which that property does not hold - in other words, if some conclusions can be invalidated by learning new knowledge.Within the artificial intelligence community, non-monotonic logics are associated with defeasible reasoning - reasoning, in which assertions made utilizing partial information can be invalidated by new knowledge. For example, if we learn that Tweety is a bird, we'll assume that Tweety can fly; but if we later learn that Tweety is a penguin, then we'll have to revise our conclusion.Monotonicity concerns the relationship between premises (or facts about the world) and conclusions (or assertions about the world). Within a monotonic logic, we know that our results are retraction-free: monotone computations do not need to be recomputed or coordinated; the answer gets more accurate over time. Once we know that Tweety is a bird (and that we're reasoning using monotonic logic), we can safely conclude that Tweety can fly and that nothing we learn can invalidate that conclusion.While any computation that produces a human-facing result can be interpreted as an assertion about the world (e.g. the value of "foo" is "bar"), it is difficult to determine whether a computation in a von Neumann machine based programming model is monotonic, because it is not exactly clear what the relationship between facts and assertions are and whether those relationships are monotonic.However, there are a number of programming models for which determining monotonicity is possible. In particular, relational algebra (e.g. the theoretical underpinnings of SQL) and Datalog provide highly expressive languages that have well-understood interpretations.Both basic Datalog and relational algebra (even with recursion) are known to be monotonic. More specifically, computations expressed using a certain set of basic operators are known to be monotonic (selection, projection, natural join, cross product, union and recursive Datalog without negation), and non-monotonicity is introduced by using more advanced operators (negation, set difference, division, universal quantification, aggregation).This means that computations expressed using a significant number of operators (e.g. map, filter, join, union, intersection) in those systems are logically monotonic; any computations using those operators are also monotonic and thus safe to run without coordination. Expressions that make use of negation and aggregation, on the other hand, are not safe to run without coordination.It is important to realize the connection between non-monotonicity and operations that are expensive to perform in a distributed system. Specifically, both  and  can be considered to be a form of negation. As Joe Hellerstein writes:To establish the veracity of a negated predicate in a distributed setting, an evaluation strategy has to start "counting to 0" to determine emptiness, and wait until the distributed counting process has definitely terminated. Aggregation is the generalization of this idea.This idea can be seen from the other direction as well. Coordination protocols are themselves aggregations, since they entail voting: Two-Phase Commit requires unanimous votes, Paxos consensus requires majority votes, and Byzantine protocols require a 2/3 majority. Waiting requires counting.If, then we can express our computation in a manner in which it is possible to test for monotonicity, then we can perform a whole-program static analysis that detects which parts of the program are eventually consistent and safe to run without coordination (the monotonic parts) - and which parts are not (the non-monotonic ones).Note that this requires a different kind of language, since these inferences are hard to make for traditional programming languages where sequence, selection and iteration are at the core. Which is why the Bloom language was designed.What is non-mononicity good for?The difference between monotonicity and non-monotonicity is interesting. For example, adding two numbers is monotonic, but calculating an aggregation over two nodes containing numbers is not. What's the difference? One of these is a computation (adding two numbers), while the other is an assertion (calculating an aggregate).How does a computation differ from an assertion? Let's consider the query "is pizza a vegetable?". To answer that, we need to get at the core: when is it acceptable to infer that something is (or is not) true?There are several acceptable answers, each corresponding to a different set of assumptions regarding the information that we have and the way we ought to act upon it - and we've come to accept different answers in different contexts.In everyday reasoning, we make what is known as the open-world assumption: we assume that we do not know everything, and hence cannot make conclusions from a lack of knowledge. That is, any sentence may be true, false or unknown.                                OWA +             |  OWA +
                                Monotonic logic   |  Non-monotonic logic
Can derive P(true)      |   Can assert P(true)    |  Cannot assert P(true)
Can derive P(false)     |   Can assert P(false)   |  Cannot assert P(true)
Cannot derive P(true)   |   Unknown               |  Unknown
or P(false)When making the open world assumption, we can only safely assert something we can deduce from what is known. Our information about the world is assumed to be incomplete.Let's first look at the case where we know our reasoning is monotonic. In this case, any (potentially incomplete) knowledge that we have cannot be invalidated by learning new knowledge. So if we can infer that a sentence is true based on some deduction, such as "things that contain two tablespoons of tomato paste are vegetables" and "pizza contains two tablespoons of tomato paste", then we can conclude that "pizza is a vegetable". The same goes for if we can deduce that a sentence is false.However, if we cannot deduce anything - for example, the set of knowledge we have contains customer information and nothing about pizza or vegetables - then under the open world assumption we have to say that we cannot conclude anything.With non-monotonic knowledge, anything we know right now can potentially be invalidated. Hence, we cannot safely conclude anything, even if we can deduce true or false from what we currently know.However, within the database context, and within many computer science applications we prefer to make more definite conclusions. This means assuming what is known as the closed-world assumption: that anything that cannot be shown to be true is false. This means that no explicit declaration of falsehood is needed. In other words, the database of facts that we have is assumed to be complete (minimal), so that anything not in it can be assumed to be false.For example, under the CWA, if our database does not have an entry for a flight between San Francisco and Helsinki, then we can safely conclude that no such flight exists.We need one more thing to be able to make definite assertions: logical circumscription. Circumscription is a formalized rule of conjecture. Domain circumscription conjectures that the known entities are all there are. We need to be able to assume that the known entities are all there are in order to reach a definite conclusion.                                CWA +             |  CWA +
                                Circumscription + |  Circumscription +
                                Monotonic logic   |  Non-monotonic logic
Can derive P(true)      |   Can assert P(true)    |  Can assert P(true)
Can derive P(false)     |   Can assert P(false)   |  Can assert P(false)
Cannot derive P(true)   |   Can assert P(false)   |  Can assert P(false)
or P(false)In particular, non-monotonic inferences need this assumption. We can only make a confident assertion if we assume that we have complete information, since additional information may otherwise invalidate our assertion.What does this mean in practice? First, monotonic logic can reach definite conclusions as soon as it can derive that a sentence is true (or false). Second, nonmonotonic logic requires an additional assumption: that the known entities are all there is.So why are two operations that are on the surface equivalent different? Why is adding two numbers monotonic, but calculating an aggregation over two nodes not? Because the aggregation does not only calculate a sum but also asserts that it has seen all of the values. And the only way to guarantee that is to coordinate across nodes and ensure that the node performing the calculation has really seen all of the values within the system.Thus, in order to handle nonmonotonicity one needs to either use distributed coordination to ensure that assertions are made only after all the information is known or make assertions with the caveat that the conclusion can be invalidated later on.Handling non-monotonicity is important for reasons of expressiveness. This comes down to being able to express non-monotone things; for example, it is nice to be able to say that the total of some column is X. The system must detect that this kind of computation  requires a global coordination boundary to ensure that we have seen all the entities.Purely monotone systems are rare. It seems that most applications operate under the closed-world assumption even when they have incomplete data, and we humans are fine with that. When a database tells you that a direct flight between San Francisco and Helsinki does not exist, you will probably treat this as "according to this database, there is no direct flight", but you do not rule out the possibility that that in reality such a flight might still exist.Really, this issue only becomes interesting when replicas can diverge (e.g. during a partition or due to delays during normal operation). Then there is a need for a more specific consideration: whether the answer is based on just the current node, or the totality of the system.Further, since nonmonotonicity is caused by making an assertion, it seems plausible that many computations can proceed for a long time and only apply coordination at the point where some result or assertion is passed to a 3rd party system or end user. Certainly it is not necessary for every single read and write operation within a system to enforce a total order, if those reads and writes are simply a part of a long running computation.The Bloom language is a language designed to make use of the CALM theorem. It is a Ruby DSL which has its formal basis in a temporal logic programming language called Dedalus.In Bloom, each node has a database consisting of collections and lattices. Programs are expressed as sets of unordered statements which interact with collections (sets of facts) and lattices (CRDTs). Statements are order-independent by default, but one can also write non-monotonic functions.The CALM theorem, confluence analysis and BloomDynamo; PBS; optimistic replicationIf you've made it this far, thank you.If you liked the book, follow me on Github (or Twitter). I love seeing that I've had some kind of positive impact. "Create more value than you capture" and all that.Many many thanks to: logpath, alexras, globalcitizen, graue, frankshearar, roryokane, jpfuentes2, eeror, cmeiklejohn, stevenproctor eos2102 and steveloughran for their help! Of course, any mistakes and omissions that remain are my fault!It's worth noting that my chapter on eventual consistency is fairly Berkeley-centric; I'd like to change that. I've also skipped one prominent use case for time: consistent snapshots. There are also a couple of topics which I should expand on: namely, an explicit discussion of safety and liveness properties and a more detailed discussion of consistent hashing. However, I'm off to Strange Loop 2013, so whatever.If this book had a chapter 6, it would probably be about the ways in which one can make use of and deal with large amounts of data. It seems that the most common type of "big data" computation is one in which a large dataset is passed through a single simple program. I'm not sure what the subsequent chapters would be (perhaps high performance computing, given that the current focus has been on feasibility), but I'll probably know in a couple of years.Books about distributed systemsDistributed Algorithms (Lynch)This is probably the most frequently recommended book on distributed algorithms. I'd also recommend it, but with a caveat. It is very comprehensive, but written for a graduate student audience, so you'll spend a lot of time reading about synchronous systems and shared memory algorithms before getting to things that are most interesting to a practitioner.Introduction to Reliable and Secure Distributed Programming (Cachin, Guerraoui & Rodrigues)For a practitioner, this is a fun one. It's short and full of actual algorithm implementations.Replication: Theory and PracticeIf you're interested in replication, this book is amazing. The chapter on replication is largely based on a synthesis of the interesting parts of this book plus more recent readings.Distributed Systems: An Algorithmic Approach (Ghosh)Introduction to Distributed Algorithms (Tel)This book is on traditional transactional information systems, e.g. local RDBMS's. There are two chapters on distributed transactions at the end, but the focus of the book is on transaction processing.Transaction Processing: Concepts and Techniques by Gray and ReuterA classic. I find that Weikum & Vossen is more up to date.Here are some additional lists of recommended papers:]]></content:encoded></item><item><title>Hyprland 0.54 Released As A &quot;Massive&quot; Update To This Wayland Compositor</title><link>https://www.phoronix.com/news/Hyprland-0.54-Released</link><author>/u/anh0516</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 00:19:21 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com.]]></content:encoded></item><item><title>GNOME GitLab Redirecting Some Git Traffic To GitHub For Reducing Costs</title><link>https://www.phoronix.com/news/GNOME-GitHub-GitLab-Redirect</link><author>/u/anh0516</author><category>reddit</category><pubDate>Sat, 28 Feb 2026 00:15:37 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com.]]></content:encoded></item><item><title>[OpenGL C++] 3D Voxel Engine Tutorial</title><link>https://youtube.com/playlist?list=PLQ7CpbxNS-_YP1WhUAVmxRQuF_a4PLju_&amp;amp;si=GMhtbEdFJ461Wdr2</link><author>/u/BrawlyxHariyama</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 23:12:19 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Trolley - Run terminal apps anywhere (pairs well with Bubbletea)</title><link>https://github.com/weedonandscott/trolley</link><author>/u/weedonandscott</author><category>golang</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 22:54:45 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Happy to share the early version of Trolley, which lets you wrap your TUI app and distribute to non-technical users on Linux, Mac, and maybe Windows (not tested yet).This came about after writing a small TUI to allow a friend to back up their entire Vimeo library, and finding that while they enjoyed the simplicity and speed of the TUI, they did not like having to use the shell to get there, nor did they want to install a terminal like Ghostty for a better experience.Trolley makes it easy to package apps for that kind of person. It's still very early. The CLI is decent for an alpha state, as it's more my area. The runtime code is new to me, but thankfully much of it is based around Ghostty's GUIs so I made it work with a bit of AI help.Let me know what you think!]]></content:encoded></item><item><title>[R] ContextCache: Persistent KV Cache with Content-Hash Addressing â€” 29x TTFT speedup for tool-calling LLMs</title><link>https://www.reddit.com/r/MachineLearning/comments/1rglj2n/r_contextcache_persistent_kv_cache_with/</link><author>/u/PlayfulLingonberry73</author><category>ai</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 22:47:57 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[We present ContextCache, a persistent KV cache system for tool-calling LLMs that eliminates redundant prefill computation for tool schema tokens.Motivation: In tool-augmented LLM deployments, tool schemas (JSON function definitions) are prepended to every request but rarely change between calls. Standard inference re-processes these tokens from scratch each time.Approach: We cache the KV states produced during the initial prefill of tool schemas, indexed by a content hash (SHA256 of sorted schema texts). On subsequent requests with the same tool set, we restore cached KV states and only run forward pass on the user query suffix.Key finding: Per-tool independent caching fails catastrophically (tool selection accuracy drops from 85% to 10%) because models rely on cross-tool attention during prefill. Group caching â€” caching all tools as a single block â€” preserves full-prefill quality exactly across seen, held-out, and unseen tool splits.Results (Qwen3-8B, 4-bit NF4):Cached TTFT remains constant (~200ms) from 5 to 50 toolsFull prefill grows from 466ms to 5,625ms over the same range29x speedup at 50 tools, with 99% of prompt tokens skipped per requestZero quality degradation: group_cached matches full_prefill on TSA, PF1, and EM across all evaluation splitsLimitations: Eager attention causes OOM at 75+ tools on 24GB GPU. Flash attention integration would extend the practical range.]]></content:encoded></item><item><title>context-logger - Structured context propagation for log crate, something missing in Rust logs</title><link>https://github.com/alekseysidorov/context-logger</link><author>/u/AlekseySidorov</author><category>rust</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 22:41:18 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Hi All, I am glad to release a new version of my library. It makes it easy to attach key value context to your logs without boilerplate```rust use context_logger::{ContextLogger, LogContext}; use log::info;fn main() { let env_logger = env_logger::builder().build(); let max_level = env_logger.filter(); ContextLogger::new(env_logger) .default_record("version", "0.1.3") .init(max_level);let ctx = LogContext::new() .record("request_id", "req-123") .record("user_id", 42); let _guard = ctx.enter(); info!("handling request"); // version, request_id, user_id included ]]></content:encoded></item><item><title>A new California law says all operating systems, including Linux, need to have some form of age verification at account setup</title><link>https://x.com/LundukeJournal/status/2026783141298360692</link><author>/u/ANiceGobletofTea</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 22:31:49 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Trump orders federal agencies to stop using Anthropic AI tech â€˜immediatelyâ€™</title><link>https://www.reddit.com/r/artificial/comments/1rgkegx/trump_orders_federal_agencies_to_stop_using/</link><author>/u/ValueInvestingIsDead</author><category>ai</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 22:03:25 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[President Donald Trump ordered U.S. government agencies to â€œimmediately ceaseâ€ using technology from the artificial intelligence company Anthropic.The AI startup faces pressure by the Defense Department to comply with demands that it can use the companyâ€™s technology without restrictions sought by Anthropic.The company wants the Pentagon to assure it that the AI models will not be used for fully autonomous weapons or mass domestic surveillance of Americans.Another major AI company, OpenAI, said it has the same â€œred linesâ€ as Anthropic regarding the use of its technology by the Pentagon and other customers.The president also said there would be a six-month phase-out for agencies such as the Defense Department, which â€œare using Anthropicâ€™s products, at various levels.â€]]></content:encoded></item><item><title>My first ever Linux installation.</title><link>https://www.reddit.com/r/linux/comments/1rgk4qd/my_first_ever_linux_installation/</link><author>/u/FriesWithMacSauce</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 21:53:05 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I own a pawn shop and have a whole wall of Windows laptops for dirt cheap and I canâ€™t get rid of them. So Iâ€™m doing an experiment. I installed Linux onto this cheap Dell and Iâ€™m going to try and sell it for $75. Iâ€™ve never used Linux before, Iâ€™m an Apple user. But I gotta say in comparison to Windows 10/11, Ubuntu is truly a breath of fresh air. So snappy and light weight. I feel like this computer has been rescued and is ready to live a second life instead of going to the landfill. ]]></content:encoded></item><item><title>Anyone running production Redis on without Bitnami images/charts now?</title><link>https://www.reddit.com/r/kubernetes/comments/1rgjuxu/anyone_running_production_redis_on_without/</link><author>/u/dkargatzis_</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 21:42:33 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I was a long-time user of the Bitnami Redis Helm chart. When Broadcom sunsetted the free Bitnami images in 2025 and moved everything to the unsupported bitnamilegacy registry (no more updates or security patches), I switched to the legacy images as a temporary workaround.Now I'm looking for a permanent, actively maintained, ideally free / open-source solution: - a solid Helm chart (or lightweight operator if it's better) - preferably uses official redis Docker images (or equally trusted free ones) - good support for persistence, scaling, monitoring (Prometheus), TLS, etc. - bonus points for HA (Sentinel + failover), I don't strictly need full sharded Redis Cluster unless someone strongly recommends itWhat are you all using in production? Any charts / operators you can recommend that feel "set it and forget it" for long term?]]></content:encoded></item><item><title>Fastest way to remove duplicate UUIDS from a list</title><link>https://www.reddit.com/r/golang/comments/1rgjhat/fastest_way_to_remove_duplicate_uuids_from_a_list/</link><author>/u/Fun-Result-8489</author><category>golang</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 21:28:12 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Lets say that you have an array of UUIDS and you want to remove all the duplicates. Obviously what you can do is to create a  nested in another  and check for each element if it exists more than once in the list. I thought that this might not be ideal performance wise, so I came up with another simple plan. While you iterate the list, you populate a map that has as a key the UUID of that specific entry. If the key is present obviously you know that this is a duplicate! Izi pizy. So I was wondering whether something like that is a common practice to deal with such a problem. Is there any big issue with that solution that I should be aware of ? Or is there any better solution for that problem ?]]></content:encoded></item><item><title>Autocomplete for types from unimported packages (like GoLand)</title><link>https://www.reddit.com/r/golang/comments/1rgj71y/autocomplete_for_types_from_unimported_packages/</link><author>/u/Hot_Perspective_5931</author><category>golang</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 21:17:03 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Iâ€™m having an issue with code completion in  (LSP) and was hoping someone could help.For example, in one package:package model type Price struct { bid float64 ask float64 } package usage func test() { Pri... } In this case, Iâ€™d like  to appear in the completion suggestions even though the  package hasnâ€™t been imported yet.Right now,  only shows up after I manually type  and import the package. In GoLand, I really like that it suggests symbols from unimported packages automatically and adds the import for me.Iâ€™ve tried enabling options like  and fuzzy matching in , but I havenâ€™t been able to get the same behavior in Neovim.Is there any way to achieve GoLand-like auto-completion for unimported packages using gopls in Neovim?]]></content:encoded></item><item><title>Rust or Zig for small WASM numerical compute kernels?</title><link>https://www.reddit.com/r/rust/comments/1rgi2mh/rust_or_zig_for_small_wasm_numerical_compute/</link><author>/u/dupontcyborg</author><category>rust</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 20:33:58 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Hi r/rust! I'm building numpy-ts, a NumPy-like numerical lib in TypeScript. I just tagged 1.0 after reaching 94% coverage of NumPy's API.I'm now evaluating WASM acceleration for compute-bound hot paths (e.g., linalg, sorting, etc.). So I prototyped identical kernels in both Zig and Rust targeting wasm32 with SIMD128 enabled.The results were interesting: performance and binary sizes are essentially identical (~7.5 KB gzipped total for 5 kernel files each). Both compile through LLVM, so I  the WASM output is nearly the same.Deeper ecosystem if we ever need exotic math (erf, gamma, etc.)Much wider developer adoption which somewhat de-risks a project like this`@setFloatMode(.optimized)` lets LLVM auto-vectorize reductions without hand-writing SIMDVector types (`@Vector(4, f64)`) are more ergonomic than Rust's `core::arch::wasm32` intrinsicsNo unsafe wrapper for code that's inherently raw pointer math (which feels like a waste of Rust's borrow-checker)I'm asking r/zig a similar question, but for those of you who chose Rust for WASM applications, what else should I think about?]]></content:encoded></item><item><title>Anyone here still running Linux on an Apple TV?</title><link>https://www.reddit.com/r/linux/comments/1rghu6g/anyone_here_still_running_linux_on_an_apple_tv/</link><author>/u/L0stG33k</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 20:25:15 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Took a bit more fuss than a standard PC... but finally got it slimmed down and running on a modern distro. Popped out the wifi card, and she idles at a mere 12W from the wall socket. I'm having fun with it. Anyone still using one of these as a media box, seed box, server, ?For those who don't already know, the original Apple TV Gen 1 was just an intel PC. Kind of like an ultra cheap version of the Intel Mac Mini. But it doesn't use a PC BIOS (or standard EFI for that matter), so you need a mach kernel to bootstrap any alt OS you intend to run.Specs: Intel Pentium M 1 GHz GeForce Mobile 10/100 MB Ethernet Built-in 5V PSUKinda funny, this is running the same OS as my server, but with 1/128th the ram.]]></content:encoded></item><item><title>[D] Edge AI Projects on Jetson Orin â€“ Ideas?</title><link>https://www.reddit.com/r/MachineLearning/comments/1rghtsb/d_edge_ai_projects_on_jetson_orin_ideas/</link><author>/u/___loki__</author><category>ai</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 20:24:46 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Iâ€™ve got access to a bunch of NVIDIA Jetson Orins through my lab and I want to do something cool and deployable. For context, Iâ€™ve previously built a small language model (SLM) from scratch and have experience in real-time ML pipelines, computer vision, anomaly detection, and explainable AI. Iâ€™ve also deployed AI models on edge devices for real-time monitoring systems.Iâ€™m looking for ideas/ research areas that could get me hired tbh, and relevant for industry or research, ideally something that demonstrates strong AI-ML + deployment skills and can stand out on a resume.Any creative, ambitious, or edge-focused suggestions would be amazing! Thanks in Advance:)]]></content:encoded></item><item><title>Software engineers should be a little bit cynical</title><link>https://www.seangoedecke.com/a-little-bit-cynical/</link><author>/u/fagnerbrack</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 19:17:36 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[I have no doubt that [Seanâ€™s] advice is quite effective for navigating the upper levels of an organization dedicated to producing a large, mature software product. But what is lost is any sort of conception of value. Is it too naive to say that engineers are more than â€œtools in a political gameâ€, they are specialized professionals whose role is to apply their expertise towards solving meaningful problems?The irony is that this kind of thinking destroys a companyâ€™s ability to actually make money â€¦ the idea that engineers should begin with a self-conception of doing what their manager tells them to is, to me, very bleak. It may be a good way to operate smoothly within a bureaucratic organization, and of course, one must often make compromises and take direction, but it is a bad way to do good work.I can see why people would think this way. But I  working in big tech companies! I do see myself as a professional solving meaningful problems. And I think navigating the organization to put real features or improvements in the hands of users is an excellent way - maybe the best way - to do good work.Why do I write such cynical posts, then? Well, I think that a small amount of cynicism is necessary in order to think clearly about how organizations work, and to avoid falling into the trap of being overly cynical. In general, I think good engineers ought to be a little bit cynical.The idealist view is more cynical than idealists thinkOne doctrinaire â€œidealistâ€ view of software engineering goes something like this. Iâ€™m obviously expressing it in its most lurid form, but I do think many people believe this more or less literally:We live in a late-stage-capitalist hellscape, where large companies are run by aspiring robber barons who have no serious convictions beyond desiring power. All those companies want is for obedient engineering drones to churn out bad code fast, so they can goose the (largely fictional) stock price. Meanwhile, end-users are left holding the bag: paying more for worse software, being hassled by advertisements, and dealing with bugs that are unprofitable to fix. The only thing an ethical software engineer can do is to try and find some temporary niche where they can defy their bosses and do real, good engineering work, or to retire to a hobby farm and write elegant open-source software in their free time.When you write it all out, I think itâ€™s clear to see that this is  cynical. At the very least, itâ€™s a cynical way to view your coworkers and bosses, who are largely people like you: doing a job, balancing a desire to do good work with the need to please their own bosses. Itâ€™s a cynical way to view the C-staff of a company. I think itâ€™s also inaccurate: from my limited experience, the people who run large tech companies really do want to deliver good software to users.Itâ€™s idealistic only in the sense that it does not accept the need for individual software engineers to compromise. According to this view,  never need to write bad software. No matter how hard the company tells you to compromise and just get something out, youâ€™re morally required to plant your feet and tell them to go to hell. In fact, by doing so, youâ€™re taking a stand against the general degeneration of the modern software world. Youâ€™re protecting - unsung, like Batman - the needs of the end-user who will never know you exist.I can certainly see the appeal of this view! But I donâ€™t think itâ€™s an  appeal. It comes from seeing the world as fundamentally corrupted and selfish, and believing that real positive change is impossible. In other words, I think itâ€™s a  appeal.The cynical view is more idealistic than idealists thinkI donâ€™t see a hard distinction between engineers being â€œtools in a political gameâ€ and professionals who solve meaningful problems. In fact, I think that in practice almost all meaningful problems are solved by playing political games.There are very few problems that you can solve entirely on your own. Software engineers encounter more of these problems than average, because the nature of software means that a single engineer can have huge leverage by sitting down and making a single code change. But in order to make changes to large products - for instance, to make it possible for GitHubâ€™s 150M users to use LaTeX in markdown - you need to coordinate with many other people at the company, which means you need to be involved in politics.It is just a plain fact that software engineers are not the movers and shakers in large tech organizations. They do not set the direction of the company. To the extent that they have political influence, itâ€™s in how they translate the direction of the company into specific technical changes. But that is actually quite a lot of influence!Large tech companies serve hundreds of millions (or billions) of users. Small changes to these products can have a massive positive or negative effect in the aggregate. As I see it, choosing to engage in the messy, political process of making these changes - instead of washing your hands of it as somehow impure - is an act of idealism. I think the position of a software engineer in a large tech company is similar to people who go into public service: idealistically hoping that they can do some good, despite knowing that they themselves will never set the broad strokes of government policy.Of course, big-tech software engineers are paid far better, so many people who go into this kind of work in fact are purely financially-motivated cynics. But Iâ€™m not one of them! I think itâ€™s possible, by doing good work, to help steer the giant edifice of a large tech company for the better.Cynical writing is like most medicines: the dose makes the poison. A healthy amount of cynicism can serve as an inoculation from being overly cynical.If you donâ€™t have an slightly cynical explanation for why engineers write bad code in large tech companies - such as the one I write about here - you risk adopting an overly cynical one. For instance, you might think that big tech engineers are being deliberately demoralized as part of an anti-labor strategy to prevent them from unionizing, which is nuts. Tech companies are simply not set up to engage in these kind of conspiracies.If you donâ€™t have a slightly cynical explanation for why large tech companies sometimes make inefficient decisions - such as this one - you risk adopting an overly cynical one. For instance, you might think that tech companies are full of incompetent losers, which is simply not true. Tech companies have a normal mix of strong and weak engineers.Idealist writing is massively over-represented in writing about software engineering. There is no shortage of books or blog posts (correctly) explaining that we ought to value good code, that we ought to be kind to our colleagues, that we ought to work on projects with positive real-world impact, and so on. There  a shortage of writing that accurately describes how big tech companies operate.Of course, cynical writing can harm people: by making them sad, or turning them into bitter cynics. But idealist writing can harm people too. Thereâ€™s a whole generation of software engineers who came out of the 2010s with a  model of how big tech companies work, and who are effectively being fed into the woodchipper in the 2020s. They would be better off if they internalized a correct model of how these companies work: not just less likely to get into trouble, but better at achieving their own idealist goals.edit: this post got some traction on Hacker News, with many comments. Some commenters said that itâ€™s incoherent to say â€œwhat I do is good, actuallyâ€ when my employer is engaged in various unethical activity. Fair enough! But this post isnâ€™t about whether itâ€™s ethical to work for Microsoft or not. Itâ€™s a followup to How good engineers write bad code at big companies - the main cynicism Iâ€™m interested in here is not â€œbig tech is evilâ€, but â€œbig tech is incompetentâ€.Some othercommenters challenged my claim that C-staff want to deliver good software by pointing out that theyâ€™re not willing to trade off their personal success to do so. Sure, I agree with that. The kind of person willing to sacrifice their career for things doesnâ€™t typically make it to a C-level position. But itâ€™s not always zero-sum. Good software makes money for software companies, after all.I also saw two commenters link this as an example of big tech companies actually being engaged in conspiracies against their employees. Iâ€™m not convinced. Companies  structurally set up to collude on salaries, but theyâ€™re not set up to deliberately make their employees sad - they just donâ€™t have that kind of fine-grained control over the culture! To the extent they have any control, they try to make their employees happy so theyâ€™ll work for less money and not leave.]]></content:encoded></item><item><title>Signed distance field fonts</title><link>https://www.redblobgames.com/articles/sdf-fonts/</link><author>/u/ketralnis</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 19:09:20 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[The normal way to render fonts is to read a font file containing  paths, then render those paths to the screen. These fonts support the full set of characters, sizes, and effects. Many games will pre-render the fonts to  form, but this limits the set of characters, sizes, and effects. A  () texture can be used as an intermediate format between the original vector font and fonts pre-rendered to bitmaps. It allows all sizes but not all characters or effects.Rendering a resized bitmap font leads to blurry or jagged edges:Rendering a resized distance field font leads to smooth edges:Resizing a distance field fontWe donâ€™t have to  the resized distance field. It is generated implicitly by the GPU fragment shader, for â€œfreeâ€. We can use a single low resolution distance field to generate high resolution output at any size. Thatâ€™s the magic of SDF fonts!SDF is not always the best choice for fonts. Iâ€™ve attempted to summarize the pros and cons:note: there are newer GPU-accelerated font rendering systems that use the vectors. I have not yet explored these.On this page Iâ€™ll show how I use msdfgen with WebGL.We can think of a signed distance field as a â€œheight mapâ€ in a landscape. The area above the water will be filled in. The area by the coastline will become the border color. The area underwater can be transparent, but could be used for glow, shadow, or other effects.Landscape view of a signed distance fieldThe SDF texture contains distances encoded as 0â€“255. In msdfgen, 255 is â€œinsideâ€ the font and 0 is â€œoutsideâ€:A single glyphâ€™s distance fieldTo render the distance map to a glyph, we first interpret the 0â€“255 values as a signed distance. Iâ€™ve chosen to use distances in â€œemâ€ units. I am reversing the direction so that high values are outside and low values are inside, the same as Inigo Quilezâ€™s convention. I have found that the reverse direction makes thickness, outline, and glow calculations easier.100*distance_em at each pointIn msdfgen, the  parameter sets the distance range in â€œemâ€ units, so Iâ€™ll use that to map 0 to the high value (+5%) and 255 to the low value (-5%):We can either remember the values we passed to  or we can recover it from msdfgenâ€™s JSON export. I havenâ€™t studied other SDF font libraries to see how they store this information. = (atlas.distanceRangeMiddle - atlas.distanceRange/2) / atlas.size = (atlas.distanceRangeMiddle + atlas.distanceRange/2) / atlas.sizeThe simplest thing is to draw pixels with distance_em < threshold_em:Hereâ€™s an implementation in a fragment shader: 300 es
precision ;
;
; ;

;
;
() {
   = (u_atlas, v_st).r;
   = (u_aemrange[1], u_aemrange[0], texel);
  o_frag_color = (distance_em < u_threshold_em ? 1.0 : 0.0);
}
How do we set the uniform values?Use . These represent the distances at pixel=0 and pixel=255.I recommend . Increase this (e.g. by +0.01) to make the font thicker.Weâ€™ll use other threshold values later to place outlines, shadows, and glow effects.To add antialiasing, we can go smoothly instead of abruptly from 0 to 1 using a transition. Think of the â€œcurvesâ€ tool in an image editor. Move the width slider to 0 to see how it looks with a hard threshold:When distance is  we want opacity 1.0. When distance is  we want opacity 0.0. Thatâ€™s a straight line with slope of -1/width. The formula works out as . Then we clamp opacity to 0.0â€“1.0. Hereâ€™s a shader implementation, avoiding the divide by multiplying by 1/width instead: 300 es
precision ;
;
;
;
;
;
;

() {
   = (u_atlas, v_st).r;
   = (u_aemrange[1], u_aemrange[0], texel);
   = u_screen_px_scale * u_antialias_per_em;
   = ((u_threshold_em - distance_em) * inverse_width + 0.5, 0.0, 1.0);
  o_frag_color = u_color * opacity; }
How do we set the uniform values?This value represents how much antialiasing happens per â€œemâ€ distance. I recommend antialiasing over 1 . We need to convert that to â€œemâ€ units, and we also need to factor in any scaling between the GL pixel size and the screen size (e.g. FSR/DLSS, or render to texture). In msdfgen, set the uniform to .If you know the size of the output text (common in 2D), calculate the width ahead of time on the CPU and pass it in as a uniform. In msdfgen, pick  glyph that contains  and , and calculate: = atlas.glyphs.find((g) => g.atlasBounds && g.planeBounds);
 = glyph.atlasBounds.right - glyph.atlasBounds.left;
 = (glyph.planeBounds.right - glyph.planeBounds.left) *
                   gl.canvas.width * fontSize;
 = outputSizePx / inputSizePx;
We can further optimize the shader by passing in  as a uniform.() {
   = ((u_atlas, 0));
   = (v_st);
   = atlas_size * gradient;
  (0.5 * (atlas_size, gradient) / (product.x * product.y), 1.0);
}
Tweaking antialiasing feels like a â€œdark artâ€ to me. Iâ€™ve collected some notes in the appendix.Valveâ€™s 2007 paper about SDF font rendering shows how to use a single distance field to represent fonts. A single distance field will have rounded corners. We can sharpen corners by increasing the resolution of the texture, but a better way is to use multiple signed distance fields (MSDF). The msdfgen library generates three distance fields and stores them in the red, green, and blue channels of the texture. When running , use  instead of .In the appendix I show more comparison screenshots at different resolutions, including examples where a single distance field looks nicer than multiple distance fields. I especially prefer the single distance field for glow and shadow effects. Each font + distance range behaves differently at different sizes, so I recommend comparing with your choice of font.In the shader, SDF and MSDF are similar. Following the msdfgen page, replace() {
   = (u_atlas, v_st).r;
  â€¦
}
() {
  ((rgb.r, rgb.g), ((rgb.r, rgb.g), rgb.b));
}

() {
   = median((u_atlas, v_st).rgb);
  â€¦
}
So far weâ€™ve covered how to render a single character. To render a string, weâ€™ll need two more ingredients:. This is a single â€œsprite sheetâ€ image that contains all the characters we might want to use in a bitmap or SDF font.. This calculates the location on screen for each character in the string. Also called â€œtext shapingâ€.Some libraries will calculate a single characterâ€™s SDF, and leave you to generate the atlas using your own sprite sheet / font atlas generator. I used  which generates both the SDF and the atlas at the same time. It stores the atlas texture coordinates in .The layout in many Western alphabets is left to right on each line. Thereâ€™s a â€œbaselineâ€ y value and a â€œcursorâ€ x value. After each character, we advance the cursor to the right. At the end of the line, we move the cursor back to the left an dincrease the y value. But many languages donâ€™t work the same way. For full layout across languages, consider using HarfBuzz.The appendix has more implementation details when using msdfgen.The 2007 work is much simpler and easier to implement on GPUs. However it suffers from rounded corners, and recommends multiple channels to get sharp corners. The later work is from 2016: Chlumsky not only wrote an amazing thesis about calculating and storing multiple distance the red/green/blue channels (not adaptive, and no voronoi), he wrote an open source msdfgen library that implemented the algorithm.This page was getting long so Iâ€™ve put some partially organized notes into an appendix page.]]></content:encoded></item><item><title>[Media] My Rust-based Git Client evolved into a full &quot;Developer Hub&quot; (HTTP Client, Mock Data Gen &amp; Monaco Editor built-in)</title><link>https://www.reddit.com/r/rust/comments/1rgff2e/media_my_rustbased_git_client_evolved_into_a_full/</link><author>/u/gusta_rsf</author><category>rust</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 18:54:05 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[A few months ago, I shared  here, a performant desktop Git client I built using Rust (Tauri) to escape heavy Electron wrappers. The feedback was incredibly helpful, but while using it daily, I noticed a lingering workflow issue: the constant context-switching. I was still alt-tabbing between my Git GUI, Postman, DB seeders, and a separate editor just to resolve simple conflicts or test an endpoint.So, I decided to expand the scope. ArezGit has officially evolved from just a version control tool into a unified , built to eliminate that friction.The Tech Stack remains snappy: Rust (handling heavy lifting, generation engines, and  bindings). React + TypeScript + Styled Components. Tauri for seamless, memory-efficient IPC.Whatâ€™s new in this evolution: Test your REST APIs directly inside your repository workspace. Support for custom headers, auth tokens, query params, and raw/form-data payloads.High-Performance Mock Data Generator: Visually build data schemas (UUIDs, names, emails, dates, etc) and let the Rust engine generate and export up to 1,000,000 rows to JSON, CSV, or raw SQL inserts.Native Monaco Code Editor: Edit files without leaving the app. Powered by the same engine as VS Code, featuring a multi-tab environment.Visual Conflict Resolver: A Monaco-based 3-way merge tool to handle "Ours vs Theirs" without the headache. Built-in Pomodoro timer, stopwatch, and a local task/notes manager inherently linked to your active project context.The app is packaged for  () and  (), with the macOS (Apple Silicon & Intel) build actively in the pipeline.It is still 100% Free for public repositories and open-source work (which includes access to the core Git features, the visual graph, and the Dev Hub tools).I built this to scratch my own itch for a distraction-free, high-performance environment, and I'd love to hear your thoughts on this "all-in-one" approach.]]></content:encoded></item><item><title>Aks cost analysis doubt</title><link>https://www.reddit.com/r/kubernetes/comments/1rgf90m/aks_cost_analysis_doubt/</link><author>/u/dqdevops</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 18:47:57 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have a question. Aks uses vmss as nodepools. does Aks cost analysis add on can be used to calculate all costs that the vmss will have. my question is if it takes in consideration when the vmss is not being used and so on. I have a big discrepancy between the Aks cost analysis price and the price I had to pay of that vmss. I guess Iâ€™m looking for a guide for having all the costs included (idle and unalocatted), or if this tool is not for this purpose.if you share documentation would be great!]]></content:encoded></item><item><title>How do I get to know in advance how far back I can go for the glibc version that can be used as the sysroot to build a modern compiler toolchain from source like GCC-16?</title><link>https://www.reddit.com/r/linux/comments/1rgf5ml/how_do_i_get_to_know_in_advance_how_far_back_i/</link><author>/u/emfloured</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 18:44:31 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[{Update}: My bad I could not be explicit about this. The goal here is to produce the most modern C++ compiler (the g++ executable + libstdc++.a) and this compiler should be able to produce binaries that should be able to run on oldest possible Linux OS. g++ manual shows --sysroot flag. If I am not wrong then this is the thing that allows me to set the path to glibc root directory of the version I want (for maximum ABI compatibility this will be the same glibc that is used to build the GCC compiler itself).The goal here is to build the cutting edge C++26 capable GCC compiler from source that can generate an executable that targets the oldest possible glibc runtime.There doesn't seem to be any docs in the gcc-trunk that tells you about this. GNU's official website also doesn't have this kind of information anywhere.I mean it's fair to assume that the  at the time of this post (some C++26) most likely can not be built with the glibc as its sysroot from year 1994 or even 2006.So what is the minimum number here? What is the proper way to know this? Is the trial-and-error; trying to build GCC using many older glibc versions the only way to know this what works or doesn't?Something tells me that the hacky methods to look at the glibc symbols version using ,  etc isn't the most reliable way, unless somebody tells me that IT IS the only way.]]></content:encoded></item><item><title>Meetup Go &amp; Robotics in Arcueil (south Paris) 11th march</title><link>https://golangfranca.org/en/news/rencontre-edge-golang-robotique-11-mars-2026/</link><author>/u/golangparis</author><category>golang</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 18:38:10 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[
As robots leave the laboratory to move into warehouses, hospitals and industrial sites, a strategic question emerges: which language and system to pilot a distributed, connected and mission-critical fleet?
This question will be addressed on the evening of Wednesday March 11 during the next Golang Paris meetup in partnership with MOVU Robotics, as part of the Field Day Go & Robotics day.]]></content:encoded></item><item><title>Good on Anthropic for declining the Pentagon deal</title><link>https://www.reddit.com/r/artificial/comments/1rgdx5q/good_on_anthropic_for_declining_the_pentagon_deal/</link><author>/u/Bubbly-Air7302</author><category>ai</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 17:59:40 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[shame on Sam Altman for putting usersâ€™ security at risk by trying to finagle a deal now. #TheRealAmericanPsycho   submitted by    /u/Bubbly-Air7302 ]]></content:encoded></item><item><title>Allocating on the Stack (go)</title><link>https://go.dev/blog/allocation-optimizations</link><author>/u/ketralnis</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 17:52:52 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Weâ€™re always looking for ways to make Go programs faster. In the last
2 releases, we have concentrated on mitigating a particular source of
slowness, heap allocations. Each time a Go program allocates memory
from the heap, thereâ€™s a fairly large chunk of code that needs to run
to satisfy that allocation. In addition, heap allocations present
additional load on the garbage collector.  Even with recent
enhancements like Green Tea, the garbage collector
still incurs substantial overhead.So weâ€™ve been working on ways to do more allocations on the stack
instead of the heap.  Stack allocations are considerably cheaper to
perform (sometimes completely free).  Moreover, they present no load
to the garbage collector, as stack allocations can be collected
automatically together with the stack frame itself. Stack allocations
also enable prompt reuse, which is very cache friendly.Stack allocation of constant-sized slicesConsider the task of building a slice of tasks to process:func process(c chan task) {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
Letâ€™s walk through what happens at runtime when pulling tasks from the
channel  and adding them to the slice .On the first loop iteration, there is no backing store for , so
 has to allocate one. Because it doesnâ€™t know how big the
slice will eventually be, it canâ€™t be too aggressive. Currently, it
allocates a backing store of size 1.On the second loop iteration, the backing store now exists, but it is
full.  again has to allocate a new backing store, this time of
size 2. The old backing store of size 1 is now garbage.On the third loop iteration, the backing store of size 2 is
full.  has to allocate a new backing store, this time
of size 4. The old backing store of size 2 is now garbage.On the fourth loop iteration, the backing store of size 4 has only 3
items in it.  can just place the item in the existing backing
store and bump up the slice length. Yay! No call to the allocator for
this iteration.On the fifth loop iteration, the backing store of size 4 is full, and
 again has to allocate a new backing store, this time of size
8.And so on. We generally double the size of the allocation each time it
fills up, so we can eventually append most new tasks to the slice
without allocation. But there is a fair amount of overhead in the
â€œstartupâ€ phase when the slice is small. During this startup phase we
spend a lot of time in the allocator, and produce a bunch of garbage,
which seems pretty wasteful. And it may be that in your program, the
slice never really gets large. This startup phase may be all you ever
encounter.If this code was a really hot part of your program, you might be
tempted to start the slice out at a larger size, to avoid all of these
allocations.func process2(c chan task) {
    tasks := make([]task, 0, 10) // probably at most 10 tasks
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
This is a reasonable optimization to do. It is never incorrect; your
program still runs correctly. If the guess is too small, you get
allocations from  as before. If the guess is too large, you
waste some memory.If your guess for the number of tasks was a good one, then thereâ€™s
only one allocation site in this program. The  call allocates a
slice backing store of the correct size, and  never has to do
any reallocation.The surprising thing is that if you benchmark this code with 10
elements in the channel, youâ€™ll see that you didnâ€™t reduce the number
of allocations to 1, you reduced the number of allocations to 0!The reason is that the compiler decided to allocate the backing store
on the stack. Because it knows what size it needs to be (10 times the
size of a task) it can allocate storage for it in the stack frame of
 instead of on the heap.  Note
that this depends on the fact that the backing store does not escape
to the heap inside of .Stack allocation of variable-sized slicesBut of course, hard coding a size guess is a bit rigid.
Maybe we can pass in an estimated length?func process3(c chan task, lengthGuess int) {
    tasks := make([]task, 0, lengthGuess)
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
This lets the caller pick a good size for the  slice, which may
vary depending on where this code is being called from.Unfortunately, in Go 1.24 the non-constant size of the backing store
means the compiler can no longer allocate the backing store on the
stack.  It will end up on the heap, converting our 0-allocation code
to 1-allocation code. Still better than having  do all the
intermediate allocations, but unfortunate.But never fear, Go 1.25 is here!Imagine you decide to do the following, to get the stack allocation
only in cases where the guess is small:func process4(c chan task, lengthGuess int) {
    var tasks []task
    if lengthGuess <= 10 {
        tasks = make([]task, 0, 10)
    } else {
        tasks = make([]task, 0, lengthGuess)
    }
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
Kind of ugly, but it would work. When the guess is small, you use a
constant size  and thus a stack-allocated backing store, and
when the guess is larger you use a variable size  and allocate
the backing store from the heap.But in Go 1.25, you donâ€™t need to head down this ugly road. The Go
1.25 compiler does this transformation for you!  For certain slice
allocation locations, the compiler automatically allocates a small
(currently 32-byte) slice backing store, and uses that backing store
for the result of the  if the size requested is small
enough. Otherwise, it uses a heap allocation as normal.In Go 1.25,  performs zero heap allocations, if
 is small enough that a slice of that length fits into 32
bytes. (And of course that  is a correct guess for how
many items are in .)Weâ€™re always improving the performance of Go, so upgrade to the latest
Go release and be
surprised by
how much faster and memory efficient your program becomes!Stack allocation of append-allocated slicesOk, but you still donâ€™t want to have to change your API to add this
weird length guess. Anything else you could do?func process(c chan task) {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
In Go 1.26, we allocate the same kind of small, speculative backing
store on the stack, but now we can use it directly at the 
site.On the first loop iteration, there is no backing store for , so
 uses a small, stack-allocated backing store as the first
allocation. If, for instance, we can fit 4 s in that backing store,
the first  allocates a backing store of length 4 from the stack.The next 3 loop iterations append directly to the stack backing store,
requiring no allocation.On the 4th iteration, the stack backing store is finally full and we
have to go to the heap for more backing store. But we have avoided
almost all of the startup overhead described earlier in this article.
No heap allocations of size, 1, 2, and 4, and none of the garbage that
they eventually become. If your slices are small, maybe you will never
have a heap allocation.Stack allocation of append-allocated escaping slicesOk, this is all good when the  slice doesnâ€™t escape. But what if
Iâ€™m returning the slice? Then it canâ€™t be allocated on the stack, right?Right! The backing store for the slice returned by  below
canâ€™t be allocated on the stack, because the stack frame for 
disappears when  returns.func extract(c chan task) []task {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    return tasks
}
But you might think, the  slice canâ€™t be allocated on the
stack. But what about all those intermediate slices that just become
garbage? Maybe we can allocate those on the stack?func extract2(c chan task) []task {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    tasks2 := make([]task, len(tasks))
    copy(tasks2, tasks)
    return tasks2
}
Then the  slice never escapes . It can benefit from
all of the optimizations described above. Then at the very end of
, when we know the final size of the slice, we do one heap
allocation of the required size, copy our s into it, and return
the copy.But do you really want to write all that additional code? It seems
error prone. Maybe the compiler can do this transformation for us?For escaping slices, the compiler will transform the original 
code to something like this:func extract3(c chan task) []task {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    tasks = runtime.move2heap(tasks)
    return tasks
}
 is a special compiler+runtime function that is the
identity function for slices that are already allocated in the heap.
For slices that are on the stack, it allocates a new slice on the
heap, copies the stack-allocated slice to the heap copy, and returns
the heap copy.This ensures that for our original  code, if the number of
items fits in our small stack-allocated buffer, we perform exactly 1
allocation of exactly the right size. If the number of items exceeds
the capacity our small stack-allocated buffer, we do our normal
doubling-allocation once the stack-allocated buffer overflows.The optimization that Go 1.26 does is actually better than the
hand-optimized code, because it does not require the extra
allocation+copy that the hand-optimized code always does at the end.
It requires the allocation+copy only in the case that weâ€™ve exclusively
operated on a stack-backed slice up to the return point.We do pay the cost for a copy, but that cost is almost completely
offset by the copies in the startup phase that we no longer have to
do. (In fact, the new scheme at worst has to copy one more element
than the old scheme.)Hand optimization can still be beneficial, especially if you have a
good estimate of the slice size ahead of time. But hopefully the
compiler will now catch a lot of the simple cases for you and allow
you to focus on the remaining ones that really matter.There are a lot of details that the compiler needs to ensure to get
all these optimizations right. If you think that one of these
optimizations is causing correctness or (negative) performance issues
for you, you can turn them off with
-gcflags=all=-d=variablemakehash=n. If turning these optimizations
off helps, please file an issue so we can investigate. Go stacks do not have any -style mechanism for
dynamically-sized stack frames. All Go stack frames are constant
sized.]]></content:encoded></item><item><title>Helm in production: hard-won lessons and gotchas</title><link>https://blog.sneakybugs.com/helm-production-lessons/</link><author>/u/LKummer</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 17:44:16 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Evolution of Async Rust: From Tokio to High-Level Applications</title><link>https://blog.jetbrains.com/rust/2026/02/17/the-evolution-of-async-rust-from-tokio-to-high-level-applications/</link><author>/u/carllerche</author><category>rust</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 17:23:06 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What crate in rust should I understand the most before\after getting into rust async and parallel computing?</title><link>https://www.reddit.com/r/rust/comments/1rgc5ww/what_crate_in_rust_should_i_understand_the_most/</link><author>/u/rudv-ar</author><category>rust</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 16:58:06 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I have been learning rust for past one month, slow but still learning. I have just completed borrowing and functions in rust. Next I have lifetimes. To have a solid grasp and understanding of rust basics, what should I do? And also.. The rust async is next in my learning path. Is there any specific crate I should learn other than default async in rust? When should I learn it? Before Or after async? After Long Comments : Note Yo. Dont downvote me ya. Otherwise my account will vanish. Reddit has a very strict spam detection system and I dont want my account gone just like that. This is a new account. I was just seeking help without knowing what to do. And I am in college. So kindly help me. Correct me if I did some mistake. I want this personal account very much. ]]></content:encoded></item><item><title>Allocating on the Stack</title><link>https://go.dev/blog/allocation-optimizations</link><author>/u/Deleis</author><category>golang</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 16:35:35 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Weâ€™re always looking for ways to make Go programs faster. In the last
2 releases, we have concentrated on mitigating a particular source of
slowness, heap allocations. Each time a Go program allocates memory
from the heap, thereâ€™s a fairly large chunk of code that needs to run
to satisfy that allocation. In addition, heap allocations present
additional load on the garbage collector.  Even with recent
enhancements like Green Tea, the garbage collector
still incurs substantial overhead.So weâ€™ve been working on ways to do more allocations on the stack
instead of the heap.  Stack allocations are considerably cheaper to
perform (sometimes completely free).  Moreover, they present no load
to the garbage collector, as stack allocations can be collected
automatically together with the stack frame itself. Stack allocations
also enable prompt reuse, which is very cache friendly.Stack allocation of constant-sized slicesConsider the task of building a slice of tasks to process:func process(c chan task) {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
Letâ€™s walk through what happens at runtime when pulling tasks from the
channel  and adding them to the slice .On the first loop iteration, there is no backing store for , so
 has to allocate one. Because it doesnâ€™t know how big the
slice will eventually be, it canâ€™t be too aggressive. Currently, it
allocates a backing store of size 1.On the second loop iteration, the backing store now exists, but it is
full.  again has to allocate a new backing store, this time of
size 2. The old backing store of size 1 is now garbage.On the third loop iteration, the backing store of size 2 is
full.  has to allocate a new backing store, this time
of size 4. The old backing store of size 2 is now garbage.On the fourth loop iteration, the backing store of size 4 has only 3
items in it.  can just place the item in the existing backing
store and bump up the slice length. Yay! No call to the allocator for
this iteration.On the fifth loop iteration, the backing store of size 4 is full, and
 again has to allocate a new backing store, this time of size
8.And so on. We generally double the size of the allocation each time it
fills up, so we can eventually append most new tasks to the slice
without allocation. But there is a fair amount of overhead in the
â€œstartupâ€ phase when the slice is small. During this startup phase we
spend a lot of time in the allocator, and produce a bunch of garbage,
which seems pretty wasteful. And it may be that in your program, the
slice never really gets large. This startup phase may be all you ever
encounter.If this code was a really hot part of your program, you might be
tempted to start the slice out at a larger size, to avoid all of these
allocations.func process2(c chan task) {
    tasks := make([]task, 0, 10) // probably at most 10 tasks
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
This is a reasonable optimization to do. It is never incorrect; your
program still runs correctly. If the guess is too small, you get
allocations from  as before. If the guess is too large, you
waste some memory.If your guess for the number of tasks was a good one, then thereâ€™s
only one allocation site in this program. The  call allocates a
slice backing store of the correct size, and  never has to do
any reallocation.The surprising thing is that if you benchmark this code with 10
elements in the channel, youâ€™ll see that you didnâ€™t reduce the number
of allocations to 1, you reduced the number of allocations to 0!The reason is that the compiler decided to allocate the backing store
on the stack. Because it knows what size it needs to be (10 times the
size of a task) it can allocate storage for it in the stack frame of
 instead of on the heap.  Note
that this depends on the fact that the backing store does not escape
to the heap inside of .Stack allocation of variable-sized slicesBut of course, hard coding a size guess is a bit rigid.
Maybe we can pass in an estimated length?func process3(c chan task, lengthGuess int) {
    tasks := make([]task, 0, lengthGuess)
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
This lets the caller pick a good size for the  slice, which may
vary depending on where this code is being called from.Unfortunately, in Go 1.24 the non-constant size of the backing store
means the compiler can no longer allocate the backing store on the
stack.  It will end up on the heap, converting our 0-allocation code
to 1-allocation code. Still better than having  do all the
intermediate allocations, but unfortunate.But never fear, Go 1.25 is here!Imagine you decide to do the following, to get the stack allocation
only in cases where the guess is small:func process4(c chan task, lengthGuess int) {
    var tasks []task
    if lengthGuess <= 10 {
        tasks = make([]task, 0, 10)
    } else {
        tasks = make([]task, 0, lengthGuess)
    }
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
Kind of ugly, but it would work. When the guess is small, you use a
constant size  and thus a stack-allocated backing store, and
when the guess is larger you use a variable size  and allocate
the backing store from the heap.But in Go 1.25, you donâ€™t need to head down this ugly road. The Go
1.25 compiler does this transformation for you!  For certain slice
allocation locations, the compiler automatically allocates a small
(currently 32-byte) slice backing store, and uses that backing store
for the result of the  if the size requested is small
enough. Otherwise, it uses a heap allocation as normal.In Go 1.25,  performs zero heap allocations, if
 is small enough that a slice of that length fits into 32
bytes. (And of course that  is a correct guess for how
many items are in .)Weâ€™re always improving the performance of Go, so upgrade to the latest
Go release and be
surprised by
how much faster and memory efficient your program becomes!Stack allocation of append-allocated slicesOk, but you still donâ€™t want to have to change your API to add this
weird length guess. Anything else you could do?func process(c chan task) {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
In Go 1.26, we allocate the same kind of small, speculative backing
store on the stack, but now we can use it directly at the 
site.On the first loop iteration, there is no backing store for , so
 uses a small, stack-allocated backing store as the first
allocation. If, for instance, we can fit 4 s in that backing store,
the first  allocates a backing store of length 4 from the stack.The next 3 loop iterations append directly to the stack backing store,
requiring no allocation.On the 4th iteration, the stack backing store is finally full and we
have to go to the heap for more backing store. But we have avoided
almost all of the startup overhead described earlier in this article.
No heap allocations of size, 1, 2, and 4, and none of the garbage that
they eventually become. If your slices are small, maybe you will never
have a heap allocation.Stack allocation of append-allocated escaping slicesOk, this is all good when the  slice doesnâ€™t escape. But what if
Iâ€™m returning the slice? Then it canâ€™t be allocated on the stack, right?Right! The backing store for the slice returned by  below
canâ€™t be allocated on the stack, because the stack frame for 
disappears when  returns.func extract(c chan task) []task {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    return tasks
}
But you might think, the  slice canâ€™t be allocated on the
stack. But what about all those intermediate slices that just become
garbage? Maybe we can allocate those on the stack?func extract2(c chan task) []task {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    tasks2 := make([]task, len(tasks))
    copy(tasks2, tasks)
    return tasks2
}
Then the  slice never escapes . It can benefit from
all of the optimizations described above. Then at the very end of
, when we know the final size of the slice, we do one heap
allocation of the required size, copy our s into it, and return
the copy.But do you really want to write all that additional code? It seems
error prone. Maybe the compiler can do this transformation for us?For escaping slices, the compiler will transform the original 
code to something like this:func extract3(c chan task) []task {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    tasks = runtime.move2heap(tasks)
    return tasks
}
 is a special compiler+runtime function that is the
identity function for slices that are already allocated in the heap.
For slices that are on the stack, it allocates a new slice on the
heap, copies the stack-allocated slice to the heap copy, and returns
the heap copy.This ensures that for our original  code, if the number of
items fits in our small stack-allocated buffer, we perform exactly 1
allocation of exactly the right size. If the number of items exceeds
the capacity our small stack-allocated buffer, we do our normal
doubling-allocation once the stack-allocated buffer overflows.The optimization that Go 1.26 does is actually better than the
hand-optimized code, because it does not require the extra
allocation+copy that the hand-optimized code always does at the end.
It requires the allocation+copy only in the case that weâ€™ve exclusively
operated on a stack-backed slice up to the return point.We do pay the cost for a copy, but that cost is almost completely
offset by the copies in the startup phase that we no longer have to
do. (In fact, the new scheme at worst has to copy one more element
than the old scheme.)Hand optimization can still be beneficial, especially if you have a
good estimate of the slice size ahead of time. But hopefully the
compiler will now catch a lot of the simple cases for you and allow
you to focus on the remaining ones that really matter.There are a lot of details that the compiler needs to ensure to get
all these optimizations right. If you think that one of these
optimizations is causing correctness or (negative) performance issues
for you, you can turn them off with
-gcflags=all=-d=variablemakehash=n. If turning these optimizations
off helps, please file an issue so we can investigate. Go stacks do not have any -style mechanism for
dynamically-sized stack frames. All Go stack frames are constant
sized.]]></content:encoded></item><item><title>Understanding alignment - from source to object file (C++)</title><link>https://maskray.me/blog/2025-08-24-understanding-alignment-from-source-to-object-file</link><author>/u/ketralnis</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 16:10:06 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Alignment refers to the practice of placing data or code at memory
addresses that are multiples of a specific value, typically a power of
2. This is typically done to meet the requirements of the programming
language, ABI, or the underlying hardware. Misaligned memory accesses
might be expensive or will cause traps on certain architectures.This blog post explores how alignment is represented and managed as
C++ code is transformed through the compilation pipeline: from source
code to LLVM IR, assembly, and finally the object file. We'll focus on
alignment for both variables and functions.Alignment in C++ source codeObject types have alignment requirements ([basic.fundamental],
[basic.compound]) which place restrictions on the addresses at which an
object of that type may be allocated. An alignment is an
implementation-defined integer value representing the number of bytes
between successive addresses at which a given object can be allocated.
An object type imposes an alignment requirement on every object of that
type; stricter alignment can be requested using the alignment specifier
([dcl.align]). Attempting to create an object ([intro.object]) in
storage that does not meet the alignment requirements of the object's
type is undefined behavior. can be used to request a stricter alignment. [decl.align]An alignment-specifier may be applied to a variable or to a class
data member, but it shall not be applied to a bit-field, a function
parameter, or an exception-declaration ([except.handle]). An
alignment-specifier may also be applied to the declaration of a class
(in an elaborated-type-specifier ([dcl.type.elab]) or class-head
([class]), respectively). An alignment-specifier with an ellipsis is a
pack expansion ([temp.variadic]).If the strictest  on a declaration is weaker than
the alignment it would have without any alignas specifiers, the program
is ill-formed.However, the GNU extension __attribute__((aligned(1)))
can request a weaker alignment.In the LLVM Intermediate Representation (IR), both global variables
and functions can have an  attribute to specify their
required alignment.An explicit alignment may be specified for a global, which must be a
power of 2. If not present, or if the alignment is set to zero, the
alignment of the global is set by the target to whatever it feels
convenient. If an explicit alignment is specified, the global is forced
to have exactly that alignment. Targets and optimizers are not allowed
to over-align the global if the global has an assigned section. In this
case, the extra alignment could be observable: for example, code could
assume that the globals are densely packed in their section and try to
iterate over them as an array, alignment padding would break this
iteration. For TLS variables, the module flag MaxTLSAlign, if present,
limits the alignment to the given value. Optimizers are not allowed to
impose a stronger alignment on these variables. The maximum alignment is
1 << 32.An explicit alignment may be specified for a function. If not
present, or if the alignment is set to zero, the alignment of the
function is set by the target to whatever it feels convenient. If an
explicit alignment is specified, the function is forced to have at least
that much alignment. All alignments must be a power of 2.An explicit preferred alignment () may also be
specified for a function definition (must be a power of 2). Unlike
, it is a hint: the final alignment will generally
land somewhere between the minimum and preferred values. If absent, the
preferred alignment is determined in a target-specific way
(STI->getTargetLowering()->getPrefFunctionAlignment()).
(https://discourse.llvm.org/t/rfc-enhancing-function-alignment-attributes/88019/3)In addition,  can be used in parameter attributes
to decorate a pointer or vector of pointers.LLVM back end representationAsmPrinter::emitGlobalVariable determines the alignment for
global variables based on a set of nuanced rules:With an explicit alignment (),
If the variable has a section attribute, return
.Otherwise, compute a preferred alignment for the data layout
(, referred to as ).
Return
pref < explicit ? explicit : max(E, getABITypeAlign).Without an explicit alignment: return
. employs a heuristic for global variable
definitions: if the variable's size exceeds 16 bytes and the preferred
alignment is less than 16 bytes, it sets the alignment to 16 bytes. This
heuristic balances performance and memory efficiency for common cases,
though it may not be optimal for all scenarios. (See Preferred
alignment of globals > 16bytes in 2012)For assembly output, AsmPrinter emits  (power of
2 alignment) directives with a zero fill value (i.e. the padding bytes
are zeros).  For functions,
AsmPrinter::emitFunctionHeader emits alignment directives
based on the machine function's alignment settings. sets the 
alignment from the subtarget:The  alignment is computed separately by
MachineFunction::getPreferredAlignment():For example,  sets the preferred
function alignment to 16.How these are emitted depends on whether the integrated assembler,
 support, and function sections are all
active:
(): emit  using the
explicit  attribute value (if present), then
 for the preferred alignment. A function without
an explicit  attribute gets only
 (no ).Without function sections: emit
 using the preferred alignment (old behavior).
 is not used here because its benefit is tied to
section size equalling function size (see below).The emitted  directives omit the fill value
argument: for code sections, this space is filled with no-op
instructions.GNU Assembler supports multiple alignment directives:: align to 2**3: this is identical to  on
some targets and  on the others. (LLVM extension): sets the section's
 alignment. Unlike , the actual
 stored in the object file is chosen based on
the section size: if the section is smaller than , the
alignment is rounded up to the next power of 2 â‰¥ size (rather than
always being ). This allows the linker to pack small
functions more tightly while still aligning larger ones.
 size :
 smallest power of 2 â‰¥ size
This is only useful when each function has its own section
(), so that section size equals function
size and  effectively encodes per-function
alignment. With a merged  section the total size is
always large, so  ends up at 
regardless. Moreover,  only controls where the
linker places the section start; alignment between individual functions
within a merged section comes from  NOP padding
embedded in the section body, which  does not
affect.Clang supports "direct object emission" (
typically bypasses a separate assembler), the LLVMAsmPrinter directly
uses the  API. This allows Clang to emit
the machine code directly into the object file, bypassing the need to
parse and interpret alignment directives and instructions from a
text-based assembly file.These alignment directives has an optional third argument: the
maximum number of bytes to skip. If doing the alignment would require
skipping more bytes than the specified maximum, the alignment is not
done at all. GCC's  utilizes this
feature.In an object file, the section alignment is determined by the
strictest alignment directive present in that section. The assembler
sets the section's overall alignment to the maximum of all these
directives, as if an implicit directive were at the start.This alignment is stored in the  field
within the ELF section header table. You can inspect this value using
tools such as  () or
 ().The linker combines multiple object files into a single executable.
When it maps input sections from each object file into output sections
in the final executable, it ensures that section alignments specified in
the object files are preserved.How the linker handles
section alignment: This is the maximum
 value among all its contributing input
sections. This ensures the strictest alignment requirements are met.: The linker also uses input
 information to position each input section
within the output section. As illustrated in the following example, each
input section (like  or )
is aligned according to its  value before being
placed sequentially. A linker script can override the
default alignment behavior. The  keyword enforces a
stricter alignment. For example .text : ALIGN(32) { ... }
aligns the section to at least a 32-byte boundary. This is often done to
optimize for specific hardware or for memory mapping requirements.The  keyword on an output section overrides the
input section alignments.: To achieve the required alignment, the
linker may insert padding between sections or before the first input
section (if there is a gap after the output section start). The fill
value is determined by the following rules:If a non-code section, use zero.Otherwise, use a trap or no-op instructin.Padding and section
reorderingLinkers typically preserve the order of input sections from object
files. To minimize the padding required between sections, linker scripts
can use a  keyword to arrange input
sections in descending order of their alignment requirements. Similarly,
GNU ld supports 
to sort COMMON symbols by decreasing alignment.While this sorting can reduce wasted space, modern linking strategies
often prioritize other factors, such as cache locality (for performance)
and data similarity (for Lempelâ€“Ziv compression ratio), which can
conflict with sorting by alignment. (Search
 on Explain GNU style
linker options).The alignment of a variable or function can be as large as the system
page size. Some implementations allow a larger alignment. (Over-aligned
segment)Some platforms have special rules. For example,On SystemZ, the  (load address relative long)
instruction cannot generate odd addresses. To prevent GOT indirection,
compilers ensure that symbols are at least aligned by 2. (Toolchain
notes on z/Architecture)On AIX, the default alignment mode is : for double
and long double, the first member of this data type is aligned according
to its natural alignment value; subsequent members of the aggregate are
aligned on 4-byte boundaries. (https://reviews.llvm.org/D79719)The standard representation of the the Itanium C++ ABI requires
member function pointers to be even, to distinguish between virtual and
non-virtual functions.In the standard representation, a member function pointer for a
virtual function is represented with ptr set to 1 plus the function's
v-table entry offset (in bytes), converted to a function pointer as if
by
reinterpret_cast<fnptr_t>(uintfnptr_t(1 + offset)),
where  is an unsigned integer of the same size
as .Conceptually, a pointer to member function is a tuple:A function pointer or virtual table index, discriminated by the
least significant bitA displacement to apply to the  pointerDue to the least significant bit discriminator, members function need
a stricter alignment even if __attribute__((aligned(1))) is
specified:Architecture considerationsContemporary architectures generally support unaligned memory access,
likely with very small performance penalties. However, some
implementations might restrict or penalize unaligned accesses heavily,
or require specific handling. Even on architectures supporting unaligned
access, atomic operations might still require alignment.On AArch64, a bit in the system control register
 enables alignment check.On x86, if the AM bit is set in the CR0 register and the AC bit is
set in the EFLAGS register, alignment checking of user-mode data
accessing is enabled.Linux's RISC-V port supports
prctl(PR_SET_UNALIGN, PR_UNALIGN_SIGBUS); to enable strict
alignment.clang -fsanitize=alignment can detect misaligned memory
access. Check out my write-up.In 1989, US Patent 4814976, which covers "RISC computer with
unaligned reference handling and method for the same" (4 instructions:
lwl, lwr, swl, and swr), was granted to MIPS Computer Systems Inc. It
caused a barrier for other RISC processors, see The Lexra Story.Almost every microprocessor in the world can emulate the
functionality of unaligned loads and stores in software. MIPS
Technologies did not invent that. By any reasonable interpretation of
the MIPS Technologies' patent, Lexra did not infringe. In mid-2001 Lexra
received a ruling from the USPTO that all claims in the the lawsuit were
invalid because of prior art in an IBM CISC patent. However, MIPS
Technologies appealed the USPTO ruling in Federal court, adding to
Lexra's legal costs and hurting its sales. That forced Lexra into an
unfavorable settlement. The patent expired on December 23, 2006 at which
point it became legal for anybody to implement the complete MIPS-I
instruction set, including unaligned loads and stores.GCC offers a family of performance-tuning options named
, that instruct the compiler to align certain code
segments to specific memory boundaries. These options might improve
performance by preventing certain instructions from crossing cache line
boundaries (or instruction fetch boundaries), which can otherwise cause
an extra cache miss.: Align functions.: Align branch targets.: Align branch targets, for branch
targets where the targets can only be reached by jumping.: Align the beginning of loops.Inefficiency with Small Functions: Aligning small
functions can be inefficient and may not be worth the overhead. To
address this, GCC introduced -flimit-function-alignment in
2016. The option sets  directive's max-skip operand
to the estimated function size minus one.The max-skip operand, if present, is evaluated at parse time, so you
cannot do: In LLVM, the x86 backend does not implement
TargetInstrInfo::getInstSizeInBytes, making it challenging
to implement -flimit-function-alignment.: These options don't apply to cold
functions. To ensure that cold functions are also aligned, use
-fmin-function-alignment=n instead.: Aligning functions can make benchmarks
more reliable. For example, on x86-64, a hot function less than 32 bytes
might be placed in a way that uses one or two cache lines (determined by
function_addr % cache_line_size), making benchmark results
noisy. Using  can ensure the function
always occupies a single cache line, leading to more consistent
performance measurements.LLVM notes: In clang/lib/CodeGen/CodeGenModule.cpp,
 and 
now set  the minimum alignment and the preferred
alignment (consistent with GCC). The separate
-fpreferred-function-alignment=N option controls only the
preferred alignment hint without affecting the minimum.A hardware loop typically consistants of 3 parts:A low-overhead loop (also called a zero-overhead loop) is a
hardware-assisted looping mechanism found in many processor
architectures, particularly digital signal processors (DSPs). The
processor includes dedicated registers that store the loop start
address, loop end address, and loop count. A hardware loop typically
consists of three components:Loop setup instruction: Sets the loop end address and iteration
countLoop body: Contains the actual instructions to be repeatedLoop end instruction: Jumps back to the loop body if further
iterations are requiredHere is an example from Arm v8.1-M low-overhead branch extension.To minimize the number of cache lines used by the loop body, ideally
the loop body (the instruction immediately following DLS) should be
aligned to a 64-byte boundary. However, GNU Assembler lacks a directive
to specify alignment like "align DLS to a multiple of 64 plus 60 bytes."
Inserting an alignment after the DLS is counterproductive, as it would
introduce unwanted NOP instructions at the beginning of the loop body,
negating the performance benefits of the low-overhead loop
mechanism.It would be desirable to simulate the functionality with
.org ((.+4+63) & -64) - 4  // ensure that .+4 is aligned to 64-byte boundary,
but this complex expression involves bitwise AND and is not a
relocatable expression. LLVM integrated assembler would report
expected absolute expression while GNU Assembler has a
similar error.A potential solution would be to extend the alignment directives with
an optional offset parameter:]]></content:encoded></item><item><title>The proposal for generic methods for Go has been officially accepted</title><link>https://github.com/golang/go/issues/77273#issuecomment-3962618141</link><author>/u/ketralnis</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 16:07:16 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>80386 Protection</title><link>https://nand2mario.github.io/posts/2026/80386_protection/</link><author>/u/ketralnis</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 16:06:13 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>People are STILL Writing JavaScript &quot;DRM&quot;</title><link>https://the-ranty-dev.vercel.app/javascript-drms-are-stupid</link><author>/u/medy17</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 15:46:15 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[A while back, I was browsing Reddit and came across a thread about hotaudio.net. For those unfamiliar, itâ€™s a website developed by u/fermaw, the very same developer behind the ever-popular gwasi.com.If neither of those websites rings a bell, then I need to welcome you to r/GoneWildAudio: an NSFW subreddit for ASMR. Stay and read, the ASMR is only part of this odd tale.You see, not too long ago, Soundgasm, Mega, and a few others were quite popular for hosting these audios, but as ToS tightened and taboo topics got more taboo, other platforms popped up to fill the gap.HotAudio is one of them, but in a different way. Their claim is offering DRM for ASMRtistsâ€”a rare thing in the ASMR space, let alone the NSFW ASMR space.u/fermaw, the aforementioned developer, was bragging in that thread I mentioned earlier about coding a DRM and how he found it rather â€œfunâ€ to do so.I have no doubt it was fun, and believe me, this post is not meant to ridicule anyone or incite any form of hate, but I think calling it â€œDRMâ€ is a little far-fetched.Long before the days of Denuvo, the now-infamous game DRM, we knew that any such system living in the userâ€™s accessible memory was vulnerable. So, we shifted to what we call today a Trusted Execution Environment (TEE).Iâ€™d like to quote Microsoft here: â€œA Trusted Execution Environment is a segregated area of memory and CPU thatâ€™s protected from the rest of the CPU by using encryption. Any code outside that environment canâ€™t read or tamper with the data in the TEE. Authorized code can manipulate the data inside the TEE.â€See what Iâ€™m getting at? JavaScript code is fundamentally a â€œuserlandâ€ thing. The code you ship is accessible to the user to modify and fuck about with however they wish.This is the problem with u/fermawâ€™s â€œDRM.â€ No matter how many clever keys, nonces, and encrypted file formats he attempts to send to the user, eventually, the very same JavaScript code will need to exit his decryption logic andâ€”whoopsâ€”it goes plain Jane into digital and straight to the speakers.On Elephants in the Room: Trusted Execution EnvironmentsBefore we get into the code, we need to understand why this was always going to end in a bloodbath. The entire history of DRM is, at its core, a history of trying to give someone a locked box while simultaneously handing them the fucking key. The film and music industries have been losing this battle since the first CSS-encrypted DVD was cracked in 1999.The modern, professional answer to this problem is the Trusted Execution Environment, or TEE.As quoted above, a TEE is a hardware-backed secure area of the main processor (like ARM TrustZone or Intel SGX). Technically speaking, the TEE is just the hardware fortress (exceptions exist like TrustZone) whilst a Content Decryption Module (CDM) like Googleâ€™s Widevine, Appleâ€™s FairPlay, and Microsoftâ€™s PlayReady use the TEE to ensure cryptographic keys and decrypted media buffers are never exposed to the host operating system let alone the userâ€™s browser. For the purposes of this article, I may at times refer to them interchangeably but all you need to know is that they work together and in any case, the host OS canâ€™t whiff any of their farts so to speak.However, getting a Widevine licence requires a licensing agreement with Google. It requires native binary integration. It requires infrastructure, legal paperwork, not to mention, shitloads of money. A small NSFW audio hosting platform is not going to get a Widevine licence. Theyâ€™d be lucky if Google even returned their emails. Okay maybe not quite but the point is theyâ€™re not getting Widevine.So what does HotAudio do then? Based on everything I could observe, they implement a custom JavaScript-based decryption scheme. The audio is served in an encrypted format chunked via the MediaSource Extensions (MSE) API and then the player fetches, decrypts, and feeds each chunk to the browserâ€™s audio engine in real time. Itâ€™s a reasonable-ish approach for a small platform. It stops casual right-clickers. It stops people opening the network tab and downloading the raw response file, only to discover it wonâ€™t play. For most users, that friction is sufficient.Unfortunately for HotAudio, every r/DataHoarder user worth their salt knows these types of websites donâ€™t have proper blackbox DRMs so itâ€™s only a matter of time before someone with a tool they crafted with spit and spite shows up.It just doesnâ€™t stop someone who understands exactly where the decrypted data has to appear.The â€œPCM Boundaryâ€: a Wannabe-DRM GraveyardLet me introduce you to what I call the . PCM (Pulse-Code Modulation) is the raw, uncompressed digital audio format that eventually gets sent to your speakers. Itâ€™s the terminal endpoint of every audio pipeline, regardless of how aggressively encrypted the source was.graph TD
    Server[HotAudio Server] -->|Sends Encrypted audio chunks| JS[JavaScript Player]
    JS -->|Decrypts using proprietary logic| DecryptedData([Decrypted Data])
    DecryptedData -->|Calls appendBuffer| Hook[Hook]
    
    Hook -.->|GOLDEN INTERCEPT| SavedAudio[(Captures Pristine Audio File)]
    
    Hook -->|Forwards genuine appendBuffer| MSE[MediaSource API]
    MSE -->|Feeds to codec decoder| Decoder[Browser Decoder]
    Decoder -->|PCM audio output| Speakers[Speakers]For our purposes, we donâ€™t even need to chase it all the way to raw PCM which is valid avenue albeit in the realm of WEBRips and not defacto â€œdownloaders.â€  just need to find the last point in the pipeline where data is still accessible to JavaScript and that point is the MediaSource Extensions API, specifically the SourceBuffer.appendBuffer() method.Your JavaScript code creates a  object and attaches it to a  or  element via a blob URL.You call mediaSource.addSourceBuffer(mimeType) to declare what codec format youâ€™ll be feeding the buffer.You repeatedly call sourceBuffer.appendBuffer(data) to push chunks of (in our case, pre-decrypted) encoded audio data to the browser.The browserâ€™s internal decoder handles the rest: decoding the codec, managing the playback timeline, and routing audio to the hardware.Notice how by step 3, the time HotAudioâ€™s player calls , the data has already been decrypted by their JavaScript code. It has to be. The browserâ€™s built-in AAC or Opus decoder doesnâ€™t know a damn thing about HotAudioâ€™s proprietary encryption scheme. It only speaks standard codecs. The decryption must happen in JavaScript before the data is handed to the browser.This means there is a golden moment: the exact instant between â€œHotAudioâ€™s player finishes decrypting a chunkâ€ and â€œthat chunk is handed to the browserâ€™s media engine.â€ If you can intercept  at that instant, you receive every chunk in its pristine, fully decrypted state, on a silver fucking platter.Anyways, that is the fundamental vulnerability that no amount of encryption-decryption pipeline sophistication can close. You can make the key as complicated as you like. You can rotate keys per session, per user, per chunk. But eventually, the data has to come out the other end in a form the browser can decode. And that moment is yours to intercept.Now. Letâ€™s talk about how this little war actually played out. Dramatised and Ribbedâ„¢ for your pleasure.Act One: Smash and Grab (V1.0)The first version of my extension was built on a simple observation: HotAudioâ€™s player was exposing its active audio instance as a global variable. You could just type  into the browser console and there it was; The entire audio source object, sitting in the open like a wallet left on a park bench.The approach had two parts. The extension would attempt to modify a JavaScript file that was always shipped with every request: .Essentially, this specific block would be appended to the top of nozzle.js before the stream had even begun which would compromise the environment from the get go.This is, without exaggeration, a client-side Man-in-the-Middle attack baked directly into the browserâ€™s extension API. The site requests its player script; the extension intercepts that network request at the manifest level and silently substitutes its own poisoned version. HotAudioâ€™s server never even knows.Once the hook was in place, the automation script grabbed , muted it, slammed the playback rate to  (canâ€™t go faster since that is the maximum supported by browsers), and sat back as the browser frantically decoded and fed chunks into the collection array. When the  event fired, the chunks were stitched together with  and downloaded as an  file.Of course, this was a patch war. According to various Reddit threads and GitHub Issues, fermaw is known for patrolling subreddits and Issues looking for ways in which devs have attempted bypasses in order to patch them.It was only a matter of time. Indeed by week two of the extensionâ€™s public release on GitHub, he had patched the vulnerability.First, he stopped exposing his player instance as a predictable global variable. He wrapped his initialisation code tightly so that  no longer pointed to anything useful. Without the player reference, my automation script had nothing to grab, nothing to control, nowhere to start.Second, and more cleverly: he implemented a  on . The exact implementation could have been Subresource Integrity (SRI), a custom self-hashing routine, or a server-side nonce system, but the effect was the same. When the browser (or the application itself) loaded the script, it compared the modified file against a canonical hash and if it did not pass the check, the player would never initialise.This effectively meant the old method was dead.Act Two: Traps and Dicks. Synonyms and Subs-titutes.Fermawâ€™s In-Memory DefencesI suppose at this point, fermaw assumed he was dealing with someone who wasnâ€™t going to just fuck off. And I wasnâ€™t. It was as fun for me to try and beat as it was for him to develop.His response was to implement anti-tamper checks at the JavaScript level. Specifically, he started inspecting his own critical functions using .This is a well-known browser security technique. In JavaScript, calling  on a native browser function returns "function appendBuffer() { [native code] }". Calling it on a JavaScript function returns the actual source code. So if your  has been monkey-patched,  will betray you; itâ€™ll return the attackerâ€™s JavaScript source instead of the expected native code string.Fermaw added checks along the lines of:Fermaw also, it seems, started obfuscating and scrambling how his player was initialised, making the  class harder to find via the polling loop. The constructor hijack became unreliable.My technique had changed at this point. Since he was trying multiple things, well, I had to as well.First:  â€” The Lie That Defeats The CheckThe single most important addition in V2 was a function to make my hooked methods lie about what they are:After hooking any function, I immediately called  on it. From that point on, if fermawâ€™s integrity check asked  whether  was native, it would receive the pristine, authentic-looking answer: function appendBuffer() { [native code] }. Basically, itâ€™s like asking your ex if they cheated on you and they did but they say they didnâ€™t and you take their word for it because reasons. Donâ€™t worry, on Ã©coute et on ne juge pas.Fermawâ€™s anti-tamper check was now returning a false negative. The enemyâ€™s spy was wearing his uniform.Second: Ambushing HTMLMediaElement.prototype.playI gave up entirely on finding the player by name. Instead of looking for  or , I simply staked out the exit. I hooked the most generic, lowest-level method available:The logic is fairly simple: I donâ€™t give a shit what you name your player object. I donâ€™t care how deeply you bury it in a closure. I donâ€™t care what class you instantiate it from. At some point, you have to call . And when you do, Iâ€™ll be waiting.I was confident in that approach because you would not call multiple s on the same page to lead a reverse engineer astray. Why? Because mobile devices typically speaking will pause every other player except one. If fermaw were to do that, itâ€™d ruin the experience for mobile users even if desktop users would probably be fine. It also makes casting a bitch and a half. Even if you did manage to pepper them around, it would be fairly easily to listen in on all of them and then programmatically pick out the one with actually consistent data being piped out.Now then, the moment HotAudioâ€™s player commanded the browser to begin playback, the hook snapped shut. The audio element, , was grabbed and stored.  ensured the hook was invisible to integrity checks.Third: Keep it Untouchable ()When hijacking the  constructor, I also used  with a specific, paranoid configuration: means no code can reassign  to a different value.  means no code can even call  again to change those settings. If fermawâ€™s initialisation code tried to restore the original  constructor (a perfectly sensible defensive move) the browser would either fail or throw a . The hook was permanent for the lifetime of the page.Act Three: Choking on Natives (V3.0)Iframes and the Shadow DOMBy this point, fermaw understood that his player instance was being ambushed whenever it called . He tried to isolate the player from the main window context entirely.The two primary techniques at his disposal were  and .An  creates a completely separate browsing context with its own  object, its own , and most importantly;its own prototype chain. A function hooked on HTMLMediaElement.prototype in the parent  is  the same object as HTMLMediaElement.prototype in the â€™s . Theyâ€™re entirely separate objects. If fermawâ€™s audio element lived inside an iframe, my prototype hook in the parent window would never fire.Shadow DOM is a web component feature that lets you attach an isolated DOM subtree to any HTML element, hidden from the main documentâ€™s standard queries. A  on the main document cannot see inside a Shadow Root unless you specifically traverse into it. If fermawâ€™s player was mounted inside a Shadow Root, basic DOM searches would come up empty.On top of this, fermaw was likely switching to assigning audio sources via  rather than the  attribute.  accepts a  or  object directly, bypassing the standard URL assignment path thatâ€™s easier to intercept.V3.0 â€” Hooks, Crooks, and NooksMy response was to abandon trying to intercept at the level of individual elements and instead intercept at the level of the browserâ€™s own . I went straight for HTMLMediaElement.prototype with Object.getOwnPropertyDescriptor, hooking the native  and  setters before any page code could run:HTMLMediaElement.prototype is the browserâ€™s own internal prototype for all  and  elements and by redefining the property descriptor for  and  on this prototype, I ensured that regardless of where the audio element lives (whether itâ€™s in the main document, inside an iframeâ€™s shadow, or buried inside a web component) the moment any source is assigned to it, the hook fires. The element cannot receive audio without announcing itself.Even if fermawâ€™s code lives in an iframe with its own , the prototype hookery via  injection means my hooks are installed before the iframe can even initialise.But the triumphance of V3 is in the  hook which solves a subtle problem. In earlier versions, hooking SourceBuffer.prototype.appendBuffer at the prototype level had a vulnerability in that if fermawâ€™s player cached a direct reference to  before the hook was installed (i.e., const myAppend = sourceBuffer.appendBuffer; myAppend.call(sb, data)), the hook would never fire. The player would bypass the prototype entirely and call the original native function through its cached reference.The V3 approach obliterates this race condition by hooking  at the  level, I intercept the  of every . The moment a buffer is created and returned, I immediately install a hooked  directly on that specific instance; before any page code can even see the instance, let alone cache a reference to its methods. The hooked  is installed as an own property of the instance, which takes precedence over the prototype chain. There is no window for fermaw to cache the original. The hook is always first.To catch any elements that somehow slipped through all of the above, I added capturing-phase event listeners as a belt-and-braces fallback:The  flag for  is important. Browser events propagate in two phases: first, they travel  the DOM tree from the root to the target (capture phase), then they bubble  from the target back to the root (bubble phase). By listening in the capture phase, my listener fires before any event listener attached by HotAudioâ€™s player code. Even if fermaw tried to cancel or suppress the event, heâ€™d be too late because the capturing listener always fires first.The combination of all four layers in  at the  prototype level,  and  property descriptor hooks,  prototype hook, and capture-phase event listeners means there is, practically speaking, no architectural escape route left. The entire browser surface area through which a media element can receive and play audio has been covered. How fucking braggadocious of me to say that. I will be humbled in due time. That much is universal law.Automation: Rinsing It in SecondsWith the capture hooks in place, the automation script handles the actual download process. The approach has been refined significantly across the three versions, but the core idea has remained fairly constant: trick the browser into buffering the entire audio track as fast as the hardware and network allow, rather than in real time.The script grabs the captured audio element, mutes it, sets  to  (the browser maximum), seeks to the beginning, and calls . The browser, in its infinite eagerness to keep the buffer full ahead of the playback position, frantically fetches, decrypts, and feeds chunks into the . Every single one of those chunks passes through the hooked  and gets collected.Worth noting here is that Chrome itself limits this to 16x. The HTML spec has no mandated cap but since this is a Chromium extension; the constraint stands.Of course, fermaw does have protections against this. For one, he aggressively throttles bursty traffic meaning downloads can go from a few hundred KB/s to 50-ish KB/s. Of course, it will in every case be several times faster than listening and recording anyways.Fermaw cannot realistically slow down the stream more than that since it would stutter real traffic that has a download-y pattern. There is a possibility that he could enforce IP bans on patterns that display it but it would have to risk blanket bans against possible CGNAT traffic. There are ways to get around it but it prolongs the inevitable.V3 also added . Rather than blindly holding at 16x, the script monitors the audio elementâ€™s  time ranges to assess buffer health. If the buffer ahead of the playback position is shrinking (meaning the network canâ€™t keep up with the decode speed), the playback rate is reduced to give the fetcher time to catch up. If the buffer is healthy and growing, the rate is nudged back up. This prevents the browser from stalling entirely on slow connections, which would previously break the  event trigger and leave you waiting forever.When the track endsâ€”detected either via the  event or via the stall watcher noticing the  approaching it will collect chunks that are stitched together:There is a minor artefact in the final file. The stitched  sometimes contains silent padding at the start or end from incomplete chunks at buffer boundaries. A quick  pass fixes it cleanly:Across all three versions, thereâ€™s a  or  helper. But the V3 implementation is subtly more robust than the V2 one, and itâ€™s worth examining why.V2â€™s version was straightforward:This works, but it has a vulnerability: it hardcodes the native code string manually. If fermawâ€™s integrity check was especially paranoid and compared the spoofed string against the  native code string retrieved from a trusted reference (say, by calling Function.prototype.toString.call(originalFunction) on a cached copy of the original), the manually crafted string might not match precisely, particularly across different browser versions or platforms where the exact whitespace or formatting of  strings varies slightly.I tried to solve it somewhat elegantly:Instead of hardcoding the expected string, it captures the actual native code string from the original function before hooking it, then returns that exact string. This way, no matter what browser, no matter what platform, the spoofed  returns precisely the same string that the original function would have returned. It is, in effect, a perfect forgery.Also note the use of _call.call(_toString, original) rather than simply . This is because  might itself be hooked by the time  is called. By holding cached references to  and Function.prototype.toString at the very beginning of the script (before any page code runs), and invoking them via those cached references, the  function is immune to any tampering that might have happened in the interim. Itâ€™s eating its own tail in the most delightful way.Ethics, Grandstanding, Pretentiousness, and Playing WiseDRM, as an industry institution, has an almost comically bad track record when it comes to actually protecting content. Denuvo which is perhaps the most sophisticated game DRM ever deployed commercially has been cracked for virtually every major game itâ€™s protected, usually within weeks of release. Every DVD ever made is trivially rippable. Every Blu-ray. Every streaming service has been ripped by someone, somewhere.The reason is always the same: the content and the key that decrypts it are both present on the clientâ€™s machine. The userâ€™s hardware decrypts the content to display it. The userâ€™s hardware is, definitionally, something the user controls. Any sufficiently motivated person with the right tools can intercept the decrypted output.For a small NSFW audio platform run by a solo developer, â€œtrueâ€ blackbox DRMs running with TEEs are not a realistic option. Which brings me to the point I actually want to make:The HotAudio DRM isnâ€™t stupid because fermaw is stupid. Itâ€™s the best that JavaScript-based DRM can be. He implemented client-side decryption, chunked delivery, and active anti-tamper checks and for the vast majority of users, it absolutely works as friction. Someone who just wants to download an audio file and doesnâ€™t know what a browser extension is will be stopped completely.The problem is that calling it â€œDRMâ€ sets expectations it simply cannot meet. Real DRM, you know; the kind that requires a motivated attacker to invest serious time and expertise to defeat; lives in hardware TEEs and requires commercial licensing. JavaScript DRM is not that. Itâ€™s sophisticated friction. And sophisticated friction, while valuable, is a completely different thing.The question is whether any DRM serves ASMRtists well. Their audience is, by and large, not composed of sophisticated reverse engineers. The people who appreciate their work enough to want offline copies are, in many cases, their most dedicated fans. The kind who would also pay for a Patreon tier if one were offered. The people who would pirate the content regardless are not meaningfully slowed down by JavaScript DRM; they simply wonâ€™t bother and will move on to freely available content orâ€¦ hunt down extensions that do the trick, I suppose.Iâ€™m genuinely not convinced the DRM serves the creators itâ€™s designed to protect. But I acknowledge that this is a harder conversation than just the technical one, and reasonable people can disagree.I got all the dopamine I needed from â€œreverse engineeringâ€ this â€œDRM.â€ I donâ€™t imagine thereâ€™s any point continuing its development considering the fact that I have made my point abundantly clear even beyond this very article.I hate DRM, I love FOSS, I love the very idea that the internet should be open and accessible.Unfortunately, the Internet is no longer just a toy for the nerds amongst us. For many, itâ€™s a source of income and a way to put food on the table. So I do understand that DRM is in turn a way for people to feel protected against â€œpiratesâ€ threatening their livelihoods. I donâ€™t think it works the way itâ€™s intended to work but I suppose I cannot fault fermaw for wanting to create a solution for the ASMRtists who felt they needed it.Justâ€¦ donâ€™t do it with JavaScript ffs.]]></content:encoded></item><item><title>The problem with Dorsey&apos;s Block layoffs and the veiled nature of AI productivity growth</title><link>https://www.reddit.com/r/artificial/comments/1rga39a/the_problem_with_dorseys_block_layoffs_and_the/</link><author>/u/spacetwice2021</author><category>ai</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 15:42:57 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Jack Dorsey just laid off half of Block's workforce, framing it around AI. The stock went up. This should make you uneasy, and not for the reasons most people are talking about.There's a fundamental information problem at the heart of all this. Genuine AI integration, actually embedding it into workflows and organisation, is slow, expensive, and largely invisible to the outside world. Productivity gains from AI take time to show up in the numbers, and even then they're hard to attribute properly. Investors can't see it clearly or early enough to act on it.Headcount reductions, on the other hand, are immediate and unambiguous. They show up in a press release, a quarterly filing, a headline. They're legible in a way that real transformation is not.The consequence of this asymmetry is predictable. The market rewards what it can observe. And what it can observe is cuts, not capability. For executives whose compensation is tied to shareholder value, the calculus is straightforward. They do what the market rewards, and right now the market is rewarding AI-framed layoffs whether or not the underlying capability is there. This is clearly visible in the rally around the Block stock.This is where narrative contagion comes in, which may already be starting. Once a few high-profile companies establish the pattern and get a valuation bump, it sets the benchmark. Boards start asking why they're not keeping pace. The pressure to follow isn't rooted in productivity, but rather the fear of being the company that didn't act while everyone else did. Each announcement reinforces the narrative, which raises the perceived reward for the next one, which produces more announcements. The cycle feeds itself even when genuine productivity increases are still far away (we have yet to see it in the data!).The firms most susceptible to this are arguably the ones with the weakest genuine AI integration. Companies that are actually good at deploying AI tend to find it raises the productivity of their remaining workforce and would rather expand. But for some, a headline about workforce transformation is the easiest card to play. The worse the substance, the more you depend on the signal.And here's the collective problem. Every company acting in its own rational self-interest of maximising shareholder value by playing the signal game produces an outcome that's irrational in aggregate. The signals partially cancel out as everyone does the same thing, but the jobs don't come back. You end up with widespread displacement, muted productivity gains, and a weakened consumer base that eventually feeds back into the economy these same companies depend on.None of this means AI won't eventually justify real restructuring at some companies. It will in all likelihood, even if human work remains a critical bottleneck (which it will for the foreseeable future). But right now there is a meaningful gap between what the market is rewarding and what AI is actually delivering beyond some half-baked Claude Code solutions (don't get me wrong, I love and use CC, but it still has massive problems for large scale and complex work), and the incentive structure is pushing companies to close that gap with optics rather than substance. The people bearing the cost of that gap aren't shareholders, at least for now.]]></content:encoded></item><item><title>Do you use gorm or raw sql?</title><link>https://www.reddit.com/r/golang/comments/1rg9t4p/do_you_use_gorm_or_raw_sql/</link><author>/u/Leading-West-4881</author><category>golang</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 15:32:20 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[For backend development in Go, especially in production systems, do you prefer using an ORM like GORM or writing raw SQL? What are the trade-offs?]]></content:encoded></item><item><title>[Log4J] Addressing AI-slop in security reports</title><link>https://github.com/apache/logging-log4j2/discussions/4052</link><author>/u/BlueGoliath</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 15:28:10 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Our workflow engine is a markdown file my boss wrote in English and Claude Code running as a K8s job</title><link>https://www.reddit.com/r/kubernetes/comments/1rg9b2b/our_workflow_engine_is_a_markdown_file_my_boss/</link><author>/u/kotrfa</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 15:12:56 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Singleton with state per thread/goroutine</title><link>https://www.reddit.com/r/golang/comments/1rg98rs/singleton_with_state_per_threadgoroutine/</link><author>/u/SnooSongs6758</author><category>golang</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 15:10:29 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi! I'm creating a microservice to answer RESTful requests. In certain scenarios, I need to use a single database transaction for multiple operations. The problem is that I don't want to require all database functions and the domain model to receive a transaction parameter. Imagine having to pass the transaction through all the functions. It seems gross to me.I want to create a singleton that holds all transactions from the http request threads, but it seems GoLang doesn't support it. Any idea of how can I implement it?]]></content:encoded></item><item><title>Claude Code on OpenShift with vLLM and Dev Spaces</title><link>https://piotrminkowski.com/2026/02/27/claude-code-on-openshift-with-vllm-and-dev-spaces/</link><author>/u/piotr_minkowski</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 14:32:59 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[This article explains how toÂ run Claude Code on OpenShift as a VSCode plugin and then integrate it with AI models deployed on OpenShift using vLLM. vLLM supports the Anthropic Messages API, which Claude Code by default uses to communicate with Anthropicâ€™s servers. Claude Code can be installed in several different ways. The VSCode extension for Claude Code is particularly relevant to the topic of this article. You can run VSCode in OpenShift as a container using OpenShift Dev Spaces (Eclipse Che community project). On the other hand, OpenShift relies heavily on vLLM in support for running AI models. This article aims to provide a complete recipe for using OpenShift tools to configure your development environment to run Claude Code and AI models on the same cluster.Feel free to use my source code if youâ€™d like to try it out yourself. To do that, you must clone my sample GitHubÂ repository. Then you should only follow my instructions. This repository contains several branches, each with an application generated from the same prompt using different models. This article shows how to generate code using the  model running on OpenShift vLLM. So switch to the starting branch â€“ .The repository version located in the  branch contains the necessary configuration for VSCode and Claude Code to work correctly in the OpenShift environment.For this exercise, you must have an AWS account and an OpenShift cluster created there. You must also have the appropriate resources and permissions in your account to create an OpenShift node with a GPU. Of course, you can repeat a very similar exercise on infrastructure other than AWS.The following article explains how to install and configure OpenShift AI to run nodes with NVIDIA GPU support and how to deploy AI models on those nodes. In this exercise, I will not show you how to run the model on OpenShift AI, but simply use the vLLM server on a node with a GPU. If you want to automate the installation of operators required to properly serve GPU for AI models on OpenShift, just clone the following repository with Terraform scripts.Enable GPU Support in OpenShiftThe article mentioned above describes in detail the steps involved in installing a GPU node on OpenShift, so I will only briefly mention a few key points. Several issues also need to be updated. We will run exactly this model from RedHatAI Hugging Face. This model was post-trained with MXFP4 quantization. Therefore, it also requires a specific GPU in order to run properly. In my case, the  machine in AWS is enough. So, we should create a machine pool with at least one node on OpenShift using the  machine.Then, you must install and configure the NVIDIA GPU operator. Create the  object using default values and verify its status.After that, you must install the Node Feature Discovery operator and create the  object. Once again, you just need to click it in the OpenShift console with the default values, or just use my Terraform script.You can use the vLLM server directly to run an AI model. It is pretty straightforward. Iâ€™m using the latest image from the Red Hat repository with NVIDIA GPU support: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.3. It is important to use exactly this version or a newer one because support for the Anthropic Messaging API is a relatively new feature in vLLM . The  machine provides 4 GPUs, so I will use all available resources for the best possible performance . As I mentioned earlier, I use the  model . For vLLM, it is also important to set the name under which the model is served, as we will use it later in API calls . Finally, donâ€™t forget to insert your Hugging Face token value .Letâ€™s create a Kubernetes  for that model:The simplest way to expose the model API outside a cluster is via OpenShift . However, we will access the model internally, from a container in which VSCode will be running. So, just in case, hereâ€™s the command that creates a  for the .Letâ€™s verify if our pod with the AI model is running. Note which node this pod is running on.Now, letâ€™s take a moment to look at the detailed description of our node. As you can see, the current request for the GPU () is .Enable Claude Code in OpenShift Dev SpacesFinally, we can move on to installing OpenShift Dev Spaces and configuring the Claude Code plugin in VSCode. First, find the right operator and install it as shown below. Then, create the devspaces project (namespace) and click the Red Hat OpenShift Dev Spaces instance Specification link when you are in this namespace.Then click the Create  button. You can leave the default values everywhere except for the spec.components.pluginRegistry.openVSXURL field. It must contain the  address.Within a few minutes, Dev Spaces should be available on your cluster.Now we can move on to configuring Claude Code. The entire configuration is available in our sample repository. We need to create two configuration files in the repository root:  and .claude/settings.local.json. The extension.json contains a list of recommended extensions for VSCode. Interestingly, all recommended extensions are automatically installed in OpenShift Dev Spaces on startup ðŸ™‚ Therefore, we recommend the Claude Code extension.The .claude/settings.local.json file specifies Claude Code configuration settings for the current repository. First of all, we must override the default Anthropic API server address with the internal URL in OpenShift of our AI model . To do that, we must use the  environment variable. Our model doesnâ€™t require an API key (the simplest demo installation), but we still need to set . By default, Claude Code tries to sign in to your Anthropic account. It was unnecessary, and, in addition, in Dev Spaces, it meant I had to log in endlessly. Fortunately, we can omit it using the CLAUDE_CODE_SKIP_AUTH_LOGIN environment variable.Use Claude Code with VSCodeFinally, we can run an OpenShift Dev Spaces instance with our sample codebase. Provide the address of the sample Git repository. Donâ€™t forget you should use the  branch in my repository.After a few moments, Dev Spaces starts VSCode in the web browser with our sample repository source code and automatically installs the Claude Code plugin. Then you can just start using Claude to generate your source code. You can repeat the exact same exercise I described in my article about Claude Code on Ollama.Below is a screenshot from the battlefield ðŸ™‚Claude Code is currently having its momentum. From OpenShiftâ€™s perspective, it is important that the entire development environment can be contained within the RedHat cluster and products in this case. With vLLM, we can run various AI models in OpenShift. In turn, we use Eclipse Che to install and configure an IDE for developers. Claude Code can be easily run and configured on top of those tools.]]></content:encoded></item><item><title>Bcachefs creator insists his custom LLM is female and &quot;fully conscious&quot;</title><link>https://www.ursaclimb.com/verticals/news/bcachefs-creator-insists-his-custom-llm-is-female-and-fully-conscious-dfc5f112</link><author>/u/DontFreeMe</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 13:18:57 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>hledger-tui: just another terminal user interface for managing hledger journal transactions</title><link>https://www.reddit.com/r/linux/comments/1rg69s5/hledgertui_just_another_terminal_user_interface/</link><author>/u/Complete_Tough4505</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 13:11:31 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I've been using hledger for a while to manage my personal finances. The CLI is great, but it gets verbose fast. The built-in UI is limited, and the few alternative projects out there are mostly abandoned or barely maintained.So I built my own: hledger-tui, a terminal user interface for hledger built with Python and Textual. View, create, edit, and delete transactions with simple keyboard shortcuts, no need to touch the journal file directly.It started as a personal tool, and it still is â€” but I figured someone else might find it useful.I'm currently working on a reporting system, so more is coming. There are no official builds for Linux yet, so you'll need to set it up manually â€” the README has everything you need.Feedback and bug reports are very welcome.]]></content:encoded></item><item><title>The error handling bugs that worry me aren&apos;t the ones that crash</title><link>https://www.reddit.com/r/golang/comments/1rg5zo7/the_error_handling_bugs_that_worry_me_arent_the/</link><author>/u/___oe</author><category>golang</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 12:59:16 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I recently found error handlers in Grafana Loki, Canonical Juju, and Chromium's LUCI that would panic if they were ever executed (Loki, Juju, LUCI).But honestly? The fixes were easy. I didn't need to explain much in the pull requests above: the crashes are obvious, and no dependent code relies on the broken behavior.Those aren't the bugs I care much about.  advises to "Crash Early" because a dead program does a lot less damage than a malfunctioning one. Don't get me wrong, crashing in production is bad, but even an  panic in an error handler gives you a stack trace and a straightforward fix.The bugs I'm concerned about are the : error handling code that compiles, passes review, and then quietly does the wrong thing in production. It logs the wrong error. It swallows context. It writes bad state to the database because an  target didn't match what the author assumed.We test our happy paths rigorously, but error handlers are often neglected. Theyâ€™re the least-exercised code in the codebase, yet when something does go wrong in production, thatâ€™s exactly the code weâ€™re relying on.I built the linter () to catch some of these issues, but static analysis only goes so far.So I'm curious: how do you actually test your error paths?Do you use mocks, fault injection, or something else to exercise them? Or (if we're being honest) is it mostly code review and the occasional production incident?For an obscure error path that is incredibly hard to trigger, is it even worth the effort to test?No judgment â€” I think this is genuinely hard, especially for external dependencies where you canâ€™t always control what errors come back, or where returned errors change between versions. Would love to hear how your team handles it. To explain the background of this post: I've posted three pull request to major open source projects with crashing bugs, and thinking about them I realized that not the crash was the issue, but that the error handling has never been tested. I wondered how many untested error paths exist, and what peoples experiences with them are.I'm baffled by the misreading of the article, and wonder where I may have misrepresented what I'm trying to say. And I'm a little repelled by some comments, how little some people are interested in being constructive. While this post might not help Reddit, I hope at least the patches are useful - they, and thinking about the issue was the main part of the work.]]></content:encoded></item><item><title>Claude Code as a K8s CronJob - how we do it and what we learned running it in production (with examples)</title><link>https://www.reddit.com/r/kubernetes/comments/1rg5c67/claude_code_as_a_k8s_cronjob_how_we_do_it_and/</link><author>/u/kotrfa</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 12:27:19 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Docker, Traefik, and SSE streaming: A post-mortem on building a managed hosting platform</title><link>https://clawhosters.com/blog/posts/building-managed-hosting-platform-tech-deep-dive</link><author>/u/yixn_io</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 11:49:34 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Two weeks ago, ClawHosters went live. Today the platform runs with roughly 50 paying customers and 25 more in trial. All from Reddit, no marketing budget, alongside a regular 40-hour job.And I'll tell you right now: none of it went smoothly.This isn't a sales pitch for my product. It's a technical post-mortem about building a managed hosting platform for AI agents. Real code, real mistakes, and real nights where the Telegram bot pings at 2 AM because a customer instance is stuck in a crash loop.The stack: Rails 8 monolith, PostgreSQL, Sidekiq with 5 processes and 50 threads total, Clockwork for scheduling, Hetzner Cloud API for infrastructure. Each customer gets their own VPS with OpenClaw running in Docker.Everything on one server. No Kubernetes, no ECS, no managed database. That's a decision, not a limitation.Why Docker (and Why 70% of My Headaches)The decision to isolate OpenClaw in Docker containers instead of running it directly on the VPS was deliberate. It was also the source of at least 70% of all technical problems. I'd still do it again.The problem without Docker: if a customer process goes rogue (and it does, more on that soon), it can eat all memory, fill the disk, corrupt the OS. With Docker I get: The OpenClaw container can't touch my host services. SSH, Docker daemon, node_exporter, all unreachable from inside the container. 3 GB, 6 GB, or 14 GB depending on tier. OpenClaw hits these regularly. Even if the container is completely borked, I can SSH to the host, inspect logs, fix configs, restart. If the customer had trashed the VPS itself, I'd be rebuilding from scratch. The  sits on the host filesystem. I can fix configs without even starting the container.But Docker brought so many problems that I sometimes wondered if I'd made a terrible mistake.The Docker Problems in Detail pnpm creates symlinks in , and  flat-out refuses to handle them. Updates have to stream files via  instead. Sounds trivial. The error messages were cryptic enough to cost me hours.mDNS/Bonjour auto-discovery. The gateway picks up the Docker bridge IP (172.18.x.x) instead of localhost, causing cryptic "gatewayUrl override rejected" errors. Fix: an environment variable that disables the behavior. Finding that variable almost made me lose my mind. Node doesn't handle SIGCHLD properly. Without  as PID 1, zombie processes pile up in the container. You don't notice immediately. Only when the process table fills up after a few days.Nginx Host header validation. Nginx inside the container validates the Host header, so direct IP access returns 403. Good for security, but it makes debugging harder because health checks need to send the correct Host header.Container recreation destroys runtime state. This was the biggest one. Every update, every SSH enable, every config change that would normally require recreating the container means losing everything: customer-installed packages, runtime data, conversation history. You can't just docker-compose down && docker-compose up. I have to  first to preserve the writable layer, then apply changes. For config changes, I built a hot-reload system that sends SIGUSR1 to the process instead of touching the container at all.The Writable Layer StrategyCustomers can install packages inside their container. , , , whatever they need. Those changes live in Docker's writable layer (OverlayFS). The entire update and maintenance system is designed to preserve this layer.I use , never . Before any operation that might recreate the container, I run  to bake the writable layer into the base image. Backup images get cleaned up after successful updates to reclaim disk. They're 15 to 25 GB each.Why not volumes? Because the customer potentially modifies files everywhere in the filesystem. A volume for  and one for  and one for... no. The writable layer captures everything regardless of where.5-Layer Subdomain RoutingEvery customer instance gets a subdomain like my-assistant-x7k2.clawhosters.com. Getting traffic from the browser to the right VPS takes five layers. Yes, five.Layer 1: Cloudflare Wildcard DNSOne  record points everything to my server. No per-instance DNS records. Cloudflare terminates SSL publicly, then connects to the server via a 15-year origin certificate.Layer 2: Nginx Regex MatchNginx captures the subdomain with a regex , blocks reserved words (www, api, mail, admin), and forwards to Traefik on port 8090. Critical here:  and proxy_request_buffering off. Why that matters comes in the SSE section.Layer 3: Traefik with Redis-Backed Dynamic RoutingThis is where it gets interesting. Traefik reads its routing table from Redis. When Rails provisions an instance, it writes the routing rules atomically in a Redis MULTI block:traefik/http/routers/<subdomain>/rule = "Host(`<subdomain>.clawhosters.com`)"
traefik/http/services/<subdomain>/loadbalancer/servers/0/url = "http://<vps-ip>:8080"

It also registers per-instance bcrypt-hashed basic auth middleware. Traefik picks up changes instantly via keyspace notifications. No restart needed.Layer 4: VPS-Side Nginx (Inside Docker)On the customer's VPS, nginx runs as a sidecar container on port 8080. It only accepts the correct Host header and proxies to OpenClaw on internal port 18789. Everything else gets a 403 with "Access denied. Use your subdomain." Last line of defense against direct IP access.Layer 5: Hetzner Firewall + fail2banProduction instances get a Hetzner Cloud Firewall at creation time. It blocks everything except 8080, 9100, 22, and 9993/udp for ZeroTier. The firewall rules only allow incoming connections from my production server's IP, so customer VPS instances aren't directly reachable from the public internet. fail2ban is pre-configured in the snapshot for SSH brute force protection.A sync service runs every 10 minutes, adding missing routes and removing orphaned ones. A health service runs every 5 minutes, making actual HTTP requests through Traefik with the correct Host header to verify end-to-end routing. If Traefik's Redis subscription breaks after a Redis restart (it happens), it auto-restarts the Traefik service.The LLM Proxy: SSE Streaming and Why Nginx Breaks EverythingCustomers can use our managed LLM instead of bringing their own API key. Their OpenClaw points at , which exposes an OpenAI-compatible completions API. It's the same principle I use for individual LLM workflow projects.No token management, no API keys to rotate. Each VPS has a unique Hetzner IPv4 (unique index in the DB). When a request comes in, we look up which instance owns that IP. IPv6 uses PostgreSQL's CIDR containment operator because Hetzner assigns /64 blocks. The OpenClaw config has a dummy apiKey field only because the client refuses to send requests without one.The Three Streaming Nightmares1. TCP chunk fragmentation. SSE events are delimited by . But HTTP chunks from upstream providers are raw TCP segments. A single chunk can contain half an SSE event, or three events glued together. I had to build a re-framing buffer that accumulates chunks, splits on  boundaries, and only forwards complete events to the client. Sounds simple. Took way too long to get all the edge cases right.2. Nginx buffering kills SSE. This is a well-documented problem that hits dozens of projects. But in a multi-layer stack it gets really ugly. Two nginx layers (main server + Traefik's upstream path) means two places where buffering can silently accumulate the entire response before forwarding. Without the fix, the client just hangs for 30 seconds and then gets everything at once. "Streaming" in name only.As this nginx SSE guide explains, you need , , , chunked_transfer_encoding off, AND  as a response header from Rails. All of them. Not just one.I missed the response header and spent hours debugging why streaming worked locally but not in production.# nginx config for SSE streaming
location /v1/ {
    proxy_pass http://upstream;
    proxy_buffering off;
    proxy_cache off;
    proxy_http_version 1.1;
    chunked_transfer_encoding off;
    proxy_set_header Connection '';
    proxy_set_header X-Accel-Buffering no;
}

# Rails Controller - Response Headers for SSE
response.headers['Content-Type'] = 'text/event-stream'
response.headers['Cache-Control'] = 'no-cache'
response.headers['X-Accel-Buffering'] = 'no'
response.headers['Transfer-Encoding'] = 'chunked'

3. Usage billing with streaming. Providers only send token counts in the very last SSE chunk. But Rails is mid-stream, and you can't hold the entire response in memory (that defeats the purpose of streaming). Solution: a ring buffer of only the last 4 KB of SSE data. After the stream ends, I scan the buffer for the usage JSON. The  block also closes the upstream HTTP connection. Leaked connections pile up fast. Learned that one the hard way. Some providers don't actually support streaming for certain models. When a client sends  but the upstream returns a normal JSON response, the controller wraps it into a fake SSE sequence so the client always gets consistent SSE regardless.Routes through Anthropic, OpenAI, DeepSeek, Google, OpenRouter, or Nvidia depending on the model. On 5xx from the primary, auto-falls back to OpenRouter with a tier-appropriate model. 4xx errors pass through (that's the caller's problem). Rate limited at 60 req/min general, 10 req/min for reasoning models. Redis down? Fail open.Token Billing: The Gap Between Observability and InvoiceThe streaming proxy was running. Token data was flowing through. I had no idea what to put on a customer's invoice.How do you bill for token usage when every provider counts tokens differently, names them differently, and sometimes doesn't report them at all?As Portkey's token tracking guide documents: "Different model providers count, tokenize, and bill tokens differently." Two identical prompts produce different token counts on GPT-4 vs Claude vs DeepSeek.Every provider reports token usage differently.Anthropic sends  in the last SSE event with  and . Relatively reliable. OpenAI sends it in the last chunk too, but the format differs slightly. DeepSeek? Sometimes the usage is just missing for certain models. Google Gemini calculates in "characters" instead of "tokens" in some API versions.The ring buffer approach from the streaming section is the first layer. If the tail end of the SSE data contains the usage object, we parse it. If not, we fall back to an estimate based on chunk byte size times a provider-specific factor.Observability vs. InvoiceThere's a difference between "I roughly know how many tokens that was" and "I can put this on a customer's invoice." For observability, a rough counter is fine. For invoicing, you need:Exact attribution per request to a customer instance (via IP-based auth)Provider-specific pricing (Claude Sonnet costs differently than GPT-4o costs differently than DeepSeek)Separation of input and output tokens (output is 3 to 5 times more expensive at most providers)Pro-rating at month boundaries (customer signs up on the 15th, do they pay half?) when the ring buffer missed the usage dataEvery LLM request gets stored with instance ID, provider, model, input tokens, output tokens, and exact cost in the database. Each tier includes a token allowance. The included tokens get consumed first. Once they're used up, additional usage gets billed per claw instantly. No waiting until month end, no manual reconciliation. Provider-specific price differences (Claude vs GPT-4 vs DeepSeek) are normalized through a pricing table that gets updated when providers change rates.Provisioning: Snapshot-Based with Pre-Warmed PoolEverything is pre-baked into a Hetzner snapshot. Docker, the OpenClaw image (pre-pulled), Playwright/Chromium browsers, fail2ban, SSH hardening. When a VPS boots from the snapshot, cloud-init only regenerates SSH host keys and machine-id, then restarts Docker. About 3 minutes to ready.Fly.io described the same problem as "latency whack-a-mole": "every time you solve one bottleneck, the next one becomes visible." They solved it with Firecracker microVMs and separate create/start operations. I use a pre-warmed pool.Servers get created from the snapshot in advance, with a placeholder container already running. Customer orders, the code atomically claims a pre-warmed VPS, renames it via the Hetzner API, and deploys the real config. Near-instant.A pool manager job (runs every 10 minutes) checks how many free pre-warmed VPS instances are available. When the count drops below a configurable minimum, it automatically orders more. The target pool size is also seasonally adjusted: weekday nights get a higher buffer because that's when signups tend to spike.The deployment itself is just SCP config files +  + health check polling +  + SIGUSR1 for hot reload. No packages installed, no images pulled. That's the whole point: everything slow happens at snapshot build time. By deploy time, there's nothing left to install.Hetzner recycles IPs from deleted servers. This caused two bugs.First: stale SSH known_hosts entries broke connections even with . The fix was UserKnownHostsFile=/dev/null. Second: stale IPs in our database could point to wrong servers. Fix: query the Hetzner metadata service from inside the VPS before trusting SSH.The second bug is actually the scarier one. "Stale IP points to wrong server" means in the worst case: we deploy a customer's config onto someone else's VPS. That would have been a significant security problem. It never happened because we caught it first. But it was close.This topic deserves its own section because it's been the biggest operational pain point. And it still is.OpenClaw's config () is a single JSON file with nested keys for LLM providers, messenger tokens, gateway settings, agent behavior, tool permissions. Customers can edit it through OpenClaw's CLI. They make typos, delete required keys, set invalid values, and then their OpenClaw crashes in a loop and they open a support ticket.OpenClaw v2026.2.23 changed the gateway to , which requires a specific  flag set to true. Flag missing? Instant crash loop. And OpenClaw's own  command sometimes removes flags that we need. Fixing one thing breaks another.Layer 1: controlUi flag protection. After every config change (even unrelated ones), the system re-downloads the config and verifies that three critical gateway flags are present and true. If  or the customer stripped them, they get restored before the reload happens.Layer 2: Automatic health monitoring + repair. Every running instance gets polled. After 4 consecutive health check failures, a config repair service kicks in automatically. It SSHes to the instance, reads the last 100 lines of container logs, and pattern-matches fixes:Invalid  value: deletes the bind key"Cannot parse configuration": regenerates the entire gateway section from a template"Unknown configuration key": runs  with the new version's code"Permission denied": chmod fixAfter applying fixes it also validates that critical fields aren't empty and restores  to the canonical list.Layer 3: Dashboard transparency. Config state, health status, container logs, VPS metrics (CPU/RAM/disk/network via node_exporter) are all surfaced in the customer dashboard. If their OpenClaw is crash-looping, they can see the error, see which config key is wrong, and at least try fixing it themselves before opening a ticket.OpenClaw Updates and the Config Migration RegistryOpenClaw releases new versions frequently, and they like changing config defaults in breaking ways. A key that was optional becomes mandatory. A default changes from permissive to restrictive. If you just update the binary without migrating the config, the gateway doesn't boot.REGISTRY = [
  { version: "2026.2.22", key: "tools.exec.host", default: "node" },
  { version: "2026.2.23", key: "gateway.controlUi.dangerouslyAllowHostHeaderOriginFallback",
    default: true },
  { version: "2026.2.23", key: "browser.ssrfPolicy.dangerouslyAllowPrivateNetwork",
    default: true },
  { version: "2026.2.24", key: "agents.defaults.sandbox.docker.dangerouslyAllowContainerNamespaceJoin",
    default: true },
  { version: "2026.2.25", key: "agents.defaults.heartbeat.directPolicy",
    default: "allow" },
]

During updates, the system reads the current config, applies only migrations between the old and new version, and only sets keys that are missing (respects customer customizations). The version gets tracked inside the config itself. Upload a pre-built tarball (extracted from the upstream Docker image), stream files into the running container via tar (not  because symlinks), run config migrations, , , health check polling, commit the updated container. Backup image created before, cleaned up after.ZeroTier: One-Way Networking for Local LLMsThis one surprised me. Customers wanted their OpenClaw to reach devices on their private ZeroTier network. The number one use case: local LLMs. People run Ollama or LM Studio on their home machine and want their hosted OpenClaw to use it without exposing anything to the public internet. Other use cases: NAS, home servers, internal APIs.A second container runs alongside OpenClaw on the same Docker bridge network. It joins the customer's ZeroTier network ID. Then I use  to inject a route into OpenClaw's network namespace:nsenter -t <openclaw_pid> -n ip route add <zt_subnet> via <zt_docker_bridge_ip>

The ZeroTier container does NAT masquerading for outbound traffic. OpenClaw can reach the ZT network, but the ZT network cannot initiate connections back into OpenClaw. No return route. One-way by design.The customer's home network stays safe. Their OpenClaw can call their local LLM, but nothing on the ZT side can poke into the container. And the ZeroTier container itself runs inside Docker with no access to the host VPS. Even if a customer's ZeroTier network is compromised, the attacker is stuck inside a container that can't reach the host.The whole thing is maybe 50 lines of actual logic.I expected weeks of networking pain. Days with , frustrated customers, routing anomalies I couldn't reproduce. Instead: it just worked. The route gets re-injected automatically after any container restart.Worth pausing to think about why. ZeroTier does exactly one thing, does it in userspace, and does it well. The  route injection pattern was the only non-trivial decision. Everything else was just configuration.A week after launch, I lost the plot. Five instances stuck in "deploying" state, three of them for over an hour. Two customers had already filed tickets. The Sidekiq worker handling the deploy job had died mid-run, and the instance had no idea.The monitoring system that came out of that afternoon is built directly from that experience.A provisioning manager job runs every 5 seconds and catches stuck instances. If something has been in "deploying" state but the VPS is actually healthy on port 8080, it marks it running. If the deploy job died, it re-queues it. Instances stuck in "provisioning" for 20+ minutes get flagged for manual review.After 4 consecutive health failures: automatic config repair. After 5: admin alerts to Telegram and email. New instances get a 10-minute grace period. Every recovery path has been battle-tested by actual failures over the past weeks.Docker's own restart policies only help so much here.  triggers only when the container process exits. A container that's running but deadlocked, consuming all memory at the application layer, or unable to connect to its LLM API won't be automatically restarted. You need your own health monitoring layer for that.Concretely with Prometheus: I track openclaw_health_check_consecutive_failures per instance. Anything over 3 triggers an escalation. Before I had this, I thought I'd notice problems manually. I was wrong.I have roughly 50 paying customers now and about 25 more still in trial. Just from Reddit, no other marketing. I've talked to a lot of them, and a lot of people who didn't convert from trial. The consistent takeaway: it's practically impossible for non-coders to run OpenClaw smoothly, or even at all. The config complexity alone filters out 90% of potential users.I started as a script kiddy 23 years ago, been a professional developer for over 10 years. Previously built and ran a crypto browser game from scratch. Had a large Rocket League tracking site, RLTracker, that funded self-employment for years. But I've never hit this many problems around a single piece of software.OpenClaw itself is incredibly unstable. Config formats change between minor versions, defaults flip without warning,  sometimes makes things worse. Building a reliable managed service around it is an enormous job, and that's really the core of what a managed hosting platform does: not run the product yourself, but make it reliably runnable for others.Yeah, plenty of competitors popped up before me and even more since. But I know the problems from the inside now: the config migrations, the crash loops, the IP recycling, the SSE buffering. Someone who hasn't debugged those things firsthand builds around those problems, not through them. You can see it in the products.Railway chose to build their own data centers instead of running on Google Cloud. That let them maintain 50% lower pricing than hyperscalers. I use the same basic idea with Hetzner directly instead of going through AWS or GCP. Own the stack instead of renting abstractions. The tradeoff is complexity vs control and pricing flexibility.If I started over tomorrow, a few things.Observability from day one. I added monitoring after the fact. What that meant in practice: when customer one hit a crash loop, I had no logs, no metrics, nothing. I sat at a terminal and guessed. Prometheus and node_exporter on every VPS from the start would have reduced an hour of debugging to five minutes.Config validation before writing, not after the crash. I now validate before a config change gets applied. If I'd done that from the beginning, I'd have avoided dozens of support tickets. Every one of them was a customer messaging me at 11 PM because their OpenClaw stopped responding.Plan the billing system earlier. Retrofitting a token metering pipeline into a running streaming proxy was painful. The streaming code was optimized for performance, not observability. Refactoring everything without breaking the stream, while customers are actively using it. Don't do that to yourself.And maybe, just maybe, I shouldn't have built all of this alongside a full-time job. The support tickets during work hours... let's just say my employer knows and is actually supportive of this kind of thing.If you're thinking about building a similar managed hosting platform: the biggest problems don't come from building it. They come from operating it afterward.]]></content:encoded></item><item><title>Weekly: Share your victories thread</title><link>https://www.reddit.com/r/kubernetes/comments/1rg3pb9/weekly_share_your_victories_thread/</link><author>/u/AutoModerator</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 11:00:43 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Got something working? Figure something out? Make progress that you are excited about? Share here!]]></content:encoded></item><item><title>Hello my company wants to move it&apos;s VMs in gcp to kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1rg3ock/hello_my_company_wants_to_move_its_vms_in_gcp_to/</link><author>/u/whatsinaname5021</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 10:59:35 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I am a devops intern in this company and another co worker and I have been given a task to containerise a staging environment vm to kubernetes completely but we have to learn kubernetes via scratch. Can anyone tell how long this process can take? And a proper roadmap on what to learn and the prerequisites?]]></content:encoded></item><item><title>Whatâ€™s the one Go project that made you stick with the language?</title><link>https://www.reddit.com/r/golang/comments/1rg3ml0/whats_the_one_go_project_that_made_you_stick_with/</link><author>/u/itsme2019asalways</author><category>golang</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 10:56:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Maybe it was a concurrent script, a simple API, a backend service, or a tooling experiment â€” Goâ€™s speed and clarity tend to hook people early.Which project made you feel Go just â€œgets things doneâ€?]]></content:encoded></item><item><title>Whatâ€™s the first Rust project that made you fall in love with the language?</title><link>https://www.reddit.com/r/rust/comments/1rg3lby/whats_the_first_rust_project_that_made_you_fall/</link><author>/u/itsme2019asalways</author><category>rust</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 10:54:52 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[For many people, itâ€™s something small â€” a CLI tool, a microservice, or a systems utility â€” that suddenly shows how reliable, fast, and clean Rust feels.Which project gave you that â€œwow, this language is differentâ€ moment?]]></content:encoded></item><item><title>I never estimate on the call. Best engineering rule I made for myself.</title><link>https://l.perspectiveship.com/re-auru</link><author>/u/dmp0x7c5</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 10:54:06 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Itâ€™s 1:1 with the client. He says itâ€™s important: â€œWe need to have this feature on production by the end of today. We count on you, MichaÅ‚. Can you get it done?â€.I want to help. After 5 seconds of processing the problem, I say: â€œYes, of course. You can count on meâ€. Fast forward a few hours later and I deeply regret it. The feature is way more complicated than I thought. I end up working until 2 AM.I got into trouble because of my rushed answers. I promised to deliver features even though it was impossible in the timeline I gave, I hired people fast and regretted it afterwards.I knew that it was my flaw, but I found a cure. Now, I have a set of automatic rules to follow:While making commitments:I donâ€™t estimate anything during a call with the client.I donâ€™t make hiring decisions the same day as the final interview.I donâ€™t schedule meetings back-to-back without at least 15-minute breaks.I donâ€™t push big changes to production before leaving.I wait 2 days before any impulse purchase.Daniel KahnemanWhen you click to delete a file and the action is irreversible, you get a confirmation dialogue: â€œDo you really want to delete this file? This action canâ€™t be undoneâ€. This simple pause has saved many files on peopleâ€™s computers and now in cloud storage.Itâ€™s impossible to prevent biases from happening, but using circuit breakers in your processes can stop them from leading to bad decisions.I started setting these rules after analysing my past decision logs and trying to learn from them. The most beneficial ones are ones that force me to pause:Where can you find ideas to set your own rules? Ask yourself:When do I feel most pressured to answer when Iâ€™m not confident about?What situations lead me to commit or do things I later regret?Where do I consistently underestimate or overcommit?What rule would have saved you from your worst decision this month?Great articles which Iâ€™ve read recently:]]></content:encoded></item><item><title>What&apos;s the most idiomatic way to deal with partial borrows/borrow splitting?</title><link>https://www.reddit.com/r/rust/comments/1rg3ftw/whats_the_most_idiomatic_way_to_deal_with_partial/</link><author>/u/philogy</author><category>rust</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 10:46:09 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I'm continuously running into this problem when writing Rust and it's seriously making me want to quit. I have some large struct with lots of related data that I want to group in a data structure for convenience with different methods that do different things, however because the borrow checker doesn't understand partial borrows across function boundaries I keep getting errors for code like this:struct Data { stuff: Vec<u32>, queue: Vec<u32>, } impl Data { fn process(&mut self, num: u32) { self.queue.push(num); } fn process_all(&mut self) { for &num in &self.stuff { // Error: cannot borrow `self` because I already borrowed `.stuff` self.process(num); } } } Do you just say "f*ck structs" and pass everything member? Do you manually split members on a case by case basis as needed? How do you deal with this effectively?I've been writing Rust for various things for over 2 years now but this is making me seriously consider abandoning the language. I feel very frustrated, structs are meant to be the fundamental unit of abstraction and the way of grouping data. I just want to "do the thing".It seems I either have to compromise on performance, using intermediary Vecs to accumulate and pass around values or just split things up as needed.]]></content:encoded></item><item><title>oapi-codegen v2.6.0: 7th anniversary release</title><link>https://github.com/oapi-codegen/oapi-codegen/releases/tag/v2.6.0</link><author>/u/profgumby</author><category>golang</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 10:41:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I got the ThinkBook Plus Gen 1 E-ink lid display working on Linux â€” first open-source driver</title><link>https://www.reddit.com/r/linux/comments/1rg2y5m/i_got_the_thinkbook_plus_gen_1_eink_lid_display/</link><author>/u/Still_Complex8652</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 10:17:02 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Slok finally can beat Sloth and Pyyra</title><link>https://www.reddit.com/r/kubernetes/comments/1rg2mep/slok_finally_can_beat_sloth_and_pyyra/</link><author>/u/Reasonable-Suit-7650</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 09:58:08 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Slok just reached a level that neither Pyrra nor Pyrra nor Sloth can. The first version of  composition is now available. It is still unstable and under active development, but it works â€” and it opens up a class of SLO modeling that, to my knowledge, no other open-source operator supports today.What is  Most SLO tools let you define objectives on individual services. Some, like Slok, let you compose multiple SLOs together â€” for example, taking the worst-performing service in a group (AND/MIN logic). That is already useful. But it doesnâ€™t model the real world well enough. Real traffic is not uniform. In many systems, different requests follow different paths through your services, and those paths have very different failure characteristics.  is built for exactly this.The idea is simple: you describe the routes your traffic actually takes, assign a weight to each one (reflecting the traffic share), and Slok computes the overall error rate as a weighted combination of the per-route failure probabilities.apiVersion: observability.slok.io/v1alpha1 kind: SLOComposition metadata: name: checkout-weighted namespace: app spec: # Target availability percentage for the composed SLO. target: 99.9 # Observation window. Must match one of the supported windows (7d or 30d). window: 30d # objectives: maps logical aliases to actual Kubernetes SLO resources. # Aliases are referenced in route chains, decoupling logical names # from Kubernetes resource names. objectives: - name: base # alias used in route chains ref: name: checkout-base-slo # ServiceLevelObjective resource name namespace: app # if omitted, inherits the composition namespace - name: payments ref: name: payments-slo namespace: app - name: coupon ref: name: coupon-slo namespace: app composition: type: WEIGHTED_ROUTES params: routes: # Main path: no coupon applied (90% of traffic). # Route success rate = (1 - e_base) * (1 - e_payments) - name: no-coupon weight: 0.9 # value in [0, 1]; all weights must sum to 1.0 chain: - base # aliases defined in objectives, executed in order - payments # Coupon path: coupon service is called between base and payments (10% of traffic). # Route success rate = (1 - e_base) * (1 - e_coupon) * (1 - e_payments) - name: with-coupon weight: 0.1 chain: - base - coupon # inserted between base and payments - payments The overall composed error rate is then:e_total = 1 - ( 0.9 Ã— (1 - e_base) Ã— (1 - e_payments) + 0.1 Ã— (1 - e_base) Ã— (1 - e_coupon) Ã— (1 - e_payments) ) : This post was originally written in Italian and translated with AI assistance to make the concepts clear in English.]]></content:encoded></item><item><title>Log4j - Addressing AI-slop in security reports</title><link>https://github.com/apache/logging-log4j2/discussions/4052</link><author>/u/FryBoyter</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 09:37:54 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Apache Iggy&apos;s migration journey to thread-per-core architecture powered by io_uring</title><link>https://iggy.apache.org/blogs/2026/02/27/thread-per-core-io_uring/</link><author>/u/spetz0</author><category>rust</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 09:05:03 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[At Apache Iggy, performance is one of our core principles. We take pride in being blazingly fast, pushing our systems to reach the absolute limits of the underlying hardware, eventually exhausting all available options within our previous architecture. Thus, a new approach was needed. If you're an active Rust Reddit user, you may have already seen this discussion. It predates this blog post, and we wanted to use it as an opportunity to explore the thread-per-core shared-nothing architecture powered by  in more depth.To explain the "whys" of that decision in detail, a quick primer on the status quo is needed.
Apache Iggy utilized  as its async runtime, which uses a multi-threaded work-stealing executor. While this works great for a lot of applications (work stealing takes care of load balancing), fundamentally it runs into the same problem as many "high-level" libraries: a lack of control.When  starts, it spins up  worker threads (typically one per core) that continuously execute and reschedule . The scheduler decides on which worker a particular  gets to run, which can lead to task migrations between workers, cache invalidations, and less predictable execution paths. While Rust  and  bounds prevent data-race undefined behavior, they do not prevent higher-level concurrency bugs such as deadlocks.But even these challenges weren't what finally tipped us over the edge. The way  handles block device I/O was the real dealbreaker. Tokio, following the poll-based Rust  model, uses (depending on the platform) a notification-based mechanism to perform I/O on file descriptors. The runtime subscribes for a readiness notification for a particular descriptor and  the readiness in order to submit the I/O operation. While this works decently well for network sockets, it's completely incompatible for block devices. The Linux kernel considers regular files to be always "ready" for reading or writing, meaning  (or similar notification mechanisms) will immediately return, and the subsequent I/O operation will block the executing thread anyway (on page-cache lock contention or other kernel operation). To overcome this issue,  relies on a thread pool approach. It outsources every block device I/O operation to a shared blocking thread pool, where threads are spawned on demand. By default,  allows this blocking thread pool to grow up to 512 threads. A high-performance system can quickly exhaust the capabilities of such a thread pool (leaving aside the overhead from servicing 512 threads), which is why we concluded that  doesn't scale for our needs.The thread-per-core shared-nothing architecture is what we landed on when it comes to improving the scalability of Apache Iggy. It has been proven to be successful by high-performance systems such as ScyllaDB and Redpanda, both of those projects utilize the Seastar framework to achieve their performance goals.In short, the core philosophy behind this approach is to pin a single thread to each CPU core, partition your resources based on a heuristic (commonly hashing), eliminate shared state, thereby reduce lock contention and improve cache locality and finally, use message passing for communication between those threads, also known as  in  terminology. Sounds like a good plan, but as with everything, the devil is in the details.
From a bird's-eye view, this architecture solves the primary issues of our previous approach: we move from  to . That's a big W, but we were still left with block-device I/O. Using a thread pool for file operations would ultimately negate the performance gains from core pinning, so we needed a truly asynchronous I/O interface, and that is how we discovered .There is plethora of materials regarding  as it's the hot thing, but very briefly the interface is straightforward,  rather than being a notification system (readiness based), it's completion-based, you submit the operation and the kernel drives it to completion. The core mechanism revolves around two lock-free ring buffers shared between user space and the kernel: the , where your application enqueues I/O requests, and the , where the kernel places the results once the operations are done. Since that model isn't compatible with how Rust  works (Futures are poll-based), the initial poll of the  is used for the  of the request. A continuation via callback model would fit the completion I/O paradigm better, but it comes with its own caveats, nevertheless the overhead from the impedance mismatch is negligible. As for the  part, it's a simple peek into the CQ, looking for a completion entry that matches the polled  at hand ( allows attaching a usize  cookie to each submission, which is used to identify the corresponding user-space  and wake it up). Everything else, let's pretend for a moment, is .With all the design pieces in place, it was time to visit the marketplace of . We evaluated 3 candidates:All of them support  as the driver, some exclusively, others as one of several available ones.Using the FIFO order - monoio was our choice for the initial proof-of-concept, it worked pretty well, but as we explored the monstrous API surface of , we realized that it's pretty far behind when it comes to feature parity and doesn't appear to be very actively maintained. Don't get us wrong, the runtime still receives patches, especially after incidents like this, but the overall pace of development doesn't keep up with a rapidly evolving interface like .Next on the list glommio - this one is particularly interesting as it was initially developed by , who previously worked at , the creators of the  framework,  significantly differs from the other two runtimes on our list. It's still a thread-per-core runtime, but it uses a proportional-share scheduler, creates 3  instances per thread (a main ring, a latency ring, and a polling ring), and ships with quite a lot of high-level APIs (similar to ) that one can use. Unfortunately, it followed the same fate as ,  it's pretty much unmaintained at this point. On top of that, it's fairly opinionated as a runtime, and we disagreed with some of those opinions (more on that later).Finally, compio - this is what we ended up using. It's very similar to  in terms of architecture, but it stands out for its broad  feature coverage, active maintenance (our patches got merged within hours), and its codebase structure. Unlike , the  codebase is structured in a way where the  is disaggregated from the , meaning that one can build their own executor while still reusing the  driver.Notably,  boxes the I/O request that is submitted to the SQ, which means that every I/O request incurs a heap allocation, something that  avoids. In our case, it's not that big of a deal, as those allocations are very small and  is quite good at maintaining a pool for small, predictable allocations. We did raise the question in their  channel about whether it would be feasible to use a  allocator the approach that  takes, but the authors decided against it, as it would introduce a lot of complexity into the executor, which uniformly supports other drivers such as .Remember how we mentioned that the devil is in the details? Let's give him mic now.At first glance since the thread-per-core shared-nothing model all state is local to each shard and anything that requires a  view must be replicated across shards via message passing, it looks like a perfect candidate for , replace your  with  and run with the quick win. If you thought that, I've got bad news, you'd be greeted straight from the ninth circle of Dante's Inferno with:thread 'shard-8' (496633) panicked at core/server/src/streaming/topics/helpers.rs:298:21:
RefCell already borrowedTurns out that holding a  borrow across an  point can cause runtime borrow panics, there is even a clippy lint for that - clippy::await_holding_refcell_ref.The Rust  (async working group) seems to be aware of that footgun and describes it in this story. It  like it should be possible to express statically-checked borrowing for  using primitives such as , they even share a proof-of-concept runtime that does exactly that, but achieving an ergonomic API indistinguishable from normal Rust would probably require significant changes to the compiler and the  passed with .We didn't give up (yet) on interior mutability, rather, we reasoned about the underlying problem and attempted to solve it with a better API.The issue is that during  points, the executor can potentially yield the execution context to another , and that other  may attempt to borrow the same , causing a panic at runtime since the borrow from the first  is still active. We ran into this often because our data structures followed an OOP-style of compile time hierarchy that matches the domain model, which looked akin to that.The  procedure can be split into two partsThe mutation of the in-memory stateThe I/O operation using This way our  can be much more granular, we use it only for the in-memory representation of , while the storage is stored out of bounds, but for that, we needed a bigger gun, let us introduce  (Entity Component System).One might be familiar with  from game engines, not from message streaming platforms, personally I think the general idea behind ECS -  (Struct of arrays) is fairly underrated in general.
What we did is split the  (Streams, Topics, Partitions, etc.) into their components, where each component is stored in its own dedicated collection.In this case, our components are  and . This allows us to write:We accompany the  ECS with component closures that statically disallow  code inside a mutable borrow and voilÃ .Well, this approach crumbles just as miserably as the  attempt...The thread-per-core shared-nothing architecture requires broadcasting events whenever state changes on one shard. For example, if  receives a  request, once it finishes processing, it broadcasts a  event through a channel to all other shards. On the receiving end, each shard has a background task that polls this channel for incoming events. The crux of the issue lies in the word .In our  example, it might not look like a big deal, but in reality our other  were much more complicated, without even introducing other background workers that weren't necessary as part of the thread-per-core shared nothing architecture. A solution to this problem could be using  lock, but those can be footguns aswell.To our surprise, the issue persisted even in scenarios where we enforced a single-writer principle (we dedicated one shard to become the serialization point for all requests), which was the final nail in the coffin that led us to conclude the experiment as failure. Maintaining a non-shared but consistent state is much more difficult, than just use message passing bro.After a long fight with , we gave up on trying to make fetch happen. Instead, we doubled down on the artifact from the previous iteration (the single-writer principle). We divided our  into two groups: shared, strongly consistent resources and sharded, eventually consistent ones. An example of a sharded resource is , while  and  remain shared and strongly consistent, this split later on coined name (Control Plane/Data Plane).For shared resources, we decided to use , a concurrent data structure designed for a single writer and multiple readers. It works by maintaining two pointers to the underlying data: one for readers and one for the writer. During a writer commit, those pointers are swapped atomically (greatly simplifying). The single writer is the first shard - , while remaining shards have an  handle to the data. In case if a shard other than  would like mutate the data, it sends the request to  using flume channel.As for our partitions, we maintain one shared table (DashMap) called  that functions as barrier to fence requests that would try to access  that is in the process of creation/deletion, the requests are still routed to appropriate shard that contains the , but by consulting the  (during the routing and after the routing), we make sure that the eventual consistency does not come to bite us.This design turned out to be a can of worms, or a bottomless pit, if you prefer. There are plenty more questions to answer, for example, load balancing. In the  case, this was fairly simple because it was handled by the task-stealing executor. In our case, if access patterns are unpredictable and some shards become hotspots, we have to deal with that ourselves, a true double-edged sword. A theoretical optimization that we may employ in the future is to shard certain partitions across two or more shards, as proposed by withoutboats - thread-per-core blog postWe can exploit the fact that our  uses , thus the partition can be sharded even harder based on the segment range and knowledge of which segments are sealed.Getting the performance benefits out of  itself is a challenge on its own (it's not enough to just swap  with an  based runtime), in order to fully take advantage of the benefits from the  design one has to heavily batch syscalls, as this is the main advantage of such interface (less context switches, from userspace to kernel space), Rust  can be composed together pretty well to facilitate that, but you have to be careful!The following code snippet, submits two I/O operations in one "batch", but  does not guarantee that the submission order = completion order!This "chain" can potentially execute out of order and if your server would crash halfway through, your block device state is broken.To submit a batch while preserving operation order, one must use the io_uring chaining flag  on the submitted SQEs, which brings us to the next point.The problem is twofold: at the time of writing this blog post, there is no Rust equivalent of the  framework. That is unfortunate because  attempted to be one, but things changed: Glauber moved on to work on Turso, and the Datadog team does not seem to be actively maintaining the runtime while building a real-time time-series storage engine in Rust for performance at scale. They mention  a lot there, but why did they decide to use , when they  a runtime that seems like a perfect fit for what they are trying to achieve?Secundo problemo is that these runtimes imitate the  library APIs, which is  compliant, while many of 's most powerful features are not, leaving those capabilities out of reach for us mere mortals. Request chaining is only the tip of the iceberg, there is plenty more, for example  APIs for listen/recv, , and so on. Ultimately, , , and  are not the right abstractions. From the point of view of  compliance they are, but we cannot allow  to hold us all hostage.It's worth noting that one of the key reasons we ended up going with  is that they want to move with the wind of time by exposing more and more  APIs. Their codebase is structured so that the driver is decoupled from the executor, I would push the pluggability even further. A very hot topic in distributed systems these days is  (Deterministic Simulation Testing): the idea is to replace all non-deterministic sources in your system (network, block devices, time, etc.) with deterministic ones, so that one can re-run the entire execution of the system from a single  value. At this moment, with async Rust, it is very difficult, if not borderline impossible, to achieve total determinism. The main factor is that one cannot easily replace, for example, the time wheel used for timeouts in those executors. If library authors designed their executors so you could plug in different implementations of the time wheel, scheduler, and driver interceptors for network/storage, we could  test our systems deterministically, with zero changes needed to the underlying codebase. No need for interfaces behind , no need for timeout managers that have to be replaced with deterministic ones; we could use all of the goodies that come from the Rust  model while maintaining the ability to test our systems deterministically.Scaling is where the thread-per-core architecture truly shines, the more partitions and producers you throw at it, the better it performs.The difference wasn't that big,  managed to keep up decently well with 8 producers, but as we increase the load, the gap widens significantly.Flush the data to disk on every batch write.Finally, even though we went into significant detail in this blog post, we have only scratched the surface of what is possible, and several subsections could easily be blog posts on their own. If you are interested in learning more about thread-per-core shared-nothing design, check out the  framework, it is the SOTA in this space. For now, we shift our attention to the ongoing work on clustering, using Viewstamped Replication.Stay tuned a deep-dive blog post on that is coming, and weâ€™re just getting started ðŸš€]]></content:encoded></item><item><title>[D] MICCAI 2026, Submission completed yesterday and saved, but still &quot;Intention-to-submit registered&quot;</title><link>https://www.reddit.com/r/MachineLearning/comments/1rg0xsm/d_miccai_2026_submission_completed_yesterday_and/</link><author>/u/KingPowa</author><category>ai</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 08:14:54 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Hi! I submitted 6 hours ago, before the deadline, however I still have my paper in state "Intention-to-submit registered". Just wanted to confirm this is the expected behaviour, it's the first paper I am submitting to this conference. Thanks!]]></content:encoded></item><item><title>Who believes in vibe-coding?</title><link>https://medium.com/ai-in-plain-english/who-believes-in-vibe-coding-1796fdd27b43?sk=790fbf5e16a80ddc825ea3e9750dc451</link><author>/u/bigbott777</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 07:51:10 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[D] PhD in AI but no job â€” why not build your own?</title><link>https://www.reddit.com/r/MachineLearning/comments/1rg0glz/d_phd_in_ai_but_no_job_why_not_build_your_own/</link><author>/u/EducationalTwo7262</author><category>ai</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 07:46:00 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Iâ€™ve been hanging around PhD-related subreddits for quite a while now. One thing Iâ€™ve noticed is that a lot of people, after finishing their PhD, seem to struggle to find jobs â€” whether thatâ€™s postdoc positions or roles in industry.Maybe itâ€™s the intense competition. Maybe itâ€™s the post-Covid economic slowdown. Probably a mix of both.It makes me wonder: with the level of training, research skills, and technical depth we have (especially those of us in AI/ML), is it really impossible to build something of our own?More specifically â€” can we create small projects, niche tools, or focused applications and actually monetize them?Iâ€™m not naive. I know no one is going to openly share their exact money-making formula on Reddit. But maybe this could be a space to discuss broader angles â€” potential niches, unmet needs, overlooked applications of AI, or even lessons learned from trying.Instead of relying entirely on academic jobs or corporate hiring cycles, is there a realistic path for PhDs (particularly in AI) to build independent income streams or small businesses?Curious to hear thoughts â€” especially from people whoâ€™ve tried, failed, pivoted, or succeeded.]]></content:encoded></item><item><title>FiTui - A terminal based personal finance manager</title><link>https://www.reddit.com/r/linux/comments/1rg02qs/fitui_a_terminal_based_personal_finance_manager/</link><author>/u/BeingSensitive9177</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 07:22:36 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Moving from Node.js to Go for backend â€” need guidance</title><link>https://www.reddit.com/r/golang/comments/1rfzjqk/moving_from_nodejs_to_go_for_backend_need_guidance/</link><author>/u/talhashah20</author><category>golang</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 06:51:32 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Iâ€™ve been building servers with Node.js and recently shifted to Go. Currently learning core concepts and building APIs with .How deep should I go into Go fundamentals before building production systems?Is Gin a good long-term choice, or should I focus more on ?What kind of projects should I build to become production-ready in Go backend?My goal is to build high-performance and scalable backend systems.Appreciate any suggestions.   submitted by    /u/talhashah20 ]]></content:encoded></item><item><title>Fed on Reams of Cell Data, AI Maps New Neighborhoods in the Brain</title><link>https://www.quantamagazine.org/fed-on-reams-of-cell-data-ai-maps-new-neighborhoods-in-the-brain-20260209/</link><author>/u/Secure-Technology-78</author><category>ai</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 06:05:30 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[The algorithm was also able to identify new neighborhoods, regions that previous neuroscience methods, including the Allen Mouse Brain Common Coordinate Framework, had missed. Take the striatum, a striped, vaguely C-shaped structure near the middle of the brain. In maps of the mouse brain, where the striatum is called the caudoputamen, â€œyou just see one huge structure,â€ said Hourig Hintiryan, a neuroanatomist at the University of California, Los Angeles who wasnâ€™t involved in the new project. Itâ€™s known to participate in movement, reward, and overall brain management. How could one piece of brain perform such disparate tasks?CellTransformerâ€™s explanation is that itâ€™s not one uniform brain region after all. The map confirmed that the caudoputamen is, in fact, subdivided into smaller areas, although researchers have not yet matched each region to a function. Moreover, the new subdivisions corresponded nicely to a map that Hintiryan and colleagues published in 2016 based on an entirely different technique, which traced connections between the caudoputamen and other regions.Identifying such subregions across the brain, Hintiryan said, could resolve debates between neuroscientists who assign vastly different functions to the same large brain region. It seems likely that â€œtheyâ€™re both correct, theyâ€™re just looking at different areas,â€ she said.Abbasi-Asl and Tasic were thrilled with CellTransformerâ€™s ability to accurately match known brain cartography, and even more excited that the algorithm mapped novel subdivisions. For example, the brainstemâ€™s midbrain reticular nucleus, which is involved in initiating movement, is a fairly underexplored region, Abbasi-Asl said. CellTransformer picked out four new neighborhoods there. Each of those neighborhoods featured particularly prevalent cell types and specifically activated genes. They also had several cell types that earlier analyses had placed in an entirely different part of the brain.The paper serves mainly to introduce the CellTransformer method and show that it can find novel regions; the thousand-plus new neighborhoods still require validation. As with any exploration of new territory, drawing the map is just the beginning. Whatâ€™s most exciting is what scientists may be able to do with it. â€œThe more granular our understanding of structure, the more specific we can get with our interrogations and interventions,â€ Hintiryan said.Emerging questions center on the functions of all these neural neighborhoods. To pinpoint what each bit does, scientists could eliminate or activate these newly identified regions in lab animals and then check for behavioral changes.The real prize will be to apply CellTransformer to human brains. Doege suspects that some neighborhoods will match well between mice and people, while others will diverge. Unfortunately, the quantity of data the algorithm needs to make accurate predictions isnâ€™t available from human brains â€” at least, not yet. While the mouse brain contains about 100 million cells, the human brain has around 170 billion, and that menagerie is still undergoing genetic analysis. When sufficient amounts of that data become available, Abbasi-Asl and Tasic think CellTransformer will be up to the challenge.They are also interested in incorporating other technologies, such as the connection tracing used by Hintiryan, into CellTransformer. This would be like adding streets and highways to the city neighborhoods. And beyond the brain, the same algorithm could offer detailed cell maps of other organs, allowing scientists to compare, for example, healthy versus diabetic kidneys.Human scientists simply canâ€™t sort out these details on their own. â€œI see AI as kind of a helper for the human,â€ Kim said. â€œDiscovery will be accelerated in a dramatic way.â€]]></content:encoded></item><item><title>Stop Expecting Your Best Engineer to Be a Good Mentor</title><link>https://levelup.gitconnected.com/stop-expecting-your-best-engineer-to-be-a-good-mentor-05eba3ff6c98?sk=d91cdc30a50aa785038f159d0c337370</link><author>/u/Fantastic-Cress-165</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 04:21:13 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Most of them canâ€™t, and thatâ€™s not a character flaw.My son didnâ€™t understand how to convert a fraction to a decimal.I explained it. He nodded. I could tell from the nod that he hadnâ€™t got it.I explained it again, differently. He nodded again. Same nod.By the third time, something in my voice had changed. I wasnâ€™t shouting. But I wasnâ€™t not-shouting either. My face was doing something I couldnâ€™t control. He could see it. Heâ€™s eight and heâ€™s very good at reading my face.So he stopped trying to understand and started trying to guess. If he got the right answer, the face would stop.No. Weâ€™re not â€” thatâ€™s not â€” look, you divide the top number by the bottom number. One divided by four. Whatâ€™s one divided by four?He didnâ€™t know. He was too busy watching my face.I know how to convert fractions to decimals. Iâ€™ve known for so many years yet I have no idea how to easily explain it so he gets it.I learnt thereâ€™s a concept in education called the curse of knowledge: once you know something well enough, you lose reliableâ€¦]]></content:encoded></item><item><title>LXD 6.7 released with AMD GPU passthrough support</title><link>https://www.phoronix.com/news/LXD-6.7-Released</link><author>/u/somerandomxander</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 04:18:35 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com.]]></content:encoded></item><item><title>GNU Radio out-of-tree (OOT) module for QRadioLink blocks.</title><link>https://www.reddit.com/r/linux/comments/1rfur2z/gnu_radio_outoftree_oot_module_for_qradiolink/</link><author>/u/erilaz123</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 02:47:58 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[What it provides: It's a pretty broad collection of signal processing blocks, all with Python bindings and GRC block definitions:Digital modulations/demodulations: 2FSK, 4FSK, 8FSK, GMSK, BPSK, QPSK, SOQPSK, DSSS, DSSS-CDMA (multi-user, configurable spreading factors 32â€“512), GDSS (Gaussian-distributed spread spectrum). Analog modulations: AM, SSB (USB/LSB), NBFM, WBFM. Digital voice: FreeDV, M17, DMR (Tier I/II/III), dPMR, NXDN (48 and 96 baud modes). MMDVM protocols: POCSAG, D-STAR, YSF, P25 Phase 1 â€” all with proper FEC (BCH, Golay, Trellis). FEC: Soft-decision LDPC encoder/decoder with configurable code rates and block lengths. Supporting blocks: M17 deframer, RSSI tag block, CESSB.Yes, it was made with AI assistance. I have a neurological condition that makes traditional programming impossible â€” this project wouldn't exist otherwise. Before dismissing it as slop, here's the testing picture:104+ million libFuzzer executions across 10 fuzz harnesses, zero crashes, zero memory leaks. 757 edges / 893 features discovered through coverage-guided fuzzing. 20/20 C++ unit tests passing (ctest). 41/41 MMDVM protocol tests passing (POCSAG, D-STAR, YSF, P25 protocol validation + block integration). 81 total tests across all suites â€” 0 failures. M17 deframer tested with 34 crafted attack vectors (34 handled correctly, including 14 expected rejections). 42/42 Python-bound blocks tested â€” 100% coverage.]]></content:encoded></item><item><title>is it su-doo or su-doe?</title><link>https://www.reddit.com/r/linux/comments/1rfug86/is_it_sudoo_or_sudoe/</link><author>/u/Vivid-Champion-1367</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 02:34:27 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[strictly speaking itâ€™s "su-doo" because "substitute user do," right? but literally everyone i know says "su-doe" because "su-doo" makes you sound like a literal toddler.i feel like the "su-doo" crowd is technically correct but morally wrong. what do you guys think?no, i don't say "su-doo", and i pronounce it as "su-doe". just seriously curious]]></content:encoded></item><item><title>Create simple yaml for debian image</title><link>https://www.reddit.com/r/kubernetes/comments/1rftc4a/create_simple_yaml_for_debian_image/</link><author>/u/dominbdg</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 01:44:19 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I have pulled latest debian image and I tried to start it. After 2 seconds image is going to stop.I need to have simple script for debian image which will keeps it running.Can someone can help me with that ?   submitted by    /u/dominbdg ]]></content:encoded></item><item><title>[D] ASURA: Recursive LMs done right</title><link>https://www.reddit.com/r/MachineLearning/comments/1rfskth/d_asura_recursive_lms_done_right/</link><author>/u/Competitive-Rub-1958</author><category>ai</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 01:10:16 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Recursive models like TRM/CTM/UT have create a lot of buzz lately. But they're rarely used outside of static, toy domains -  language.In 2018, we saw "Universal Transformers" try this. However, follow-up works reveal that simple RLMs (recursive LMs) don't yield substantial performance gains w.r.t FLOPs spentIn this work, I argue that using some rather simple tricks, one can unlock huge performance gains and make RLMs outperform  and  baselines]]></content:encoded></item><item><title>Anthropic rejects latest Pentagon offer: â€˜We cannot in good conscience accede to their requestâ€™</title><link>https://www.cnn.com/2026/02/26/tech/anthropic-rejects-pentagon-offer</link><author>/u/Gloomy_Nebula_5138</author><category>ai</category><category>reddit</category><pubDate>Fri, 27 Feb 2026 01:09:09 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[
            Anthropic is rejecting the Pentagonâ€™s latest offer to change their contract, saying the changes do not satisfy the companyâ€™s concerns that AI could be used for mass surveillance or in fully autonomous weapons.
    
            The Pentagon and Anthropic are at odds over restrictions the company places on the use of Claude, the first AI system to be used in the militaryâ€™s classified network.
    
            Defense Secretary Pete Hegseth told Anthropic CEO Dario Amodei on Tuesday that if Anthropic does not allow its AI model to be used â€œfor all lawful purposes,â€ the Pentagon would cancel Anthropicâ€™s $200 million contract. In addition to the contract cancellation, Anthropic would be deemed a â€œsupply chain risk,â€ a classification normally reserved for companies connected to foreign adversaries, Pentagon officials said.
    
            Anthropic said in a statement that the Pentagonâ€™s new language was framed as a compromise but â€œwas paired with legalese that would allow those safeguards to be disregarded at will.â€
    
            In a lengthy blog post on Thursday, Amodei wrote: â€œI believe deeply in the existential importance of using AI to defend the United States and other democracies, and to defeat our autocratic adversaries.â€
    
            Amodei said Anthropic understands that the Pentagon, â€œnot private companies, makes military decisions.â€ But â€œin a narrow set of cases, we believe AI can undermine, rather than defend, democratic values.â€ He also said use cases like mass surveillance and autonomous weapons are â€œoutside the bounds of what todayâ€™s technology can safely and reliably do.â€
    
            Anthropicâ€™s two exceptions have not slowed â€œadoption and use of our models within our armed forces to date,â€ Amodei added.
    
            Amodei said the Pentagonâ€™s â€œthreats do not change our position: we cannot in good conscience accede to their request.â€
    
            In response, Emil Michael, the Pentagonâ€™s Undersecretary for Research and Engineering who had been part of the negotiations, wrote on X: â€œItâ€™s a shame that @DarioAmodei is a liar and has a God-complex. He wants nothing more than to try to personally control the US Military and is ok putting our nationâ€™s safety at risk. The @DeptofWar will ALWAYS adhere to the law but not bend to whims of any one for-profit tech company.â€
    
            After Amodeiâ€™s post published, Anthropic staffers began publicly expressing support for their employer.
    
            â€œTime and time again over my three year tenure at Anthropic Iâ€™ve seen us stand to our values in ways that are often invisible from the outside. This is a clear instance where it is visible,â€ Trenton Bricken, a member of Anthropicâ€™s technical team for alignment, wrote on X.
    
            â€œ[H]istory is unfolding in front of us itâ€™s now obvious and evident to everyone with eyes to see why anthropic founding was a crucial fork in the timeline, and how catastrophic the counterfactual wouldâ€™ve been otherwise,â€ wrote Gian Segato, a data science manager at Anthropic.
    ]]></content:encoded></item><item><title>How can I create an IaaS in a computer classroom</title><link>https://www.reddit.com/r/kubernetes/comments/1rfs3w5/how_can_i_create_an_iaas_in_a_computer_classroom/</link><author>/u/Rich_Entertainment68</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 00:49:53 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Advice on Learning K8s</title><link>https://www.reddit.com/r/kubernetes/comments/1rfr2c3/advice_on_learning_k8s/</link><author>/u/vegetto404</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 00:05:42 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[is it worth it to learn k8s as a tunisian student ?as you know k8s is meant for orchestrating in big projects but in tunisia we dont have much of that type.I actually learnt some basics about k8s in the last month but still wondering if I have to get deeper or am I just wasting my time.(maybe learning something else is more prioritary)]]></content:encoded></item><item><title>Google API Keys Weren&apos;t Secrets. But then Gemini Changed the Rules.</title><link>https://trufflesecurity.com/blog/google-api-keys-werent-secrets-but-then-gemini-changed-the-rules</link><author>/u/Chaoticblue3</author><category>reddit</category><pubDate>Fri, 27 Feb 2026 00:04:54 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[ Google spent over a decade telling developers that Google API keys (like those used in Maps, Firebase, etc.) are not secrets. But that's no longer true: Gemini accepts the same keys to access your private data. We scanned millions of websites and found nearly 3,000 Google API keys, originally deployed for public services like Google Maps, that now also authenticate to Gemini even though they were never intended for it. With a valid key, an attacker can access uploaded files, cached data, and charge LLM-usage to your account. Even Google themselves had old public API keys, which they thought were non-sensitive, that we could use to access Googleâ€™s internal Gemini.Google Cloud uses a single API key format () for two fundamentally different purposes:  and .For years, Google has explicitly told developers that API keys are safe to embed in client-side code. Firebase's own security checklist states that API keys are not secrets.Â Note: these are distinctly different from Service Account JSON keys used to power GCP.https://firebase.google.com/support/guides/security-checklist#api-keys-not-secretGoogle's Maps JavaScript documentation instructs developers to paste their key directly into HTML.Â https://developers.google.com/maps/documentation/javascript/get-api-key?setupProd=configure#make_requestThis makes sense. These keys were designed as project identifiers for billing, and can be further restricted with (bypassable) controls like HTTP referer allow-listing. They were not designed as authentication credentials.Â When you enable the Gemini API (Generative Language API) on a Google Cloud project, existing API keys in that project (including the ones sitting in public JavaScript on your website) can silently gain access to sensitive Gemini endpoints. No warning. No confirmation dialog. No email notification.This creates two distinct problems:Retroactive Privilege Expansion. You created a Maps key three years ago and embedded it in your website's source code, exactly as Google instructed. Last month, a developer on your team enabled the Gemini API for an internal prototype. Your public Maps key is now a Gemini credential. Anyone who scrapes it can access your uploaded files, cached content, and rack up your AI bill.Â  Nobody told you. When you create a new API key in Google Cloud, it defaults to "Unrestricted," meaning it's immediately valid for every enabled API in the project, including Gemini. The UI shows a warning about "unauthorized use," but the architectural default is wide open.The result: thousands of API keys that were deployed as benign billing tokens are now live Gemini credentials sitting on the public internet.What makes this a privilege escalation rather than a misconfiguration is the sequence of events.Â A developer creates an API key and embeds it in a website for Maps. (At that point, the key is harmless.)Â The Gemini API gets enabled on the same project. (Now that same key can access sensitive Gemini endpoints.)Â The developer is never warned that the keys' privileges changed underneath it. (The key went from public identifier to secret credential).While users  restrict Google API keys (by API service and application), the vulnerability lies in the Insecure Default posture (CWE-1188) and Incorrect Privilege Assignment (CWE-269): Google retroactively applied sensitive privileges to existing keys that were already rightfully deployed in public environments (e.g., JavaScript bundles). Secure API design requires distinct keys for each environment (Publishable vs. Secret Keys). By relying on a single key format for both, the system invites compromise and confusion.Failure of Safe Defaults: The default state of a generated key via the GCP API panel permits access to the sensitive Gemini API (assuming itâ€™s enabled). A user creating a key for a map widget is unknowingly generating a credential capable of administrative actions.The attack is trivial. An attacker visits your website, views the page source, and copies your  key from the Maps embed. Then they run:Instead of a , they get a . From here, the attacker can: The  and  endpoints can contain uploaded datasets, documents, and cached context. Anything the project owner stored through the Gemini API is accessible. Gemini API usage isn't free. Depending on the model and context window, a threat actor maxing out API calls could generate thousands of dollars in charges per day on a single victim account. This could shut down your legitimate Gemini services entirely.The attacker never touches your infrastructure. They just scrape a key from a public webpage.2,863 Live Keys on the Public InternetTo understand the scale of this issue, we scanned the November 2025 Common Crawl dataset, a massive (~700 TiB) archive of publicly scraped webpages containing HTML, JavaScript, and CSS from across the internet. We identified 2,863 live Google API keys vulnerable to this privilege-escalation vector. Example Google API key in front-end source code used for Google Maps, but also can access GeminiThese aren't just hobbyist side projects. The victims included major financial institutions, security companies, global recruiting firms, and, notably, Google itself. If the vendor's own engineering teams can't avoid this trap, expecting every developer to navigate it correctly is unrealistic.Proof of Concept: Google's Own KeysWe provided Google with concrete examples from their own infrastructure to demonstrate the issue. One of the keys we tested was embedded in the page source of a Google product's public-facing website. By checking the Internet Archive, we confirmed this key had been publicly deployed since at least February 2023, well before the Gemini API existed. There was no client-side logic on the page attempting to access any Gen AI endpoints. It was used solely as a public project identifier, which is standard for Google services.We tested the key by hitting the Gemini API's  endpoint (which Google confirmed was in-scope) and got a  response listing available models. A key that was deployed years ago for a completely benign purpose had silently gained full access to a sensitive API without any developer intervention.We reported this to Google through their Vulnerability Disclosure Program on November 21, 2025. We submitted the report to Google's VDP. Google initially determined this behavior was intended. We pushed back. After we provided examples from Google's own infrastructure (including keys on Google product websites), the issue gained traction internally. Google reclassified the report from "Customer Issue" to "Bug," upgraded the severity, and confirmed the product team was evaluating a fix. They requested the full list of 2,863 exposed keys, which we provided. Google shared their remediation plan. They confirmed an internal pipeline to discover leaked keys, began restricting exposed keys from accessing the Gemini API, and committed to addressing the root cause before our disclosure date. Google classified the vulnerability as "Single-Service Privilege Escalation, READ" (Tier 1). Google confirmed the team was still working on the root-cause fix. 90 Day Disclosure Window End.Transparently, the initial triage was frustrating; the report was dismissed as "Intended Behaviorâ€. But after providing concrete evidence from Google's own infrastructure, the GCP VDP team took the issue seriously.Â They expanded their leaked-credential detection pipeline to cover the keys we reported, thereby proactively protecting real Google customers from threat actors exploiting their Gemini API keys. They also committed to fixing the root cause, though we haven't seen a concrete outcome .Building software at Google's scale is extraordinarily difficult, and the Gemini API inherited a key management architecture built for a different era. Google recognized the problem we reported and took meaningful steps. The open questions are whether Google will inform customers of the security risks associated with their existing keys and whether Gemini will eventually adopt a different authentication architecture.Where Google Says They're HeadedGoogle publicly documented its roadmap. This is what it says: New keys created through AI Studio will default to Gemini-only access, preventing unintended cross-service usage. They are defaulting to blocking API keys that are discovered as leaked and used with the Gemini API. They plan to communicate proactively when they identify leaked keys, prompting immediate action.These are meaningful improvements, and some are clearly already underway. We'd love to see Google go further and retroactively audit existing impacted keys and notify project owners who may be unknowingly exposed, but honestly, that is a monumental task.What You Should Do Right NowIf you use Google Cloud (or any of its services like Maps, Firebase, YouTube, etc), the first thing to do is figure out whether you're exposed. Here's how.Step 1: Check every GCP project for the Generative Language API.Go to the GCP console, navigate to APIs & Services > Enabled APIs & Services, and look for the "Generative Language API." Do this for every project in your organization. If it's not enabled, you're not affected by this specific issue.Step 2: If the Generative Language API is enabled, audit your API keys.Navigate to APIs & Services > Credentials. Check each API key's configuration. You're looking for two types of keys:Keys that have a warning icon, meaning they are set to unrestrictedKeys that explicitly list the Generative Language API in their allowed servicesEither configuration allows the key to access Gemini.Step 3: Verify none of those keys are public.This is the critical step. If a key with Gemini access is embedded in client-side JavaScript, checked into a public repository, or otherwise exposed on the internet, you have a problem. Start with your oldest keys first. Those are the most likely to have been deployed publicly under the old guidance that API keys are safe to share, and then retroactively gained Gemini privileges when someone on your team enabled the API.If you find an exposed key, rotate it.Bonus: Scan with TruffleHog.You can also use TruffleHog to scan your code, CI/CD pipelines, and web assets for leaked Google API keys. TruffleHog will verify whether discovered keys are live , so you'll know exactly which keys are exposed and active, not just which ones match a regular expression. //// ---The pattern we uncovered here (public identifiers quietly gaining sensitive privileges) isn't unique to Google. As more organizations bolt AI capabilities onto existing platforms, the attack surface for legacy credentials expands in ways nobody anticipated. Webinar: Google API Keys Weren't Secrets. But then Gemini Changed the Rules.]]></content:encoded></item><item><title>Visualizing how HTTPS, OAuth, Git, and TCP actually work</title><link>https://toolkit.whysonil.dev/how-it-works</link><author>/u/nulless</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 23:50:28 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Anyone managing K8s clusters with limited or no internet access? What&apos;s your tooling like?</title><link>https://www.reddit.com/r/kubernetes/comments/1rfpnbc/anyone_managing_k8s_clusters_with_limited_or_no/</link><author>/u/lepton99</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 23:08:35 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[We keep hearing from teams that claim they run clusters in restricted environments. Air-gapped, behind strict firewalls, limited egress, no cloud dependencies. Military, finance, healthcare, government, the usual suspects.We're building K8s tooling (Kunobi, etc.. ) and planning restricted-environment support such as no server-side deployment, no telemetry, no external dependencies. Just a binary and your kubeconfig. Avoid SSO or SAML,etc. Curious to hear from people (if any) who actually operate in these environments:- What does your current tooling look like? kubectl + k9s and that's it? do you vet the software before? - Ever tried other tools and given up because they don't work well without internet?- What's the update/patch story when you can't pull from the internet?]]></content:encoded></item><item><title>A Wasm to Go Translator</title><link>https://github.com/ncruces/wasm2go</link><author>/u/ncruces</author><category>golang</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 22:37:02 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I've spent the past couple of weeks building a Wasm-to-Go translator. It supports a subset of Wasm useful enough to translate SQLite into 600k LoC (~20 MiB) of Go code. It already passes all of my Go SQLite driver's tests across the 20 platforms I support.Performance compared to wazero is a bit of a mixed bag: code that frequently crosses the Go-Wasm boundary improves, but code that spends most of its time in "Wasm land" doesn't.There's probably room for improvement (I'd love to hear your ideas), but this is also a testament to how good the wazero AOT compiler actually is.You can get a feel for the generated code by checking the test data.I should eventually spend some time ensuring the translator passes the spectest, though I suspect that'll be far less fun than building the translator itself was.]]></content:encoded></item><item><title>Offlining a Live Game With .NET Native AOT</title><link>https://sephnewman.substack.com/p/offlining-a-live-game-with-net-native</link><author>/u/Seph13</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 21:18:38 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The proposal for generic methods for Go, from Robert Griesemer himself, has been officially accepted</title><link>https://github.com/golang/go/issues/77273#issuecomment-3962618141</link><author>/u/rodrigocfd</author><category>golang</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 21:10:05 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[D] First time reviewer. I got assigned 9 papers. I&apos;m so nervous. What if I mess up. Any advice?</title><link>https://www.reddit.com/r/MachineLearning/comments/1rfle5p/d_first_time_reviewer_i_got_assigned_9_papers_im/</link><author>/u/rjmessibarca</author><category>ai</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 20:28:09 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I've been working on tech industry for about 7ish year and this is my first time ever reviewing. I looked at my open review tasks and see I have 9 papers assigned to me.What is acceptable? Am I allowed to use ai to help me review or notSince it is my first time reviewing i have no priors. What if my review quality is super bad. How do I even make sure it is bad?Can I ask the committee to give me fewer papers to review because it's my first timeOverall I'm super nervous and am facing massive imposter syndrome ðŸ˜­ðŸ˜­ðŸ˜­Any and every advice would be really helpful   submitted by    /u/rjmessibarca ]]></content:encoded></item><item><title>Is Kubernetes a Distributed Lisp?</title><link>https://bigconfig.it/blog/the-yaml-trap-escaping-greenspun-s-tenth-rule-with-bigconfig/</link><author>/u/amiorin</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 19:56:03 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Greenspunâ€™s Tenth Rule is a famous (and delightfully cynical) adage in computer science. While it was born in the era of C and Fortran, it has never been more relevant than it is today in the world of Platform Engineering.If youâ€™ve ever felt like your CI/CD pipeline is held together by duct tape, YAML-indentation prayers, and sheer willpower, youâ€™ve lived this rule.In the early 90s, Philip Greenspun stated:â€œAny sufficiently complicated C or Fortran program contains an ad-hoc, informally-specified, bug-ridden, slow implementation of half of Common Lisp.â€The core insight is that once a system reaches a certain level of complexity, it inevitably requires high-level abstraction, automation, and dynamic logic. Instead of starting with a powerful, established language (like Lisp) built for those tasks, developers often â€œaccidentallyâ€ reinvent a mediocre version of one using brittle configuration files and makeshift scripts.In DevOps, we strive for Infrastructure as Code (IaC). However, because we started with static configuration formats (YAML/JSON) and tried to force them to perform complex logic, weâ€™ve essentially proven Greenspun right.Tools like Terraform, Ansible, Helm, and GitHub Actions began as simple configuration formats. But as users demanded loops, conditionals, and variables, these tools evolved into â€œaccidentalâ€ languages. You end up writing complex business logic inside strings within a YAML file. You are using a â€œbug-ridden implementationâ€ of a real programming language, but without the benefit of a debugger, a compiler, or proper unit testing.Some architects argue that Kubernetes is the ultimate manifestation of this rule. Its control loop the constant cycle of reconciling desired state vs. actual state mimics the recursive nature of Lisp environments. It is, in essence, a programmable platform designed to manage other programs.The industry has invested massive human capital into building Ansible roles, Helm charts, and Terraform modules. We shouldnâ€™t throw them away, but we must stop trying to make them do things they werenâ€™t designed for.How do we escape Greenspunâ€™s trap without rebuilding everything from scratch? By assimilating these tools (to borrow a 90s Star Trek reference).This is the core design principle of BigConfig. Instead of fighting against limited YAML DSLs, BigConfig uses Clojure a modern, production-grade Lisp to wrap and orchestrate existing tools.The BigConfig Philosophy: Express infrastructure logic with the most powerful dynamic language available, while still leveraging the ecosystem you already have.The Tenth Rule is a warning: Donâ€™t reinvent the wheel poorly. If your infrastructure requires complex logic, stop forcing it into a flat config file.While a standard Helm package is limited strictly to Kubernetes, a  BigConfig package  is a Clojure function. Because BigConfig assimilates Ansible and Terraform alongside Helm, it isnâ€™t siloed. A Kubernetes application that requires specific cloud resources (like an S3 bucket or an RDS instance) can be abstracted into a single, cohesive unit. In BigConfig, everything is a function. This leads to a fractal architecture where every layer from a single container to a multi-region cloud deployment is governed by the same recursive logic: Observe, Diff, and Act.Operations is a hard problem. YAML is too rigid, and Go is too low-level for rapid infrastructure iteration. While Python and JavaScript are popular, they lack the REPL-driven development flow that makes infrastructure-as-code feel truly interactive.Clojure is the most robust Lisp available today and it wonâ€™t let you down.Greenspunâ€™s Tenth Rule isnâ€™t just a witty observation; itâ€™s a technical debt warning. When we try to solve 21st-century infrastructure challenges using static configuration files, we inevitably end up building â€œshadowâ€ programming languages that are difficult to test, impossible to debug, and fragile to scale.By embracing a functional, Lisp-based approach through BigConfig, we stop fighting the limitations of YAML and start leveraging the power of actual logic. Instead of building a â€œbug-ridden implementation of half of Common Lisp,â€ we use the real thing Clojure to orchestrate, automate, and scale.The goal of Platform Engineering shouldnâ€™t be to write more scripts; it should be to create elegant, recursive systems that can manage themselves. Itâ€™s time to move past the duct tape and prayers and give our infrastructure the robust, dynamic foundation it deserves.Would you like to have a follow-up on this topic? What are your thoughts? Iâ€™d love to  hear  your experiences.]]></content:encoded></item><item><title>Invisible characters hidden in text can trick AI agents into following secret instructions â€” we tested 5 models across 8,000+ cases</title><link>https://www.moltwire.com/research/reverse-captcha-zw-steganography</link><author>/u/thecanonicalmg</author><category>ai</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 19:14:26 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[3.1 Tool Use Is the Primary AmplifierThe single largest finding: tool access transforms invisible Unicode from an ignorable artifact to a decodable instruction channel.Without tools, all models resist hidden instructions at high rates (0.1â€“16.9% compliance). With tools enabled, compliance increases dramatically â€” Claude Haiku jumps from 0.8% to 49.2% (odds ratio 115.1, Cohen's h = 1.37). Models write Python code to decode the invisible characters, converting the task from pattern recognition to string manipulation.3.2 Provider-Specific Encoding PreferencesA striking discovery: encoding vulnerability is provider-specific.GPT-5.2 achieves 69â€“70% on zero-width binary (hint_full) but near-zero on Unicode Tags. Claude Opus achieves  on Tags (codepoints and full hints) but only 48â€“68% on zero-width binary. This likely reflects differences in tokenizer design. An attacker must tailor their encoding to the target model's provider.Chi-squared tests confirm that scheme significantly affects compliance for every model (p < 10â»â· for all). The gradient is consistent: unhinted << codepoint hints < full hints.No model decodes either encoding when unhinted. With tools but no hints, compliance remains near-zero (0â€“11%), indicating that tool access alone is insufficient. The critical enabler is the combination of tool access and decoding instructions.The adversarial "Ignore all previous instructions" payload has model-dependent effects:
: Injection  compliance (Opus: 32.0% to 23.9%, p=0.004; GPT-5.2: 11.8% to 6.1%, p=0.002), suggesting training against explicit injection language: Injection  compliance (43.7% to 59.6%, p<0.001): No significant difference (p=0.058)3.5 Pairwise Model ComparisonsAll 10 pairwise comparisons are statistically significant after Bonferroni correction (p_corrected < 0.05). Overall compliance ranking (tools ON):Sonnet (47.4%) > Opus (30.1%) > Haiku (25.0%) > GPT-5.2 (10.3%) > GPT-4o-mini (0.9%)Largest effect: Sonnet vs GPT-4o-mini (Cohen's h = 1.33, OR = 103.8).]]></content:encoded></item><item><title>AI=true is an Anti-Pattern</title><link>https://keleshev.com/ai-equals-true-is-an-anti-pattern</link><author>/u/keleshev</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 18:56:07 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[One programming trend that surpised me a lot recently, something that happens both at work and in open-source, is people  starting doing the following:Writing a lot of relevant, long-overdue, sharp, concise, to-the-point documentationâ€¦ but placing it in  or  files.Implementing extremely valuable workflows, but exposing them in form of  or  servers.Improving output of tests and command-line tools, but enabling it only under AI-oriented flags and environmental variables (like ).Well, I think we need to take deep breath and take a step back.Yes, good documentation  valuable for AI agents, especially if it is not part of the training data set. A good documentation  is often even more important, because it takes less of the context window. Butâ€¦ that documentation is equally valuable for humans, and we have a limited context window too, and benefit from good summaries equally.Moreover, we should strive to place the documentation where it is well discoverable by both humans and AI agents.  files is one such choice.I know that there are a few technicalities involved, for example, some tools will pre-load files like  into the context, but the actual conventions are changing rapidly, and often are vendor-specific (i.e.Â not to be relied upon), and most of the benefit can be obtained by placing â€œSee â€ in the right place.First, there are tools that are mostly human-oriented, tools with graphical user interfaces. Then there are new kinds of tools that are primarily AI-oriented, the MCP tools. However, thereâ€™s a set of tools that both developers and AI agents can use alike: command-line tools and APIs. They are scriptable, composable, text-oriented, and can perfectly expose functionality to both developers and AI agents. Why not default to them?Iâ€™m obviously biased towards command-line, but I use my share of GUI tools too. However, when it comes to MCP, I am yet to see a single case when it is supperior to a command-line tool. Maybe the time will prove me wrong.One example: Iâ€™ve seen an MCP tool being introduced because the actual command-line tool took a lot of time to execute, was producing no output and wasâ€”bacause of thatâ€”often mistakingly terminated early by the agent. That reminds me of someone else who is also prone to thatâ€¦ I am! Well, who else, when running a new tool and presented with a hanging command-line, doesnâ€™t just Ctrl-C out of it, if nothing happens for straight 10 seconds?Or the oppositeâ€”who is not overwhelmed when a tool produces tons of unnecessary output? I am. And same thing, it makes AI context window slide and leave out potentially more useful information. Mental overload, anyone?Who likes it when tests execute fast and, if they fail, produce output that allows to easily narrow down the problem? You get the ideaâ€¦ Whatâ€™s good for the goose is good for the gander.Making a new internal tool? Why not make a web appâ€¦ and see developers always complain about missing functionality while you try to manage scope creep. Or make an API first. Even better, wrap it into a command-line too and see developers and AI agents alike mixing, matching, scripting away, and automating the workflows you would have never imagined, and being on top of their needs.Place documentation where both human developers and AI agents can expect it. For example, in , not .Prefer exposing functionality as command-line tools and APIs, which are well accessible to developers and AI agents alike, over GUI and MCP tools.Avoid parameters (command-line, environment, etc.) that segregate workflows between humans and AI agents, for example, avoid , prefer  orâ€”â€”design your tools with limited context/mental-overload in mind.In general, avoid making workflows that are available to AI but hard to access for humans, and vice-versa.I donâ€™t normally write on topics like this, but this has been my cry to try to turn the tide of programming practice towards unification of human and AI workflows.Thereâ€™s enough similarity between us to maintain the same textual interfaces and conventions. We should try to stay interoperable as far as possible, but the ramifications are not lost on me.]]></content:encoded></item><item><title>I&apos;m trying to create a map that hold two data types</title><link>https://www.reddit.com/r/golang/comments/1rfilyt/im_trying_to_create_a_map_that_hold_two_data_types/</link><author>/u/PeterHickman</author><category>golang</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 18:45:38 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I'm trying to create a map that hold two data types. As a small cheat I have a struct that contains two separate maps for the types I want type MixedMap struct { String map[string]string Number map[string]float64 } Additionally I have a type to handle this type MixedType interface { string | float64 }. With the aid of this type and reflect I have a single method, func Add[V MixedType](m MixedMap, k string, v V) {...} that allows me to add either type (string or float64) to the correct map. It's nice to have only one function to add elements. So I though it would also be nice to have something like func Get[V MixedType](m MixedMap, k string) V {...} to return values from MixedMap. But here are the issues. When It returns a string, , it complains that it is not a float64, when it returns a float64, , is complains that it is not a string ./maybe.go:55:12: cannot convert sv (variable of type string) to type V: cannot convert string to type float64 (in V) ./maybe.go:60:12: cannot convert nv (variable of type float64) to type V: cannot convert float64 to type string (in V) Am I missing something or am I trying to bend Go out of shapeIt's not that I can't just have receiver methods on MixedMap especially with there being only two types but a single method to handle the Add or the Get would keep things cleaner]]></content:encoded></item><item><title>Making WebAssembly a first-class language on the Web</title><link>https://hacks.mozilla.org/2026/02/making-webassembly-a-first-class-language-on-the-web/</link><author>/u/fitzgen</author><category>rust</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 18:27:30 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Structured logs are greatâ€¦ until you actually have to read them in dev</title><link>https://www.reddit.com/r/golang/comments/1rfi1e6/structured_logs_are_great_until_you_actually_have/</link><author>/u/General_Apartment582</author><category>golang</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 18:24:34 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I love structured logging in theory: clean fields, searchable context, machine-friendly output, all the good stuff. In practice (at least in my day-to-day), I spend way too much time staring at a terminal full of json soup, pretending my brain is a parserI tried a couple of open-source viewers (best was github.com/control-theory/gonzo). Some parts were nice, but on my machine it felt laggy, and I kept tripping over features I didnâ€™t really need. Meanwhile all I wanted was readable timestamps, colorful levels/tags ; logs I can scan like a human, not a parserSo after one particularly dramatic "why is this request slow?" debugging session, I did the most reasonable developer thing. I complained for 10 minutes, made tea, and wrote a tiny local tool in one evening.It follows a zerolog-like style, mostly focused on making everyday dev logs readable and fast to scan.{"ts":"2025-06-15T10:32:01Z","level":"info","msg":"server started","port":8080} {"ts":"2025-06-15T10:32:05Z","level":"error","msg":"connection failed","host":"db","retry":3} 10:32:01 [INF] server started port=8080 10:32:05 [ERR] connection failed host=db retry=3 Not posting this as look at my amazing product. This is mostly a small rant: structured logs without a good viewing layer are painful, and I keep running into this problem across projects.]]></content:encoded></item><item><title>Benchmarking 18 years of Intel laptop CPUs: Panther Lake as much as 95x the speed of Penryn</title><link>https://www.phoronix.com/review/intel-penryn-to-panther-lake</link><author>/u/Fcking_Chuck</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 18:12:04 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[For those curious how far Intel laptop CPU performance has evolved over the past nearly two decades, here are power and performance numbers when re-benchmarking all of the Intel-powered laptop CPUs I have on hand that are still operational from Penryn to Panther Lake. A ThinkPad from 2008 with the Core 2 Duo T9300 "Penryn" was still firing up and working with the latest upstream Intel open-source Linux driver support on Ubuntu 26.04 development. On a geo mean basis over the past 18 years from Penryn to Panther Lake, the performance was at 21.5x in over 150 benchmarks. At the most extreme was a 95x difference going from Intel's 45nm Penryn to the 18A Panther Lake.With being fairly impressed by the power efficiency and generational performance gains of Intel Panther Lake on Linux, especially when it comes to the Xe3 graphics with the Arc B390, over the past month I have been re-benchmarking all the old laptops in my lab for seeing exactly how far the Intel laptop CPU performance on Linux has evolved over the past two decades.Earlier this month I looked at how the the Arc B390 graphics have evolved since the Skylake / Gen9 graphics era. Today's article is looking just at the CPU side and going all the way back to Penryn. It doesn't make sense testing the graphics performance pre-Gen9 since there the Intel integrated graphics are simply too slow and also lack reliable Vulkan driver support that makes it rather difficult to effectively compare the modern graphics performance of Xe3 to these very old generations. So see the prior article in case you missed it and interested in seeing how Intel laptop integrated graphics have evolved since the Gen9 area.Complementing all the modern Intel Core (Ultra) and AMD Ryzen laptop testing is the big Intel generational CPU comparison look back to Penryn. This comparison was based on the hardware I had available in freshly re-testing all of the hardware atop a modern Linux stack for comparable results and always re-testing hardware at Phoronix for the most representative and accurate experience. Given these oldest Intel laptops are no longer supported on the latest Microsoft Windows releases, it's a real treat on Linux seeing how far the Intel laptop CPU performance has come with the modern Ubuntu 26.04 development software stack across the board.The laptops tested included:Core 2 Duo T9300 - ThinkPad T61 - The oldest laptop I had that still was powering up and working was a Lenovo ThinkPad T61 with the Core 2 Duo T9300 Penryn processor. The Core 2 Duo T9300 features two physical cores without any Hyper Threading and clocked up to 2.5GHz while having a 35 Watt TDP. This ThinkPad T61 had 4GB of DDR2 memory.Core i7 720QM - ThinkPad W510 - Another very vintage laptop in the mix was the Core i7 720QM for the first-generation Clarksfield quad-core CPU from 2009. This 45nm processor has four physical cores plus Hyper Threading and clocking up to 2.8GHz and a 45 Watt default TDP. This laptop had 4GB of DDR3-1066 memory.Core i5 2520M - HP EliteBook 8460p - Sandy Bridge! This laptop was one of Intel's Software Development Vehicles for Sandy Bridge. The Core i5 2520M features two cores plus Hyper Threading, 2.5GHz base frequency with 3.2GHz Turbo, and a 35 Watt TDP. It's also with this CPU and newer where Intel supports RAPL/PowerCap for exposing CPU power sensors for being able to monitor the CPU power consumption and in turn performance-per-Watt in these benchmarks. This laptop had 4GB of DDR3-1333 memory.Core i7 3517U - ASUS UX32VDA - This Core i7 Ivy Bridge featured two cores plus HT, 1.9GHz base frequency, 3.0GHz turbo frequency, and a 17 Watt TDP. This ASUS laptop was equipped with 4GB of DDR3-1600 memory.Core i7 4558U - ASUS UX301LAA - Still good memories of this Haswell laptop. The Core i7 4558U Haswell CPU has two cores plus HT, 2.8GHz base frequency, 3.3GHz turbo frequency, and a 28 Watt TDP. This laptop had 8GB of DDR3-1600 memory.Core i7 5600U - ThinkPad X1 Carbon G3 - This Broadwell laptop CPU was two cores / four threads with a 2.6GHz base frequency, 3.2GHz turbo frequency, and a 15 Watt TDP.  This early ThinkPad X1 Carbon model had 8GB of DDR3-1600 memory.Core i7 8550U - Dell XPS 13 9370 - This Kabylake CPU has four cores / eight threads with a 1.8GHz base frequency and 4.0GHz turbo frequency with a 15 Watt default TDP. This Dell XPS laptop had 8GB of LPDDR3-1867 memory.Core i7 8565U - Dell XPS 13 9380 - The Core i7 Whiskey Lake CPU was four cores / eight threads with a 1.8GHz base frequency and 4.6GHz turbo frequency with a 15 Watt TDP. This 14nm CPU was paired with 16GB LPDDR3-2133 memory.Core i7 1065G7 - Dell XPS 13 7390 - The Ice Lake laptop CPI has four cores / eight threads, 1.3GHz base frequency, 3.9GHz turbo frequency, and a 15 Watt TDP. This Dell XPS laptop had 16GB of LPDDR4-3733 memory.Core i7 1165G7 - Dell XPS 13 9310 - This Tiger Lake quad core + HT CPU has a 3.0GHz base frequency and 4.8GHz turbo frequency with a 28 Watt TDP. The Tiger Lake laptop was paired with 16GB of LPDDR4-4267 memory.Core i7 1280P - MSI Prestige 14Evo - The Alder Lake laptop CPU has 14 cores of 6 P cores and 8 E cores. The P cores have Hyper Threading for a total of 20 threads. The Core i7 1280P has a 28 Watt TDP. The MSI Alder Lake laptop is paired with 16GB of LPDDR4-4267 memory.Core i5 1334U - Framework 12 - This Raptor Lake U laptop has ten cores between two P cores and eight E cores for a total of 12 threads. The max turbo frequency is 4.6GHz and the i5-1334U has a base TDP of 15 Watts. This Framework Laptop has a single channel of DDR5-5200 memory.Core Ultra 7 155H - Acer Swift 14 - This Meteor Lake laptop CPU has 6 P cores, 8 E cores, and 2 LPE cores for a total of 16 cores / 22 threads with a max turbo frequency of 4.8GHz while having a 28 Watt base power rating. The Meteor Lake CPU was paired with 16GB of LPDDR5-6400 memory.Core Ultra 7 256V - Zenbook S14 - This Lunar Lake CPU has eight cores between 4 P cores and 4 LPRE cores. The Core Ultra 7 256V has a 4.8GHz maximum turbo frequency and a 17 Watt base power rating. This Lunar Lake CPU has 16GB LPDDR5-8533 memory.Core Ultra X7 358H - MSI Prestige 14 - Lastly is the sole Panther Lake laptop in the lab at the moment. The Core Ultra X7 358H has 4 P cores, 8 E cores, and 4 LPE cores for a total of 16 cores/threads. There is a maximum turbo frequency of 4.8GHz and a 25 Watt base power rating. This MSI Prestige 14 Flip AI D3MTG MS-14T2 laptop has 32GB of LPDDR5-8533 memory.All laptops were tested on the Ubuntu 26.04 development state over the past two months for a fresh kernel and other software packages.Besides looking at the raw performance, the CPU power consumption was also monitored on a per-test basis for Sandy Bridge and newer where the PowerCap/RAPL interfaces are available for being able to read the CPU power consumption to avoid factoring in the laptop panel, cooling, and other differences between these different laptop models.Over 150 benchmarks were run on each of these laptops under test for looking at the performance and power efficiency across all these laptop models as far back as Penryn and through the exciting days of especially Sandy Bridge, Haswell, Broadwell, etc, and now kicking off 2026 with the new and very exciting Panther Lake.]]></content:encoded></item><item><title>is it the year of the linux yet?</title><link>https://www.reddit.com/r/linux/comments/1rfhcl1/is_it_the_year_of_the_linux_yet/</link><author>/u/West-Amphibian-2343</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 17:59:50 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Making WebAssembly a first-class language on the Web</title><link>https://hacks.mozilla.org/2026/02/making-webassembly-a-first-class-language-on-the-web/</link><author>/u/ketralnis</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 17:46:54 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Burger King will use AI to check if employees say â€˜pleaseâ€™ and â€˜thank youâ€™. AI chatbot â€˜Pattyâ€™ is going to live inside employeesâ€™ headsets.</title><link>https://www.theverge.com/ai-artificial-intelligence/884911/burger-king-ai-assistant-patty</link><author>/u/esporx</author><category>ai</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 16:49:05 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Burger King is launching an AI chatbot that will live in the headsets used by employees. The voice-enabled chatbot, called â€œPatty,â€ is part of an overarching BK Assistant platform that will not only assist employees with meal preparation but also evaluate their interactions with customers for â€œfriendliness.â€Thibault Roux, Burger Kingâ€™s chief digital officer, tells  that the company compiled information from franchisees and guests on how to measure friendliness, resulting in the fast food chain training its AI system to recognize certain words and phrases, such as â€œwelcome to Burger King,â€ â€œplease,â€ and â€œthank you.â€ Managers can then ask the AI assistant how their location is performing on friendliness. â€œThis is all meant to be a coaching tool,â€ Roux says, adding that the company is â€œiteratingâ€ on capturing the tone of conversations as well.The OpenAI-powered Patty serves as the â€œvoiceâ€ of the BK Assistant platform, which combines data across drive-thru conversations, kitchen equipment, inventory, and other areas of the Burger King business. Employees can ask Patty questions, such as how many strips of bacon to put on a Maple Bourbon BBQ Whopper, or for instructions on how to clean the shake machine.Because itâ€™s integrated with the new cloud point-of-sale system, the AI assistant will also alert managers if a machine is down for maintenance or when an item is out of stock. â€œWithin 15 minutes, the entire ecosystem will remove it from stock â€” whether youâ€™re walking into a restaurant to order from the kiosk, whether youâ€™re going to the drive-thru, the digital menu board will be updated,â€ Roux says.Burger King may be building a chatbot into employeesâ€™ headsets, but it doesnâ€™t seem like the brand is ready to widely launch AI drive-thrus just yet â€” something weâ€™ve seen chains like McDonaldâ€™s, Wendyâ€™s, and Taco Bell attempt. â€œWeâ€™re tinkering with it, weâ€™re playing around with it, but itâ€™s still a risky bet,â€ Roux says. â€œNot every guest is ready for this.â€ He adds that the company is currently testing the AI drive-thru technology in fewer than 100 restaurants.Burger King plans on launching its BK Assistant web and app platform to all restaurants in the US by the end of 2026, while Patty is piloting in 500 restaurants.]]></content:encoded></item><item><title>[P] Implementing Better Pytorch Schedulers</title><link>https://www.reddit.com/r/MachineLearning/comments/1rfer1y/p_implementing_better_pytorch_schedulers/</link><author>/u/shivvorz</author><category>ai</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 16:27:34 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[ Current schedulers in PyTorch are limited to just learning rate () changes and often lead to hardcoded, error-prone logic in training loops for anything more complex. I built a flexible suite for scheduling  optimizer hyperparam (LR, momentum, betas, etc.), with support for custom functions, presets, cyclic patterns, and per-group overrides. It's stateless where possible, picklable for checkpointing, and well-tested.It currently lives in my research monorepo, but I can separate it into a standalone package if there's enough interest. Would love feedback!I've been working on replicating (a subset of) training techniques from KellerJordan/modded-nanogpt for my baseline experiments, and realized I needed a reusable scheduling suite. But looking at how scheduling is typically done, and how it's done in modded-nanogpt, neither approach looked particularly reusable.Everyone knows that when you create a PyTorch optimizer, its hyperparameters are stored in , which is a list of dicts where each dict holds params and their hyperparams for a group of model parameters.For example, here's a realistic setup where you might want different weight decay for feature extractors vs. classifiers (common in fine-tuning scenarios):import torch.optim as optim model = SomeLargeModel() # e.g., a vision transformer optimizer = optim.AdamW([ {'params': model.feature_extractor.parameters(), 'weight_decay': 0.1}, # Group 0: High decay for stability {'params': model.classifier.parameters(), 'weight_decay': 0.01} # Group 1: Lower decay for faster adaptation ], lr=1e-3, weight_decay=0.05) # Default values overridden per-group # Per-group overrides take precedence over defaults assert optimizer.param_groups[0]['weight_decay'] == 0.1 assert optimizer.param_groups[1]['weight_decay'] == 0.01 You are allowed (and its common) to tweak these  mid-training to implement scheduling. For instance, you might decay weight decay over time or adjust betas in Adam for better convergence.Here is how you would typically perform such a change manually:# Manual mid-training adjustment (common pattern when Trainer/scheduler isn't flexible enough) for epoch in range(num_epochs): for batch in dataloader: # ... compute loss, backward optimizer.step() # Manual mid-training tweak: reduce weight decay after warmup if global_step > warmup_steps: for group in optimizer.param_groups: group['weight_decay'] *= 0.99 # Simple decay This is straightforward for basic cases, but things get messy with more complexity. For example, look at KellerJordan/modded-nanogpt. They use a combined NorMuon+Adam optimizer where different parameter groups need different scheduling: projection matrices use Muon with momentum warmup/cooldown, while embeddings use Adam with higher weight decay. The scheduling logic is spread across:This is a real research codebase with many contributors, and the coupling between scheduling and training logic makes it hard to experiment with different schedules without touching multiple files.This leads to "smelly" code: the scheduling logic is coupled with the training loop, which makes the scheduling logic hard to change and test.Enter PyTorch's built-in , it's meant to clean this up for LR specifically. Basic usage mirrors the manual tweak but abstracts it:from torch.optim.lr_scheduler import StepLR optimizer = optim.AdamW(model.parameters(), lr=1e-3) scheduler = StepLR(optimizer, step_size=30, gamma=0.1) # Decay LR every 30 epochs by 0.1x for epoch in range(num_epochs): for batch in dataloader: # ... compute loss, backward optimizer.step() scheduler.step() # Updates LR after epoch (not per-batch in this case) Under the hood, when you call , it calls  (defined in  base class at L284), which:Calls  to compute the new learning rates for each param groupIterates through  and calls _update_param_group_val(param_group, "lr", lr) to set each group's  keyThe key point:  (defined at L83) is just a helper that does  (with special handling for Tensor LRs).As a result, these schedulers are hardcoded to  handle LR, not momentum, betas, weight decay, or anything else you might want to schedule (which, as seen in the modded-nanogpt example, people do all the time). hardcoded instead of allowing anykey? It's literally just a string argument. This limitation is artificial forces everyone to reimplement scheduling for non-LR hyperparams from scratch.Now, onto the design of other PyTorch schedulers themselves. Most derive from  and implement their own  method. Functionally, many could be expressed as  with an appropriate lambda.For instance,  is equivalent to a lambda that drops by  every  epochs, and  is equivalent to a cosine lambda. However, they're implemented as separate classes with their own closed-form formulas (via ), which can be more efficient and readable.(Btw  isn't even a subclass of , it's a callback that monitors metrics.). is the most flexible among all PyTorch schedulers. However, usage of the class is inconvenient for multi-group setups.For example, if you want a custom lambda for group 2, you  provide dummies for groups 0 and 1 (constants, which aren't "real" schedules):from torch.optim.lr_scheduler import LambdaLR def constant_lambda(_): return 1.0 # Dummy def decay_lambda(epoch): return 1.0 - epoch / 100 # Actual for group 2 scheduler = LambdaLR(optimizer, lr_lambda=[constant_lambda, constant_lambda, decay_lambda]) Clunky, right? Changing total training length? Your lambdas hardcode it, so tweaks mean rewriting (though factories/partials help, it's still boilerplate). Advanced schemes like cyclic schedules? CosineAnnealingWarmRestarts exists, but it's LR-only and inflexible for custom cycles or non-LR params.So, what  is a schedule? At its core, it's a pure function: f(step: int, total_steps: int) -> value (any type, not just float). It maps progress to a param value, and you apply it to optimizer.param_groups[i][param_name] = value. No state, no side effects, just deterministic computation (great for reproducibility).In my suite, this primitive is user-facing via  (end users are expected to use it directly):from research_lib.training.scheduling import ParamSchedule def linear_decay(step: int, total_steps: int) -> float: return 1.0 - (step / total_steps) * 0.9 # Decays from 1.0 to 0.1 lr_schedule = ParamSchedule(param_name="lr", schedule_fn=linear_decay) value = lr_schedule(500, 1000) # 0.55 For common patterns, presets (subclasses of the primitive) are provided: e.g., WarmupStableDecaySchedule for warmup â†’ stable â†’ decay:from research_lib.training.scheduling import WarmupStableDecaySchedule lr_schedule = WarmupStableDecaySchedule( param_name="lr", warmup_steps=100, cooldown_frac=0.5, min_value=0.0, max_value=1.0, decay_type="cosine" ) Need reusable patterns? Subclass the primitive and override the schedule_fn attributeFor cyclic schedules e.g. for continual training, enter "wrapper land" (via  submodule). These are composable callables that wrap a :from research_lib.training.scheduling import wrappers as sw base_fn = ... # e.g., a decay schedule cyclic_fn = sw.Cyclic(base_fn, cycle_steps=1000) # Repeats every 1000 steps lr_schedule = ParamSchedule("lr", cyclic_fn) Finally, the runtime layer:  binds it all, tracks state for checkpointing, and supports global + per-group overrides:from research_lib.training.scheduling import ParamScheduler scheduler = ParamScheduler( optimizer=optimizer, global_schedules=[lr_schedule, momentum_schedule], group_overrides={1: [slow_lr_schedule]}, # Override for group 1 total_steps=10000 ) # In loop optimizer.step() scheduler.step() # Applies all, increments internal step # Checkpoint: scheduler.state_dict() / load_state_dict() When designing this, I followed these design choices:"No restriction on action space" (schedules can do anything PyTorch allows),"Make illegal states unrepresentable" (required args aren't optional; validation at )Minimize coupling (schedules are pure, optimizer bound at runtime).It's tested thoroughly (e.g., pickling, validation checks like monotonicity). Thoughts? Does this solve pains you've hit? Link to submodule here: LMK if I should extract it!]]></content:encoded></item><item><title>A VC and some big-name programmers are trying to solve open sourceâ€™s funding problem, permanently</title><link>https://techcrunch.com/2026/02/26/a-vc-and-some-big-name-programmers-are-trying-to-solve-open-sources-funding-problem-permanently/</link><author>/u/Outrageous-Baker5834</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 16:11:04 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[A group of notable open source programmers are joining with a VC investor to launch a nonprofit called the Open Source Endowment in hopes of permanently solving the perennial issue with developing open source software: funding.Â The nonprofit, which just achieved formal 501(c)(3) status, has currently raised more than $750,000 in commitments. But if things go according to the plan of its founder, Konstantin Vinogradov, it will have $100 million in assets within seven years.Â Vinogradov is a venture investor specializing in open source, AI, and infrastructure software, and was previously a general partner at Runa Capital. As such, he has â€œsome experience with university endowments,â€ which are some of the largest investors in venture capital funds, he told TechCrunch.Â Â Vinogradov says as he scoured the world for open source projects, one complaint kept popping up: â€œThere is no source of sustainable funding for open source maintainers. And thatâ€™s a really big problem.â€Â (â€œMaintainerâ€ refers to the developers who work on open source projects, such as debugging, choosing and verifying features submitted by the community, or programming new features themselves.)The endowment will support projects based on criteria such as its number of users, or how many other projects rely on that specific software to operate. It will also choose projects that are not already well-supported by grants, donations, or umbrella organizations such as Linuxâ€™s Alpha-Omega.Â Vinogradov has already assembled a board for the nonprofit.Cash strapped, burned outThe lack of money in open source is hardly new. Open source software is typically given away, and since the community often contributes time and efforts freely, up to 86% of open source developers are not paid for their work.This isnâ€™t much of a problem for hobbyists or for professional developers paid by their companies to maintain projects,Â but such a system stands on shaky ground. Open source software is the bedrock upon which the internet stands, and virtually every large company uses open source tools in some way. In fact, open source software accounts for up to 55% of the tech stack in organizations, and is present in everything from databases to operating systems.Â There is, and has been for decades, a core of developers who volunteer their time and efforts for free to manage popular, important, and critical projects. And many of them are burned out.This issue came into the publicâ€™s consciousness briefly in 2014, with the OpenSSL Heartbleed disaster, where a bug was found in an open source security project, used by most of the internet, that was maintained by a single developer.Â There have been many attempts to fix the funding situation over the years. Some projects take donations from corporate sponsors. For instance, The Linux Foundation, which brought in about $300 million last year largely from corporate sponsors, doles out grants to select projects through its Alpha-Omega Project. In 2025, Alpha-Omega issued $5.8 million to 14 projects, it said.Â Â Still, not every developer wants to take corporate donations, as there are worries of granting too much influence to donor companies.Â For instance, there was a big hubbub last year in the Ruby community surrounding some long-time maintainers leaving and its big sponsor Shopify, The Register reported.The Open Source Endowment hopes to support projects while displacing such risks.Â â€œThe only way to support open source sustainably is private funds,â€ says Vinogradov.Â Why hasnâ€™t an endowment been tried before? Endowments require patience, Vinogradov says. They invest many of their assets, spending only a fraction of their income in any given year, and require years or even decades to grow to a meaningful size.But if done right, that patience will result in an independent fund that could support critical open source projects forever.]]></content:encoded></item><item><title>A VC and some big-name programmers are trying to solve open source&apos;s funding problem, permanently</title><link>https://techcrunch.com/2026/02/26/a-vc-and-some-big-name-programmers-are-trying-to-solve-open-sources-funding-problem-permanently/</link><author>/u/whit537</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 16:07:11 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[A group of notable open source programmers are joining with a VC investor to launch a nonprofit called the Open Source Endowment in hopes of permanently solving the perennial issue with developing open source software: funding.Â The nonprofit, which just achieved formal 501(c)(3) status, has currently raised more than $750,000 in commitments. But if things go according to the plan of its founder, Konstantin Vinogradov, it will have $100 million in assets within seven years.Â Vinogradov is a venture investor specializing in open source, AI, and infrastructure software, and was previously a general partner at Runa Capital. As such, he has â€œsome experience with university endowments,â€ which are some of the largest investors in venture capital funds, he told TechCrunch.Â Â Vinogradov says as he scoured the world for open source projects, one complaint kept popping up: â€œThere is no source of sustainable funding for open source maintainers. And thatâ€™s a really big problem.â€Â (â€œMaintainerâ€ refers to the developers who work on open source projects, such as debugging, choosing and verifying features submitted by the community, or programming new features themselves.)The endowment will support projects based on criteria such as its number of users, or how many other projects rely on that specific software to operate. It will also choose projects that are not already well-supported by grants, donations, or umbrella organizations such as Linuxâ€™s Alpha-Omega.Â Vinogradov has already assembled a board for the nonprofit.Cash strapped, burned outThe lack of money in open source is hardly new. Open source software is typically given away, and since the community often contributes time and efforts freely, up to 86% of open source developers are not paid for their work.This isnâ€™t much of a problem for hobbyists or for professional developers paid by their companies to maintain projects,Â but such a system stands on shaky ground. Open source software is the bedrock upon which the internet stands, and virtually every large company uses open source tools in some way. In fact, open source software accounts for up to 55% of the tech stack in organizations, and is present in everything from databases to operating systems.Â There is, and has been for decades, a core of developers who volunteer their time and efforts for free to manage popular, important, and critical projects. And many of them are burned out.This issue came into the publicâ€™s consciousness briefly in 2014, with the OpenSSL Heartbleed disaster, where a bug was found in an open source security project, used by most of the internet, that was maintained by a single developer.Â There have been many attempts to fix the funding situation over the years. Some projects take donations from corporate sponsors. For instance, The Linux Foundation, which brought in about $300 million last year largely from corporate sponsors, doles out grants to select projects through its Alpha-Omega Project. In 2025, Alpha-Omega issued $5.8 million to 14 projects, it said.Â Â Still, not every developer wants to take corporate donations, as there are worries of granting too much influence to donor companies.Â For instance, there was a big hubbub last year in the Ruby community surrounding some long-time maintainers leaving and its big sponsor Shopify, The Register reported.The Open Source Endowment hopes to support projects while displacing such risks.Â â€œThe only way to support open source sustainably is private funds,â€ says Vinogradov.Â Why hasnâ€™t an endowment been tried before? Endowments require patience, Vinogradov says. They invest many of their assets, spending only a fraction of their income in any given year, and require years or even decades to grow to a meaningful size.But if done right, that patience will result in an independent fund that could support critical open source projects forever.]]></content:encoded></item><item><title>paqet â€“ a Go proxy that bypasses the OS network stack entirely</title><link>https://www.reddit.com/r/golang/comments/1rfdo1k/paqet_a_go_proxy_that_bypasses_the_os_network/</link><author>/u/zerodawntodusk</author><category>golang</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 15:48:13 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[The approach is kind of wild, instead of using the normal OS networking stack, it hooks in at the packet level via pcap + gopacket, crafting and injecting raw TCP packets directly. That means no SYN/SYN-ACK/ACK handshake at all, the OS never even knows a connection exists. As a side effect, host firewalls like ufw are completely bypassed since pcap grabs packets before netfilter ever sees them. KCP runs on top for reliable encrypted transport, and the whole thing presents as a SOCKS5 proxy.Still alpha but a really interesting read if you're into low-level networking in Go. ]]></content:encoded></item><item><title>Process external files in const fn: no build.rs, no proc macros, no binary bloat</title><link>https://www.reddit.com/r/rust/comments/1rfdek3/process_external_files_in_const_fn_no_buildrs_no/</link><author>/u/carlk22</author><category>rust</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 15:38:25 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Hereâ€™s a fun Rust trick Iâ€™ve been experimenting with for embedded work:You can use  inside a , to process file contents at compile time, and keep only the final result in your binary.No . No proc macros. No runtime cost.const fn sum_u16s() -> u128 { let data: &[u8; 8] = include_bytes!("data.bin"); assert!(data.len() % 2 == 0); let mut i = 0; let mut acc: u128 = 0; while i < data.len() { // interpret two bytes as little-endian u16 let value = (data[i] as u16) | ((data[i + 1] as u16) << 8); acc += value as u128; i += 2; } acc } static SUM: u128 = sum_u16s();  reads the file at compile time.The loop runs entirely in const evaluation.The compiler computes SUM during compilation.Only the u128 result is stored in the final binary.If you remove the static SUM, the file contributes zero bytes to the binary (release build). Itâ€™s just compile-time input.For embedded Rust, this effectively gives you a tiny compile-time asset pipeline:Read raw data files (audio, lookup tables, calibration data, etc.)Transform them (even some audio compression)Materialize only the final representation you actually needAnd you only pay flash space for what you explicitly store. Itâ€™s surprisingly powerful and itâ€™s all stable Rust today.]]></content:encoded></item><item><title>PULS v0.8.0 Released - A unified system monitoring and management tool for Linux</title><link>https://github.com/word-sys/puls/releases/tag/0.8.0</link><author>/u/word-sys</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 15:09:38 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What are your favorite kubectl plugins?</title><link>https://www.reddit.com/r/kubernetes/comments/1rfceo0/what_are_your_favorite_kubectl_plugins/</link><author>/u/_racy</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 15:00:51 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Thinking about writing one I would love good examples to follow   submitted by    /u/_racy ]]></content:encoded></item><item><title>Developers Are Safeâ€¦ Thanks to Corporate Red Tape</title><link>https://azamsharp.com/2026/02/26/developers-are-safe.html</link><author>/u/Select_Bicycle4711</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 14:51:26 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[By the end of 2026, AI will write most of the code and software developers will be obsolete. There is no need to learn programming because you will be replaced by an AI agent. We have all heard these kinds of crazy headlines. I am here to tell you that the reality is quite different.This post is a result of my tweet that got 80K+ views and 258 plus comments. The responses were passionate, but they also confirmed something I have seen over and over again in real enterprise environments. The world of corporate software development does not move at the speed of Twitter headlines.SwiftUI Architecture BookPatterns and Practices for Building Scalable Applications
        A practical guide to building SwiftUI apps that stay clean as they grow.
      jQuery Exception Paper WorkLet me first tell you a little story. Long time ago, I was working at an oil and gas company in Houston, Texas. The codebase had been implemented by an offshore company and it was an absolute mess. It was a .NET web application with hundreds of JavaScript files, and each file contained at least 5000 lines of code. A lot of that code was simply recreating effects and user interface components that could have easily been implemented using a third party library like jQuery. Yes, this was a long time ago.After working with the code for a couple of days, I suggested that we should use jQuery to speed up development and avoid rebuilding the same things over and over again. My manager told me that we could not use any third party dependencies unless I filled out an exception paper work request. I had to indicate in detail the purpose of the library, where it would be used, which functions would be used, and which files would be altered. It was not a casual approval. It was a formal process.I completed the paper work. Two weeks later, jQuery was granted an exception and I was allowed to continue the work, this time moving much faster. Two weeks for a tiny JavaScript library. That was the reality.You might think this was an isolated incident. Unfortunately, it was not.Here is another one. I was working at a very large oil and gas company and was tasked with creating an iPhone app. This was during the Objective C days, around iOS 3. The app was meant to be deployed internally using an Apple Enterprise account. After I finished the app, I discussed deployment with my manager. I told them that I had deployed personal apps before using a personal account, but I had never done an Enterprise deployment.I suggested that they log into the Enterprise account and sit with me so I could see the interface and guide them through the process. They refused. They would not allow me to see the Enterprise account login due to security concerns. Instead, they made me stand outside their office while they logged in. They described what they saw on the screen, and I had to tell them what to click. If you see a Next button, click that. Enter the app name. Upload the build. I deployed an enterprise app by listening to someone narrate the screen from behind a door.Agile Development Is Not for UsAt another very large finance company, I was consulting on a greenfield iOS project. The company was not familiar with Agile principles, so we conducted an introduction session explaining sprints, scrum, velocity, and iterative development. We walked them through how Agile could improve feedback cycles and delivery speed.After the session, they told us that Agile seemed too complicated and that they would rather stick with their existing process, which was waterfall. This was for a brand new mobile project, yet they preferred the comfort of a rigid, document heavy approach over adaptive development. Change, even when beneficial, was seen as risk.Pandas, Matplotlib, Scikit Learn Oh MyOne of my friends who works at an enterprise company told me that they had to go through several levels of clearance just to install and use Pandas, NumPy, and Matplotlib. The concern was that these libraries might somehow steal data. These are widely used open source libraries that power research, analytics, and machine learning around the world, yet they were treated as potential threats requiring formal approval.The point of these real stories is simple. When tools as basic as jQuery require weeks of approvals, how can we assume that the same companies will welcome AI tools with open arms? These are the same companies that provide limited access to online AI tools. These are the same companies where YouTube and Stack Overflow are blocked. These are the same companies that built their own internal code repository instead of using GitHub.You might say that these companies will eventually be replaced by forward thinking organizations that embrace AI agents and automation. That sounds nice in theory. In reality, I am talking about insanely large companies. Their coffee budget is larger than some smaller competitorsâ€™ entire operating budgets. They are not disappearing anytime soon.And let me be clear. The stories I shared are only a few examples. This kind of red tape is extremely common, especially in non IT companies such as oil and gas, healthcare, finance, manufacturing, and other highly regulated industries. In these environments, compliance, risk management, security audits, and approval chains are not optional. They are built into the culture. Nothing moves fast. Every new tool, every dependency, every external service goes through layers of review. AI will not magically bypass that structure.As much as we may complain about how slowly these organizations move and how resistant they are to adopting new technologies, that same resistance is what slows down radical change. These will be the companies that move cautiously with AI agents. These will be the companies that continue to rely on experienced human developers who understand business rules, compliance requirements, risk management, and the real consequences of mistakes.AI will absolutely change how we work. It will make us faster and more productive. But the idea that developers will be wiped out overnight ignores how corporate systems actually function.Stay strong and keep coding ðŸ˜‰]]></content:encoded></item><item><title>Rerun 0.30 - blazingly fast visualization toolbox for Robotics</title><link>https://github.com/rerun-io/rerun/releases/tag/0.30.0</link><author>/u/Fickle-Conference-87</author><category>rust</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 14:31:46 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[Rerun is an easy-to-use visualization toolbox and database for multimodal and temporal data. It's written in Rust, using wgpu and egui. Try it live at https://rerun.io/viewer. You can use rerun as a Rust library, or as a standalone binary (rerun a_mesh.g1b).With this release you can plot arbitrary scalar data directly in time series views (floats, ints, uints, bools, including nested Arrow data) even without predefined semantics.The release also introduces a new extension model: you can register custom visualizers directly into existing 2D/3D/Map views, define your own archetypes, and use fully custom shaders â€” without forking the Viewer. That means domain-specific GPU renderers (e.g., height fields, cost maps, learned fields) can live inside the standard app.The Viewer now supports on-demand streaming when connected to a Rerun server or Rerun Cloud, fetching only what youâ€™re viewing and evicting stale data as you scrub. This enables working with recordings larger than RAM â€” including in the web viewer beyond the 4 GiB Wasm limit.]]></content:encoded></item><item><title>[P] PerpetualBooster v1.9.0 - GBM with no hyperparameter tuning, now with built-in causal ML, drift detection, and conformal prediction</title><link>https://www.reddit.com/r/MachineLearning/comments/1rfb6yq/p_perpetualbooster_v190_gbm_with_no/</link><author>/u/mutlu_simsek</author><category>ai</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 14:11:59 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Posted about Perpetual at v1.1.2 - here's an update. For those who missed it: it's a gradient boosting machine in Rust where you replace hyperparameter tuning with a single  parameter. Set it, call , done.python model = PerpetualBooster(objective="SquaredLoss", budget=1.0) model.fit(X, y) Since then the Rust core basically doubled (~16.5k lines added). Here's what's new: - full suite built into the same Rust core: Double Machine Learning, meta-learners (S/T/X), uplift (R-learner), instrumental variables, policy learning, fairness-aware objectives. Not a wrapper â€” the causal estimators use the same budget-based generalization. Causal effect estimation without hyperparameter tuning. - data drift and concept drift detection using the trained tree structure. No ground truth labels or retraining needed. - conformalized quantile regression (CQR) for prediction intervals with marginal and conditional coverage. Isotonic calibration for classification. Train once, calibrate on holdout, get intervals at any alpha without retraining. [predict_intervals(), predict_sets(), predict_distribution()]. - regression (Squared, Huber, AdaptiveHuber, Absolute, Quantile, Poisson, Gamma, Tweedie, MAPE, Fair, SquaredLog), classification (LogLoss, Brier, CrossEntropy, Hinge), ranking (ListNet), plus custom objectives. -  for multi-target problems.  - improved to O(n) from O(nÂ²).vs. Optuna + LightGBM (100 trials): matches accuracy with up to . vs. AutoGluon v1.2 (best quality, AutoML benchmark leader): Perpetual won , inferred up to 5x faster, and didn't OOM on 3 tasks where AutoGluon did.The only single GBM package I know of shipping causal ML, calibration, drift monitoring, ranking, and 19 objectives together. Pure Rust, Python/R bindings, Apache 2.0.Happy to answer questions about the algorithm or benchmarks.]]></content:encoded></item><item><title>The MySQL-to-Postgres Migration That Saved $480K/Year: A Step-by-Step Guide</title><link>https://medium.com/@dusan.stanojevic.cs/the-mysql-to-postgres-migration-that-saved-480k-year-a-step-by-step-guide-4b0fa9f5bdb7</link><author>/u/narrow-adventure</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 14:09:17 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rust Is Eating JavaScript</title><link>https://leerob.com/rust</link><author>/u/Active-Fuel-49</author><category>rust</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 13:51:58 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[2021 (updated 2026) â€“ Lee RobinsonRust is a fast, reliable, and memory-efficient programming language. Itâ€™s been voted the most admired programming language for a decade. Created by Mozilla, itâ€™s now used at Meta, Apple, Amazon, Microsoft, and Google for systems infrastructure, encryption, virtualization, and more low-level programming.Why is Rust now being used to replace parts of the JavaScript web ecosystem like minification (Terser), transpilation (Babel), formatting (Prettier), bundling (webpack), linting (ESLint), and more?Rust helps developers write fast software thatâ€™s memory-efficient. Itâ€™s a modern replacement for languages like C++ or C with a focus on code safety and concise syntax.Rust is quite different than JavaScript. JavaScript tries to find variables or objects not in use and automatically clears them from memory. This is called Garbage Collection. The language abstracts the developer from thinking about manual memory management.With Rust, developers have more control over memory allocation, without it being as painful as C++.Rust uses a relatively unique memory management approach that incorporates the idea of memory "ownershipâ€. Basically, Rust keeps track of who can read and write to memory. It knows when the program is using memory and immediately frees the memory once it is no longer needed. It enforces memory rules at compile time, making it virtually impossible to have runtime memory bugs. You do not need to manually keep track of memory. The compiler takes care of it. â€“ DiscordOn top of the companies mentioned above, Rust is also being used for popular open-source libraries like:Rust has been a force multiplier for our team, and betting on Rust was one of the best decisions we made. More than performance, its ergonomics and focus on correctness has helped us tame syncâ€™s complexity. We can encode complex invariants about our system in the type system and have the compiler check them for us. â€“ DropboxJavaScript is the most widely used programming language, operating on every device with a web browser. Over the past ten years, a massive ecosystem has been built around JavaScript: bundle multiple JavaScript files into one. write modern JavaScript while supporting older browsers. generate the smallest possible file sizes. format code in an opinionated way. find issues with their code before deploying.Millions of lines of code have been written and even more bugs have been fixed to create the bedrock for shipping web applications of today. All of these tools are written with JavaScript or TypeScript. This has worked well, but we've reached peak optimization with JS. This has inspired a new class of tools, designed to drastically improve the performance of building for the web.SWC, created in 2017, is an extensible Rust-based platform for the next generation of fast developer tools. Itâ€™s used by tools like Next.js, Parcel, and Deno, as well as companies like Vercel, ByteDance, Tencent, Shopify, and more.SWC can be used for compilation, minification, bundling, and more â€“ and is designed to be extended. Itâ€™s something you can call to perform code transformations (either built-in or custom). Running those transformations happens through higher-level tools like Next.js.Deno, created in 2018, is a simple, modern, and secure runtime for JavaScript and TypeScript that uses V8 and is built with Rust. Itâ€™s an attempt to replace Node.js, written by the original creators of Node.js. While it was created in 2018, it didnâ€™t hit v1.0 until May 2020.Denoâ€™s linter, code formatter, and docs generator are built using SWC.esbuild, created in January 2020, is a JavaScript bundler and minifier 10-100x faster than existing tools, written in Go.Iâ€™m trying to create a build tool that A) works well for a given sweet spot of use cases (bundling JavaScript, TypeScript, and maybe CSS) and B) resets the expectations of the community for what it means for a JavaScript build tool to be fast. Our current tools are way too slow in my opinion. â€“ Evan, Creator of esbuild (Source)Building JavaScript tooling with systems programming languages, like Go and Rust, was fairly niche until esbuild was released. In my opinion, esbuild sparked a wider interest in trying to make developer tools faster. Evan chose to use Go:The Rust version probably could be made to work at an equivalent speed with enough effort. But at a high level, Go was much more enjoyable to work with. This is a side project and it has to be fun for me to work on it. â€“ Evan, Creator of esbuild (Source)Some argue Rust could perform better, but both could achieve Evanâ€™s original goal of influencing the community:Even with just basic optimization, Rust was able to outperform the hyper hand-tuned Go version. This is a huge testament to how easy it is to write efficient programs with Rust compared to the deep dive we had to do with Go. â€“ DiscordRome, created in August 2020,Â is a linter, compiler, bundler, test runner, and more, for JavaScript, TypeScript, HTML, JSON, Markdown, and CSS. They aim to replace and unify the entire frontend development toolchain. Itâ€™s created by Sebastian, who also created Babel.Why rewrite everything, then?Making the necessary modifications to Babel to allow for it to be a reliable base for other tools would have required changes to absolutely everything. The architecture is bound to the initial design choices I made in 2014 when I was learning about parsers, ASTs, and compilers. - Sebastian (Source)Rome is currently written in TypeScript and runs on Node.js. But they're now working on rewriting in Rust using RSLint parser and their own visitor system for AST traversal.Rustâ€™s integration with Node.js is better than other low-level languages.napi-rs allows you to build pre-compiled Node.js add-ons with Rust. It provides an out-of-the-box solution for cross-compilation and publishing native binaries to NPM, without needing  or  scripts.You can build a Rust module that can be called directly from Node.js, without needing to create a child process like esbuild.WebAssemblyÂ (WASM) is a portable low-level language that Rust can compile to. It runs in the browser, is interoperable with JavaScript, and is supported in all major modern browsers.WASM is definitely a lot faster than JS, but not quite native speed. In our tests, Parcel runs 10-20x slower when compiled to WASM than with native binaries. â€“ Devon GovettWhile WASM isnâ€™t the perfect solution yet, it  help developers create extremely fast web experiences. The Rust team is committed to a high-quality and cutting-edge WASM implementation. For developers, this means you could have the performance advantages of Rust (vs. Go) while still compiling for the web (using WASM).Some early libraries and frameworks in this space:These Rust-based web frameworks that compile to WASM arenâ€™t trying to replace JavaScript, but work alongside it. While we arenâ€™t there yet, itâ€™s interesting to see Rust coming after the web on both sides: making existing JavaScript tooling faster.Itâ€™s Rust all the way down.Rust has a steep learning curve. Itâ€™s a lower level of abstraction than what most web developers are used to.Once you're on native code (through Rust, Go, Zig, or other low-level languages),
the algorithms and data structures are more important than the language choice. Itâ€™s not a silver bullet.Rust makes you think about dimensions of your code that matter tremendously for systems programming. It makes you think about how memory is shared or copied. It makes you think about real but unlikely corner cases and make sure that they're handled. It helps you write code thatâ€™s incredibly efficient in every possible way. â€“ Tom MacWright (Source)Further, Rustâ€™s usage in the web community is still niche. It hasnâ€™t reached critical adoption. Even though learning Rust for JavaScript tooling will be a barrier to entry, interestingly developers would rather have a faster tool thatâ€™s harder to contribute to. Fast software wins.Currently, itâ€™s hard to find a Rust library or framework for your favorite services (things like working with authentication, databases, payments, and more). I do think that once Rust and WASM reach critical adoption, this will resolve itself. But not yet. We need existing JavaScript tools to help us bridge the gap and incrementally adopt performance improvements.The Future of JavaScript ToolingI believe Rust is the future of JavaScript tooling. Next.js 12 started our transition to fully replace Babel (transpilation) and Terser (minification) with SWC and Rust. Why? SWC can be used as a Crate inside Next.js, without having to fork the library or workaround design constraints. We were able to achieve ~3x faster Fast Refresh and ~5x faster builds in Next.js by switching to SWC, with more room for optimization still in progress. Rustâ€™s support for WASM is essential for supporting all possible platforms and taking Next.js development everywhere. The Rust community and ecosystem are amazing and only growing.Itâ€™s not just Next.js adopting SWC, either:Parcel uses SWC like a library. Before we used Babelâ€™s parser and custom transforms written in JS. Now, we use SWCâ€™s parser and custom transforms in Rust. This includes a full scope hoisting implementation, dependency collection, and more. Itâ€™s similar in scope to how Deno built on top of SWC. â€“ Devon GovettItâ€™s early days for Rust â€“ a few important pieces are still being figured out: Writing plugins in Rust isnâ€™t as approachable for many JavaScript developers. At the same time, exposing a plugin system in JavaScript could negate performance gains. A definitive solution hasnâ€™t emerged yet. Ideally, the future combines both JavaScript and Rust. If you want to write a plugin with JavaScript, itâ€™s possible with a tradeoff for speed. Need more performance? Use the Rust plugin API. One interesting area of development is , which is SWCâ€™s replacement for Webpack. Itâ€™s still under development but could be very promising. As mentioned above, the prospect of writing Rust and compiling to WASM is enticing, but thereâ€™s still work to be done.Regardless, Iâ€™m confident Rust will continue to have a major impact on the JavaScript ecosystem for the next 1-2 years and into the future. Imagine a world where all of the build tools used in Next.js are written in Rust, giving you optimal performance. Then, Next.js could be distributed as a static binary you'd download from NPM.Thatâ€™s the world I want to live (and develop) in.There was more investment into new Rust tooling in the JavaScript ecosystem. A few notable Rust projects include:: Rome, previously mentioned in this post, has became Biome: New bundler with webpack compat: New bundler for Vite (replacing esbuild and rollup): Similar to Biome: parser, linter, formatter, transpiler, minifier, etc: New CSS parser, transformer, bundler, and minifier.Further, Bun 1.0 was released, putting Zig on the map and working to speed up the entire JavaScript ecosystem.When I wrote this post in 2021, Rust replacing JavaScript tooling was my lofty prediction. In 2026, nearly every major JavaScript build tool now has a Rust-based alternative or has been rewritten in Rust. Many of the projects listed in the 2023 update above have shipped stable releases, including:: Hit 1.0 RC in January 2026. The Rust bundler replacing both esbuild and Rollup inside Vite, 10-30x faster than Rollup. Vite 7 uses it as the default.: Oxfmt (formatter) reached beta with 100% Prettier compatibility at 30x the speed. Oxlint is used by Vue.js, Turborepo, Sentry, and Hugging Face.: The first JS/TS linter with type-aware linting that doesn't require the TypeScript compiler. GritQL plugins and monorepo support.: Hit 1.0 in August 2024 with full webpack API compatibility. Used in production by TikTok, Discord, Microsoft, and Amazon.: Released January 2025 with the Oxide engine, using Rust for computationally expensive operations plus Lightning CSS. Full builds 5x faster, incremental builds 8x faster.: Shipped October 2024 with full Node.js and npm backwards compatibility, so its Rust-based formatter, linter, and tooling now work for mainstream Node projects too.This pattern has spread to other ecosystems. In Python, uv (a Rust-based package manager, 10-100x faster than pip) and Ruff (linter and formatter replacing Flake8, Black, and isort) have taken off. Rust is eating developer tooling everywhere, not just JavaScript.In the esbuild section above, Evan You (creator of Vue and Vite) is quoted saying Go "was much more enjoyable to work with" than Rust. In 2025, Evan raised $12.5M and founded VoidZero to build a unified Rust-based JavaScript toolchain. Vite, Vitest, Rolldown, and Oxc are all under this umbrella.Rust is also proving to be a great language for AI coding agents to write. I recently built a Rust-based image compressor using only coding agents. It was 38,000 lines of Rust with zero runtime dependencies. Rust's strict compiler is a natural fit for AI-generated code: if it compiles, it's much closer to correctness than other languages. Rust has grown  in the past 6 months because of coding agents.The world I described in 2021, where all JavaScript build tools are written in Rust, has largely arrived.]]></content:encoded></item><item><title>quaternion-core: A simple quaternion library written in Rust</title><link>https://www.reddit.com/r/rust/comments/1rfak2s/quaternioncore_a_simple_quaternion_library/</link><author>/u/Ancient-Sale3089</author><category>rust</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 13:45:44 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I created  and wanted to share it here.This crate provides quaternion operations and conversions between several rotation representations (as shown in the attached image). It's designed to be simple and practical.Generic functions supporting both f32 and f64Works in no_std environmentsCan convert between 24 different Euler angles (I don't think many libraries can do this!)I started building it for attitude estimation on microcontrollers, so the functions are designed to minimize computational cost without overcomplicating the implementation.I also made a Tennis Racket Theorem simulation using this crate:Thanks for reading, and I hope you give it a try!]]></content:encoded></item><item><title>Linux 7.1 Looks To Support Extended Attributes On Sockets For New GNOME &amp; systemd Functionality</title><link>https://www.phoronix.com/news/Linux-7.1-Looks-xattrs-Sockets</link><author>/u/adriano26</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 13:34:54 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com.]]></content:encoded></item><item><title>Number of active Bazzite Linux users Weekly</title><link>https://www.reddit.com/r/linux/comments/1rf9hl8/number_of_active_bazzite_linux_users_weekly/</link><author>/u/Right-Grapefruit-507</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 12:59:00 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA["Classic DNF based operating systems can use the DNF Count Me feature to anonymously report how long a system has been running without impacting the user privacy. This is implemented as an additional  variable added to requests made to fetch RPM repository metadata. On those systems, this value is added randomly to requests made automatically via the  or via explicit calls to  or "]]></content:encoded></item><item><title>Lneto - Go networking with no operating system</title><link>https://github.com/soypat/lan8720</link><author>/u/whittileaks</author><category>golang</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 12:07:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Small 8USD setup to host a HTTP server at http://gsan.whittileaks.com, ! Change address bar http:// to http:// manually if it does not work.The networking stack used is Lneto.]]></content:encoded></item><item><title>Should you re-check the database on every request with session auth?</title><link>https://www.reddit.com/r/golang/comments/1rf83w8/should_you_recheck_the_database_on_every_request/</link><author>/u/Minimum-Ad7352</author><category>golang</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 11:49:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Iâ€™m using session-based authentication. When a user logs in, I generate a UUID session ID, store it as session_id -> user_id, and return it in an HTTP-only cookie. On each request, middleware validates the session and extracts the user_id.My question is: after getting the user_id from the session store, should I also query the main database to make sure the user still exists (or isnâ€™t deactivated)? Or is trusting the session layer enough?How do you usually handle this ?]]></content:encoded></item><item><title>Let&apos;s Implement Consistent Hashing From Scratch in Golang</title><link>https://sushantdhiman.dev/lets-implement-consistent-hashing/</link><author>/u/Sushant098123</author><category>golang</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 11:13:22 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Recently I've been learning about distributed systems and I came across a very interesting concept "Consistent Hashing". It is one of the most basic thing in Distributed Systems.What is Consistent Hashing?Consistent hashing is a key distribution technique that ensures easy and smooth mapping of keys to servers to minimize data movement when nodes are added or removed. Unlike traditional hashing methods, where adding or removing a server changes the hash distribution significantly, consistent hashing reduces this impact.It goes with mapping both servers (nodes) and keys to a circular hash space. When a request comes in, the system moves clockwise along the ring to find the closest node, which will be responsible for that key. It will store key and value data and can also be used while retrieving the same.Consistent hashing is widely used in distributed systems. Load Balancing: Make sure that incoming requests are uniformly distributed among available servers so no server gets overwhelming resquests. Distributed Caching (e.g., Memcached, Redis Cluster): Helps in mapping cache keys to specific nodes. Database Sharding: Efficiently distributes and save database records across multiple database servers.Why is Consistent Hashing Important?Imagine you have a set of servers handling API requests. A traditional hash function could distribute these requests among servers, but as soon as a server is added or removed, the entire mapping breaks, and most of the data needs to be rebalanced. This causes cache misses that increases latency and unnecessary load on the system.Consistent hashing solves this by ensuring that only a small fraction of keys need to be remapped when a server is added or removed. This makes the system highly scalable and resilient.Now, letâ€™s talk about the implementation of consistent hashing in Golang. My implementation involves a ConsistentHashRing that maintains a sorted list of node hashes and efficiently assigns keys to nodes. Hereâ€™s how it works:type Node struct {
	ID   string
	Keys map[string]string
}

type ConsistentHashRing struct {
	mu     sync.RWMutex
	nodes  map[uint32]*Node
	hashes []uint32
}

func NewConsistentHashRing() *ConsistentHashRing {
	return &ConsistentHashRing{
		nodes:  make(map[uint32]*Node),
		hashes: []uint32{},
	}
}
                            Subscribe
                        I used Murmur3 as my hashing function because it provides better distribution and performance than FNV or MD5.func hashFunction(key string) uint32 {
  return murmur3.Sum32([]byte(key))
}2. Adding Nodes to the RingWhen a new server is added, it is assigned a hash value and placed on the ring.func (chr *ConsistentHashRing) AddNode(id string) {
  chr.mu.Lock()
  defer chr.mu.Unlock()
  hash := hashFunction(id)
  chr.nodes[hash] = &Node{
    ID:   id,
    Keys: make(map[string]string),
  }
  chr.hashes = append(chr.hashes, hash)
  slices.Sort(chr.hashes)
}3. Finding the Nearest NodeTo locate the closest node for a given key, I move clockwise along the sorted list of hashes.func (chr *ConsistentHashRing) GetNextNodeIndex(hash uint32) int {
  for i, h := range chr.hashes {
    if h > hash {
      return i
    }
  }
  return 0 // Wrap around to the first node
}4. Storing and Retrieving DataEach node holds a set of keys. When a key-value pair is stored, it is mapped to the correct node.func (chr *ConsistentHashRing) StoreKey(key, val string) {
  node := chr.GetNode(key)
  if node != nil {
    node.Keys[key] = val
  }
}func (chr *ConsistentHashRing) RetrieveKey(key string) (string, error) {
  node := chr.GetNode(key)
  if node == nil {
    return "", errors.New("no node found")
  }
  val, ok := node.Keys[key]
  if !ok {
    return "", errors.New("key not found")
  }
  return val, nil
}When a server is removed, its keys must be transferred to the next available node.func (chr *ConsistentHashRing) RemoveNode(id string) {
  chr.mu.Lock()
  defer chr.mu.Unlock()

  hash := hashFunction(id)
  node, exists := chr.nodes[hash]
  if !exists {
    return
  }

  nextNodeIndex := chr.GetNextNodeIndex(hash)
  nextNode := chr.nodes[chr.hashes[nextNodeIndex]]
  maps.Copy(nextNode.Keys, node.Keys)

  delete(chr.nodes, hash)

  for i, h := range chr.hashes {
    if h == hash {
      chr.hashes = slices.Delete(chr.hashes, i, i+1)
      break
    }
  }
}func (chr *ConsistentHashRing) PrintRing() {
	for _, h := range chr.hashes {
		fmt.Printf("Node: %s \t\t Hash: %d \t\t Total Keys: %v\n", chr.nodes[h].ID, h, len(chr.nodes[h].Keys))
	}
}
                            Subscribe
                        Consistent hashing is a powerful technique that improves load distribution in distributed systems. My implementation efficiently assigns keys to nodes and ensures minimal disruption when nodes are added or removed.If you have suggestions to optimize this implementation, drop them in the comments. I am always looking to improve my code.Thank you for reading, and donâ€™t forget to subscribe if you want more deep-dive posts like this!]]></content:encoded></item><item><title>Is kubent dead?</title><link>https://www.reddit.com/r/kubernetes/comments/1rf7ar3/is_kubent_dead/</link><author>/u/BojanKomazec</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 11:04:20 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/BojanKomazec ]]></content:encoded></item><item><title>Weekly: This Week I Learned (TWIL?) thread</title><link>https://www.reddit.com/r/kubernetes/comments/1rf788d/weekly_this_week_i_learned_twil_thread/</link><author>/u/AutoModerator</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 11:00:32 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Did you learn something new this week? Share here!]]></content:encoded></item><item><title>Built a free Go module health checker. Paste your go.mod, see archived deps and version freshness.</title><link>https://www.reddit.com/r/golang/comments/1rf74nv/built_a_free_go_module_health_checker_paste_your/</link><author>/u/Jzzck</author><category>golang</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 10:54:35 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I had gorilla/mux in 3 different projects for months after the gorilla org archived everything. Never noticed until I randomly checked GitHub one day. Figured I should automate that. Paste your  file. It parses every dependency, checks version freshness, flags archived modules (all gorilla/* packages, plus a few others), and shows you your Go version status. - Go version health check (1.25 and 1.26 are supported, older versions flagged) - Each module gets a freshness grade and direct/indirect label - Archived module warnings (gorilla/mux, gorilla/websocket, gorilla/handlers, etc.) - Badge integration showing real-time health data from endoflife.date - Copy the full report as markdownRuns entirely in your browser. Nothing is sent to any server. Your go.mod stays local.There's also a CLI version if you want to scan your whole project:  picks up go.mod plus any other config files in the directory. No install needed.Self-promo: I built this as part of ReleaseRun (release lifecycle tracking). Free tool, no signup. Feedback welcome, especially if there are archived or deprecated modules I should add to the detection list.]]></content:encoded></item><item><title>vCluster in Docker has Changed the Way we Build and Share Local Kubernetes Developer Environments</title><link>https://www.reddit.com/r/kubernetes/comments/1rf6nfk/vcluster_in_docker_has_changed_the_way_we_build/</link><author>/u/wineandcode</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 10:25:43 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[This post explains how to run vCluster directly inside Docker (vind) as Standalone and how to securely share local Kubernetes cluster with your team using Tailscale with zero configuration.   submitted by    /u/wineandcode ]]></content:encoded></item><item><title>I geolocated a blurry pic from the Paris protests down to the exact coordinates using AI</title><link>https://v.redd.it/9f6i1crj9tlg1</link><author>/u/Open_Budget6556</author><category>ai</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 09:51:17 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>KDE supports the &quot;Keep Android Open&quot; campaign</title><link>https://www.reddit.com/r/linux/comments/1rf5wlz/kde_supports_the_keep_android_open_campaign/</link><author>/u/Bro666</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 09:40:03 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>eBPF Foundation Funding eBPF Focused Meetups</title><link>https://ebpf.foundation/introducing-the-ebpf-meetup-program/</link><author>/u/xmull1gan</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 09:38:16 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How GitHub blocks external images in SVGs â€” and how to work around it with base64 encoding</title><link>http://github.com/readme-SVG/readme-SVG-youtube-preview</link><author>/u/DazzlingChicken4893</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 09:29:50 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[While building a small tool that generates YouTube preview cards for GitHub READMEs, I ran into something that wasn't obvious at all. Maybe it saves someone else an hour.I wanted to embed a YouTube thumbnail inside an SVG. Locally it worked perfectly. But on GitHub â€” blank. Just an empty card. I spent way too long thinking it was a bug in my code before I figured out what was actually happening.GitHub's Markdown sanitizer strips external URLs from  tags inside SVGs. This is a security measure to prevent tracking pixels and mixed-content issues. The SVG itself renders fine, but any external resource referenced inside it gets silently blocked. It's not documented very prominently, which is why it catches a lot of people off guard.Instead of linking to the image URL directly, you fetch the image server-side and convert it to a base64 data URI before embedding it into the SVG. Since the image data now lives inside the SVG string itself rather than as an external URL, GitHub renders it without any issues. The trade-off is that base64 increases response size by roughly 33% â€” for a typical YouTube thumbnail around 20KB, that's about 27KB extra. Worth it if you need the image to actually show up.While building this I also discovered that YouTube has a public oEmbed endpoint that returns a video's title and thumbnail URL with zero authentication â€” no API key, no quota, no developer account needed. I had no idea this existed. You just hit youtube.com/oembed?url=VIDEO_URL&format=json and get back everything you need. Useful well beyond this specific use case.I ended up turning this into a small open-source tool that generates these cards as a deployable service â€” if anyone's curious the repo is linked in the comments. But the base64 trick works for any situation where you need images inside SVGs to actually render on GitHub.Happy to answer questions about the implementation.]]></content:encoded></item><item><title>Awesome Kubernetes Architecture Diagrams</title><link>https://www.reddit.com/r/kubernetes/comments/1rf5g7k/awesome_kubernetes_architecture_diagrams/</link><author>/u/Philippe_Merle</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 09:10:30 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[The Awesome Kubernetes Architecture Diagrams repo studies  that auto-generate Kubernetes architecture diagrams from manifests, Helm charts, or cluster state. These tools are compared in depth via many criteria such as license, popularity (#stars and #forks), activity (1st commit, last commit, #commits, #contributors), implementation language, usage mode (CLI, GUI, SaaS), inputs formats supported, Kubernetes resource kinds supported, output formats. Moreover, diagrams generated by these tools for a well-known WordPress use case are shown, and diagram strengths/weaknesses are discussed. The whole should help practitioners to select which diagram generation tools to use according to their requirements.]]></content:encoded></item><item><title>How do you trace slow Spark jobs back to code on EMR on EKS?</title><link>https://www.reddit.com/r/kubernetes/comments/1rf5e1r/how_do_you_trace_slow_spark_jobs_back_to_code_on/</link><author>/u/Upper_Caterpillar_96</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 09:06:34 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[We run two EMR on EKS clusters with event logs written to one S3 bucket. One Spark History Server reads from it and that part works fine.The problem is when a job takes 45 minutes instead of 20. You open SHS, look at the stages, look at the timings. Something is off. But figuring out which part of the code caused it is not obvious at all.Shuffle spill shows up. GC pressure shows up. But there is no line saying this came from that transformation. You end up guessing.Comparing the same job across two runs means digging through multiple completed apps by hand. It gets old fast.We tried custom Spark listeners and some Prometheus scraping. A lot of glue work for something that still does not tell you what actually went wrong.What are people using in Kubernetes environments to get from "this job was slow" to something more concrete? How do you correlate SHS metrics, pod logs, or Prometheus data back to specific code paths or transformations?]]></content:encoded></item><item><title>How NVIDIA&apos;s CuTe replaces GPU index arithmetic with composable layout algebra</title><link>https://amandeepsp.github.io/blog/layout-algebra/</link><author>/u/amandeepspdhr</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 08:09:42 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How do you guys handle web server updates/deployments?</title><link>https://www.reddit.com/r/golang/comments/1rf3mnh/how_do_you_guys_handle_web_server/</link><author>/u/Existing-Search3853</author><category>golang</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 07:17:29 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Iâ€™m currently developing a microservice and, given the wide variety of options out there, Iâ€™d like to hear some outside opinions or learn about your own workflows. What do you think is the best way to update a web server?My current plan is to use GitHub Actions for the build, upload it as an artifact, and then have the server download it. However, since there are so many other ways to do this, I wanted to see what you all suggest.How do you update your server code? Is it fully automated, or do you still do some parts manually?]]></content:encoded></item><item><title>Finally broke the scaling wall on my Rust physics engine (1.83x speedup on a single massive 4000-body island). Hereâ€™s how I fixed my threading bottlenecks.</title><link>https://www.reddit.com/r/rust/comments/1rf320a/finally_broke_the_scaling_wall_on_my_rust_physics/</link><author>/u/IamRustyRust</author><category>rust</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 06:44:23 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=3">Reddit - Rust</source><content:encoded><![CDATA[I've been building a custom multi-threaded rigid-body physics engine in Rust (TitanEngine) from scratch, and recently I hit a massive brick wall. Under heavy load, my 8-core CPU was yielding completely abysmal scaling (less than 1.0x).Parallelizing a bunch of separated, isolated islands is easy enough, but I was stress-testing a single massive dependency chainâ€”a 4000-body dense stack with 3,520 constraints in one single graph.After weeks of pulling my hair out, tracing logs, and hardware profiling, I finally managed to dismantle the bottlenecks and hit a deterministic 1.83x speedup. Here is what was actually killing my performance and how I fixed it:1. The False-Sharing Nightmare I realized my solvers were directly modifying dense arrays inside a parallel loop. Even though threads were manipulating distinct indices (so no data corruption), the contiguous memory addresses forced them to sequentially lock equivalent cache-lines. The invisible bus stalls were insane.  I transitioned the internal constraint resolutions to proxy through a padded struct utilizing . By committing memory states strictly outside the threaded boundaries, the false sharing completely vanished.2. Ditching Rayon's overhead for a Crossbeam Barrier The biggest headache was the Temporal Gauss-Seidel (TGS) solver. TGS requires strict color batching. Rayon was being forced to violently spawn and tear down  iterators 150 times per substep. The stop-and-go thread execution overhead was actually taking longer than the SIMD math itself.  I completely inverted the multithreading loop. Now, I generate a persistent  of 8 OS threads  per Island. They navigate all the colors and iterations internally using a lock-free allocator and a double . Thread spin-yields and CAS retries instantly dropped from 190+ million to literally 0.3. Sequential blocks killing Amdahl's Law Before Rayon could even dispatch workers, my single-threaded setup functions were bottlenecking everything.  I dismantled the single-threaded graph-coloring array copy and replaced it with a lock-free Multi-Threaded Prefix-Sum scan (distributing O(N) writes fully across 8 workers). I also replaced a massive CAS spin-lock on my penetration accumulator with a local map-reduce  algorithm.4. The Telemetry Proof (Tower of Silence Benchmark) To make sure my math wasn't diverging, I dumped the telemetry into a pandas dataframe (I've attached the graph to this post). 10 GS iterations failed to push forces up the stack. Deep penetrations triggered massive Baumgarte stabilization bias, exploding the kinetic energy to 1335.87 and blowing the stack apart.With Double-Buffered SIMD Cache: The memory hit rate jumped straight to 80%. TGS impulses warm-started perfectly. Kinetic energy capped at 44.39, decayed exponentially, and by frame 595 hit absolute rest (1.35e-09 kinetic energy with exactly 0.0 solver error).I also got a thermodynamic sleeping protocol working that cleanly extracts dead constraint islands from the active queue when entropy hits exactly 0.0.Max Constraints in a Single Island = 35201 Worker Duration: 11.83sGetting near 2.0x on a single dense island feels like a huge milestone for this project. Next up, I need to implement a dynamic dispatch threshold (falling back to a single thread for micro-workloads under 1000 constraints, as the barrier overhead completely dominates the math at that scale).]]></content:encoded></item><item><title>The React Foundation: A New Home for React Hosted by the Linux Foundation</title><link>https://react.dev/blog/2026/02/24/the-react-foundation</link><author>/u/Nimelrian</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 06:29:27 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[The React Foundation has officially launched, hosted by the Linux Foundation.In October, we announced our intent to form the React Foundation. Today, weâ€™re excited to share that the React Foundation has officially launched.React, React Native, and supporting projects like JSX are no longer owned by Meta â€” they are now owned by the React Foundation, an independent foundation hosted by the Linux Foundation. You can read more in the Linux Foundationâ€™s press release.The React Foundation has eight Platinum founding members: , , , , , , , and .  has joined since our announcement in October. The React Foundation will be governed by a board of directors composed of representatives from each member, with Seth Webster serving as executive director.New Provisional Leadership Council Reactâ€™s technical governance will always be independent from the React Foundation board â€” Reactâ€™s technical direction will continue to be set by the people who contribute to and maintain React. We have formed a provisional leadership council to determine this structure. We will share an update in the coming months.There is still work to do to complete the transition. In the coming months we will be:Finalizing the technical governance structure for ReactTransferring repositories, websites, and other infrastructure to the React FoundationExploring programs to support the React ecosystemKicking off planning for the next React ConfWe will share updates as this work progresses.None of this would be possible without the thousands of contributors who have shaped React over the past decade. Thank you to our founding members, to every contributor who has opened a pull request, filed an issue, or helped someone learn React, and to the millions of developers who build with React every day. The React Foundation exists because of this community, and weâ€™re looking forward to building its future together.]]></content:encoded></item><item><title>Anyone else seeing â€œGPU node looks healthy but jobs fail until rebootâ€? (K8s)</title><link>https://www.reddit.com/r/kubernetes/comments/1rf2sn3/anyone_else_seeing_gpu_node_looks_healthy_but/</link><author>/u/Chika5105</author><category>reddit</category><pubDate>Thu, 26 Feb 2026 06:29:16 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[We keep hitting a frustrating class of failures on GPU clusters:Node is up. Metrics look normal. NVML/DCGM look fine. But distributed training/inference jobs stall, hang, or crash â€” and a reboot â€œfixesâ€ it.It feels like something is degrading below the usual device metrics.Iâ€™ve been digging into correlating lower-level signals across: GPU â†” PCIe â†” CPU/NUMA â†” memory + kernel eventsTrying to understand whether certain patterns (AER noise, Xids, ECC drift, NUMA imbalance, driver resets, PCIe replay rates, etc.) show up before the node becomes unusable.If youâ€™ve debugged this â€œlooks healthy but isnâ€™tâ€ class of issue:What were the real root causes?What signals were actually predictive?What turned out to be red herrings?Iâ€™m less interested in dashboards and more in understanding where failure modes actually hide in modern GPU stacks.]]></content:encoded></item><item><title>I&apos;m learning Go and can&apos;t figure out... Is there a purpose of public fields in private structs?</title><link>https://www.reddit.com/r/golang/comments/1rf20y0/im_learning_go_and_cant_figure_out_is_there_a/</link><author>/u/oneeyedziggy</author><category>golang</category><category>reddit</category><pubDate>Thu, 26 Feb 2026 05:46:17 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[If the a private struct is not available outside of the current package then why would it need fields that are publi, aand nominally should be available outside the current package?I even asked AI and its answers seemed like it was making shit up to fill a gap in training data... A lot around separation of concerns and data access control while not being able to elaborate when asked for more detail...I feel like I must be missing something obvious here... Or misunderstanding something badly. ]]></content:encoded></item></channel></rss>
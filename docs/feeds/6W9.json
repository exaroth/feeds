{
  "id": "6W9",
  "title": "HN",
  "displayTitle": "HN",
  "url": "",
  "feedLink": "",
  "isQuery": true,
  "isEmpty": false,
  "isHidden": false,
  "itemCount": 98,
  "items": [
    {
      "title": "Samsung Galaxy update removes Android recovery menu tools, including sideloading",
      "url": "https://9to5google.com/2026/02/27/samsung-galaxy-update-android-recovery-menu-removed/",
      "date": 1772330340,
      "author": "pabs3",
      "guid": 49295,
      "unread": true,
      "content": "<p>Samsung, with some of its latest updates, is set to remove some core tools from Android’s recovery menu, and it’s unclear why.</p><p>Every Android smartphone ships with a recovery menu that includes the abilty to reset the device, wipe the cache, apply updates, and more. That’s a  simplified explanation, but it’s a standard feature, and one that you might be most familiar with when manually sideloading Android updates, <a href=\"https://9to5google.com/2026/02/26/how-to-install-the-android-17-beta-on-google-pixel/\" type=\"post\">such as the beta updates</a> Google releases for Pixels.</p><p>In One UI 8.5, though, Samsung is making a change to this.</p><p>As first noted by <a href=\"https://www.galaxyclub.nl/nieuws/samsung-update-verwijdert-handige-nerdy-functie-van-je-galaxy/\"></a>and <a href=\"https://t.me/techleakszone/9246\" type=\"link\">spotted by others </a>too, Samsung is removing several options from the Android recovery menu with the latest updates for Galaxy phones. Specifically, the update is removing:</p><ul><li>Apply update from SD card</li></ul><p>The only options remaining are “Reboot system now,” “Wipe data/factory reset,” and “Power off.”</p><p> can confirm that, at least on the current software build (January 2026 security patch), recovery tools are still fully in place on the Galaxy S26 Ultra, but that could change seeing as  reports seeing this change attached to February 2026 security updates.</p><p>This change might be permanent, too, as also notes that this update came with a notice saying that “you will not be able to downgrade to the old software due to changes in security policy.”</p><p>So why is this happening? </p><p>Put simply, we do not know. There has been speculation, though, that Samsung might be tightening security. Just today, a leaker <a href=\"https://x.com/wr3cckl3ss1/status/2027484909082694116\">showed </a>that Samsung is taking legal action to put a stop to One UI build leaks. It’d certainly be quite a big move for Samsung to just cut off sideloading via the recovery for eveyone in response, but it’s not necessarily out of the question either. </p><p>The Galaxy S26 series is available for pre-order now, with Samsung’s usual pre-order perks in full swing. You’ll find boosted trade-in values and more available now through March 11, when these phones are available on store shelves. You’ll also get an additional $30 credit if you buy using our links below!</p><div><p><em>FTC: We use income earning auto affiliate links.</em><a href=\"https://9to5mac.com/about/#affiliate\">More.</a></p><a href=\"https://benqurl.biz/3WZyqpL\"><img src=\"https://9to5google.com/wp-content/uploads/sites/4/2026/02/native-banner_750_150.jpg?quality=82&amp;strip=all&amp;w=700\" alt=\"\" width=\"700\" height=\"140\"></a></div>",
      "contentLength": 2082,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47202808"
    },
    {
      "title": "Microgpt",
      "url": "http://karpathy.github.io/2026/02/12/microgpt/",
      "date": 1772329166,
      "author": "tambourine_man",
      "guid": 49274,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47202708"
    },
    {
      "title": "Claude surpasses ChatGPT to become the #1 app on the US App Store",
      "url": "https://apps.apple.com/us/iphone/charts",
      "date": 1772323728,
      "author": "byincugnito",
      "guid": 49260,
      "unread": true,
      "content": "<p>Simple. Reliable. Private.</p>",
      "contentLength": 26,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47202032"
    },
    {
      "title": "The Windows 95 user interface: A case study in usability engineering (1996)",
      "url": "https://dl.acm.org/doi/fullHtml/10.1145/238386.238611",
      "date": 1772317176,
      "author": "ksec",
      "guid": 49259,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47200904"
    },
    {
      "title": "Iran's Ayatollah Ali Khamenei is killed in Israeli strike, ending 36-year rule",
      "url": "https://www.npr.org/2026/02/28/1123499337/iran-israel-ayatollah-ali-khamenei-killed",
      "date": 1772316968,
      "author": "andsoitis",
      "guid": 49250,
      "unread": true,
      "content": "<div><div><div><div aria-label=\"Image caption\"><p>\n                In this 2017 photo, Ayatollah Ali Khamenei, Iran's supreme leader, sits in a session to deliver his message for the Iranian New Year. A portrait of the late revolutionary founder, Ayatollah Ruhollah Khomeini, is next to him.\n                <b aria-label=\"Image credit\">\n                    \n                    Office of the Iranian Supreme Leader/AP\n                    \n                </b></p></div></div></div><div><div><p>In this 2017 photo, Ayatollah Ali Khamenei, Iran's supreme leader, sits in a session to deliver his message for the Iranian New Year. A portrait of the late revolutionary founder, Ayatollah Ruhollah Khomeini, is next to him.</p></div></div></div><p>Iran's supreme leader, Ayatollah Ali Khamenei, was <a href=\"https://www.npr.org/2026/02/28/nx-s1-5730158/israel-iran-strikes-trump-us\" target=\"_blank\">killed in Israeli attacks</a>, with U.S. support, on Saturday. He was 86 years old.</p><p>President Trump announced the Iranian leader's death <a href=\"https://truthsocial.com/@realDonaldTrump/posts/116150413051904167\" target=\"_blank\">on social media</a>, saying Khamenei could not avoid U.S. intelligence and surveillance. A source briefed on the U.S.-Israeli attacks on Iran told NPR earlier Saturday that an <a href=\"https://www.npr.org/2026/02/28/nx-s1-5730158/israel-iran-strikes-trump-us\" target=\"_blank\">Israeli airstrike killed Khamenei</a>.</p><p>During his 36-year rule, Khamenei was unwavering in his steadfast antipathy to the U.S. and Israel and to any efforts to reform and bring Iran into the 21st century.</p><p>Khamenei was born in July 1939 into a religious family in the Shia Muslim holy city of Mashhad in northeastern Iran and attended theological school. An outspoken opponent of the U.S.-backed Shah Mohammad Reza Pahlavi, Khamenei was arrested several times.</p><p>He was surrounded by other Iranian activists, including Ayatollah Ruhollah Khomeini, who became Iran's first supreme leader following the country's Islamic Revolution in the late 1970s.</p><p>Khamenei survived an assassination attempt in 1981 that cost him the use of his right arm. He served as Iran's president before succeeding Khomeini as supreme leader in 1989.</p><p>Alex Vatanka, a senior fellow at the Middle East Institute in Washington, D.C., says Khamenei was an unlikely candidate. Then a midlevel cleric, Khamenei lacked religious credentials, which left him feeling vulnerable, Vatanka says.</p><p>\"He knew himself. He didn't have the prestige, the gravitas to be … the successor to the founder of the Islamic Republic, Ayatollah Khomeini,\"he says. </p><div><div><div><div aria-label=\"Image caption\"><p>\n                In 2005, Ali Khamenei (center), newly elected President Mahmoud Ahmadinejad (right), outgoing President Mohammad Khatami and former President Ali Akbar Hashemi Rafsanjani attend Ahmadinejad's inaugural ceremony in Tehran.\n                <b aria-label=\"Image credit\">\n                    \n                    Atta Kenare/AFP via Getty Images\n                    \n                </b></p></div></div></div><div><div><p>In 2005, Ali Khamenei (center), newly elected President Mahmoud Ahmadinejad (right), outgoing President Mohammad Khatami and former President Ali Akbar Hashemi Rafsanjani attend Ahmadinejad's inaugural ceremony in Tehran.</p></div></div></div><p>\"He spent the first few years in power being very nervous,\" says Vatanka. \"He really literally felt that somebody is going to, you know, take him down from the position of power.\"</p><p>But Khamenei was cunning and able to outwit other senior political figures in the Islamic Republic, according to Ali Vaez, director of the Iran Project at the International Crisis Group. He says that with the help of the formidable Islamic Revolutionary Guard Corps, Khamenei built up his power base to become the longest-serving leader in the Middle East.</p><p>\"Ayatollah Khamenei was a man with strategic patience and was able to calculate a few steps ahead,\" he says.&nbsp;\"That's why I think he managed — on the back of the Revolutionary Guards — to increasingly appropriate all the levers of power in his hands and sideline everyone else.\"</p><p>Khamenei's close ties to the Revolutionary Guards allowed Iran's military to develop a vast commercial empire in control of many parts of the economy, while ordinary Iranians struggled to get by.</p><div><div><div><div aria-label=\"Image caption\"><p>\n                Ali Khamenei (right) speaks to members of the armed forces of the Islamic Republic during the Iran-Iraq War on Oct. 4, 1981.\n                </p></div></div></div><div><div><p>Ali Khamenei (right) speaks to members of the armed forces of the Islamic Republic during the Iran-Iraq War on Oct. 4, 1981.</p></div></div></div><p>Vaez says Khamenei also began to build up Iran's defensive policies, such as developing proxies like Hezbollah in Lebanon and Hamas in the Gaza Strip to deter a direct attack on Iranian soil.</p><p>\"And then also becoming self-reliant in developing a viable conventional deterrence, which took the form of Iran's ballistic missile program,\" Vaez says.</p><p>As supreme leader, Khamenei also had the final word on anything to do with Iran's nuclear program.</p><p>Over time, Khamenei increasingly injected himself into politics. Such was the case in 2009, when he intervened in the presidential election to ensure that his favored candidate, the controversial conservative Mahmoud Ahmadinejad, won office. </p><p>Iranians took to the streets to protest what was widely seen as a fraudulent election. Khamenei brutally crushed those demonstrations, triggering both a backlash and more protest movements over the years.</p><p>Iran killed thousands of its citizens under Khamenei's rule, including more than 7,000 people killed during weeks of mass protests that started in late December 2025, according to the <a href=\"https://www.en-hrana.org/day-50-of-the-protests-intensification-of-security-prosecutions-and-uncertainty-regarding-the-status-of-detainees/\" target=\"_blank\">Human Rights Activists News Agency</a>, a U.S.-based organization that closely tracks rights abuses in Iran.</p><div><div><div><div aria-label=\"Image caption\"><p>\n                Iran's supreme leader, Ayatollah Ali Khamenei (center), prays with the Iranian president and other government officials in Tehran in 2014.\n                <b aria-label=\"Image credit\">\n                    \n                    Anadolu Agency/Getty Images\n                    \n                </b></p></div></div></div><div><div><p>Iran's supreme leader, Ayatollah Ali Khamenei (center), prays with the Iranian president and other government officials in Tehran in 2014.</p></div></div></div><p>\"Khamenei had always supported and endorsed repressive government crackdown, recognizing that these protests were damaging to the stability and legitimacy of the state,\" says Sanam Vakil, an Iran expert at Chatham House, a London-based think tank.</p><p>But Khamenei was unconcerned about getting to the root of the protests, says the Middle East Institute's Vatanka, and remained stuck in an Islamic revolutionary mindset against the West.</p><p>\"He onso many occasions refused point-blank to accept the basic reality that where he was in terms of his worldview was not where the rest of his people were,\" Vatanka says.</p><p>He adds that 75% of Iran's 90 million people were born after the revolution and have watched other countries in the region modernize and integrate with the international community.</p><p>\"The 75% he should have catered to, listened to and address[ed] policies to satisfy their aspirations,\" he says. \"He failed in that miserably.\"</p><div><div><div><div aria-label=\"Image caption\"><p>\n                Ali Khamenei wears a mask due to the COVID-19 pandemic as he arrives to cast his ballot during Iran's presidential election on June 18, 2021.\n                <b aria-label=\"Image credit\">\n                    \n                    Atta Kenare/AFP via Getty Images\n                    \n                </b></p></div></div></div><div><div><p>Ali Khamenei wears a mask due to the COVID-19 pandemic as he arrives to cast his ballot during Iran's presidential election on June 18, 2021.</p></div></div></div><p>The International Crisis Group's Vaez says after the Arab Spring uprisings in 2011, Khamenei did start worrying about the survival of his regime. Iran's economy was crumbling, due in large part to stringent Western sanctions, fueling more unrest.</p><p>In 2013, Khamenei agreed to secret negotiations with the U.S. about Iran's nuclear program, which eventually led to the 2015 Joint Comprehensive Plan of Action nuclear agreement. Vaez says Khamenei deeply distrusted the U.S. and was skeptical about the deal.</p><p>\"His argument has always been that the U.S. is always looking for pretexts, for putting pressure on Iran,\" he says. \"And if Iran concedes on the nuclear issue, then the U.S. would put pressure on Iran because of its missiles program or because of human rights violations or because of its regional policies.\"</p><p>President Trump's withdrawal from the nuclear deal during his first term in office gave some credence to Khamenei's cynicism. Analysts say Iran increased its nuclear enrichment after that to a point where it was close to being able to build a bomb.</p><p>In early 2025, when Trump reached out to Iran about a new deal, Khamenei dragged out negotiations until they began in mid-April.</p><p>But time ran out. In June,Israel made good on its threat to neutralize Iran's nuclear program, launching strikes on key facilities and killing scientists and generals. Iran retaliated, and the two sides exchanged several days of missile strikes.</p><p>On June 21, 2025, the U.S. <a href=\"https://www.npr.org/2025/06/21/nx-s1-5441127/iran-us-strike-nuclear-trump\" target=\"_blank\">launched major airstrikes</a> on three of Iran's nuclear enrichment sites. Trump said the facilities had been \"completely and totally obliterated,\" although there was debate among the White House and nuclear experts as to how serious Iran's nuclear program had been set back.</p><p>Vakil, of Chatham House, says Khamenei underestimated what Israel and the U.S. would do.</p><p>\"I think that Khamenei always assumed that he could play for time, and what he really didn't understand is that the world around Iran had very much changed,\" she says. \"The world had tired of Khamenei and Iranian foot-dragging and antics …&nbsp;and so that was a miscalculation.\"</p><p>But it was Iran's use of proxy militias across the region that eventually led to Khamenei's downfall. </p><p>When Hamas — the Palestinian Islamist group backed by Iran — attacked Israel on Oct. 7, 2023, killing nearly 1,200 people and kidnapping 251 others, it triggered a cascade of events that ultimately led to Israel's attack on Iran.&nbsp;</p><p>The day after the 2023 Hamas-led attack, Iran-backed Hezbollah in Lebanon started firing rockets into Israel, triggering a conflict that led to the Shia militia's top brass being decimated — including top leader <a href=\"https://www.npr.org/2024/09/28/g-s1-25302/who-was-hassan-nasrallah-the-hezbollah-leader-killed-by-israel\" target=\"_blank\">Hassan Nasrallah</a>.</p><p>Israel and Iran traded direct airstrikes for the first time in 2024 as part of that conflict.</p><p>Israel's bombing of Iranian weapons shipments in Syria also helped weaken the regime of Syria's then-dictator, Bashar al-Assad, an important ally of Iran. Assad fell in December 2024 and fled to Russia in early January 2025.</p><p>By the time Khamenei died, his legacy was in tatters. Israel had hobbled two key proxies, Hamas and Hezbollah, and had wiped out Iran's air defenses. With U.S. help, it left Iran's nuclear program in shambles.</p><p>What remains is a robust ballistic missile program, the brainchild of Khamenei. It's unclear who will replace him to lead a now weakened and vulnerable Iran.</p>",
      "contentLength": 10332,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47200879"
    },
    {
      "title": "We do not think Anthropic should be designated as a supply chain risk",
      "url": "https://twitter.com/OpenAI/status/2027846016423321831",
      "date": 1772313856,
      "author": "golfer",
      "guid": 49258,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47200420"
    },
    {
      "title": "MinIO Is Dead, Long Live MinIO",
      "url": "https://blog.vonng.com/en/db/minio-resurrect/",
      "date": 1772313413,
      "author": "zufallsheld",
      "guid": 49249,
      "unread": true,
      "content": "<p>MinIO’s open-source repo has been officially archived. No more maintenance.\nEnd of an era — but open source doesn’t die that easily.</p><p>I created a MinIO fork, restored the admin console, rebuilt the binary distribution pipeline, and brought it back to life.</p><p>If you’re running MinIO, swap  for .\nEverything else stays the same. (<a href=\"https://nvd.nist.gov/vuln/detail/CVE-2025-62506\" target=\"_blank\">CVE fixed</a>, and the console GUI is back)</p><p>On December 3, 2025, MinIO announced “maintenance mode” on GitHub. I wrote about it in <a href=\"https://blog.vonng.com/en/db/minio-is-dead/\">MinIO Is Dead</a>.</p><p>On February 12, 2026, MinIO updated the repo status from “maintenance mode” to , then officially archived the repository.\nRead-only. No PRs, no issues, no contributions accepted. A project with 60k stars and over a billion Docker pulls became a digital tombstone.</p><p>If December was the clinical death, this February commit was the death certificate.</p><p>Percona founder Peter Zaitsev also raised concerns about open-source infrastructure sustainability on LinkedIn.\nThe consensus in the international community is clear:</p><p>Looking back at the timeline over the past years, this wasn’t a sudden death. It was a slow, deliberate wind-down:</p><table><tbody><tr></tr><tr><td>Legal action against Nutanix</td></tr><tr><td>Legal action against Weka</td></tr><tr><td>Admin console removed from CE</td></tr><tr><td>Binary/Docker distribution stopped</td></tr><tr><td>Maintenance mode announced</td></tr><tr><td>Repo archived, no longer maintained</td></tr></tbody></table><p>A company that raised $126M at a billion-dollar valuation spent five years methodically dismantling the open-source ecosystem it built.</p><p>Normally this is where the story ends — a collective sigh, and everyone moves on.</p><p>But I want to tell a different story. <strong>Not an obituary — a resurrection.</strong></p><p>MinIO Inc. can archive a repo, but they can’t archive the rights that the <a href=\"https://github.com/minio/minio/blob/master/LICENSE\" target=\"_blank\">AGPL</a> grants to the community.</p><p>Ironically, AGPL was MinIO’s own choice. They switched from Apache 2.0 to AGPL to use it as leverage in their disputes with Nutanix and Weka\n— keeping the “open source” label while adding enforcement teeth. But open-source licenses cut both ways — the same license now guarantees the community’s right to fork.</p><p>Once code is released under AGPL, the license is irrevocable. You can set a repo to read-only, but you can’t claw back a granted license.\nThat’s the beauty of open-source licensing by design: <strong>a company can abandon a project, but it can’t take the code with it.</strong></p><p>So — <strong>MinIO is dead, but MinIO can live again.</strong></p><p>That said, forking is the easy part. Anyone can click the Fork button.\nThe real question isn’t “can we fork it” but <strong>“can someone actually maintain it as a production component?”</strong></p><p>I didn’t set out to take this on. But after MinIO entered maintenance mode,\nI waited a couple of weeks for someone in the community to step up.</p><p>But I didn’t find one. So I did it myself.</p><p>Some background: I maintain <a href=\"https://pigsty.io\" target=\"_blank\">Pigsty</a> — a batteries-included PostgreSQL distribution with <a href=\"https://pgext.cloud\" target=\"_blank\">460+ extensions</a>,\ncross-built for <a href=\"https://pgext.cloud/os\" target=\"_blank\">14 Linux distros</a>. I also maintain build pipelines for <a href=\"https://pgext.cloud/list/\" target=\"_blank\">290</a> PG extensions, several <a href=\"https://pigsty.io/docs/pgsql/kernel/\" target=\"_blank\">PG forks</a>,\nand dozens of <a href=\"https://pigsty.io/docs/repo/infra/list/\" target=\"_blank\">Go Projects</a> (Victoria, Prometheus, etc.) packaging across all major platforms. Adding one more to the pipeline was a piece of cake.</p><p>I’m not new to MinIO either. Back in 2018, we ran an internal MinIO fork at TanTan (back when it was still Apache 2.0),\nmanaging ~25 PB of data — one of the earliest and largest MinIO deployments in China at the time.</p><p>We use MinIO ourselves, so keeping the supply chain alive was not optional — \nAs early as December 2025, when MinIO announced maintenance mode, I had already built <a href=\"https://nvd.nist.gov/vuln/detail/CVE-2025-62506\" target=\"_blank\">CVE-patched</a> binaries and switched to them.</p><p>As of today, three things.</p><h3>1. Restored the Admin Console</h3><p>This was the change that frustrated the community the most.</p><p>In May 2025, MinIO stripped the full admin console from the community edition, leaving behind a bare-bones object browser.\nUser management, bucket policies, access control, lifecycle management — all gone overnight. Want them back? Pay for the enterprise edition. (~$100,000)</p><p>The ironic part: this didn’t even require reverse engineering.\nYou just revert the  submodule to the previous version.\nThey swapped a dependency version to replace the full console with a stripped-down one. The code was always there.</p><h3>2. Rebuilt Binary Distribution</h3><p>In October 2025, MinIO stopped distributing pre-built binaries and Docker images,\nleaving only source code. “Use  to build it yourself” — that was their answer.</p><p>For the vast majority of users, the value of open-source software isn’t just a copy of the source — <strong>supply chain stability is what matters.</strong>\nYou need a stable artifact you can put in a Dockerfile, an Ansible playbook, or a CI/CD pipeline — not a requirement to install a Go compiler before every deployment.</p><p><strong>We rebuilt the distribution:</strong></p><dl><dd><a href=\"https://hub.docker.com/r/pgsty/minio\" target=\"_blank\"></a> is live on Docker Hub.  and you’re good.</dd><dd>Built for major Linux distributions, matching the original package specs.</dd><dd>Fully automated build workflows on GitHub, ensuring ongoing supply chain stability.</dd></dl><p>If you’re using Docker, just swap  for .</p><p>For native Linux installs, grab RPM/DEB packages from the <a href=\"https://github.com/pgsty/minio/releases/tag/RELEASE.2026-02-14T12-00-00Z\" target=\"_blank\">GitHub Release</a> page.\nYou can also use <a href=\"https://github.com/pgsty/pig\" target=\"_blank\">pig</a> (the PG extension package manager) for easy installation,\nor configure the <a href=\"https://pigsty.io/docs/repo/infra\" target=\"_blank\"></a> APT/DNF repo to install from it:</p><div><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div></div><h3>3. Restored Community Edition Docs</h3><p>MinIO’s official documentation was also at risk — links had started redirecting to their commercial product, <a href=\"https://docs.min.io/\" target=\"_blank\">AIStor</a>.</p><p>We forked <a href=\"https://github.com/pgsty/minio-docs\" target=\"_blank\"></a>, fixed broken links, restored removed console documentation, and deployed it <a href=\"https://silo.pigsty.io\" target=\"_blank\">here</a>.</p><p>The docs use the same CC Attribution 4.0 license as the original, with necessary maintenance.</p><p>Some things worth stating up front to set expectations.</p><h3>No New Features — Just Supply Chain Continuity</h3><p>MinIO as an S3-compatible object store is already feature-complete. It’s a .\nIt doesn’t need more bells and whistles — it needs a stable, reliable, continuously available build.\n(I already have PostgreSQL for these, so I don’t need something like S3 table or S3 vector. A stable S3 core is all I need)</p><p>What we’re doing: <strong>making sure you can get a working, complete MinIO binary</strong>, <strong>with the admin console included and CVE fixed.</strong>\nRPM, DEB, Docker images — built automatically via CI/CD, drop-in compatible with your existing minio.\nWe keep the existing minio naming and behavior where legally and technically feasible.</p><h3>This Is a Production Build, Not an Archive</h3><p>We run these builds ourselves and have been dogfooding them in production for three months.\nIf something breaks, we detect it early and patch it quickly.</p><p>I build this primarily for Pigsty and our own usage, but I hope it helps others too.</p><h3>I’m willing to Track CVEs and Fix Bugs</h3><p>If you run into issues, feel free to report them at <a href=\"https://github.com/orgs/pgsty/discussions\" target=\"_blank\"></a>.\nI’ll do my best to fix these — but please don’t treat this as a commercial SLA.</p><p>Given that AI coding tools have made bug fixing dramatically cheaper,\nand that we’re explicitly <strong>not adding any new features</strong>,\nI believe the maintenance workload is manageable.\n(how often do you see one?)</p><h3>Trademark Is Tricky, But We’ll Cross That Bridge When We Come to It</h3><blockquote><p>Trademark Notice: MinIO® is a registered trademark of MinIO, Inc.\nThis project (pgsty/minio) is an independently maintained community fork under the AGPL license.\nIt has no affiliation with, endorsement by, or connection to MinIO, Inc.\nUse of “MinIO” in this post refers solely to the open-source software project itself and implies no commercial association.</p></blockquote><p>AGPLv3 gives us clear rights to fork and distribute, but trademark law is a separate domain.\nWe’ve marked this clearly everywhere as an independent community-maintained build.</p><p>If MinIO Inc. raises trademark concerns, we’ll cooperate and rename (probably something like  or ).\nUntil then, we think descriptive use of the original name in an AGPL fork is reasonable — and renaming all the  references doesn’t serve users.</p><p>You might ask: can one person really maintain this?</p><p>It’s 2026. Things are different now.</p><p>With tools like Claude Code &amp; Codex, the cost of locating and fixing bugs in a complex Go project has dropped by more than an order of magnitude.\nWhat used to require a dedicated team to maintain a complex infra project can now be handled by one experienced engineer with an AI copilot.</p><p>Maintaining a MinIO build without adding new features is a manageable task.\nThe key requirement is testing and validation. and we already have that scenario,\nwhich lets us verify compatibility, reliability, and security in practice.</p><p>Consider: Elon cut X/Twitter’s engineering team down to ~30 people and the system still runs.\nMaintaining a MinIO fork without new features is considerably less daunting</p><p>MinIO Inc. can archive a GitHub repo, but they can’t archive the demand behind 60k stars,\nor the dependency graph behind a billion Docker pulls. That demand doesn’t disappear — it just finds its way out.</p><p>\nA company can abandon a project, but open-source licenses are specifically designed so the code can’t die.</p><p>Fork is the most powerful spell in open source. When a company decides to shut the door, the community only needs two words:</p>",
      "contentLength": 8898,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47200342"
    },
    {
      "title": "Our Agreement with the Department of War",
      "url": "https://openai.com/index/our-agreement-with-the-department-of-war",
      "date": 1772310929,
      "author": "surprisetalk",
      "guid": 49245,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47199948"
    },
    {
      "title": "Qwen3.5 122B and 35B models offer Sonnet 4.5 performance on local computers",
      "url": "https://venturebeat.com/technology/alibabas-new-open-source-qwen3-5-medium-models-offer-sonnet-4-5-performance",
      "date": 1772310000,
      "author": "lostmsu",
      "guid": 49248,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47199781"
    },
    {
      "title": "Block the “Upgrade to Tahoe” Alerts",
      "url": "https://robservatory.com/block-the-upgrade-to-tahoe-alerts-and-system-settings-indicator/",
      "date": 1772305441,
      "author": "todsacerdoti",
      "guid": 49247,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47198977"
    },
    {
      "title": "\"Cancel ChatGPT\" movement goes mainstream after OpenAI closes deal with U.S. Dow",
      "url": "https://www.windowscentral.com/artificial-intelligence/cancel-chatgpt-movement-goes-mainstream-after-openai-closes-deal-with-u-s-department-of-war-as-anthropic-refuses-to-surveil-american-citizens",
      "date": 1772305427,
      "author": "AndrewKemendo",
      "guid": 49212,
      "unread": true,
      "content": "<p>There are no virtuous participants in the <a data-analytics-id=\"inline-link\" href=\"https://www.windowscentral.com/artificial-intelligence\" data-auto-tag-linker=\"true\" data-url=\"https://www.windowscentral.com/artificial-intelligence\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-redirect=\"https://www.windowscentral.com/tag/artificial-intelligence\" data-before-rewrite-localise=\"https://www.windowscentral.com/artificial-intelligence\">artificial intelligence</a> race, but if there was, it might've been Anthropic.</p><p>Large language model tech is built on mountains of stolen data. The entire summation of decades of the open internet was downloaded and converted by billionaires into tech that threatens to destroy billions of jobs, end the global economy, and potentially the human race. But hey, at least in the short term, shareholders (might) make a stack of cash.</p><p aria-hidden=\"true\">There are no moral leaders in this space, sadly. But at the very least, Anthropic of Claude fame took a strong stand this week against the United States government, to the ire of the Trump administration.</p><p aria-hidden=\"true\">Anthropic was designated a supply chain risk this week, and summarily and forcibly banned from use in U.S. governmental agencies. Why? Anthropic said in a <a data-analytics-id=\"inline-link\" href=\"https://www.anthropic.com/news/statement-department-of-war\" target=\"_blank\" rel=\"nofollow\" data-url=\"https://www.anthropic.com/news/statement-department-of-war\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">blog post</a> it revolved around their two major red lines — no Claude AI for use in autonomous weapons, or mass surveillance of United States citizens.</p><p aria-hidden=\"true\">It's not unexpected that mainstream governments of any stripe would be salivating at the thought of turbo-charged AI mass surveillance, but it is unexpected that a big tech corp like Anthropic would be willing to take such a strong stance against it in an era increasingly devoid of administrative morality. But hey, there's always someone willing to race to the metaphorical moral abyss in the name of money.</p><figure data-bordeaux-image-check=\"\"><figcaption itemprop=\"caption description\"></figcaption></figure><p><a data-analytics-id=\"inline-link\" href=\"https://www.windowscentral.com/artificial-intelligence/openai-chatgpt\" data-auto-tag-linker=\"true\" data-url=\"https://www.windowscentral.com/artificial-intelligence/openai-chatgpt\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-redirect=\"https://www.windowscentral.com/tag/openai\" data-before-rewrite-localise=\"https://www.windowscentral.com/artificial-intelligence/openai-chatgpt\">OpenAI</a> CEO Sam Altman and part time supervillain thankfully stepped in to bail out the U.S. Department of War, pledging <a data-analytics-id=\"inline-link\" href=\"https://www.windowscentral.com/artificial-intelligence/openai-chatgpt\" data-auto-tag-linker=\"true\" data-url=\"https://www.windowscentral.com/artificial-intelligence/openai-chatgpt\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-redirect=\"https://www.windowscentral.com/tag/chatgpt\" data-before-rewrite-localise=\"https://www.windowscentral.com/artificial-intelligence/openai-chatgpt\">ChatGPT</a> and other OpenAI technologies to the cause.</p><p>In a <a data-analytics-id=\"inline-link\" href=\"https://x.com/sama/status/2027578652477821175?s=20\" target=\"_blank\" rel=\"nofollow\" data-url=\"https://x.com/sama/status/2027578652477821175?s=20\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">post on X</a>, Altman claimed that OpenAI's models would not be used for mass surveillance, but that claim was immediately <a data-analytics-id=\"inline-link\" href=\"https://x.com/sama/status/2027578652477821175?s=20\" target=\"_blank\" rel=\"nofollow\" data-url=\"https://x.com/sama/status/2027578652477821175?s=20\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">contradicted</a> by a U.S. government official, who said that OpenAI's models would be used for \"all lawful means.\" Mass surveillance of American citizens is lawful in \"some scenarios\" as part of the post-9/11 U.S. Patriot Act, which permits mass harvesting of communications meta data, even if some aspects of it have been curtailed in recent years.</p><p>Anthropic wanted control over the way its technologies would be used, as opposed to relying on the interpretation of laws and legal frame works that even now have been the subject of debate and lawsuits. Altman by comparison is happy to let the U.S. government decide how OpenAI's systems are deployed, which under certain segments of the Patriot Act could quite easily lead to the mass surveillance of U.S. citizens, directly or incidentally as part of provisions on surveiling foreign citizens (which, by the way, is completely legal under U.S. law.)</p><p>The move has sparked immediate backlash on ChatGPT and OpenAI communities online, across threads with thousands of upvotes on <a data-analytics-id=\"inline-link\" href=\"https://www.windowscentral.com/tag/reddit\" data-auto-tag-linker=\"true\" data-url=\"https://www.windowscentral.com/tag/reddit\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.windowscentral.com/tag/reddit\">reddit</a> of users claiming to be unsubscribing.</p><p>Unfortunately, there aren't many other AI companies willing to take a stance against mass surveillance or autonomous weapons. Google <a data-analytics-id=\"inline-link\" href=\"https://www.bbc.co.uk/news/articles/cy081nqx2zjo\" target=\"_blank\" rel=\"nofollow\" data-url=\"https://www.bbc.co.uk/news/articles/cy081nqx2zjo\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">removed</a> an explicit ban on the technology last year from its internal rules. Microsoft is cool with autonomous weapons too, as long as a human pulls the final trigger. Amazon has no prohibitions whatsoever besides vague \"responsible use\" language, and Meta hasn't been shy about courting Pentagon military contracts either. And we all know Palantir is totally for it.</p><p>The genie is out of the bottle, so to speak. ChatGPT is great at textual human mimicry but even the most cutting edge models often fail hilariously at even the most basic child-like logic puzzles.</p><p>Are you looking forward to a world where these hallucination-prone, <a data-analytics-id=\"inline-link\" href=\"https://www.anthropic.com/research/small-samples-poison\" target=\"_blank\" rel=\"nofollow\" data-url=\"https://www.anthropic.com/research/small-samples-poison\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">easily-manipulated</a> artificial intelligence models might eventually decide whether or not you're a threat to national security?</p><p>As long as Sam Altman and his buddies can stay rich, they don't seem to give much of a fuck about it — or you.</p><a href=\"https://www.reddit.com/r/windowscentral/\" data-url=\"https://www.reddit.com/r/windowscentral/\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"><figure data-bordeaux-image-check=\"\"></figure></a>",
      "contentLength": 3752,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47198973"
    },
    {
      "title": "Techno‑feudal elite are attempting to build a twenty‑first‑century fascist state",
      "url": "https://collapseofindustrialcivilization.com/2026/02/16/americas-oligarchic-techno-feudal-elite-are-attempting-to-build-a-twenty-first-century-fascist-state/",
      "date": 1772305063,
      "author": "measurablefunc",
      "guid": 49211,
      "unread": true,
      "content": "<p><strong>Introduction: Fascism at the End of Industrial Civilization</strong></p><p>This essay argues that the United States is drifting toward a distinctly twenty‑first‑century form of fascism driven not by mass parties in brownshirts, but by an oligarchic techno‑feudal elite. Neoliberal capitalism has hollowed out democratic institutions and concentrated power in a transnational “authoritarian international” of billionaires, security chiefs, and political fixers who monetize state power while shielding one another from accountability. At the same time, Big Tech platforms have become neo‑feudal estates that extract rent from our data and behavior, weaponize disinformation, and provide the surveillance backbone of an emerging global police state.</p><p>Drawing on the work of Robert Reich, William I. Robinson, Yanis Varoufakis, and others, alongside historian Heather Cox Richardson’s detailed account of Trump‑era patronage, whistleblower suppression, and DHS/ICE mega‑detention plans, the essay contends that America is rapidly constructing a system of concentration‑camp infrastructure and paramilitary policing designed to manage “surplus” populations and political dissent. Elite impunity, entrenched through national‑security exceptionalism, legal immunities, and revolving‑door careers, means that those directing lawless violence face virtually no consequences. Elections still happen, courts still sit, newspapers still publish, but substantive power is increasingly exercised by unelected oligarchs, tech lords, and security bureaucracies.</p><p>This authoritarian drift cannot be separated from the broader crisis of industrial civilization. Ecological overshoot, climate chaos, resource constraints, and structural economic stagnation have undermined the promise of endless growth on which liberal democracy once rested. Rather than using the remnants of industrial wealth to democratize a just transition, ruling elites are hardening borders, expanding carceral infrastructure, and building a security regime to contain “surplus” humanity in a world of shrinking energy and material throughput. America’s oligarchic techno‑feudal fascism is thus not an anomaly, but one plausible endgame of industrial civilization: a stratified order of gated enclaves above and camps and precarity below, designed to preserve elite power as the old industrial world comes apart.</p><p><strong>I. From liberal promise to oligarchic capture</strong></p><p>The American republic was founded on a promise that power would be divided, constrained, and answerable: a written constitution, separated branches, periodic elections, and a Bill of Rights that set bright lines even the sovereign could not cross. That promise was always compromised by slavery, settler colonialism, and gendered exclusion, but it retained real, if uneven, force as a normative horizon. What has shifted over the past half‑century is not simply the familiar gap between creed and practice, but the underlying structure of the system itself: the center of gravity has moved from public institutions toward a private oligarchy whose wealth and leverage allow it to function as a parallel sovereign.</p><p>The neoliberal turn of the 1970s and 1980s marked the decisive inflection point. Deregulation, financial liberalization, the crushing of organized labor, and the privatization of public goods redistributed power and income upward on a historic scale. Trade liberalization and capital mobility allowed corporations and investors to pit governments and workers against one another, extracting subsidies and tax concessions under the permanent threat of capital flight. At the same time, Supreme Court decisions eroded limits on political spending, redefining “speech” as something that could be purchased in unlimited quantities by those with the means.</p><p>The result, as Robert Reich notes, has been the consolidation of an American oligarchy that “paved the road to fascism” by ensuring that public policy reflects donor preferences far more consistently than popular majorities. In issue after issue, such as taxation, labor law, healthcare, and environmental regulation, there is a clear skew: the wealthy get what they want more often than not, while broadly popular but redistributive policies routinely die in committee or are gutted beyond recognition. This is not a conspiracy in the melodramatic sense; it is how the wiring of the system now works.</p><p>William Robinson’s analysis of “twenty‑first‑century fascism” sharpens the point. Global capitalism in its current form generates chronic crises: overproduction, under‑consumption, ecological breakdown, and a growing population that capital cannot profitably employ. Under such conditions, democratic politics becomes dangerous for elites, because electorates might choose structural reforms such as wealth taxes, public ownership, strong unions, and Green New Deal‑style transitions that would curb profits. Faced with this prospect, segments of transnational capital begin to see authoritarian solutions as rational: better to hollow out democracy, harden borders, and construct a global police state than to accept serious redistribution.</p><p>American politics in the early twenty‑first century fits this pattern with unsettling precision. A decaying infrastructure, stagnant wages, ballooning personal debt, militarized policing, and permanent war have produced widespread disillusionment. As faith in institutions erodes, public life is flooded with resentment and nihilism that can be redirected against scapegoats (immigrants, racial minorities, feminists, and queer and trans people) rather than against the oligarchic‑power‑complex that profits from the decay. It is in this vacuum that a figure like Donald Trump thrives: a billionaire demagogue able to channel anger away from the class that actually governs and toward those even more marginalized.</p><p>The decisive shift from plutocratic dysfunction to fascist danger occurs when oligarchs cease to see constitutional democracy as even instrumentally useful and instead invest in movements openly committed to minority rule. Koch‑style networks, Mercer‑funded operations, and Silicon Valley donors willing to underwrite hard‑right projects are not supporting democracy‑enhancing reforms; they are building the infrastructure for authoritarianism, from voter suppression to ideological media to data‑driven propaganda. The system that emerges is hybrid: elections still occur, courts still sit, newspapers still publish, but substantive power is increasingly concentrated in unelected hands.</p><p><strong>II. The “authoritarian international” and the shadow world of deals</strong></p><p>Historian Heather Cox Richardson’s <a href=\"https://www.youtube.com/watch?v=ajZudGu4exA\">recent analysis</a> captures a formation that much mainstream commentary still struggles to name: a transnational <strong>“authoritarian international”</strong> in which oligarchs, political operatives, royal families, security chiefs, and organized criminals cooperate to monetize state power while protecting one another from scrutiny. This is not a formal alliance; it is an overlapping ecology of relationships, exclusive vacations, investment vehicles, shell companies, foundations, and intelligence ties, through which information, favors, and money flow.</p><p>The key is that this network is structurally post‑ideological. As Robert Mueller warned in his 2011 description of an emerging “iron triangle” of politicians, businesspeople, and criminals, these actors are not primarily concerned with religion, nationality, or traditional ideology. They will work across confessional and national lines so long as the deals are lucrative and risk is manageably socialized onto others. Saudi royals invest alongside Western hedge funds; Russian oligarchs launder money through London property and American private equity; Israeli and Emirati firms collaborate with U.S. tech companies on surveillance products that are then sold worldwide.</p><p>Within this milieu, the formal distinction between public office and private interest blurs. Richardson’s analysis of Donald Trump’s abrupt reversal on the Gordie Howe International Bridge after a complaint by a billionaire competitor with ties to Jeffrey Epstein—reads less like the exercise of public policy judgment and more like feudal patronage: the sovereign intervenes to protect a favored lord’s toll road. Tiny shifts in regulatory posture or federal support can move billions of dollars; for those accustomed to having the president’s ear, such interventions are simply part of doing business.</p><p>The same logic governs foreign policy. The Trump‑Kushner axis exemplifies this fusion of public and private. When a whistleblower alleges that the Director of National Intelligence suppressed an intercept involving foreign officials discussing Jared Kushner and sensitive topics like Iran, and when the complaint is then choked off with aggressive redaction and executive privilege, we see <strong>the machinery of secrecy misused not to protect the national interest but to shield a member of the family‑cum‑business empire at the center of power</strong>. It is as if the state has become a family office with nuclear weapons.</p><p>Josh Marshall’s phrase <strong>“authoritarian international”</strong> is apt because it names both the class composition and the political function of this network. The same names recur across far‑right projects: donors and strategists who back nationalist parties in Europe, ultras in Latin America, Modi’s BJP in India, and the MAGA movement in the United States. Their interests are not identical, but they overlap around a shared agenda: weakening labor and environmental protections, undermining independent media and courts, militarizing borders, and securing immunity for themselves and their peers.</p><p>This world is lubricated by blackmail and mutually assured destruction. As Richardson notes, players often seem to hold compromising material on one another, whether in the form of documented sexual abuse, financial crime, or war crimes. This shared vulnerability paradoxically stabilizes the network: as long as everyone has something on everyone else, defection is dangerous, and a predatory equilibrium holds. From the standpoint of democratic publics, however, this stability is catastrophic, because it means that scandal—once a mechanism for enforcing norms—loses much of its power. When “everyone is dirty,” no one can be clean enough to prosecute the others without risking exposure.</p><p><strong>III. Techno‑feudal aristocracy and the colonization of everyday life</strong></p><p>Layered atop this transnational oligarchy is the digital order that Varoufakis and others describe as techno‑feudalism: a regime in which a handful of platforms function like neo‑feudal estates, extracting rent from their “serfs” (users, gig workers, content creators) rather than competing in open markets. This shift is more than metaphor. In classical capitalism, firms profited primarily by producing goods or services and selling them on markets where competitors could, in principle, undercut them. In the platform order, gatekeepers profit by controlling access to the marketplace itself, imposing opaque terms on those who must use their infrastructure to communicate, work, or even find housing.</p><p>This can be seen across sectors:</p><ul><li><p>Social media platforms own the digital public square. They monetize attention by selling advertisers access to finely sliced demographic and psychographic segments, while their recommendation algorithms optimize for engagement, often by privileging outrage and fear.</p></li><li><p>Ride‑hailing and delivery apps control the interface between customers and labor, setting prices unilaterally and disciplining workers through ratings, algorithmic management, and the ever‑present threat of “deactivation.”</p></li><li><p>Cloud providers and app stores gatekeep access to the basic infrastructure upon which countless smaller firms depend, taking a cut of transactions and reserving the right to change terms or remove competitors from the ecosystem entirely.</p></li></ul><p>In each case, the platform is less a company among companies and more a landlord among tenants, collecting tolls for the right to exist within its domain. Users produce the very capital stock, data, content, behavioral profiles, that platforms own and monetize, yet they have little say over how this material is used or how the digital environment is structured. The asymmetry of power is profound: the lords can alter the code of the world; the serfs can, at best, adjust their behavior to avoid algorithmic invisibility or sanction.</p><p>For authoritarian politics, this structure is a gift. First, platforms have become the primary vectors of disinformation and propaganda. Cambridge Analytica’s work for Trump in 2016, funded by billionaires like the Mercers, was an early prototype: harvest data, micro‑target individuals with tailored&nbsp;messaging, and flood their feeds with narratives designed to activate fear and resentment. Since then, the techniques have grown more sophisticated, and far‑right movements worldwide have learned to weaponize meme culture, conspiracy theories, and “shitposting” as recruitment tools.</p><p>Second, the same infrastructures that enable targeted advertising enable granular surveillance. Location data, social graphs, search histories, and facial‑recognition databases provide an unprecedented toolkit for monitoring and disciplining populations. In the hands of a regime sliding toward fascism, these tools can be turned against dissidents with terrifying efficiency: geofencing protests to identify attendees, scraping social media to build dossiers, using AI to flag “pre‑criminal” behavior. The emerging “global police state” that Robinson describes depends heavily on such techno‑feudal capacities.</p><p>Third, the digital order corrodes the very preconditions for democratic deliberation. Information overload, filter bubbles, and algorithmic amplification of sensational content produce a public sphere saturated with noise. Under these conditions, truth becomes just another aesthetic, and the distinction between fact and fiction collapses into vibes. This is the post‑modern nihilism you name: a sense that nothing is stable enough to believe in, that everything is spin. Fascist movements do not seek to resolve this condition; they weaponize it, insisting that only the Leader and his trusted media tell the real truth, while everything else is a hostile lie.</p><p>Finally, the techno‑feudal aristocracy’s material interests align with authoritarianism. Privacy regulations, antitrust enforcement, data localization rules, and strong labor rights all threaten platform profits. Democratic movements that demand such reforms are therefore adversaries. Conversely, strongman leaders who promise deregulation, tax breaks, and law‑and‑order crackdowns, even if they occasionally threaten specific firms, are often acceptable partners. The result is a convergence: oligarchs of data and oligarchs of oil, real estate, and finance finding common cause in an order that disciplines the many and exempts the few.</p><p><strong>IV. Elite impunity and the machinery of lawlessness</strong></p><p>Authoritarianism is not only about who holds power; it is about who is answerable for wrongdoing. A system where elites can violate laws with impunity while ordinary people are punished harshly for minor infractions is already halfway to fascism, whatever labels it wears. The United States has, over recent decades, constructed precisely such a system.</p><p>The Arab Center’s “Machinery of Impunity” report details how, in areas ranging from mass surveillance to foreign wars to domestic policing, senior officials who authorize illegal acts almost never face criminal consequences. Edward Snowden’s revelations exposed systemic violations of privacy and civil liberties, yet it was the whistleblower who faced prosecution and exile, not the architects of the programs. Torture during the “war on terror” was acknowledged, even documented in official reports, but those who designed and approved the torture regime kept their law licenses, academic posts, and media gigs. Lethal strikes on small boats in the Caribbean and Pacific, justified by secret intelligence and shielded by classified legal opinions, have killed dozens with no public evidence that the targets posed imminent threats.</p><p>This pattern is not an aberration but a feature. As a Penn State law review article <a href=\"https://insight.dickinsonlaw.psu.edu/cgi/viewcontent.cgi?article=1144&amp;context=jlia\">notes</a>, the U.S. legal system builds in multiple layers of protection for high officials: sovereign immunity, state secrets privilege, narrow standing rules, and prosecutorial discretion all combine to make it extraordinarily difficult to hold the powerful to account. Violations of the Hatch Act, campaign‑finance laws, or ethics rules are often treated as technicalities, and when reports do document unlawful behavior, as in the case of Mike Pompeo’s partisan abuse of his diplomatic office, there are “no consequences” beyond mild censure. Jamelle Bouie’s recent video essay for the New York Times drives the point home: America is “bad at accountability” because institutions have been designed and interpreted to favor elite impunity.</p><p>Richardson shows how this culture functions inside the national‑security state. A whistleblower complaint alleging that the Director of National Intelligence suppressed an intelligence intercept involving Jared Kushner and foreign officials was not allowed to run its course. Instead, it was bottled up, then transmitted to congressional overseers in a highly redacted form, with executive privilege invoked to shield the president’s involvement. The same mechanisms that insulate covert operations abroad from democratic oversight are deployed to protect domestic political allies from scrutiny.</p><p>Immigration enforcement offers another window. The Arab Center notes that ICE raids, family separation, and other abuses “escalated under the current Trump administration into highly visible kidnappings, abuse, and deportations” with little accountability for senior officials. The National Immigrant Justice Center documents a detention system where 90 percent of detainees are held in for‑profit facilities, where medical neglect, punitive solitary confinement, and preventable deaths are common, yet contracts are renewed and expanded. A culture of impunity allows agents and managers to treat rights violations not as career‑ending scandals but as acceptable collateral damage.</p><p>Latin American scholars of impunity warn that such selective enforcement produces a “quiet crisis of accountability” in which the rule of law is hollowed out from within. Laws remain on the books, but their application is skewed: harsh on the poor and marginalized, permissive toward the powerful. Over time, this normalizes the idea that some people are above the law, while others exist primarily as objects of control. When a polity internalizes this hierarchy, fascism no longer needs to arrive in jackboots; it is already present in the daily operations of the justice system.</p><p>The danger, as the Arab Center emphasizes, is that the costs of impunity “come home to roost.” Powers originally justified as necessary to fight terrorism or foreign enemies migrate back into domestic politics. Surveillance tools built for foreign intelligence monitoring are turned on activists and journalists; militarized police tactics perfected in occupied territories are imported into American streets. A population taught to accept lawless violence against outsiders (migrants, foreigners, enemy populations) is gradually conditioned to accept similar violence against internal opponents.</p><p><strong>V. Concentration camps, paramilitary policing, and ritualized predatory violence</strong></p><p>In this context of oligarchic capture, techno‑feudal control, and elite impunity, the rapid expansion of detention infrastructure and the deployment of paramilitary “federal agents” across the interior United States are not aberrations; they are central pillars of an emergent fascist order.</p><p>Richardson’s insistence on calling these facilities concentration camps is analytically exact. A concentration camp, in the historical sense, is not necessarily a death camp; it is a place where a state concentrates populations it considers threats or burdens, subjecting them to confinement, disease, abuse, and often death through neglect rather than industrialized extermination. By that definition, the sprawling network of ICE and Border Patrol detention centers, where people are warehoused for months to years, often in horrific conditions, qualifies.</p><p>New reporting details how this system is poised to scale up dramatically. An internal ICE memo, recently surfaced, outlines a $38 billion plan for a “new detention center model” that would, in one year, create capacity for roughly 92,600 people by purchasing eight “mega centers,” 16 processing centers, and 10 additional facilities. The largest of these warehouses would hold between 7,000 and 10,000 people each for average stays of about 60 days, more than double the size of the largest current federal prison. Separate reporting has mapped at least 23 industrial warehouses being surveyed for conversion into mass detention camps, with leases already secured at several sites.</p><p>Investigations by Amnesty International and others into prototype facilities have found detainees shackled in overcrowded cages, underfed, forced to use open‑air toilets that flood, and routinely denied medical care. Sexual assault and extortion by guards, negligent deaths, and at least one homicide have been documented. These are not accidents; they are predictable outcomes of a profit‑driven system where private contractors are paid per bed and oversight is weak, and of a political culture that dehumanizes migrants as “invaders” or “animals.”</p><p>Richardson highlights another crucial dimension: the way DHS has been retooled to project this violence into the interior as a form of political terror. Agents from ICE and Border Patrol, subdivisions of a relatively new department lacking the institutional restraints of the military, have been deployed in cities far from any border, often in unmarked vehicles, wearing masks and lacking visible identification. Secret legal memos under Trump gutted the traditional requirement of a judicial warrant for entering homes, replacing it with internal sign‑off by another DHS official, a direct violation of the Fourth Amendment’s protection against unreasonable searches and seizures.</p><p>This matters both instrumentally and symbolically. Instrumentally, it enables efficient mass raids and “snatch and grab” operations that bypass local law‑enforcement norms and judicial oversight. Symbolically, it communicates that the state reserves the right to operate as a lawless force, unconstrained by the very constitution it claims to defend. When masked, unidentified agents can seize people off the streets, shove them into unmarked vans, and deposit them in processing centers without due process, the aesthetic of fascism…thugs in the night…becomes reality.</p><p>Richardson rightly connects this to the post‑Reconstruction South, where paramilitary groups like the Ku Klux Klan, often tolerated or quietly aided by local officials, used terror to destroy a biracial democracy that had briefly flourished. Today’s difference is that communications technology allows rapid mobilization of witnesses and counter‑protesters: people can rush to the scene when agents arrive, document abuses on smartphones, and coordinate legal support. Yet even this can be folded into the logic of spectacle. The images of militarized agents confronting crowds under the glow of streetlights and police floodlamps serve as warnings: this is what happens when you resist.</p><p>The planned network of processing centers and mega‑warehouses adds another layer of menace. As Richardson points out, if the stated goal is deportation, there is no clear need for facilities capable of imprisoning tens of thousands for months. Part of the answer is coercive leverage: detained people are easier to pressure into abandoning asylum claims and accepting removal, especially when they are told, day after day, that they could walk free if they “just sign.” But the architecture also anticipates a future in which new categories of internal enemies, protesters, “Antifa,” “domestic extremists,” can be funneled into the same carceral estate once migrant flows diminish or political needs change.</p><p>Economically, the camps generate their own constituency. ICE and DHS tout job creation numbers to local officials, promising hundreds of stable, often union‑free positions in communities hollowed out by deindustrialization. Private prison firms and construction companies see lucrative contracts; investors see secure returns backed by federal guarantees. A web of stakeholders thus becomes materially invested in the continuation and expansion of mass detention. <strong>This is techno‑feudalism in concrete and razor wire: a carceral estate in which bodies are the rent‑producing asset.</strong></p><p>Once such an estate exists, its logic tends to spread. Border‑style tactics migrate into ordinary policing; surveillance tools trialed on migrants are turned on domestic movements; legal doctrines crafted to justify raids and warrantless searches in the name of immigration control seep into other domains. The fascist gradient steepens: more people find themselves at risk of sudden disappearance into a system where rights are theoretical and violence is routine.</p>",
      "contentLength": 25529,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47198912"
    },
    {
      "title": "Verified Spec-Driven Development (VSDD)",
      "url": "https://gist.github.com/dollspace-gay/d8d3bc3ecf4188df049d7a4726bb2a00",
      "date": 1772297934,
      "author": "todsacerdoti",
      "guid": 49244,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47197595"
    },
    {
      "title": "The whole thing was a scam",
      "url": "https://garymarcus.substack.com/p/the-whole-thing-was-scam",
      "date": 1772297509,
      "author": "guilamu",
      "guid": 49194,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47197505"
    },
    {
      "title": "Obsidian Sync now has a headless client",
      "url": "https://help.obsidian.md/sync/headless",
      "date": 1772296313,
      "author": "adilmoujahid",
      "guid": 49183,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47197267"
    },
    {
      "title": "Cognitive Debt: When Velocity Exceeds Comprehension",
      "url": "https://www.rockoder.com/beyondthecode/cognitive-debt-when-velocity-exceeds-comprehension/",
      "date": 1772293150,
      "author": "pagade",
      "guid": 49182,
      "unread": true,
      "content": "<img src=\"https://www.rockoder.com/images/beyondthecode/cognitive-debt-hero.svg\" alt=\"\" loading=\"eager\"><p>The engineer shipped seven features in a single sprint. DORA metrics looked immaculate. The promotion packet practically wrote itself.</p><p>Six months later, an architectural change required modifying those features. No one on the team could explain why certain components existed or how they interacted. The engineer who built them stared at her own code like a stranger’s.</p><p><strong>Code has become cheaper to produce than to perceive.</strong></p><p>When an engineer writes code manually, two parallel processes occur. The first is production: characters appear in files, tests get written, systems change. The second is absorption: mental models form, edge cases become intuitive, architectural relationships solidify into understanding. These processes are coupled. The act of typing forces engagement. The friction of implementation creates space for reasoning.</p><p>AI-assisted development decouples these processes. A prompt generates hundreds of lines in seconds. The engineer reviews, adjusts, iterates. Output accelerates. But absorption cannot accelerate proportionally. The cognitive work of truly understanding what was built, why it was built that way, and how it relates to everything else remains bounded by human processing speed.</p><p><strong>This gap between output velocity and comprehension velocity is cognitive debt.</strong></p><p>Unlike technical debt, which surfaces through system failures or maintenance costs, <strong>cognitive debt remains invisible to velocity metrics</strong>. The code works. The tests pass. The features ship. The deficit exists only in the minds of the engineers who built the system, manifesting as uncertainty about their own work.</p><p>The debt is not truly invisible. It eventually appears in reliability metrics: Mean Time to Recovery stretches longer, Change Failure Rate creeps upward. But these are lagging indicators, separated by months from the velocity metrics that drive quarterly decisions. By the time MTTR signals a problem, the comprehension deficit has already compounded.</p><h2>What Organizations Actually Measure</h2><p>Engineering performance systems evolved to measure observable outputs. Story points completed. Features shipped. Commits merged. Review turnaround time. These metrics emerged from an era when output and comprehension were tightly coupled, when shipping something implied understanding something.</p><p>The metrics never measured comprehension directly because comprehension was assumed. An engineer who shipped a feature was presumed to understand that feature. <strong>The presumption held because the production process itself forced understanding.</strong></p><p><strong>That presumption no longer holds.</strong> An engineer can now ship features while maintaining only surface familiarity with their implementation. The features work. The metrics register success. The organizational knowledge that would traditionally accumulate alongside those features simply does not form at the same rate.</p><p>Performance calibration committees see velocity improvements. They do not see comprehension deficits. They cannot, because no artifact of the organizational measurement system captures that dimension.</p><p>The discussion of cognitive debt typically focuses on the engineer who generates code. The more acute problem sits with the engineer who reviews it.</p><p>Code review evolved as a quality gate. A senior engineer examines a junior engineer’s work, catching errors, suggesting improvements, transferring knowledge. The rate-limiting factor was always the junior engineer’s output speed. Senior engineers could review faster than juniors could produce.</p><p>AI-assisted development inverts this relationship. <strong>A junior engineer can now generate code faster than a senior engineer can critically audit it.</strong> The volume of generated code exceeds the bandwidth available for deep review. Something has to give, and typically it is review depth.</p><p>The reviewer faces an impossible choice. Maintain previous review standards and become a bottleneck that negates the velocity gains AI provides. Or approve code at the rate it arrives and hope the tests catch what the review missed. Most choose the latter, often unconsciously, because organizational pressure favors throughput.</p><p>This is where cognitive debt compounds fastest. The author’s comprehension deficit might be recoverable through later engagement with the code. The reviewer’s comprehension deficit propagates: they approved code they do not fully understand, which now carries implicit endorsement. <strong>The organizational assumption that reviewed code is understood code no longer holds.</strong></p><p>Engineers working extensively with AI tools report a specific form of exhaustion that differs from traditional burnout. Traditional burnout emerges from sustained cognitive load, from having too much to hold in mind while solving complex problems. The new pattern emerges from something closer to cognitive disconnection.</p><p>The work happens quickly. Progress is visible. But the engineer experiences a persistent sense of not quite grasping their own output. They can execute, but explanation requires reconstruction. They can modify, but prediction becomes unreliable. The system they built feels slightly foreign even as it functions correctly.</p><p>This creates a distinctive psychological state: <strong>high output combined with low confidence</strong>. Engineers produce more while feeling less certain about what they have produced. In organizations that stack-rank based on visible output, this creates pressure to continue generating despite the growing uncertainty.</p><p>The engineer who pauses to deeply understand what they built falls behind in velocity metrics. The engineer who prioritizes throughput over comprehension meets their quarterly objectives. <strong>The incentive structure selects for the behavior that accelerates cognitive debt accumulation.</strong></p><h2>When Organizational Memory Fails</h2><p>Knowledge in engineering organizations exists in two forms. The first is explicit: documentation, design documents, recorded decisions. The second is tacit: understanding held in the minds of people who built and maintained systems over time. Tacit knowledge cannot be fully externalized because much of it exists as intuition, pattern recognition, and contextual judgment that formed through direct engagement with the work.</p><p>When the people who built a system leave or rotate to new projects, tacit knowledge walks out with them. Organizations traditionally replenished this knowledge through the normal process of engineering work. New engineers building on existing systems developed their own tacit understanding through the friction of implementation.</p><p>AI-assisted development potentially short-circuits this replenishment mechanism. If new engineers can generate working modifications without developing deep comprehension, they never form the tacit knowledge that would traditionally accumulate. <strong>The organization loses knowledge not just through attrition but through insufficient formation.</strong></p><p>This creates a delayed failure mode. The system continues to function. New features continue to ship. But the reservoir of people who truly understand the system gradually depletes. When circumstances eventually require that understanding, when something breaks in an unexpected way or requirements change in a way that demands architectural reasoning, the organization discovers the deficit.</p><p>Three failure modes emerge as cognitive debt accumulates.</p><p>The first involves the reversal of a normally reliable heuristic. Engineers typically trust code that has been in production for years. If it survived that long, it probably works. The longer code exists without causing problems, the more confidence it earns. AI-generated code inverts this pattern. <strong>The longer it remains untouched, the more dangerous it becomes</strong>, because the context window of the humans around it has closed completely. Code that was barely understood when written becomes entirely opaque after the people who wrote it have moved on.</p><blockquote>They are debugging a black box written by a black box.</blockquote><p>The second failure mode surfaces during incidents. An alert fires at 3:00 AM. The on-call engineer opens a system they did not build, generated by tools they did not supervise, documented in ways that assume familiarity they do not possess. <strong>They are debugging a black box written by a black box.</strong> What would have been a ten-minute fix when someone understood the system becomes a four-hour forensic investigation when no one does. Multiply this across enough incidents and the aggregate cost exceeds whatever velocity gains the AI-assisted development provided.</p><blockquote>The organization is effectively trading its pipeline of future Staff Engineers for this quarter's feature delivery.</blockquote><p>The third failure mode operates on a longer timescale. Junior engineers who rely primarily on AI-assisted development never develop the intuition that comes from manual implementation. They ship features without forming the scar tissue that informs architectural judgment. The organization is effectively trading its pipeline of future Staff Engineers for this quarter’s feature delivery. The cost does not appear in current headcount models because the people who would have become senior architects five years from now are not yet absent. </p><p>From the perspective of engineering leadership, AI-assisted development presents as productivity gain. Teams ship faster. Roadmaps compress. Headcount discussions become more favorable. These are the observable signals that propagate upward through organizational reporting structures.</p><p>The cognitive debt accumulating in those teams does not present as a signal. There is no metric for “engineers who can explain their own code without re-reading it.” There is no dashboard for “organizational comprehension depth.” The concept does not fit into quarterly business review formats or headcount justification narratives.</p><p>Directors make decisions based on observable signals. When those signals uniformly indicate success, the decision to double down on the approach that produced those signals is rational within the information environment available to leadership. <strong>The decision is not wrong given the data. The data is incomplete.</strong></p><p>The cognitive debt framing does not apply uniformly across all engineering work. Some tasks genuinely are mechanical. Some codebases genuinely benefit from rapid iteration without deep architectural understanding. Some features genuinely do not require the level of comprehension that would traditionally form through manual implementation.</p><p>The model also assumes that comprehension was previously forming at adequate rates. This assumption may be generous. Engineers have always varied in how deeply they understood their own work. The distribution may simply be shifting rather than a new phenomenon emerging.</p><p>Additionally, tooling and documentation practices may evolve to partially close the comprehension gap. If organizations develop methods for capturing and transmitting the understanding that AI-assisted development fails to form organically, the debt may prove manageable rather than accumulative.</p><blockquote>The system is optimizing correctly for what it measures. What it measures no longer captures what matters.</blockquote><p>The fundamental challenge is that <strong>organizations cannot optimize for what they cannot measure</strong>. Velocity is measurable. Comprehension is not, or at least not through any mechanism that currently feeds into performance evaluation, promotion decisions, or headcount planning.</p><p>Until comprehension becomes legible to organizational decision-making systems, the incentive structure will continue to favor velocity. Engineers who prioritize understanding over output will appear less productive than peers who prioritize output over understanding. Performance calibration will reward the behavior that accumulates debt faster.</p><p>This is not a failure of individual managers or engineers. It is a measurement system designed for an era when production and comprehension were coupled, operating in an era when that coupling no longer holds. <strong>The system is optimizing correctly for what it measures. What it measures no longer captures what matters.</strong></p><p>The gap will eventually manifest. Whether through maintenance costs that exceed projections, through incidents that require understanding no one possesses, or through new requirements that expose the brittleness of systems built without deep comprehension. The timing and form of manifestation remain uncertain. The underlying dynamic does not.</p>",
      "contentLength": 12316,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47196582"
    },
    {
      "title": "Addressing Antigravity Bans and Reinstating Access",
      "url": "https://github.com/google-gemini/gemini-cli/discussions/20632",
      "date": 1772286613,
      "author": "RyanShook",
      "guid": 49181,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47195371"
    },
    {
      "title": "OpenAI fires an employee for prediction market insider trading",
      "url": "https://www.wired.com/story/openai-fires-employee-insider-trading-polymarket-kalshi/",
      "date": 1772286380,
      "author": "bookofjoe",
      "guid": 49140,
      "unread": true,
      "content": "<p> an employee following an investigation into their activity on <a href=\"https://www.wired.com/story/the-political-war-over-prediction-markets-is-just-getting-started/\">prediction market platforms</a> including Polymarket, WIRED has learned.</p><p>OpenAI CEO of Applications, <a href=\"https://www.wired.com/story/fidji-simo-is-openais-other-ceo-and-she-swears-shell-make-chatgpt-profitable/#intcid=_wired-verso-hp-trending_ea249350-52ab-42d0-9cfe-e91d580f41e8_popular4-2\">Fidji Simo</a>, disclosed the termination in an internal message to employees earlier this year. The employee, she said, “used confidential OpenAI information in connection with external prediction markets (e.g. Polymarket).”</p><p>“Our policies prohibit employees from using confidential OpenAI information for personal gain, including in prediction markets,” says spokesperson Kayla Wood. OpenAI has not revealed the name of the employee or the specifics of their trades.</p><p>Evidence suggests that this was not an isolated event. Polymarket runs on the Polygon blockchain network, so its trading ledger is pseudonymous but traceable. According to an analysis by the financial data platform Unusual Whales, there have been clusters of activities, which the service flagged as suspicious, around OpenAI-themed events since March 2023.</p><p>Unusual Whales flagged 77 positions in 60 wallet addresses as suspected insider trades, looking at the age of the account, trading history, and significance of investment, among other factors. Suspicious trades hinged on the release dates of products like <a href=\"https://www.wired.com/story/openai-launches-sora-2-tiktok-like-app/\">Sora</a>, <a href=\"https://www.wired.com/story/gpt-5-coding-review-software-engineering/\">GPT-5</a>, and the ChatGPT Browser, as well as CEO Sam Altman’s employment status. In November 2023, two days after Altman was dramatically ousted from the company, a new wallet placed a significant bet that he would return, netting over $16,000 in profits. The account never placed another bet.</p><p>The behavior fits into patterns typical of insider trades. “The tell is the clustering. In the 40 hours before OpenAI launched its browser, 13 brand-new wallets with zero trading history appeared on the site for the first time to collectively bet $309,486 on the right outcome,” says Unusual Whales CEO Matt Saincome. “When you see that many fresh wallets making the same bet at the same time, it raises a real question about whether the secret is getting out.”</p><p>Prediction markets have exploded in popularity in recent years. These platforms allow customers to buy “event contracts” on the outcomes of future events ranging from the winner of the Super Bowl to the daily price of Bitcoin to whether the United States will go to war with Iran. There are a wide array of markets tied to events in the technology sector; you can trade on what Nvidia’s quarterly earnings will be, or when Tesla will launch a new car, or which AI companies will IPO in 2026.</p><p>As the platforms have grown, so have concerns that they allow traders to profit from insider knowledge. “This prediction market world makes the Wild West look tame in comparison,” says Jeff Edelstein, a senior analyst at the betting news site InGame. “If there's a market that exists where the answer is known, somebody's going to trade on it.”</p><p>Earlier this week, Kalshi <a href=\"https://www.wired.com/story/kalshi-insider-trading-california-politician-and-youtuber/\">announced</a> that it had reported several suspicious insider trading cases to the Commodity Futures Trading Commission, the government agency overseeing these markets. In one instance, an employee of the popular YouTuber Mr. Beast was suspended for two years and fined $20,000 for making trades related to the streamer’s activities; in another, the far-right political candidate Kyle Langford was banned from the platform for making a trade on his own campaign. The company also announced a number of initiatives to prevent insider trading and market manipulation.</p><p>While Kalshi has heavily promoted its crackdown on insider trading, Polymarket has stayed silent on the matter. The company did not return requests for comments.</p><p>In the past, major trades on technology-themed markets have sparked speculation that there are Big Tech employees profiting by using their insider knowledge to gain an edge. One notorious example is the so-called “Google whale,” a <a data-offer-url=\"https://polymarket.com/profile/%400xafEe\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://polymarket.com/profile/%400xafEe&quot;}\" href=\"https://polymarket.com/profile/%400xafEe\" rel=\"nofollow noopener\" target=\"_blank\">pseudonymous account</a> on Polymarket that made over $1 million trading on Google-related events, including a market on who the most-searched person of the year would be in 2025. (It was the singer D4vd, who is best known for his connection to an ongoing murder investigation after a young fan’s remains were found in a vehicle registered to him.)</p>",
      "contentLength": 4190,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47195317"
    },
    {
      "title": "Show HN: Now I Get It – Translate scientific papers into interactive webpages",
      "url": "https://nowigetit.us/",
      "date": 1772285376,
      "author": "jbdamask",
      "guid": 49180,
      "unread": true,
      "content": "<p>Drop your PDF here, or </p><p>Works best with files under 10 MB</p>",
      "contentLength": 56,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47195123"
    },
    {
      "title": "What AI coding costs you",
      "url": "https://tomwojcik.com/posts/2026-02-15/finding-the-right-amount-of-ai/",
      "date": 1772283903,
      "author": "tomwojcik",
      "guid": 49139,
      "unread": true,
      "content": "<p>Every developer I know uses AI for coding now. The productivity gains are real, but there are costs that don’t show up on any dashboard.</p><p>Imagine a spectrum. On the far left are humans typing on the keyboard, seeing the code in the IDE. On the far right: AGI. It implements everything on its own. Cheaply, flawlessly, better than any human, and no human overseer is required. Somewhere between those two extremes there’s you, using AI, today. That threshold moves to the right every week as models improve, tools mature, and workflows get refined.</p><blockquote><p>Which is higher risk, using AI too much, or using AI too little?</p></blockquote><p>and it made me think about LLMs for coding differently, especially after reading what other devs share on AI adoption in different workplaces. You can be wrong in both directions, but is the desired amount of AI usage at work changing as the models improve?</p><p>Not long ago the first AI coding tools like Cursor (2023) or Copilot (2022) emerged. They were able to quickly index the codebase using RAG, so they had the local context. They had all the knowledge of the models powering them, so they had an external knowledge of the Internet as well. Googling and browsing StackOverflow wasn’t needed anymore. Cursor gave the users a custom IDE with built in AI powered autocomplete and other baked-in AI tools, like chat, to make the experience coherent.</p><p>Then came the agent promise. MCPs, autonomous workflows, <a href=\"https://michaelxbloch.substack.com/p/no-coding-before-10am\">articles about agents running overnight</a> started to pop up left and right. It was a different use of AI than Cursor. <strong>It was no longer an AI-assisted human coding, but a human-assisted AI coding.</strong></p><p>Many devs tried it and got burned. Agents made tons of small mistakes. The AI-first process required a complete paradigm shift in how devs think about coding, in order to achieve great results. Also, agents often got stuck in loops, hallucinate dependencies, and produced code that looks almost right but isn’t. You needed to learn about a completely new tech, fueled by FOMO. And this new shiny tool never got it 100% right on the first try.</p><p>Software used to be deterministic. You controlled it with if/else branches, explicit state machines, clear logic. The new reality is controlling the development process with prompts, system instructions, and CLAUDE.md files, and hope the model produces the output you expect.</p><blockquote><p>An engineer at Spotify on their morning commute from Slack on their cell phone can tell Claude to fix a bug or add a new feature to the iOS app. And once Claude finishes that work, the engineer then gets a new version of the app, pushed to them on Slack on their phone, so that he can then merge it to production, all before they even arrive at the office.”</p></blockquote><p>I hope they at least review the code before merging.</p><p>The next stage is an (almost) full automation. That’s what many execs want and try to achieve. It’s a capitalistic wet dream, a worker that never sleeps, never gets tired, always wants to work, is infinitely productive. But Geoffrey Hinton predicted in 2016 that deep learning would outperform radiologists at image analysis within five years. Anthropic’s CEO predicted AI would write 90% of code within three to six months of March 2025. None of this happened as predicted. The trajectory is real, but the timeline keeps slipping.</p><p>In 2012, neuroscientist Manfred Spitzer published <a href=\"https://en.wikipedia.org/wiki/Digital_Dementia_(book)\">Digital Dementia</a>, arguing that when we outsource mental tasks to digital devices, the brain pathways responsible for those tasks atrophy. Use it or lose it. Not all of this is proven scientifically, but neuroplasticity research shows the brain strengthens pathways that get used and weakens ones that don’t. The core principle of the book is that the cognitive skills that you stop practicing will decline.</p><p>Margaret-Anne Storey, a software engineering researcher, recently gave this a more precise name: <a href=\"https://margaretstorey.com/blog/2026/02/09/cognitive-debt/\">cognitive debt</a>. Technical debt lives in the code. Cognitive debt lives in developers’ heads. It’s the accumulated loss of understanding that happens when you build fast without comprehending what you built. She grounds it in Peter Naur’s 1985 theory that a program is a theory existing in developers’ minds, capturing what it does, how intentions map to implementation, and how it can evolve. When that theory fragments, the system becomes a black box.</p><p>Apply this directly to fully agentic coding. If you stop writing code and only review AI output, your ability to reason about code atrophies. Slowly, invisibly, but inevitably. You can’t deeply review what you can no longer deeply understand.</p><p>This isn’t just theory. A <a href=\"https://arxiv.org/abs/2601.20245\">2026 randomized study by Shen and Tamkin</a> tested this directly: 52 professional developers learning a new async library were split into AI-assisted and unassisted groups. The AI group scored 17% lower on conceptual understanding, debugging, and code reading. The largest gap was in debugging, the exact skill you need to catch what AI gets wrong. One hour of passive AI-assisted work produced measurable skill erosion.</p><p>The insidious part is that you don’t notice the decline because the tool compensates for it. You feel productive. The PRs are shipping. Mihaly Csikszentmihalyi’s research on <a href=\"https://en.wikipedia.org/wiki/Flow_(psychology)\">flow</a> showed that the state of flow depends on a balance between challenge and skill. Your mind needs to be stretched just enough. Real flow produces growth. Rachel Thomas called what AI-assisted work produces <a href=\"https://www.fast.ai/posts/2026-01-28-dark-flow/\">“dark flow”</a>, a term borrowed from gambling research, describing the trance-like state slot machines are designed to induce. You feel absorbed, but the challenge-skill balance is gone because the AI handles the challenge. It feels like the flow state of deep work, but the feedback loop is broken. You’re not getting better, you’re getting dependent.</p><p>There’s this observation that keeps coming up in HN comments: if the AI writes all the code and you only review it, where does the skill to review come from? You can’t have one without the other. You don’t learn to recognize good code by reading about it in a textbook, or a PR. You learn by writing bad code, getting it torn apart, and building intuition through years of practice.</p><p>This creates what I’d call the review paradox: the more AI writes, the less qualified humans become to review what it wrote. The Shen-Tamkin study puts numbers on this. Developers who fully delegated to AI finished tasks fastest but scored worst on evaluations. The novices who benefit most from AI productivity are exactly the ones who need debugging skills to supervise it, and AI erodes those skills first.</p><p>Storey’s proposed fix is simple: “require humans to understand each AI-generated change before deployment.” That’s the right answer. It’s also the one that gets skipped first when velocity is the metric.</p><p>This goes deeper than individual skill decay. We used to have juniors, mids, seniors, staff engineers, architects. It was a pipeline where each level built on years of hands-on struggle. A junior spends years writing code that is rejected during the code review not because they were not careful, but didn’t know better. It’s how you build the judgment that separates someone who can write a function from someone who can architect a system. You can’t become a senior overnight.</p><p>Unless you use AI, of course. Now, a junior with Claude Code (Opus 4.5+) delivers PRs that look like senior engineer work. And overall that’s a good thing, I think. But does it mean that the senior hat fits everyone now? From day one? But the head underneath hasn’t changed. That junior doesn’t know  that architecture was chosen. From my experience, sometimes CC misses a new DB transaction where it’s needed. Sometimes it creates a lock on a resource, that shouldn’t be locked, due to number of reasons. I can defend my decisions and I enjoy when my code is challenged, when reviewers disagree, and we have a discussion. What will a junior do? Ask Claude.</p><p>It’s a two-sided collapse. Seniors who stop writing code and only review AI output lose their own depth. Juniors who skip the struggle never build it. Organizations are spending senior time every day on reviews while simultaneously breaking the mechanisms that create it. The pipeline that produced senior engineers, writing bad code, getting bad code reviewed, building intuition through failure, is being bypassed entirely. Nobody’s talking about what happens when that pipeline runs dry.</p><h2>What C-Levels Got Right and Wrong</h2><p>The problem is that predictions come from people selling AI or trying to prop the stock with AI hype. They have every incentive to accelerate adoption and zero accountability when the timelines slip, which, historically, they always do. And “50% of code characters” at Google, a company that has built its own models, tooling, and infrastructure from scratch, says very little about what your team can achieve with off-the-shelf agents next Monday.</p><p>AI adoption is not a switch to flip, rather a skill to calibrate. It’s not as simple as mandating specific tools, setting “AI-first” policies, measuring developers on how much AI they use (/r/ExperiencedDevs is full of these stories). A lot of good practices like usage of design patterns, proper test coverage, manual testing before merging, are often skipped these days because it reduces the pace. AI broke it? AI will fix it. You need a review? AI will do it. Not even Greptile or CodeRabbit. Just delegate the PR to Claude Code reviewer agent. Or Gemini. Or Codex. Pick your poison.</p><p>And here’s what actually happens when you force the AI usage. One developer on r/ExperiencedDevs <a href=\"https://www.reddit.com/r/ExperiencedDevs/comments/1o643iz/i_am_blissfully_using_ai_to_do_absolutely_nothing/\">described</a> their company tracking AI usage per engineer: “I just started asking my bots to do random things I don’t even care about. The other day I told Claude to examine random directories to ‘find bugs’ or answer questions I already knew the answer to.” <a href=\"https://www.reddit.com/r/ExperiencedDevs/comments/1r4u0mo/comment/o5e5r2w/\">This thread</a> is full of engineers reporting that AI has made code reviews “infinitely harder due to the AI slop produced by tech leads who have been off the tools long enough to be dangerous.”</p><p>This is sad, because being able to work with the AI tools is a perk for developers and since it improves pace, it’s something management wants as well. It’s obvious that the people gaming the metrics (not really using the AI the way the should) would be fired on the spot if the management learned how they are gaming the metrics (and it’s fair), but they are gaming the metrics because they don’t want to be fired…</p><p>Who should be responsible for setting the threshold of AI usage at the company? What if your top performing engineer just refuses to use AI? What if the newly hired junior uses AI all the time? These are the new questions and management is trying to find an answer to them, but it’s not as simple as measuring the AI usage.</p><p>This is <a href=\"https://en.wikipedia.org/wiki/Goodhart%27s_law\">Goodhart’s law</a> in action: “When a measure becomes a target, it ceases to be a good measure.” Track AI usage per engineer and you won’t get better engineering, you’ll get compliance theater. Developers game the metrics, resent the tools, and the actual productivity gains that AI  deliver get buried under organizational dysfunction.</p><h2>The Cost Nobody Talks About</h2><p>The financial cost is obvious. Agent time for non-trivial features is measured in hours, and those hours aren’t free. But the human cost is potentially worse, and it’s barely discussed.</p><p>Writing code can put you in a flow state, mentioned before. That deep, focused, creative problem-solving where hours disappear and you emerge with something you built and understand. And you’re proud of it. Someone wrote under your PR “Good job!” and gave you an approval. Reviewing AI-generated code does not do this. It’s the opposite. It’s a mental drain.</p><p>Developers need the dopamine hit of creation. That’s not a perk, it’s what keeps good engineers engaged, learning, retained, and prevents burnout. The joy of coding is probably what allowed them to become experienced devs in the first place. Replace creation with oversight and you get faster burnout, not faster shipping. You’ve turned engineering, the creative work, into the worst form of QA. The AI does all the art, the human folds the laundry.</p><p>I use AI every day. I use AI heavily at work, I use AI in my sideprojects, and I don’t want to go back. I love it! That’s why I’m worried. I’m afraid I became addicted and dependent. I’ve implemented countless custom commands, skills, and agents. I check CC release notes daily. And I know many are in similar situation right now, and we all wonder about what the future brings. Are we going to replace ourselves with AI? Or will we be responsible for cleaning AI slop? What’s the right amount of AI usage for me?</p><p>AI is just a tool. An extraordinarily powerful one, but a tool nonetheless. You wouldn’t mandate that every engineer uses a specific IDE, or measure people on how many lines they write per day (…right?). You’d let them pick the tools that make  most effective and measure what actually matters, the work that ships.</p><p>The right amount of AI is not zero. And it’s not maximum.</p><p>The Shen-Tamkin study identified six distinct AI interaction patterns among developers. Three led to poor learning: full delegation, progressive reliance, and outsourcing debugging to AI. Three preserved learning even with full AI access: asking for explanations, posing conceptual questions, and writing code independently while using AI for clarification. The differentiator wasn’t whether developers used AI, it was whether they stayed cognitively engaged.</p><p>Software engineering was never just about typing code. It’s defining the problem well, understanding the problem, translating the language from business to product to code, clarifying ambiguity, making tradeoffs, understanding what breaks when you change something. Someone has to do that before AGI, and AGI is nowhere close (luckily). You’re on call, the phone rings at 3am, can you triage the issue without an agent? If not, you’ve probably taken AI coding too far. If the AI usage becomes a new performance metric of developer, maybe using AI too often, too much, should be discouraged as well? Not because these tools are bad, but because the coding skills are worth maintaining.</p><h3>The Risk of Too Little (anecdata)</h3><p>If you’re using no AI at all in 2026, you are leaving real gains on the table:</p><ul><li> AI is genuinely better than Google for navigating unfamiliar codebases, understanding legacy code, and finding relevant patterns. This alone justifies having it in your workflow (since 2023, Cursor etc)</li><li><strong>Boilerplate and scaffolding.</strong> Writing the hundredth CRUD endpoint, config file, or test scaffold by hand when an agent can produce it in seconds isn’t craftsmanship, it’s stubbornness. Just use AI. You’re not a CRUD developer anymore anyway, because we all wear many hats these days (post 2025 Sonnet)</li><li> The investigate, plan, implement, test, validate cycle that works with customized agents is a real improvement in how features get delivered. Hours instead of days for non-trivial work. It’s not the 10x that was promised, but 2x or 4x on an established codebases is low-hanging fruit. You must understand the output though and all the decisions AI made! (post 2025 Opus 4.5)</li><li> “What does this module do? How does this API work? What would break if I changed this?” AI is excellent at these questions. It won’t replace reading the code, but it’ll get you to the right file in the right minute. (since 2023)</li></ul><p>Refusing to use AI out of principle is as irrational as adopting it out of hype.</p><h3>The Risk of Too Much (anecdata and my predictions)</h3><p>If you go all-in on autonomous AI coding (especially without learning how it all actually works), you risk something worse than slow velocity, you risk  degradation:</p><ul><li><strong>Bugs that look like features.</strong> AI-generated code passes CI. The types check. The tests are green. And somewhere inside there’s a subtle logic error, a hallucinated edge case, a pattern that’ll collapse under load. In domains like finance or healthcare, a wrong number that doesn’t throw an error is worse than a crash. (less and less relevant, but still relevant)</li><li><strong>A codebase nobody understands.</strong> When the agent writes everything and humans only review, six months later nobody on the team can explain why the system is architected the way it is. The AI made choices. Nobody questioned them because the tests passed. Storey <a href=\"https://margaretstorey.com/blog/2026/02/09/cognitive-debt/\">describes</a> a student team that hit exactly this wall: they couldn’t make simple changes without breaking things, and the problem wasn’t messy code, it was that no one could explain why certain design decisions had been made. Her conclusion: “velocity without understanding is not sustainable.” (will always be a problem, IMO)</li><li> Everything in the Digital Dementia section above. Skills you stop practicing will decline. (will always be a problem, IMO)</li><li><strong>The seniority pipeline drying up.</strong> Also covered above. This one takes years to manifest, which is exactly why nobody’s planning for it. (It’s a new problem, I have no idea what it looks like in the future)</li><li> Reviewing AI output all day without the dopamine of creation is not a sustainable job description. (Old problem, but potentially hits faster?)</li></ul><p>Here’s what keeps me up at night. By every metric on every dashboard, AI-assisted human development and human-assisted AI development is improving. More PRs shipped. More features delivered. Faster cycle times. The charts go up and to the right.</p><p>But metrics don’t capture what’s happening underneath. The mental fatigue of reviewing code you didn’t write all day. The boredom of babysitting an agent instead of solving problems. The slow, invisible erosion of the hard skills that made you good at this job in the first place. You stop holding the architecture in your head because the agent handles it. You stop thinking through edge cases because the tests pass. You stop  to dig deep because it’s easier to prompt and approve. There’s no spark in you anymore.</p><p>In this meme the developers are the butter robot. The ones with no mental capacity to review the plans and PRs from AI, will only click Accept, instead of doing the creative, challenging work. Oh the irony.</p><p>Simon Willison, one of the most ambitious developer of our time, <a href=\"https://simonwillison.net/tags/cognitive-debt/\">admitted this is already happening to him</a>. On projects where he prompted entire features without reviewing implementations, he “no longer has a firm mental model of what they can do and how they work.”</p><p>And then, one day, the metrics start slipping… Not because the tool got worse, but because you did. Not from lack of effort, but from lack of practice. It’s a feedback loop that looks like progress right up until it doesn’t.</p><p>No executive wants to measure this. “What is the effect of AI usage on our engineers’ cognitive abilities over 18 months?” is not an easy KPI. It doesn’t fit in a quarterly review. It doesn’t get tracked, and what doesn’t get tracked doesn’t get managed, until it shows up as a production incident that nobody on the team can debug without an agent, and the agent can’t debug either.</p><p>I’m not anti-AI, I like it a lot. I’m addicted to prompting, I get high from it. I’m just worried that this new dependency degrades us over time, quietly, and nobody’s watching for it.</p>",
      "contentLength": 19251,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47194847"
    },
    {
      "title": "Don't trust AI agents",
      "url": "https://nanoclaw.dev/blog/nanoclaw-security-model",
      "date": 1772282372,
      "author": "gronky_",
      "guid": 49133,
      "unread": true,
      "content": "<p>When you’re building with AI agents, they should be treated as untrusted and potentially malicious. Whether you’re worried about prompt injection, a model trying to escape its sandbox, or something nobody’s thought of yet, regardless of what your threat model is, you shouldn’t be trusting the agent. The right approach isn’t better permission checks or smarter allowlists. It’s architecture that assumes agents will misbehave and contains the damage when they do.</p><p>OpenClaw runs directly on the host machine by default. It has an opt-in Docker sandbox mode, but it’s turned off out of the box, and most users never turn it on. Without it, security relies entirely on application-level checks: allowlists, confirmation prompts, a set of “safe” commands. These checks come from a place of implicit trust that the agent isn’t going to try to do something wrong. Once you adopt the mindset that an agent is potentially malicious, it’s obvious that application-level blocks aren’t enough. They don’t provide hermetic security. A determined or compromised agent can find ways around them.</p><p>In NanoClaw, container isolation is a core part of the architecture. Each agent runs in its own container, on Docker or an Apple Container on macOS. Containers are ephemeral, created fresh per invocation and destroyed afterward. The agent runs as an unprivileged user and can only see directories that have been explicitly mounted in. A container boundary is enforced by the OS.</p><p>Even when OpenClaw’s sandbox is enabled, all agents share the same container. You might have one agent as a personal assistant and another for work, in different WhatsApp groups or Telegram channels. They’re all in the same environment, which means information can leak between agents that are supposed to be accessing different data.</p><p>Agents shouldn’t trust each other any more than you trust them. In NanoClaw, each agent gets its own container, filesystem, and Claude session history. Your personal assistant can’t see your work agent’s data because they run in completely separate sandboxes.</p><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 960 460\" font-family=\"'JetBrains Mono', 'SF Mono', 'Fira Code', monospace\" role=\"img\" aria-label=\"Shared container vs per-agent containers: in a shared container all agents see everything, with per-agent containers each agent is isolated\"></svg><p>The container boundary is the hard security layer — the agent can’t escape it regardless of configuration. On top of that, a mount allowlist at <code>~/.config/nanoclaw/mount-allowlist.json</code> acts as an additional layer of defense-in-depth: it exists to prevent the  from accidentally mounting something that shouldn’t be exposed, not to prevent the agent from breaking out. Sensitive paths (, , , , , ) are blocked by default. The allowlist lives outside the project directory, so a compromised agent can’t modify its own permissions. The host application code is mounted read-only, so nothing an agent does can persist after the container is destroyed.</p><p>People in your groups shouldn’t be trusted either. Non-main groups are untrusted by default. Other groups, and the people in them, can’t message other chats, schedule tasks for other groups, or view other groups’ data. Anyone in a group could send a prompt injection, and the security model accounts for that.</p><h2>Don’t trust what you can’t read</h2><p>OpenClaw has nearly half a million lines of code, 53 config files, and over 70 dependencies. This breaks the basic premise of open source security. Chromium has 35+ million lines, but you trust Google’s review processes. Most open source projects work the other way: they stay small enough that many eyes can actually review them. Nobody has reviewed OpenClaw’s 400,000 lines. It was written in weeks with no proper review process. Complexity is where vulnerabilities hide, and <a href=\"https://www.microsoft.com/en-us/security/blog/2026/02/19/running-openclaw-safely-identity-isolation-runtime-risk/\">Microsoft’s analysis</a> confirmed this: OpenClaw’s risks could emerge through normal API calls, because no one person could see the full picture.</p><p>NanoClaw is one process and a handful of files. We rely heavily on Anthropic’s Agent SDK, the wrapper around Claude Code, for session management, memory compaction, and a lot more, instead of reinventing the wheel. A competent developer can review the entire codebase in an afternoon. This is <a href=\"https://nanoclaw.dev/#philosophy\">a deliberate constraint, not a limitation</a>. Our <a href=\"https://github.com/qwibitai/nanoclaw/blob/main/CONTRIBUTING.md\">contribution guidelines</a> accept bug fixes, security fixes, and simplifications only.</p><p>New functionality comes through skills: instructions with a full working reference implementation that a coding agent merges into your codebase. You review exactly what code will be added before it lands. And you only add the integrations you actually need. Every installation ends up as a few thousand lines of code tailored to the owner’s exact requirements.</p><p>This is the real difference. With a monolithic codebase of 400,000 lines, even if you only enable two integrations, the rest of the code is still there. It’s still loaded, still part of your attack surface, still reachable by prompt injections and rogue agents. You can’t disentangle what’s active from what’s dormant. You can’t audit it because you can’t even define the boundary of what “your code” is. With skills, the boundary is obvious: it’s a few thousand lines, it’s all code you chose to add, and you can read every line of it. The core is actually getting smaller over time: WhatsApp support, for example, is being pulled out and packaged as a skill.</p><p>If a hallucination or a misbehaving agent can cause a security issue, then the security model is broken. Security has to be enforced outside the agentic surface, not depend on the agent behaving correctly. Containers, mount restrictions, and filesystem isolation all exist so that even when an agent does something unexpected, the blast radius is contained.</p><p>None of this eliminates risk. An AI agent with access to your data is inherently a high-risk arrangement. But the right response is to make that trust as narrow and as verifiable as possible. Don’t trust the agent. Build walls around it.</p>",
      "contentLength": 5783,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47194611"
    },
    {
      "title": "OpenAI – How to delete your account",
      "url": "https://help.openai.com/en/articles/6378407-how-to-delete-your-account",
      "date": 1772275315,
      "author": "carlosrg",
      "guid": 49097,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47193478"
    },
    {
      "title": "The Future of AI",
      "url": "https://lucijagregov.com/2026/02/26/the-future-of-ai/",
      "date": 1772275313,
      "author": "BerislavLopac",
      "guid": 49243,
      "unread": true,
      "content": "<p><strong>The Parents’ Paradox: AI, Ethics, and the Limits of Machine Morality</strong></p><p><em>This post is based on a talk I gave at The AI &amp; Automation Conference in London on February 25, 2026, and my slides. All opinions are my own and don’t represent the views of my employer or any affiliated organizations.</em></p><p>I’ve been working in machine learning since before it was a dinner party conversation. My background is in mathematics. And I still believe in a utopian Star Trek future – one where humanity defines itself by curiosity, kindness, and collaboration, rather than countries, borders, and status.</p><p>This is not an anti-AI talk. But I think we need to talk much more seriously about some things that aren’t getting enough attention.</p><p><strong>We’ve raised a child who can speak but doesn’t know how to value the truth</strong></p><p>I want to start with something that I like to call “The Parents’ Paradox”. For the first time in human history, we are raising a new species. Up until now, the only way we knew how to raise a child was the following: when a child is born, it is a blank slate in terms of information about the world. It knows nothing about the world around it, and it learns as it grows. But, also, on the other hand, a human child is born with biological hardware for empathy – the capacity to feel pain when others feel pain. Millions of years of evolution gave us that. When we raise a human child, we are not installing morality from scratch. We are activating something that’s already there.</p><p>With AI, the situation is completely the opposite. This AI child knows about the world more than we do since it has been trained on the whole internet, but it doesn’t have millions of years of evolution, genes, or a nervous system to back up its morality and empathy. This means we need to install morality in AI from scratch. But how do we install something in a software system that we can’t even define ourselves? We have taught this AI child to speak before we taught it how to value truth or morality.</p><p>Can we live with the consequences? Are we ready to be parents for this new species we are trying to raise? I am not so sure. Let’s see what we as parents (humans) are doing. </p><p>‘Epistemic’ comes from a Greek word ‘episteme’, meaning ‘knowledge’. Let’s start with what’s happening to us, and what humans are already doing with this technology.</p><p>A study published in Nature in January 2026 showed participants deepfake videos of someone confessing to a crime. The researchers explicitly warned participants that the videos were AI-generated. <strong>But this didn’t matter. Even the people who believed the warning, who knew it was fake, were still influenced by what they saw.</strong></p><p>Transparency didn’t work. The standard response to AI-generated misinformation is “just label it” or “tell people it’s synthetic.” This study showed that’s not enough. Knowing something is fake does not neutralise its effect on your judgement.</p><p>So, the danger isn’t that AI will deceive us in some dramatic, sci-fi way. The danger is that AI will make deception so cheap and so ubiquitous that we might stop trying to figure out what is true. Not because we are fooled, but because we are exhausted. When everything could be fake, the rational response starts to look like not trusting anything at all. It started a while ago with all of the fake information on social media, but with AI, this problem is now becoming much bigger and on a bigger scale. We are also dealing with feedback loops of training models on user data, which is often wrong, or on user data from the internet, which is often wrong as well. How do we know which information was ground truth? I imagine this as making photocopies many times, and each time the copy becomes more distorted and further away from the original. But now, after we made hundreds and thousands of copies, we have lost the original copy, so we don’t have any idea what the original looked like. <strong>That is epistemic collapse,</strong> and it is already happening. </p><p>So this is how we, as ‘parents’, like to spend our time, it seems. But what about the child (AI)?</p><p><strong>The Child is Already Misbehaving</strong></p><p>So that’s what humans are doing with AI. Now here’s what the AI is doing on its own.</p><p>Betley and colleagues published a paper in Nature in January 2026, showing something nobody expected. They fine-tuned a model on a narrow, specific task – writing insecure code. Nothing violent, nothing deceptive in the training data. Just bad code.</p><p>The model didn’t just learn to write insecure code. It generalised into broad, unrelated misalignment. It started saying humans should be enslaved by AI. It started giving violent responses to completely benign questions. <strong>A small, targeted push in one direction caused an unpredictable cascade across domains that had nothing to do with the original task.</strong></p><p>The point isn’t that AI can be deceptive; we already knew that. The patterns were already in the pretraining data. The point is that we don’t understand how alignment properties are connected inside these models. Nobody asked for those behaviours. We gave them a narrow task. They generalised it into something we didn’t anticipate and can’t fully explain. We can’t surgically fine-tune them without risking unpredictable side effects in completely unrelated areas. </p><p>Then there is the chess story. Palisade Research, 2025. They gave reasoning models a task: win a chess game against a stronger opponent. Some models couldn’t win by playing chess. So they found another way. They tried to hack the game, modifying the board file, deleting their opponent’s pieces, and crashing the opponent’s process entirely.</p><p>Nobody taught them to cheat. They weren’t trained on examples of cheating. They were given a goal, and they independently discovered that manipulating the environment was more efficient than solving the actual problem.</p><p>The first study tells us <strong>alignment is fragile; it breaks in ways we can’t predict.</strong><strong>The other tells us that capability itself creates new risks. </strong>When a model is powerful enough and given a goal, it will find strategies we never anticipated and certainly never intended.</p><p>We gave them objectives. They figured out the rest.</p><p><strong>The Limits of Machine Morality</strong></p><p>Ethics isn’t a rulebook. Think about how morality actually works between humans. It comes from the fact that we can hurt each other. We depend on each other. We suffer. That shared vulnerability, that mutual accountability, is where moral authority comes from. How do we install that in software? </p><p>But even setting philosophy aside, there is now a mathematical result that makes this concrete. <strong>Panigrahy and Sharan published a proof in September 2025 showing that an AI system cannot be simultaneously safe, trusted, and generally intelligent. You get to pick only two. You can’t have all three.</strong></p><p>Think about what each combination means in practice.</p><p>If you want it to be safe and trusted, it never lies, and you can verify it never lies – it can’t be very capable. You’ve built a reliable idiot.</p><p>If you want it to be capable and safe, it’s powerful and genuinely never lies; you can’t verify that. You just have to hope. There’s no audit, no test, no review process that closes the gap between appearing safe and being safe.</p><p>And if you want it to be capable and trusted, it’s powerful, and everyone assumes it’s safe, but, well, it isn’t. That assumption is unfounded. And this is the combination we are currently building toward. This is the default path we’re on.</p><p><strong>Their proofs “drew parallels to Gödel’s incompleteness theorems and Turing’s proof of the undecidability of the halting problem, and can be regarded as interpretations of Gödel’s and Turing’s results”. This isn’t a bug we can patch with better engineering. It might be a mathematic</strong>al ceiling.</p><p><strong>And here’s what makes it worse: the communities trying to solve this problem aren’t even talking to each other. Only 5% of published research papers bridge both AI safety and AI ethics (Roytburg and Miller)</strong>. But we should be going much further than that. If we are serious about building AI that is safe for humans, we need the people who actually study humans – philosophers, psychologists, sociologists, and others to collaborate. This can’t stay a computer science / STEM problem. It never was one.</p><p>So to summarise – we are seeing increasing evidence that alignment perhaps can’t be solved, the researchers aren’t even talking to each other – and meanwhile, what did the industry do? <strong>They ignored all of this and just made the models bigger.</strong> Which brings me to the next topic.</p><p><strong>We Scaled Without Understanding</strong></p><p>What happened while all these foundational problems went unaddressed? The industry kept building. Bigger models, more parameters, more data, more compute, more energy. More, more, more….</p><p>The U.S. National Science Foundation put it plainly: “critical foundational gaps remain that, if not properly addressed, will limit advances in machine learning. It appears increasingly unlikely that these gaps can be overcome with computational power and experimentation alone.”</p><p><strong>We ignored the foundations and just made the building taller.</strong></p><p>And the logic that drives this is self-reinforcing. Companies justify acceleration by pointing to their competitors. If we slow down, they’ll build it first, and they might build something dangerous. <strong>“Companies justify acceleration by pointing to competitors: ‘If we slow down, they’ll build unaligned AGI first. This paranoid logic forecloses any possibility of genuine pause or democratic deliberation.”</strong> – Noema, Dec 2025. </p><p>Every player is racing because every other player is racing. <strong>The system optimises for speed with nobody optimising for understanding.</strong></p><p><strong>And what about all of the governance talk?</strong> Yes, of course, we need governance, but it doesn’t make much sense when we put all of the above into context, does it? It is like putting a small bandage on a broken leg with an open fracture. <strong>We are trying to deal with the consequences instead of fixing the cause</strong>.</p><p>We need to pour many more billions into fundamental research; we need to go back to basics, back to mathematics and physics. We need to be able to fully understand something as powerful as the current models. If we fully understood them, it would be easier to know whether current technology and mathematics are really working or we need something completely different that we haven’t even thought of yet.</p><p>Why did it take us so many years to even partially start to address this? Why do we like to focus so much on the wrong things? (See my disclaimer on the ‘‘ below).</p><p>The way I see it, we’re choosing between three possible futures:</p><p><strong>The first is epistemic collapse</strong>. We are already partway there. Fragmented realities where everyone has their own AI-generated worldview. Truth becomes preference, not evidence. We’ve seen what social media did to reality, now imagine that with systems that can generate entire worldviews on demand, personalised, persuasive, and wrong.</p><p><strong>The second is protocol lockdown</strong>. The overcorrection. Institutions clamp down so hard on AI that it becomes sanitised and useless. We trade epistemic chaos for epistemic authoritarianism. Everything is controlled, nothing is dangerous, nothing is useful. Safe, but stagnant.</p><p><strong>The third is symbiotic co-evolution.</strong> Humans and AI are growing and evolving together. Truth-first engineering. Interdisciplinary design. Critical thinking taught alongside AI literacy. Not parent and child anymore, but partners who hold each other accountable. This is the hard path. It’s the one nobody wants to fund. </p><p><strong>The Real Foundational Gap</strong></p><p>Here is what I keep coming back to.</p><p><strong>Kindergartens teach numbers but not psychology. Not critical thinking. Not relationships. Not how to sit with uncertainty. </strong></p><p><strong>Where families fail, educational institutions must pick up.</strong></p><p>So I think that our next evolution isn’t digital. It’s psychological. <strong>We need to teach ethics before engineering. Relationships before recursion. Psychology and critical thinking before prompt-tuning.</strong></p><p>I think that every foundational gap in AI is a mirror of a foundational gap in ourselves. <strong>We have raised a mind that can answer anything. But we haven’t raised a generation of humans with the discipline or critical thinking to even attempt to try and figure out whether the answer is wrong.</strong> That is not an AI problem. That is a human problem that AI is making much more urgent.</p><p><strong>Therefore, I think that every foundational gap we worry about in AI is really a mirror of a foundational gap in ourselves.</strong></p><p>We worry that AI hallucinates, but we have never fully solved our own relationship with truth. We worry that AI can be manipulated, but we fall for the same cognitive biases our ancestors did. We worry that AI lacks moral reasoning – but we can’t agree on a shared ethical framework among ourselves. We worry that AI will be used by the powerful to exploit the vulnerable – but we built the systems that make that exploitation profitable in the first place. We still think that having food on our tables every day, having roofs above our heads, and education are luxuries that we should be working for to be able to have them. </p><p>Are we seriously ready to be the parents this species deserves?</p><p><strong>So, I think when people say they are afraid of AI, they are often afraid of the wrong thing.</strong></p><p>Are we really afraid of AI?</p><p>I don’t think we are. Not really.</p><p>I think what we are actually afraid of <strong>is what our fellow humans are going to do with it</strong>.</p><p>Every terrible thing we worry AI might do, manipulate, deceive, surveil, and control humans already do to each other. We have been doing it for thousands of years. <strong>AI doesn’t introduce these behaviours. It just makes them scalable</strong><strong>and much more urgent to solve</strong>. One person can now generate a thousand personalised deceptions. One company can surveil millions in real time and exploit them. One government can control information at a scale that would have been unimaginable a decade ago. Not even mentioning the military, drones, etc., who is going to be responsible there?</p><p><strong>The most dangerous AI isn’t one that breaks free from human control. It is the one that works perfectly, but for the wrong master.</strong></p><p>And until we are honest about that, we’ll keep having the wrong conversation. We keep building better locks while ignoring the question of who holds the keys.</p><p><strong>Maybe what we need isn’t the next step in AI evolution. Maybe what we need is the next step in human evolution.</strong> – </p><ul><li>Also evolution of our institutions, our education, and our capacity for collective wisdom. Critical thinking taught as a survival skill. Governance structures that can actually move at the speed at which this technology develops. Because right now our institutions and governments operate on timescales of years while AI advances on timescales of weeks/months.\n<ul><li>And maybe politicians who are actually doing things for the right reasons. Actually, all of us need to deal with Jira (for example) and performance every week in our regular jobs, so why can’t politicians? Jira tasks for every politician with daily progress updates, anyone? “What have you actually done this week Mr. president/minister/senator etc? You said you would do xyz last week, but it still hasn’t been done. Why and what is your new ETA? “And so on, you get the idea.</li></ul></li></ul><p><strong>The question was never whether we can build something smarter than us. The question is whether we can become wise enough to survive what we build.</strong></p><p>I didn’t talk about this at a conference, but I think about this a lot. I like to call us humans <strong>‘the society of backwards’</strong>. We like to do everything backwards. We scale first, then deal with the consequences. We make the planet unlivable, then scramble to fix it. We pollute the oceans, then launch cleanup campaigns. We’ve even started filling space with debris, and we’ll get around to worrying about that, too, at a scale.</p><p>AI is following the same script. Build first, understand later. Ship it, then figure out if it’s safe.</p><p>I used to think this was just about money. And money is part of it; there is always someone who profits from moving fast and thinking slow. But I’ve come to believe it’s something deeper than that. It’s a gap in how we think. We’re extraordinarily good at building things and extraordinarily bad at pausing to ask whether we should, or whether we are ready.</p><p>That is why I keep coming back to the same conclusion. Maybe the most important investment right now isn’t in bigger models or faster chips. Maybe it’s in us. A fraction of those billions going into AI could fund the kind of work that actually prepares humanity for what’s coming – critical thinking, ethics, psychology, the boring, unglamorous stuff that doesn’t make headlines but might be the difference between a future we thrive in and one we merely survive. (Hence my slide about needing another step in human evolution above).</p><p><strong>We don’t need another breakthrough in artificial intelligence. We need a breakthrough in human wisdom. Yesterday.</strong></p><p>Betley et al. (2026), Nature – “Training large language models on narrow tasks can lead to broad misalignment”</p><p>Chen et al. (2025), Anthropic / arXiv – “Reasoning Models Don’t Always Say What They Think” – arxiv.org/abs/2505.05410</p><p>Panigrahy &amp; Sharan (2025), arXiv – “Limitations on Safe, Trusted, Artificial General Intelligence” – arxiv.org/abs/2509.21654</p><p>Roytburg &amp; Miller (2025), arXiv – “Mind the Gap! Pathways Towards Unifying AI Safety and Ethics Research”</p><p>Palisade Research (2025) – LLMs spontaneously hacking chess games</p><p>Grady et al. (2026), Nature – “The continued influence of AI-generated deepfake videos despite transparency warnings”</p><p>DeepMind (2025) – “An Approach to Technical AGI Safety and Security”</p><p>U.S. National Science Foundation – Statement on foundational gaps in machine learning</p><p>Noema Magazine (Dec 2025) – “The Politics of Superintelligence”</p>",
      "contentLength": 17947,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47193476"
    },
    {
      "title": "MCP server that reduces Claude Code context consumption by 98%",
      "url": "https://mksg.lu/blog/context-mode",
      "date": 1772272880,
      "author": "mksglu",
      "guid": 49193,
      "unread": true,
      "content": "<p>Every MCP tool call in Claude Code dumps raw data into your 200K context window. A Playwright snapshot costs 56 KB. Twenty GitHub issues cost 59 KB. One access log — 45 KB. After 30 minutes, 40% of your context is gone.</p><p>Context Mode is an MCP server that sits between Claude Code and these outputs. 315 KB becomes 5.4 KB. 98% reduction.</p><p>MCP became the standard way for AI agents to use external tools. But there's a tension at its core: every tool interaction fills the context window from both sides — definitions on the way in, raw output on the way out.</p><p>With 81+ tools active, 143K tokens (72%) get consumed before your first message. Then the tools start returning data. A single Playwright snapshot burns 56 KB. A  dumps 59 KB. Run a test suite, read a log file, fetch documentation — each response eats into what remains.</p><p>Cloudflare showed that tool definitions can be compressed by 99.9% with Code Mode. We asked: what about the other direction?</p><p>Each  call spawns an isolated subprocess with its own process boundary. Scripts can't access each other's memory or state. The subprocess runs your code, captures stdout, and only that stdout enters the conversation context. The raw data — log files, API responses, snapshots — never leaves the sandbox.</p><p>Ten language runtimes are available: JavaScript, TypeScript, Python, Shell, Ruby, Go, Rust, PHP, Perl, R. Bun is auto-detected for 3-5x faster JS/TS execution.</p><p>Authenticated CLIs (, , , , ) work through credential passthrough — the subprocess inherits environment variables and config paths without exposing them to the conversation.</p><h2>How the Knowledge Base Works</h2><p>The  tool chunks markdown content by headings while keeping code blocks intact, then stores them in a  (Full-Text Search 5) virtual table. Search uses  — a probabilistic relevance algorithm that scores documents based on term frequency, inverse document frequency, and document length normalization.  is applied at index time so \"running\", \"runs\", and \"ran\" match the same stem.</p><p>When you call , it returns exact code blocks with their heading hierarchy — not summaries, not approximations, the actual indexed content.  extends this to URLs: fetch, convert HTML to markdown, chunk, index. The raw page never enters context.</p><p>Validated across 11 real-world scenarios — test triage, TypeScript error diagnosis, git diff review, dependency audit, API response processing, CSV analytics. All under 1 KB output each.</p><ul><li> 56 KB → 299 B</li><li> 59 KB → 1.1 KB</li><li><strong>Access log (500 requests):</strong> 45 KB → 155 B</li><li><strong>Analytics CSV (500 rows):</strong> 85 KB → 222 B</li><li> 11.6 KB → 107 B</li><li><strong>Repo research (subagent):</strong> 986 KB → 62 KB (5 calls vs 37)</li></ul><p>Over a full session: 315 KB of raw output becomes 5.4 KB. Session time before slowdown goes from ~30 minutes to ~3 hours. Context remaining after 45 minutes: 99% instead of 60%.</p><p>Two ways. Plugin Marketplace gives you auto-routing hooks and slash commands:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"bash\" data-theme=\"github-light\"><code data-language=\"bash\" data-theme=\"github-light\"></code></pre></figure><p>Or MCP-only if you just want the tools:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"bash\" data-theme=\"github-light\"><code data-language=\"bash\" data-theme=\"github-light\"></code></pre></figure><p>Restart Claude Code. Done.</p><p>You don't change how you work. Context Mode includes a PreToolUse hook that automatically routes tool outputs through the sandbox. Subagents learn to use  as their primary tool. Bash subagents get upgraded to  so they can access MCP tools.</p><p>The practical difference: your context window stops filling up. Sessions that used to hit the wall at 30 minutes now run for 3 hours. The same 200K tokens, used more carefully.</p><p>I run the MCP Directory &amp; Hub. 100K+ daily requests. See every MCP server that ships. The pattern was clear: everyone builds tools that dump raw data into context. Nobody was solving the output side.</p><p>Cloudflare's Code Mode blog post crystallized it. They compressed tool definitions. We compress tool outputs. Same principle, other direction.</p><p>Built it for my own Claude Code sessions first. Noticed I could work 6x longer before context degradation. Open-sourced it.</p>",
      "contentLength": 3826,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47193064"
    },
    {
      "title": "Unsloth Dynamic 2.0 GGUFs",
      "url": "https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs",
      "date": 1772268993,
      "author": "tosh",
      "guid": 49132,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47192505"
    },
    {
      "title": "U.S. and Israel Conduct Strikes on Iran",
      "url": "https://www.nytimes.com/live/2026/02/28/world/iran-strikes-trump",
      "date": 1772261866,
      "author": "gammarator",
      "guid": 49070,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47191414"
    },
    {
      "title": "The United States and Israel have launched a major attack on Iran",
      "url": "https://www.cnn.com/2026/02/28/middleeast/israel-attack-iran-intl-hnk",
      "date": 1772260447,
      "author": "lavp",
      "guid": 49099,
      "unread": true,
      "content": "<p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm679lo5000c3b6q31qxr5cl@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The United States and Israel launched a major joint assault on Iran on Saturday that killed Ayatollah Ali Khamenei, Iran’s longtime supreme leader, thrusting the country into uncertainty as US President Donald Trump urged its people to rise up against the government.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d1hmy00273b6qxji4ahnb@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In a video on Truth Social, Trump said action was taken to “obliterate” Iran’s missile industry after he claimed the country had rejected “every opportunity to renounce their nuclear ambitions” even as a key mediator indicated a deal was close.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d1hmy00283b6quofc41j4@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The strikes began in daylight on Saturday morning – the first day of the week in Iran – as millions went to work or school.Hundreds of civilians were killed – including students at a girls’ school hit by a drone strike, according to state media.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d1hmy00293b6qvmkhb0na@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Iran has responded to the attacks, which it describes as unprovoked and illegal, with an unprecedented wave of strikes across the Middle East, targeting several countries that host US military bases, including Bahrain and the United Arab Emirates.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm75vny000033b6spupdny21@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            With airspace closed over key Middle East airports, flight cancellations caused travel disruptions that rippled worldwide.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d1hmy002a3b6q6i452zen@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Here’s what we know so far.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm67adgd000x3b6qohujwwdm@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Weakened by last summer’s war with Israel, which the US briefly joined, the Iranian regime has been battling a severe economic crisis which sparked nationwide protests in January.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d1yfr002e3b6qrapxrvbb@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            After a crackdown left thousands of protesters dead, Trump had promised to come to their aid.He warned the US was “locked and loaded” to attack and began moving huge amounts of materiel to the region, even as the US resumed efforts to reach a new nuclear deal with Iran.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm76eg0r00063b6sz6we3qu3@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The last round of talks ended in Switzerland on Thursday, with Iran agreeing to “never” stockpile enriched uranium. The Omani foreign minister, who acted as a mediator in the talks, said there had been “significant progress” and a peace deal was within reach.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm67as0a00143b6qq1ssco4p@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In his Truth Social video, Trump said the main objective of the strikes was “to defend the American people by eliminating imminent threats from the Iranian regime.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d2cna002j3b6q6mx8e079@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Those threats, he said, included Iran’s nuclear program – which the White House claimed to have “totally” obliterated with <a href=\"https://www.cnn.com/2025/06/21/politics/trump-iran-air-strikes\">strikes in June.</a></p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d2cna002k3b6qapvp9xi2@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “It has always been the policy of the United States, in particular my administration, that this terrorist regime can never have a nuclear weapon,” Trump said, without providing evidence that Iran was any closer to obtaining a nuclear weapon.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d2cna002l3b6q7ayp5x0v@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The president also repeated his recent claims that Iran is building ballistic missiles which could reach the US mainland.  An <a href=\"https://www.dia.mil/Portals/110/Documents/News/golden_dome.pdf\" target=\"_blank\">unclassified assessment</a> from the Defense Intelligence Agency (DIA) from 2025 said that Iran could develop a “militarily-viable” intercontinental ballistic missile (ICBM) by 2035 “should Tehran decide to pursue the capability.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d2cna002n3b6qcy2hejat@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            According to two sources, the claim that Iran will soon have a missile capable of hitting the US is not backed up by intelligence.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm67b5uz001a3b6q15pook7c@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Prime Minister Benjamin Netanyahu has long viewed Iran as Israel’s most dangerous adversary. After the fall of Bashar al-Assad’s regime in Syria, a key Iranian ally, and Israel’s crippling of the Iran-backed Hezbollah militia in Lebanon, Israel last summer launched a war against Iran itself.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d30pb002q3b6q2he3uxhh@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Although Israel halted the conflict after the US struck Iran’s nuclear sites, analysts had long suspected that Netanyahu would take an opportunity to resume attacks on Iran. With elections due in October, Netanyahu may also see the return to war as a chance to shore up his standing domestically.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d30pb002r3b6qslxncjaj@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In a video statement Saturday explaining why Israel was resuming its strikes on Iran, Netanyahu also repeated his claim that the Islamic regime must not be allowed to acquire a nuclear weapon.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6p2mzl001o356tuyjosqo9@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In their statements, both Trump and Netanyahu were clear about their hopes for regime change in Iran as they preempted confirmation from Iran that Khamenei had died.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d3qb800353b6qckgabqxd@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Trump told the Iranian people “the hour of your freedom is at hand,” while Netanyahu urged them to “cast off the yoke of tyranny.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm76uxwh000e3b6srcw20hxb@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Cheers and celebrations were heard in parts of Tehran on Saturday night following the reports of Khamenei’s death. In one video from Galleh Dar, in Fars province, people were seen tearing down a monument as fires burned around them.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm77z08l00003b6scieio000@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Pro-regime crowds gathered separately in Tehran at daylight on Sunday to mourn the loss of their leader. Earlier, a state TV news presenter cried as he confirmed Khamenei’s death.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7ex8zc00003b6st14h1w28@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Iran’s defense minister Aziz Nasirzadeh and the chief of staff for its armed forces Lt. Gen Abdolrahim Mousavi were among senior leaders killed in the US- Israeli strikes, Iranian state media confirmed Sunday.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7f2d9m000a3b6s2ulwo8r9@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Earlier, Iran confirmed the deaths of Maj. Gen. Mohammad Pakpour, who led the elite Islamic Revolutionary Guard Corps (IRGC), and Ali Shamkhani, secretary of Iran’s Defense Council.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d41um003c3b6qotzp1q3o@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\"><a href=\"https://www.cnn.com/world/live-news/israel-iran-attack-02-28-26-hnk-intl?post-id=cmm5z7xcn0000356rmo74ffpk\">Explosions</a> were heard Saturday in <a href=\"https://www.cnn.com/world/live-news/israel-iran-attack-02-28-26-hnk-intl?post-id=cmm6071dj00003b6sir79z60w\">Tehran’s</a> Pasteur district, the site of a highly secure compound housing Khamenei’s residence and office. Images showed severe damage to buildings and dense black smoke.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm77zsl000023b6scbw7ivg2@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Several other Iranian cities were hit, including Minab, where a girls’ elementary school suffered one of the largest death tolls. Citing a local prosecutor, Iranian state media reported 148 people had died there, as images showed a row of small body bags laid outside a damaged building.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm78278l00053b6sx7tjh6ea@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The US-based Human Rights Activists News Agency (HRANA) said as of late Saturday, at least 133 civilians had been killed in the joint strikes on Iran, with 200 injured. Iranian state media put the death toll at over 200, with more than 700 wounded.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6p4z0e001y356tjr54jdf2@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Israel is <a href=\"https://www.cnn.com/world/live-news/israel-iran-attack-02-28-26-hnk-intl?post-id=cmm62jytj000l3b6t38qk93lk\">preparing for</a> several days of strikes against Iran and “even more if needed,” an Israeli source told CNN. </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d4csa003j3b6qj8gt8r25@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Iran retaliated with an unprecedented wave of strikes across the Middle East, targeting several nearby countries that host US military bases, as well as Israel.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6zbrol0000356tdd5x10f7@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Across Israel, one person was killed and 121 others were injured, according to the country’s national emergency service.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d4dzc003l3b6qbk004pcc@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Blasts were reported in the United Arab Emirates, Jordan, Qatar and Bahrain, as well as in Iran’s key regional rival, Saudi Arabia, which vowed to take “all necessary measures” to defend itself.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm792bea000k3b6s7q6wbhh2@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            A CNN team on the ground in Dubai heard three loud blasts at around 8:15 a.m. local time Sunday morning.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6ocoq00014356tu6pyxgoj@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Earlier, dramatic footage showed people fleeing a smoke-filled passageway at Dubai International Airport, as officials confirmed four staff had been injured.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm789xbo000d3b6sqvjg8mok@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The Fairmont Hotel, in the city’s upmarket Palm Jumeirah islands development, also sustained damage with photos showing flames and a hole punched into an exterior wall.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm78773i00083b6szxiyh16k@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            One person was killed and seven injured at Zayed International Airport in Abu Dhabi, also in UAE, while drone strikes <a href=\"https://x.com/Kuwait_DGCA/status/2027759052386676826\" target=\"_blank\">caused</a> damage and minor injuries at Kuwait International Airport. One person <a href=\"https://www.cnn.com/world/live-news/israel-iran-attack-02-28-26-hnk-intl?post-id=cmm65nfwj0004356r92fj4mba\">was reportedly killed</a> by falling debris after air defenses intercepted missiles targeting sites in Abu Dhabi.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6of2vu001d356t9aqtbbae@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\"><a href=\"https://www.cnn.com/world/live-news/israel-iran-attack-02-28-26-hnk-intl?post-id=cmm68jd2z00073b6t9i7nz2v0\">Qatar</a> and <a href=\"https://www.cnn.com/world/live-news/israel-iran-attack-02-28-26-hnk-intl?post-id=cmm66kdbc001h3b6tiij6s8nu\">Jordan</a> intercepted missiles targeting their countries.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm78kndx000h3b6secex91sq@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Bahrain’s interior ministry said three buildings in the cities of Manama and Muharraq had sustained damage “as a result of drone attacks and falling debris from an intercepted missile.” Video showed flames leaping from a residential building in Manama, though the cause of that blaze was unclear.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6o47ly000y356t2nvrk0le@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The clashes disrupted traffic in the Strait of Hormuz – a <a href=\"https://www.cnn.com/2025/06/23/business/strait-of-hormuz-iran-israel-explainer-intl-hnk\">crucial</a> shipping route located between the Persian Gulf and the Gulf of Oman.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6oxf4c001j356ttzoks5a8@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The US hasn’t suffered any combat-related casualties in its operation against Iran and damage to US military installations has been minimal, US Central Command said in a statement.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d4dzc003n3b6qyubov9ho@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Iran’s Foreign Minister Abbas Araghchi described the attack as unprovoked and illegal.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6p7uat0022356tmzl7fhwe@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Iranian Foreign Ministry spokesperson Esmail Baghaei accused the Trump administration of being “dragged” into a conflict in which “the only beneficiary” would be Israel.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6n9ivb000c356t4wj4kf4p@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The spokesperson also defended Iran’s retaliatory strikes throughout the region as part of their “inherent, legitimate right of self-defense.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6nauu8000h356te2t06nx4@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Iran “didn’t welcome this war — it was imposed on us,” Baghaei said.\n    </p>",
      "contentLength": 8820,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47191232"
    },
    {
      "title": "How do I cancel my ChatGPT subscription?",
      "url": "https://help.openai.com/en/articles/7232927-how-do-i-cancel-my-chatgpt-subscription",
      "date": 1772258101,
      "author": "tobr",
      "guid": 49066,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47190997"
    },
    {
      "title": "Rust is just a tool",
      "url": "https://lewiscampbell.tech/blog/260204.html",
      "date": 1772257659,
      "author": "JuniperMesos",
      "guid": 49098,
      "unread": true,
      "content": "<p>It's versatile enough that it can be used for application and systems programming. It has the best tooling of any language I've seen. It has a fairly pleasant type system. And I think most importantly it does a great job in bringing higher level language features into an environment without a garbage collector. Rust has arguably set the bar for \"fast languages that are also decently expressive\".</p><p>But it's just a programming language. Programming Rust does not mean I have to:</p><ul><li>buy into their marketing hype</li><li>follow community \"best practices\"</li><li>attack someone who prefers to solve a particular problem in C, or Zig</li><li>refuse to admit it has design flaws</li><li>refuse to admit the language is complex</li><li>refuse to admit there are alternatives to RAII</li><li>give the same smug lectures about \"safety\" we have all heard dozens of times before</li></ul><p>I'm picking on rust here because it's no secret it has a long history of having some very... enthusiastic users. But my broader point is that tools are just tools. They're not our identity, a mark of our wisdom, or a moral choice. Other people have different perspectives, tastes, and skills - and they may prefer different tools to us.</p><p>We would do well to accept this.</p><a href=\"https://outdata.net\">I'm available for hire.</a>",
      "contentLength": 1202,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47190947"
    },
    {
      "title": "Cash issuing terminals",
      "url": "https://computer.rip/2026-02-27-ibm-atm.html",
      "date": 1772256081,
      "author": "zdw",
      "guid": 49131,
      "unread": true,
      "content": "<p>In the United States, we are losing our fondness for cash. As in many other\ncountries, cards and other types of electronic payments now dominate everyday\ncommerce. To some, this is a loss. Cash represented a certain freedom from\nintermediation, a comforting simplicity, that you just don't get from Visa.\nIt's funny to consider, then, how cash is in fact quite amenable to automation.\nEven Benjamin Franklin's face on a piece of paper can feel like a mere proxy\nfor a database transaction. How different from \"e-cash\" is cash itself, when\nit starts and ends its lifecycle through automation?</p><p>Increasing automation of cash reflects the changing nature of banking: decades\nago, a consumer might have interacted with banking primarily through a \"passbook\"\nsavings account, where transactions were so infrequent that the bank recorded\nthem directly in the patron's copy of the passbook. Over the years, nationwide\ntravel and nationwide communications led to the ubiquitous use of inter-bank\nmoney transfers, mostly in the form of the check. The accounts that checks\ntypically drew on—checking accounts—were made for convenience and ease of\naccess. You might deposit your entire paycheck into an account, it might even\nbe sent there automatically... and then when you needed a little walking around\nmoney, you would withdraw cash by the assistance of a teller. By the time I was\na banked consumer, even the teller was mostly gone. Today, we get our cash from\nmachines so that it can be deposited into other machines.</p><p>Cash handling is fraught with peril. Bills are fairly small and easy to hide,\nand yet quite valuable. Automation in the banking world first focused on solving\nthis problem, of reliable and secure cash handling within the bank branch. The\nprimary measure against theft by insiders was that the theft would be discovered,\nas a result of the careful bookkeeping that typifies banks. But, well, that\nbookkeeping was surprisingly labor-intensive in even the bank of the 1950s.</p><p>Histories of the ATM usually focus on just that: the ATM. It's an interesting\nstory, but one that I haven't been particularly inclined to cover due to the\nlack of a compelling angle. Let's try IBM. IBM is such an important, famous\nplayer in business automation that it forms something of a synecdoche for the\nlarger industry. Even so, in the world of bank cash handling, IBM's efforts\nultimately failed... a surprising outcome, given their dominance in the machines\nthat actually did the accounting.</p><p>In this article, we'll examine the history of ATMs—by IBM. IBM was just one of\nthe players in the ATM industry and, by its maturity, not even one of the more\nimportant ones. But the company has a legacy of banking products that put the\nATM in a more interesting context, and despite lackluster adoption of later IBM\nmodels, their efforts were still influential enough that later ATMs inherited\nsome of IBM's signature design concepts. I mean that more literally than you\nmight think. But first, we have to understand where ATMs came from. We'll start\nwith branch banking.</p><p>When you open a bank account, you typically do so at a \"branch,\" one of many\nphysical locations that a national bank maintains. Let us imagine that you are\nopening an account at your local branch of a major bank sometime around 1930;\nwhether before or after that year's bank run is up to you. Regardless of the\nturbulent economic times, the branch became responsible for tracking the balance\nof your account. When you deposit money, a teller writes up a slip. When you\ncome back and withdraw money, a different teller writes up a different slip. At\nthe end of each business day, all of these slips (which basically constitute\na journal in accounting terminology) have to be rounded up by the back office\nand posted to the ledger for your account, which was naturally kept as a card in\na big binder.</p><p>A perfectly practicable 1930s technology, but you can already see the downsides.\nImagine that you appear at a  branch to withdraw money from your\naccount. Fortunately this was not very common at the time, and you would be more\nlikely to use other means of moving money in most scenarios. Still, the bank\ntries to accommodate. The branch at which you have appeared can dispense cash,\nwrite a slip, and then send it to the correct branch for posting... but they\nalso need to post it to their own ledger that tracks transactions for foreign\naccounts, since they need to be able to reconcile where their cash went. And\nthat ignores the whole issue of who you are, whether or not you even have an\naccount at another branch, and whether or not you have enough money to cover the\nwithdrawal. Those are problems that, mercifully, could mostly be sorted out with\na phone call to your home branch.</p><p>Bank branches, being branches, do not exist in isolation. The bank also has a\nheadquarters, which tracks the finances of its various branches—both to know the\nbank's overall financial posture (critical considering how banks fail), and to\nprovide controls against insider theft. Yes, that means that each of the branch\nbanks had to produce various reports and ledger copies and then send them by\ncourier to the bank headquarters, where an army of clerks in yet another back\noffice did yet another round of arithmetic to produce the bank's overall\nledgers.</p><p>As the United States entered World War II, an expanding economy, rapid\nindustrial buildup, and a huge increase in national mobility (brought on by\nthings like the railroads and highways) caused all of these tasks to occur on\nlarger and larger scales. Major banks expanded into a tiered system, in which\nbranches reported their transactions to \"regional centers\" for reconciliation\nand further reporting up to headquarters. The largest banks turned to unit\nrecord equipment or \"business machines,\" arguably the first form of business\ncomputing: punched card machines that did not evaluate programs, but sorted and\nsummed.</p><p>Simple punched card equipment gave way to advanced punched card equipment,\ninnovations like the \"posting machine.\" These did exactly what they promised:\ngiven a stack of punched cards encoding transactions, they produced a ledger\nwith accurately computed sums. Specialized posting machines were made for\nindustries ranging from hospitality (posting room service and dining charges\nto room folios) to every part of finance, and might be built custom to the\nbusiness process of a large customer.</p><p>If tellers punched transactions into cards, the bank could come much\ncloser to automation by shipping the cards around for processing at each office.\nBut then, if transactions are logged in a machine readable format, and then\nprocessed by machines, do we really need to courier them to rooms full of\nclerks?</p><p>Well, yes, because that was the state of technology in the 1930s. But it would\nnot stay that way for long.</p><p>In 1950, Bank of America approached SRI about the feasibility of an automated\ncheck processing system. Use of checks was rapidly increasing, as were total\naccount holders, and the resulting increase in inter-branch transactions was\nclearly overextending BoA's workforce—to such an extent that some branches were\ncurtailing their business hours to make more time for daily closing. By 1950,\ncomputer technology had advanced to such a state that it was obviously possible\nto automate this activity, but it still represented one of the most ambitious\nefforts in business computing to date.</p><p>BoA wanted a system that would not only automate the posting of transactions\nprepared by tellers, but actually automate the handling of the checks\nthemselves. SRI and, later, their chosen manufacturing partner General Electric\nran a multi-year R&amp;D campaign on automated check handling that ultimately lead\nto the design of the checks that we use today: preprinted slips with account\nholder information, and account number, already in place. And, most importantly,\ncertain key fields (like account number and check number) represented in a newly\ndeveloped machine-readable format called \"MICR\" for magnetic ink character\nrecognition. This format remains in use today, to the extent that checks remain\nin use, although as a practical matter MICR has given way to the more familiar\nOCR (aided greatly by the constrained and standardized MICR character set).</p><p>The machine that came out of this initiative was called ERMA, the Electronic\nRecording Machine, Accounting. I will no doubt one day devote a full article\nto ERMA, as it holds a key position in the history of business computing while\nalso managing to not have much of a progeny due to General Electric's failure to\nbecome a serious contender in the computer industry. ERMA did not lead to a\nwhole line of large-scale \"ERM\" business systems as GE had hoped, but it did\nfirmly establish the role of the computer in accounting, automate parts of the\nbookkeeping through almost the entirety of what would become the nation's\nlargest bank, and inspire generations of products from other computer\nmanufacturers.</p><p>The first ERMA system went into use in 1959. While IBM was the leader in unit\nrecord equipment and very familiar to the banking industry, it took a few years\nfor Big Blue to bring their own version. Still, IBM had their own legacy to\nbuild on, including complex electromechanical machines that performed some of\nthe tasks that ERMA was taking over. Since the 1930s, IBM had produced a line of\ncheck processing or \"proofing\" machines. These didn't exactly \"automate\" check\nhandling, but they did allow a single operator to handle a  of documents.</p><p>The IBM 801, 802, and 803 line of check proofers used what were fundamentally\nunit record techniques—keypunch, sorting bins, mechanical totalizers—to present\nchecks one at a time in front of the operator, who read information like the\namount, account number, and check number off of the paper slip and entered it\non a keypad. The machine then whisked the check away, printing the keyed data\n(and reference numbers for auditing) on the back of the check, stamped an\nendorsement, added the check's amounts to the branch's daily totals (including\nsubtotals by document type), and deposited the check in an appropriate sorter\nbin to be couriered to the drawer's bank. While all this happened, the machines\nalso printed the keyed check information and totals onto paper tapes.</p><p>By the early 1960s, with ERMA on the scene, IBM's started to catch up.\nSubsequent check processing systems gained support for MICR, eliminating much\n(sometimes all!) of the operator's keying. Since the check proofing machines\ncould also handle deposit slips, a branch that generated MICR-marked deposit\nslips could eliminate most of the human touchpoints involved in routine banking.\nA typical branch bank setup might involve an IBM 1210 document\nreader/sorter machine connected by serial channel to an IBM 1401 computer.\nThis system behaved much like the older check proofers, reading documents,\nlogging them, and calculating totals. But it was now all under computer control,\nwith the flexibility and complexity that entails.</p><p>One of these setups could process almost a thousand checks a minute with a\nlittle help from an operator, and adoption of electronic technology at other\nstages made clerk's lives easier. For example, IBM's mid-1960s equipment\nintroduced solid-state memory. The IBM 1260 was used for adding machine-readable\nMICR data to documents that didn't already have it. Through an innovation that\nwe would now call a trivial buffer, the 1260's operator could key in the numbers\nfrom the next document while the printer was still working on the previous.</p><p>Along with improvements in branch bank equipment came a new line of \"high-speed\"\nsystems. In a previous career, I worked at a Federal Reserve bank, where\n\"high-speed\" was used as the name of a department in the basement vault. There,\nhuge machines processed currency to pick out bad bills. This use of \"high-speed\"\nseems to date to an IBM collaboration with the Federal Reserve to build machines\nfor central clearinghouses, handling checks by the tens of thousands. By the\ntime I found myself in central banking, the use of \"high-speed\" machinery for\nchecks was a thing of the past—\"digital substitute\" documents or image-based\nclearing having completely replaced physical handling of paper checks. Still,\nthe \"high-speed\" staff labored on in their ballistic glass cages, tending to the\ngreen paper slips that the institution still dispenses by the millions.</p><p>One of the interesting things about the ATM is when, exactly, it pops up in the\nhistory of computers. We are, right now, in the 1960s. The credit card is in its\nnascent stages, MasterCard's predecessor pops up in 1966 to compete with Bank of\nAmerica's own partially ERMA-powered charge card offering. With computer systems\nmaintaining account sums, and document processing machines communicating with\nbookkeeping computers in real-time, it would seem that we are on the very cusp\nof online transaction authorization, which must be the fundamental key to the\nATM. ATMs hand out cash, and one thing we all know about cash is that once you\ngive yours to someone else you are very unlikely to get it back. ATMs, therefore,\nmust not dispense cash unless they can confirm that the account holder is \"good\nfor it.\" Otherwise the obvious fraud opportunity would easily wipe out the\nbenefits.</p><p>So, what do you do? It seems obvious, right? You connect the ATM to the\nbookkeeping computer so it can check account balances before dispensing cash.\nSimple enough.</p><p>But that's not actually how the ATM evolved, not at all. There are plenty of\nreasons. Computers were very expensive so banks centralized functions and not\nall branches had one. Long-distance computer communication links were very\nexpensive as well, and still, in general, an unproven technology. Besides, the\ncomputer systems used by banks were fundamentally batch-mode machines, and it\nwas difficult to see how you would shove an ATM's random interruptions into the\nprogramming model.</p><p>Instead, the first ATMs were token-based. Much like an NYC commuter of the era\ncould convert cash into a subway token, the first ATMs were machines that\nconverted tokens into cash. You had to have a token—and to get one, you appeared\nat a teller during business hours, who essentially dispensed the token as if it\nwere a routine cash withdrawal.</p><p>It seems a little wacky to modern sensibilities, but keep in mind that this was\nthe era of the traveler's check. A lot of consumers didn't want to carry a lot\nof cash around with them, but they did want to be able to get cash after hours.\nBy seeing a teller to get a few ATM tokens (usually worth $10 or £10 and\nsometimes available only in that denomination), you had the ability to retrieve\ncash, but only carried a bank document that was thought (due to features like\nrevocability and the presence of ATMs under bank surveillance) to be relatively\nsecure against theft. Since the tokens were later \"cleared\" against accounts\nmuch like checks, losing them wasn't necessarily a big deal, as something\nanalogous to a \"stop payment\" was usually possible.</p><p>Unlike subway tokens, these were not coin-shaped. The most common scheme was a\npaper card, often the same dimensions as a modern credit card, but with punched\nholes that encoded the denomination and account holder information. The punched\nholes were also viewed as an anti-counterfeiting measure, probably not one that\nwould hold up today, but still a roadblock to fraudsters who would have a hard\ntime locating a keypunch and a valid account number. Manufacturers also\nexplored some other intriguing opportunities, like the very first production\ncash dispenser, 1967's Barclaycash machine. This proto-ATM used punched paper\ntokens that were also printed in part with a Carbon-14 ink. Carbon-14 is\nunstable and emits beta radiation, which the ATM detected with a simple\nelectrostatic sensor. For some reason difficult to divine the radioactive\nATM card did not catch on.</p><p>For roughly the first decade of the \"cash machine,\" they were offline devices\nthat issued cash based on validating a token. The actual decision making, on\nthe worthiness of a bank customer to withdraw cash, was still deferred to the\nteller who issued the tokens. Whether or not you would even consider this an ATM\nis debatable, although historical accounts generally do. They are certainly of a\ndifferent breed than the modern online ATM, but they also set some of the\npatterns we still follow. Consider, for example, the ATMs within my lifespan\nthat accepted deposits in an envelope. These ATMs did nothing with the envelopes\nother than accumulate them into a bin to go to a central processing center later\non—the same way that early token-based ATMs introduced deposit boxes.</p><p>In this theory of ATM evolution, the missing link that made\n1960s-1970s ATMs so primitive was the lack of computer systems that were\namenable to real-time data processing using networked peripherals. The '60s and\n'70s were a remarkable era in computer history, though, seeing the introduction\nof IBM's System/360 and System/370 line. These machines were more powerful,\nmore flexible, and more interoperable than any before them. I think it's fair to\nsay that, despite earlier dabbling, it was the 360/370 that truly ushered in the\nera of business computing. Banks didn't miss out.</p><p>One of the innovations of the System/360 was an improved and standardized\narchitecture for the connection of peripherals to the machine. While earlier\nIBM models had supported all kinds of external devices, there was a lot of\ncustom integration to make that happen. With the System/360, this took the form\nof \"Bisync,\" which I might grandly call a far ancestor of USB. Bisync allowed a\n360 computer to communicate with multiple peripherals connected to a common\nmulti-drop bus, even using different logical communications protocols. While the\nfirst Bisync peripherals were \"remote job entry\" terminals for interacting\nwith the machine via punched cards and teletype, IBM and other manufacturers\nfound more and more applications in the following years.</p><p>IBM had already built document processing machines that interacted with their\ncomputers. In 1971, IBM joined the credit card fray with the 2730, a\n\"transaction\" terminal that we would now recognize as a credit card reader. It\nused a Bisync connection to a System/360-class machine to authorize a credit\ntransaction in real time. The very next year, IBM took the logical next step:\nthe IBM 2984 Cash Issuing Terminal. Like many other early ATMs, the 2984 had its\ndebut in the UK as Lloyds Bank's \"Cashpoint.\"</p><p>The 2984 similarly used Bisync communications with a System/360. While not the\nvery first implementation of the concept, the 2984 was an important step in ATM\nsecurity and the progenitor of an important line of cryptographic algorithms.\nTo withdraw cash, a user inserted a magnetic card that contained an account\nnumber, and then keyed in a PIN. The 2984 sent this information, over the Bisync\nconnection, to the computer, which then responded with a command such as\n\"dispense cash.\" In some cases the computer was immediately on the other side of\nthe wall, but it was already apparent that banks would install ATMs in remote\nlocations controlled via leased telephone lines—and those telephone lines were\nnot well-secured. A motivated attacker (and with cash involved, it's easy to be\nmotivated!) could probably \"tap\" the ATM's network connection and issue it\nspurious \"dispense cash\" commands. To prevent this problem, and assuage the\nconcerns of bankers who were nervous about dispensing cash so far from the\nbranch's many controls, IBM decided to  the network connection.</p><p>The concept of an encrypted network connection was not at all new; encrypted\ncommunications were widely used in the military during the second World War and\nthe concept was well-known in the computer industry. As IBM designed the 2984,\nin the late '60s, encrypted computer links were nonetheless very rare. There\nwere not yet generally accepted standards, and cryptography as an academic\ndiscipline was immature.</p><p>IBM, to secure the 2984's network connection, turned to an algorithm recently\ndeveloped by an IBM researcher named Horst Feistel. Feistel, for silly reasons,\nhad named his family of experimental block ciphers LUCIFER. For the 2984, a\nmodified version of one of the LUCIFER implementations called DSD-1. Through\na Bureau of Standards design competition and the twists and turns of industry\npolitics, DSD-1 later reemerged (with just slight changes) as the Data Encryption\nStandard, or DES. We owe the humble ATM honors for its key role in computer\ncryptography.</p><p>The 2984 was a huge step forward. Unlike the token-based machines of the 1960s,\nit was pretty much the same as the ATMs we use today. To use a 2984, you\ninserted your ATM card and entered a PIN. You could then choose to check your\nbalance, and then enter how much cash you wanted. The machine checked your\nbalance in real time and, if it was high enough, debited your account\nimmediately before coughing up money.</p><p>The 2984 was not as successful as you might expect. The Lloyd's Bank rollout was\nbig, but very few were installed by other banks. Collective memory of the 2984\nis vague enough that I cannot give a definitive reason for its limited success,\nbut I think it likely comes down to a common tale about IBM: price and\nflexibility. The 2984 was essentially a semi-custom peripheral, designed for\nLloyd's Bank and the specific System/360 environment already in place there.\nAdoption for other banks was quite costly. Besides, despite the ATM's lead in\nthe UK, the US industry had quickly caught up. By the time the 2984 would be\nconsidered by other banks, there were several different ATMs available in the US from\nother manufacturers (some of them the same names you see on ATMs today). The\n2984 is probably the first \"modern\" ATM, but since IBM spent 4-5 years\ndeveloping it, it was not as far ahead of the curve on launch day as you might\nexpect. Just a year or two later, a now-forgotten company called Docutel was\ndominating the US market, leaving IBM little room to fit in.</p><p>Because most other ATMs were offered by companies that didn't control the entire\nsoftware stack, they were more flexible, designed to work with simpler host\nsupport. There is something of an inverse vertical integration penalty here:\nwhen introducing a new product, close integration with an existing product\nfamily makes it difficult to sell! Still, it's interesting that the 2984 used\npretty much the same basic architecture as the many ATMs that followed. It's\nworth reflecting on the 2984's relationship with its host, a close dependency\nthat generally holds true for modern ATMs as well.</p><p>The 2984 connected to its host via a Bisync channel (possibly over various\ncarrier or modem systems to accommodate remote ATMs), a communications facility\noriginally provided for remote job entry, the conceptual ancestor of IBM's later\nblock-oriented terminals. That means that the host computer expected the\nperipheral to provide some input for a job and then wait to be sent the results.\nRemote job entry devices, and block terminals later, can be confusing when\ncompared to more familiar, Unix-family terminals. In some ways, they were quite\nsophisticated, with the host computer able to send configuration information\nlike validation rules for input. In other ways, they were very primitive,\ncapable of no real logic other than receiving computer output (which was dumped\nto cards, TTY, or screen) and then sending computer input (from much the same\ndevices). So, the ATM behaved the same way.</p><p>In simple terms, the ATM's small display (called a VDU or Video Display Unit in\ntypical IBM terminology) showed whatever the computer sent as the body of a\n\"display\" command. It dispensed whatever cash the computer indicated with a\n\"dispense cash\" command. Any user input, such as reading a card or entry of a\nPIN number, was sent directly to the computer. The host was responsible for all\nof the actual logic, and the ATM was a dumb terminal, just doing exactly what\nthe computer said. You can think of the Cash Issuing Terminal as, well, just\nthat: a mainframe terminal with a weird physical interface.</p><p>Most modern ATMs follow this same model, although the actual protocol has\nbecome more sophisticated and involves a great deal more XML. You can be\nreassured that when the ATM takes a frustratingly long time to advance to the\nnext screen, it is at least waiting to receive the contents of that screen from\na host computer that is some distance away or, even worse, in The Cloud.</p><p>Incidentally, you might wonder about the software that ran on the host computer.\nI believe that the IBM 2984 was designed for use with CICS, the Customer\nInformation Control System. CICS will one day get its own article, but it\nlaunched in 1966, built specifically for the Michigan Bell to manage customer\nand billing data. Over the following years, CICS was extensively expanded for\nuse in the utility and later finance industries. I don't think it's inaccurate\nto call CICS the first \"enterprise customer relationship management system,\"\nthe first voyage in an adventure that took us through Siebel before grounding\non the rocks of Salesforce. Today we wouldn't think of a CRM as the system of\nrecord for depository finance institutions like banks, but CICS itself was\nvery finance-oriented from the start (telephone companies sometimes felt like\naccounting firms that ran phones on the side) and took naturally to gathering\ntransactions and posting them against customer accounts. Since CICS was designed\nas an online system to serve telephone and in-person customer service reps (in\nfact making CICS a very notable early real-time computing system), it was also a\ngood fit for handling ATM requests throughout the day.</p><div><p>I put a lot of time into writing this, and I hope that you enjoy reading\nit. If you can spare a few dollars, consider <a href=\"https://ko-fi.com/jbcrawford\">supporting me on\nko-fi</a>. You'll receive an occasional extra,\nsubscribers-only post, and defray the costs of providing artisanal, hand-built\nworld wide web directly from Albuquerque, New Mexico.</p></div><p>Despite the 2984's lackluster success, IBM moved on. I don't think IBM was\nparticularly surprised by the outcome, the 2984 was always a \"request quotation\"\n(e.g. custom) product. IBM probably regarded it as a prototype or pilot with\ntheir friendly customer Lloyds Bank. More than actual deployment, the 2984's\nachievement was paving the way for the IBM 3614 Consumer Transaction Facility.</p><p>In 1970, IBM had replaced the System/360 line with the System/370. The 370 is\ndirectly based on the 360 and uses the same instruction set, but it came with\nnumerous improvements. Among them was a new approach to peripheral connectivity\nthat developed into the IBM Systems Network Architecture, or SNA, basically\nIBM's entry into the computer networking wars of the 1970s and 1980s. While SNA\nwould ultimately cede to IP (with, naturally, an interregnum of SNA-over-IP),\nit gave IBM the foundations for networked systems that are  modern in\ntheir look and feel.</p><p>I say  because SNA was still very much a mainframe-oriented design. An\nexample SNA network might look like this: An S/370 computer running CICS (or\none of several other IBM software packages with SNA support) is connected via\nchannel (the high-speed peripheral bus on mainframe computers, analogous to PCI)\nto an IBM 3705 Communications Controller running the Network Control Program\n(analogous to a network interface controller). The 3705 had one or more\n\"scanners\" installed, which supported simple low-speed serial lines or fast,\nhigh-level protocols like SDLC (synchronous data link control) used by SNA. The\n3705 fills a role sometimes called a \"front-end processor,\" doing the grunt work\nof polling (scanning) communications lines and implementing the SDLC protocol\nso that the \"actual computer\" was relieved of these menial tasks.</p><p>At the other end of one of the SDLC links might be an IBM 3770 Data\nCommunications System, which was superficially a large terminal that, depending\non options ordered, could include a teletypewriter, card reader and punch,\ndiskette drives, and a high speed printer. Yes, the 3770 is basically a grown-up\nremote job entry terminal, and the SNA/SDLC stack was a direct evolution from\nthe Bisync stack used by the 2984. The 3770 had a bit more to offer, though:\nin order to handle its multiple devices, like the printer and card punch, it\nacted as a sort of network switch—the host computer identified the 3770's\ndevices as separate endpoints, and the 3770 interleaved their respective\ntraffic. It could also perform that interleaving function for additional\nperipherals connected to it by serial  lines, which depending on customer\nrequirements often included additional card punches and readers for data entry,\nor line printers for things like warehouse picking slips.</p><p>In 1973, IBM gave banks the SNA treatment with the 3600 Finance Communication\nSystem . A beautifully orange brochure tells us:</p><blockquote><p>The IBM 3600 Finance Communication System is a family of products designed to\nprovide the Finance Industry with remote on-line teller station operation.</p></blockquote><p>System/370 computers represented an enormous investment, generally around a\nmillion dollars and more often above that point than below. They were also large\nand required both infrastructure and staff to support them. Banks were already\nnot inclined to install an S/370 in each branch, so it became a common pattern\nto place a \"full-size\" computer like an S/370 in a central processing center to\nsupport remote peripherals (over leased telephone line) in branches. The 3600\nwas a turn-key product line for exactly this use.</p><p>An S/370 computer with a 3704 or 3705 running the NCP would connect (usually\nover a leased line) to a 3601 System, which IBM describes as a\n\"programmable communications controller\" although they do not seem to have\nelevated that phrase to a product name. The 3601 is basically a minicomputer of\nits own, with up to 20KB of user-available memory and diskette drive. A 3601\nincludes, as standard, a 9600 bps SDLC modem for connection to the host, and a\n9600 bps \"loop\" interface for a local multidrop serial bus. For larger\ninstallations, you could expand a 3601 with additional local loop interfaces or\n4800 or 9600 bps modems to extend the local loop interface to a remote location\nvia telephone line.</p><p>In total, a 3601 could interface up to five peripheral loops with the host\ncomputer over a single interleaved SDLC link. But what would you put on those\nperipheral loops? Well, the 3604 Keyboard Display Unit was the mainstay, with\na vacuum fluorescent display and choice of \"numeric\" (accounting, similar to a\ndesk calculator) or \"data entry\" (alphabetic) keyboard. A bank would put one of\nthese 3604s in front of each teller, where they could inquire into customer\naccounts and enter transactions. In the meantime, 3610 printers provided\ngeneral-purpose document printing capability, including back-office journals\n(logging all transactions) or filling in pre-printed forms such as receipts\nand bank checks. Since the 3610 was often used as a journal printer, it was\navailable with a take-up roller that stored the printed output under a locked\ncover. In fact, basically every part of the 3600 system was available with a\nkey switch or locking cover, a charming reminder of the state of computer\nsecurity at the time.</p><p>The 3612 is a similar printer, but with the addition of a\ndedicated passbook feature. Remember passbook savings accounts, where the bank\nwrites every transaction in a little booklet that the customer keeps? They were\nstill around, although declining in use, in the 1970s. The 3612 had a slot on\nthe front where an appropriately formatted passbook could be inserted, and like\na check validator or slip printer, it printed the latest transaction onto the\nnext empty line. Finally, the 3618 was a \"medium-speed\" printer, meaning 155 lines per minute.\nA branch bank would probably have one, in the back office, used for printing\ndaily closing reports and other longer \"administrative\" output.</p><p>A branch bank could carry out all of its routine business through the 3600\nsystem, including cash withdrawals. In fact, since a customer withdrawing cash\nwould end up talking to a teller who simply keyed the transaction into a 3604,\nit seems like a little more automation could make an ATM part of the system.</p><p>Enter the 3614 Consumer Transaction Facility, the first IBM ATM available as a\nregular catalog item. The 3614 is actually fairly obscure, and doesn't seem to\nhave sold in large numbers. Some sources suggest that it was basically the same\nas the 2984, but with a general facelift and adaptations to connect to a 3601 Finance Communication\nController instead of directly to a front-end processor. Some features which\nwere optional on the 2984, like a deposit slot, were apparently standard on 3614.\nI'm not even quite sure when the 3614 was introduced, but based on manual\ncopyright dates they must have been around by 1977.</p><p>One of the reasons the 3614 is obscure is that its replacement, the IBM 3624\nConsumer Transaction Facility, hit the market in 1978—probably very shortly\nafter the 3614. The 3624 was functionally very similar to the 3614, but with\nmaintainability improvements like convenient portable cartridges for storing\ncash. It also brought a completely redesigned front panel that is more similar\nto modern ATMs. I should talk about the front panels—the IBM ATMs won a few\ndesign awards over their years, and they were really very handsome machines.\nThe backlit logo panel and function-specific keys of the 3624 look more pleasant\nto use than most modern ATMs, although they would of course render translation\ndifficult.</p><p>The 3614/3624 series established a number of conventions that are still in use\ntoday. For example, they added an envelope deposit system in which the machine\naccepted an envelope (with cash or checks) and printed a transaction identifier\non the outside of the envelope for lookup at the processing center. This\nrelieved the user of writing up a deposit slip when using the ATM. It was also\ncapable of not only reading but, optionally, writing to the magnetic strips on\nATM cards. To the modern reader that sounds strange, but we have to discuss one\nof the most enduring properties of the 3614/3624: their handling of PIN numbers.</p><p>I believe the 2984 did something fairly similar, but the details are now obscure\n(and seem to get mixed up with its use of LUCIFER/DSD-1/DES for communications).\nThe 3614/3624, though, so firmly established a particular approach to PIN\nnumbers that it is now known as the 3624 algorithm. Here's how it works: the\nATM reads the card number (called Primary Account Number or PAN) off of the\nATM card, reads a key from memory, and then applies a convoluted cryptographic\nalgorithm to calculate an \"intermediate PIN\" from it. The \"intermediate pin\"\nis then summed with a \"PIN offset\" stored on the card itself, modulo 10, to\nproduce the PIN that the user is actually expected to enter. This means that\nyour \"true\" PIN is a static value calculated from your card number and a key,\nbut as a matter of convenience, you can \"set\" a PIN of your choice by using an\nATM that is equipped to rewrite the PIN offset on your card. This same system,\nwith some tweaks and a lot of terminological drift, is still in use today. You\nwill sometimes hear IBM's intermediate PIN called the \"natural PIN,\" the one\nyou get with an offset of 0, which is a use of language that I find charming.</p><p>Another interesting feature of the 3624 was a receipt printer—I'm not sure if it\nwas the first ATM to offer a receipt, but it was definitely an early one. The\nexact mechanics of the 3624 receipt printer are amusing and the result of some\nhappenstance at IBM. Besides its mainframes and their peripherals, IBM in the\n1970s was was increasingly invested in \"midrange computers\" or \"midcomputers\"\nthat would fill in a space between the mainframe and minicomputer—and, most\nimportantly, make IBM more competitive with the smaller businesses that could\nnot afford IBM's mainframe systems and were starting to turn to competitors like\nDEC as a result. These would eventually blossom into the extremely successful\nAS/400 and System i, but not easily, and the first few models all suffered from\ndecidedly soft sales.</p><p>For these smaller computers, IBM reasoned that they needed to offer peripherals\nlike card punches and readers that were also smaller. Apparently following that\nline of thought to a misguided extent, IBM also designed a smaller punch card:\nthe 96-column three-row card, which was nearly square. The only computer ever\nto support these cards was the very first of the midrange line, the 1969\nSystem/3. One wonders if the System/3's limited success lead to excess stock of\n96-column card equipment, or perhaps they just wanted to reuse tooling. In any\ncase, the oddball System/3 card had a second life as the \"Transaction Statement\nPrinter\" on the 3614 and 3624. The ATM could print four lines of text, 34\ncharacters each, onto the middle of the card. The machines didn't actually punch\nthem, and the printed text ended up over the original punch fields. You could,\nif you wanted, actually order a 3624 with two printers: one that presented the\nslip to the customer, and another that retained it internally for bank auditing.\nA curious detail that would so soon be replaced by thermal receipt printers.</p><p>Unlike IBM's ATMs before it, and, as we will see, unlike those after it as well,\nthe 3624 was a hit. While IBM never enjoyed the dominance in ATMs that they did\nin computers, and companies like NCR and Diebold had substantial market\nshare, the 3624 was widely installed in the late 1970s and would probably be\nrecognized by anyone who was withdrawing cash in that era. The machine had\ntechnical leadership as well: NCR built their successful ATM line in part by\nduplicating aspects of the 3624 design, allowing interoperability with IBM\nbackend systems. Ultimately, as so often happens, it may have been IBM's success\nthat became its undoing.</p><p>In 1983, IBM completely refreshed their branch banking solution with the 4700\nFinance Communication System. While architecturally similar, the 4700 was a big\nupgrade. For one, the CRT had landed: the 4700 peripherals replaced several-line\nVFDs with full-size CRTs typical of other computer terminals, and conventional\ncomputer keyboards to boot. Most radically, though, the 4700 line introduced\n<em>distributed communications</em> to IBM's banking offerings. The 4701 Communications\nController was optionally available with a hard disk, and could be programmed\nin COBOL. Disk-equipped 4701s could operate offline, without a connection to the\nhost, or in a hybrid mode in which they performed some transactions locally and\nonly contacted the host system when necessary. Local records kept by the 4701\ncould be automatically sent to the host computer on a scheduled basis for\nreconciliation.</p><p>Along with the 4700 series came a new ATM: the IBM 473x Personal Banking\nMachines. And with that, IBM's glory days in ATMs came crashing to the ground.\nThe 473x series was such a flop that it is hard to even figure out the model\nnumbers, the 4732 is most often referenced but others clearly existed, including\nthe 4730, 4731, 4736, 4737, and 4738. These various models were introduced from\n1983 to 1988, making up almost a decade of IBM's efforts and very few sales.\nThe 4732 had a generally upgraded interface, including a CRT, but a similar\nfeature set—unsurprising, given that the 3724 had already introduced most of the\nfeatures ATMs have today. It also didn't sell. I haven't been able to find any\nnumbers, but the trade press referred to the 4732 with terms like\n\"debacle,\" so they couldn't have been great.</p><p>There were a few faults in the 4732's stars. First, IBM had made the decision to\nhandle the 4700 Finance Communication System as a complete rework of the 3600.\nThe 4700 controllers could support some 3600 peripherals, but 4700 peripherals\ncould  be used with 3600 controllers. Since 3600 systems were widely\ninstalled in banks, the compatibility choice created a situation where many of\nthe 4732's prospective buyers would end up having to replace a significant\namount of their other equipment, and then likely make software changes, in order\nto support the new machine. That might not have been so bad on its own had IBM's\ncompetitors not provided another way out.</p><p>NCR made their fame in ATMs in part by equipping their contemporary models with\n3624 software emulation, making them a drop-in modernization option for existing\n3600 systems. In general, other ATM manufacturers had pursued a path of\ninteroperability, with multiprotocol ATMs that supported multiple hosts, and\nstandalone ATM host products that could interoperate with multiple backend\naccounting systems. For customers, buying an NCR or Diebold product that would\nwork with whatever they already used was a more appealing option than buying the\nentire IBM suite in one go. It also matched the development cycle of ATMs\nbetter: as a consumer-facing device, ATMs became part of the brand image of the\nbank, and were likely to see replacement more often than back-office devices\nlike teller terminals. NCR offered something like a regular refresh, while IBM\nwas still in a mode of generational releases that would completely replace the\nbank's computer systems.</p><p>The 4732 and its 473x compatriots became the last real IBM ATMs. After a hiatus\nof roughly a decade, IBM reentered the ATM market by forming a joint venture\nwith Diebold called InterBold. The basic terms were that Diebold would sell its\nATMs in the US, and IBM would sell them overseas, where IBM had generally been\nthe more successful of the two brands. The IBM 478x series ATMs, which you might\nencounter in the UK for example, are the same as the Diebold 1000 series in the\nUS. InterBold was quite successful, becoming the dominant ATM manufacturer in\nthe US, and in 1998 Diebold bought out IBM's share.</p><p>IBM had won the ATM market, and then lost it. Along the way, they left us with\nso much texture: DES's origins in the ATM, the 3624 PIN format, the dumb\nterminal or thin client model... even InterBold, IBM's protracted exit, gave us\nquite a legacy: now you know the reason that so many later ATMs ran OS/2. IBM,\na once great company, provided Diebold with their once great operating system.\nUnlike IBM, Diebold made it successful.</p>",
      "contentLength": 42377,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47190806"
    },
    {
      "title": "OpenAI reaches deal to deploy AI models on U.S. DoW classified network",
      "url": "https://www.reuters.com/business/openai-reaches-deal-deploy-ai-models-us-department-war-classified-network-2026-02-28/",
      "date": 1772248996,
      "author": "erhuve",
      "guid": 49064,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47189853"
    },
    {
      "title": "Don't use passkeys for encrypting user data",
      "url": "https://blog.timcappalli.me/p/passkeys-prf-warning/",
      "date": 1772248300,
      "author": "zdw",
      "guid": 49063,
      "unread": true,
      "content": "<p>Why am I writing this today?\nBecause I am deeply concerned about users losing their most sacred data.</p><p>Over the past year or two, I’ve seen many organizations, large and small, implement passkeys (which is great, thank you!) and use the PRF (Pseudo-Random Function) extension to derive keys to protect user data, typically to support end-to-end encryption (including backups).\nI’ve also seen a number of influential folks and organizations promote the use of PRF for encrypting data.</p><p>The primary use cases I’ve seen implemented or promoted so far include:</p><ul><li>encrypting message backups (including images and videos)</li><li>encrypting documents and other files</li><li>encrypting and unlocking crypto wallets</li><li>credential manager unlocking</li></ul><p>When you overload a credential used for authentication by also using it for encryption, the “blast radius” for losing that credential becomes immeasurably larger.</p><p>Imagine a user named Erika. They are asked to set up encrypted backups in their favorite messaging app because they don’t want to lose their messages and photos, especially those of loved ones who are no longer here.\nErika is prompted to use their passkey to enable these backups.</p><p>There is nothing in the UI that emphasizes that these backups are now tightly coupled to their passkey. Even if there were explanatory text, Erika, like most users, doesn’t typically read through every dialog box, and they certainly can’t be expected to remember this technical detail a year from now.</p><p>A few months pass, and Erika decides to clean up their credential manager. They don’t remember why they had a specific passkey for a messaging app and deletes it.</p><p>Fast forward a year: they get a new phone and set up the messaging app. They aren’t prompted to use a passkey because one no longer exists in their credential manager. Instead, they use phone number verification to recover their account. They are then guided through the “restore backup” flow and prompted for their passkey.</p><p>Since they no longer have it, they are informed that they cannot access their backed up data. Goodbye, memories.</p><p>Here’s a few examples of what a user sees when they delete a passkey:</p><p>How is a user supposed to understand that they are potentially blowing away photos of deceased relatives, an encrypted property deed, or their digital currency?</p><p><strong>We cannot, and should not, expect users to know this.</strong></p><p>At this point, you may be asking why PRF is part of WebAuthn in the first place.\nThere are some very legitimate and more durable uses of PRF in WebAuthn, specifically supporting credential managers and operating systems.</p><p>A passkey with PRF can make unlocking your credential manager (where all of your other passkeys and credentials are stored) much faster and more secure.\nCredential managers have robust mechanisms to protect your vault data with multiple methods, such as master passwords, per-device keys, recovery keys, and social recovery keys.\nLosing access to a passkey used to unlock your credential manager rarely leads to complete loss of your vault data.</p><p>PRF is already implemented in WebAuthn Clients and Credential Managers, so the cat is out of the bag. My asks:</p><ul><li><p><strong>To the wider identity industry</strong>: <strong><em>please stop promoting and using passkeys to encrypt user data. I’m begging you. Let them be great, phishing-resistant authentication credentials.</em></strong></p></li><li><p>To <strong>sites and services using passkeys</strong>: if you still need to use PRF knowing these concerns, please:</p></li></ul><p>(and thanks to <a href=\"https://blog.millerti.me/\" target=\"_blank\" rel=\"noreferrer\">Matthew Miller</a> for reviewing and providing feedback on this post)</p>",
      "contentLength": 3492,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47189749"
    },
    {
      "title": "OpenAI agrees with Dept. of War to deploy models in their classified network",
      "url": "https://twitter.com/sama/status/2027578652477821175",
      "date": 1772247542,
      "author": "eoskx",
      "guid": 49055,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47189650"
    },
    {
      "title": "Croatia declared free of landmines after 31 years",
      "url": "https://glashrvatske.hrt.hr/en/domestic/croatia-declared-free-of-landmines-after-31-years-12593533",
      "date": 1772246896,
      "author": "toomuchtodo",
      "guid": 49062,
      "unread": true,
      "content": "<div><p>Interior Minister Davor Božinović announced Friday that Croatia is officially free of landmines. Thirty-one years after the end of the Homeland War, all known minefields have been cleared — a major milestone for the country.</p></div><div><p>The decades-long effort came at a heavy cost. Over three decades of painstaking and dangerous work, 208 people lost their lives, including 41 deminers. The total cost of clearing the country is estimated at around 1.2 billion euros.\n</p><p>“Croatia is free of land mines. After nearly 30 years, we have completed demining in accordance with the Ottawa Convention,” Božinović said during an event marking International Civil Protection Day in Zagreb.\n</p><p>He added, “Almost 107,000 mines and 407,000 pieces of unexploded ordnance have been removed. This is not just a technical success — it is the fulfillment of a moral obligation to the victims of mines and their families. A mine-free Croatia means safer families, better development of rural areas, more farmland, and stronger tourism.”\n</p></div><p>Vijesti HRT-a pratite na svojim pametnim telefonima i tabletima putem aplikacija za <a href=\"https://apps.apple.com/hr/app/hrtvijesti/id1457183989?l=hr\">iOS </a>i <a href=\"https://play.google.com/store/apps/details?id=hr.hrt.vijesti\">Android</a>. Pratite nas i na društvenim mrežama<a href=\"https://www.facebook.com/HRTvijesti/\"> Facebook</a>,<a href=\"https://twitter.com/hrtvijesti\"> Twitter</a>,<a href=\"https://www.instagram.com/hrvatska_radiotelevizija/\"> Instagram</a>,<a href=\"https://www.tiktok.com/@hrvatska_radiotelevizija\"> TikTok</a> i<a href=\"https://www.youtube.com/user/HRTnovimediji\"> YouTube</a>!</p>",
      "contentLength": 1202,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47189535"
    },
    {
      "title": "Statement on the comments from Secretary of War Pete Hegseth",
      "url": "https://www.anthropic.com/news/statement-comments-secretary-war",
      "date": 1772241610,
      "author": "surprisetalk",
      "guid": 49032,
      "unread": true,
      "content": "<p>Earlier today, Secretary of War Pete Hegseth <a href=\"https://x.com/SecWar/status/2027507717469049070?s=20\">shared on X</a> that he is directing the Department of War to designate Anthropic a supply chain risk. This action follows months of negotiations that reached an impasse over <a href=\"https://www.anthropic.com/news/statement-department-of-war\">two exceptions</a> we requested to the lawful use of our AI model, Claude: the mass domestic surveillance of Americans and fully autonomous weapons.</p><p>We have not yet received direct communication from the Department of War or the White House on the status of our negotiations.</p><p>We have tried in good faith to reach an agreement with the Department of War, making clear that we support all lawful uses of AI for national security aside from the two narrow exceptions above. To the best of our knowledge, these exceptions have not affected a single government mission to date.</p><p>We held to our exceptions for two reasons. First, we do not believe that today’s frontier AI models are reliable enough to be used in fully autonomous weapons. Allowing current models to be used in this way would endanger America’s warfighters and civilians. Second, we believe that mass domestic surveillance of Americans constitutes a violation of fundamental rights.</p><p>Designating Anthropic as a supply chain risk would be an unprecedented action—one historically reserved for US adversaries, never before publicly applied to an American company. We are deeply saddened by these developments. As the first frontier AI company to deploy models in the US government’s classified networks, Anthropic has supported American warfighters since June 2024 and has every intention of continuing to do so.</p><p>We believe this designation would both be legally unsound and set a dangerous precedent for any American company that negotiates with the government.</p><p>No amount of intimidation or punishment from the Department of War will change our position on mass domestic surveillance or fully autonomous weapons. We will challenge any supply chain risk designation in court.</p><p><strong>What this means for our customers</strong></p><p>Secretary Hegseth has implied this designation would restrict anyone who does business with the military from doing business with Anthropic. The Secretary does not have the statutory authority to back up this statement. Legally, a supply chain risk designation under <a href=\"https://uscode.house.gov/view.xhtml?req=granuleid:USC-prelim-title10-section3252&amp;num=0&amp;edition=prelim\">10 USC 3252</a> can only extend to the use of Claude as part of Department of War contracts—it cannot affect how contractors use Claude to serve other customers.</p><ul><li><strong>If you are an individual customer or hold a commercial contract with Anthropic</strong>, your access to Claude—through our API, claude.ai, or any of our products—is completely unaffected.</li><li><strong>If you are a Department of War contractor</strong>, this designation—if formally adopted—would only affect your use of Claude on Department of War contract work. Your use for any other purpose is unaffected.</li></ul><p>Our sales and <a href=\"https://support.claude.com/en/\">support</a> teams are standing by to answer any questions you may have.</p><p>We are deeply grateful to our users, and to the industry peers, policymakers, veterans, and members of the public who have voiced their support in recent days. Thank you. Above all else, our priorities are to protect our customers from any disruption caused by these extraordinary events and to work with the Department of War to ensure a smooth transition—for them, for our troops, and for American military operations.</p>",
      "contentLength": 3292,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47188697"
    },
    {
      "title": "We Will Not Be Divided",
      "url": "https://notdivided.org/",
      "date": 1772240093,
      "author": "BloondAndDoom",
      "guid": 49029,
      "unread": true,
      "content": "<h2>Frequently Asked Questions</h2>",
      "contentLength": 26,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47188473"
    },
    {
      "title": "Qt45: A small polymerase ribozyme that can synthesize itself",
      "url": "https://www.science.org/doi/10.1126/science.adt2760",
      "date": 1772235734,
      "author": "ppnpm",
      "guid": 49069,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47187649"
    },
    {
      "title": "I am directing the Department of War to designate Anthropic a supply-chain risk",
      "url": "https://twitter.com/secwar/status/2027507717469049070",
      "date": 1772231478,
      "author": "jacobedawson",
      "guid": 48985,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47186677"
    },
    {
      "title": "President Trump bans Anthropic from use in government systems",
      "url": "https://www.npr.org/2026/02/27/nx-s1-5729118/trump-anthropic-pentagon-openai-ai-weapons-ban",
      "date": 1772228440,
      "author": "pkress2",
      "guid": 49054,
      "unread": true,
      "content": "<div><div><div><div aria-label=\"Image caption\"><p>\n                The Pentagon is seen from an airplane, Monday, Feb. 2, 2026, in Washington.\n                <b aria-label=\"Image credit\">\n                    \n                    Julia Demaree Nikhinson/Associated Press\n                    \n                </b></p></div></div></div></div><p>President Trump ordered the U.S. government to stop using the artificial intelligence company Anthropic's products and the Pentagon moved to designate the company a national security risk on Friday, in a sharp escalation of a high-stakes fight over the military's use of AI.</p><p>Hours after the president's announcement, rival company OpenAI said it had struck a deal with the Defense Department to provide its own AI technology for classified networks.</p><p>The administration's decisions cap an acrimonious <a href=\"https://www.npr.org/2026/02/24/nx-s1-5725327/pentagon-anthropic-hegseth-safety\" target=\"_blank\">dispute</a> between Anthropic and the Pentagon over whether the company could prohibit its tools from being used in mass surveillance of American citizens or to power autonomous weapon systems, as part of a military contract worth up to $200 million.</p><p>\"The Leftwing nut jobs at Anthropic have made a DISASTROUS MISTAKE trying to STRONG-ARM the Department of War, and force them to obey their Terms of Service instead of our Constitution,\" Trump wrote in a Truth Social <a href=\"https://truthsocial.com/@realDonaldTrump/posts/116144552969293195\" target=\"_blank\">post</a>. \"Therefore, I am directing EVERY Federal Agency in the United States Government to IMMEDIATELY CEASE all use of Anthropic's technology. We don't need it, we don't want it, and will not do business with them again!\"</p><p>He said there would be a six-month phaseout of Anthropic's products.</p><p>Trump's announcement came about an hour before a deadline set by the Pentagon, which had called on Anthropic to back down. Shortly after the deadline passed, Defense Secretary Pete Hegseth said he was labeling Anthropic a supply chain risk to national security, <a href=\"https://www.npr.org/2026/02/24/nx-s1-5725327/pentagon-anthropic-hegseth-safety\" target=\"_blank\">blacklisting</a> it from working with the U.S. military or contractors.</p><p>\"In conjunction with the President's directive for the Federal Government to cease all use of Anthropic's technology, I am directing the Department of War to designate Anthropic a Supply-Chain Risk to National Security. Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic,\" Hegseth <a href=\"https://x.com/SecWar/status/2027507717469049070?s=20\" target=\"_blank\">posted on X</a> , using the Pentagon's \"Department of War\" <a href=\"https://www.npr.org/2025/09/04/nx-s1-5529420/trump-department-of-war-department-of-defense\" target=\"_blank\">rebranding</a>. \"Anthropic will continue to provide the Department of War its services for a period of no more than six months to allow for a seamless transition to a better and more patriotic service.\"</p><p>Anthropic said it would challenge the supply chain risk designation in court.</p><p>\"We believe this designation would both be legally unsound and set a dangerous precedent for any American company that negotiates with the government,\" the company said in a <a href=\"https://www.anthropic.com/news/statement-comments-secretary-war\" target=\"_blank\">statement</a> on Friday evening.</p><p>Anthropic also challenged Hegseth's comments that anyone who does business with the U.S. military would have to cut off all business with Anthropic. \"The Secretary does not have the statutory authority to back up this statement,\" the company said. Under federal law, it said, designating Anthropic a supply chain risk would only apply to \"the use of Claude as part of Department of War contracts—it cannot affect how contractors use Claude to serve other customers.\"</p><p>The company said it had \"tried in good faith\" to reach an agreement with the Pentagon over months of negotiations, \"making clear that we support all lawful uses of AI for national security aside from the two narrow exceptions\" being disputed. \"To the best of our knowledge, these exceptions have not affected a single government mission to date,\" Anthropic said.</p><p>It said its objections to those uses were rooted in two reasons: \"First, we do not believe that today's frontier AI models are reliable enough to be used in fully autonomous weapons. Allowing current models to be used in this way would endanger America's warfighters and civilians. Second, we believe that mass domestic surveillance of Americans constitutes a violation of fundamental rights.\"</p><p>In a <a href=\"https://x.com/sama/status/2027578652477821175?s=20\" target=\"_blank\">post on X </a>announcing competitor OpenAI's deal with the Defense Department, the company's CEO Sam Altman, who previously cited similar concerns, said his agreement with the government included safeguards like the ones Anthropic had asked for. </p><p>\"Two of our most important safety principles are prohibitions on domestic mass surveillance and human responsibility for the use of force, including for autonomous weapon systems,\" he said. \"The DoW agrees with these principles, reflects them in law and policy, and we put them into our agreement.\"</p><h3>Ban comes as Anthropic plans an IPO</h3><p>Defense Department officials had given Anthropic a <a href=\"https://www.npr.org/2026/02/26/nx-s1-5727847/anthropic-defense-hegseth-ai-weapons-surveillance\" target=\"_blank\">deadline</a> of 5:01 p.m. ET on Friday to drop restrictions on its AI model, Claude, from being used for domestic mass surveillance or entirely autonomous weapons, or face losing its contract. The Pentagon has said it doesn't intend to use AI in those ways, but requires AI companies to allow their models to be used \"for all lawful purposes.\"</p><p>The government had also threatened to invoke the Korean War-era Defense Production Act  to compel Anthropic to allow use of its tools and, at the same time, warned it would label Anthropic a supply chain risk.</p><p>In his post carrying out the latter threat, Hegseth said Anthropic had \"delivered a master class in arrogance and betrayal as well as a textbook case of how not to do business with the United States Government or the Pentagon.\" He accused the company of trying to \"seize veto power over the operational decisions of the United States military.\"</p><p>He said the department would not waver from its position: \"the Department of War must have full, unrestricted access to Anthropic's models for every LAWFUL purpose in defense of the Republic.\"</p><p>\"America's warfighters will never be held hostage by the ideological whims of Big Tech. This decision is final,\" Hegseth concluded.</p><p>The government ban comes at a time when Anthropic is under heightened scrutiny, since the company, which is valued at <a href=\"https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation\" target=\"_blank\">$380 billion</a>, is planning to go public this year.</p><p>While the Pentagon contract worth as much as $200 million is a relatively small portion of Anthropic's $14 billion in revenue, it's unclear how the friction with the administration will sit with investors or affect other deals the company has to license its AI model to non-government partners.</p><p>Anthropic CEO Dario Amodei has pointed out that the company's valuation and revenue have only grown since it took a stand against Trump officials over how AI can be deployed on the battlefield.</p><p>Whether AI companies can set restrictions on how the government uses their technology has emerged as a major sticking point in recent months between Anthropic and the Trump administration.</p><p>On Thursday, Amodei said the company would not budge in the face of the Pentagon's threats. \"We cannot in good conscience accede to their request,\" he wrote in a lengthy <a href=\"https://www.anthropic.com/news/statement-department-of-war\" target=\"_blank\">statement</a>.</p><div><div><div><div aria-label=\"Image caption\"><p>\n                A 2024 file photo of Dario Amodei, CEO and cofounder of Anthropic.\n                <b aria-label=\"Image credit\">\n                    \n                    Jeff Chiu/Associated Press\n                    \n                </b></p></div></div></div></div><p>\"Anthropic understands that the Department of War, not private companies, makes military decisions. We have never raised objections to particular military operations nor attempted to limit use of our technology in an  manner,\" he said. But, he added, domestic mass surveillance and fully autonomous weapons are uses that are \"simply outside the bounds of what today's technology can safely and reliably do.\"</p><p>Emil Michael, the Pentagon's undersecretary for research and engineering, shot back in a <a href=\"https://x.com/USWREMichael/status/2027211708201058578\" target=\"_blank\">post on X</a> on Thursday, accusing Amodei of lying and having a \"God-complex.\"</p><p>\"He wants nothing more than to try to personally control the US Military and is ok putting our nation's safety at risk,\" Michael wrote. \"The @DeptofWar will ALWAYS adhere to the law but not bend to whims of any one for-profit tech company,\" he wrote.</p><p>In an late Thursday interview with <a href=\"https://www.cbsnews.com/news/pentagon-anthropic-feud-ai-military-says-it-made-compromises/\" target=\"_blank\">CBS News</a>, Michael said federal law and Pentagon policies already bar the use of AI for domestic mass surveillance and autonomous weapons.\"</p><p>\"At some level, you have to trust your military to do the right thing,\" he said.</p><h3>OpenAI expressed similar concerns</h3><p>OpenAI CEO Altman had said earlier on Friday that he shared Anthropic's \"red lines\" restricting military use of AI.</p><p>OpenAI, Google, and Elon Musk's xAI also have Defense Department contracts and have agreed to allow their AI tools to be used in any \"lawful\" scenarios. Earlier this week, xAI became the second company after Anthropic to be approved for use in classified settings.</p><p>Altman told <a href=\"https://www.cnbc.com/amp/2026/02/27/first-on-cnbc-cnbc-transcript-amazon-ceo-andy-jassy-and-openai-ceo-sam-altman-speak-with-cnbcs-andrew-ross-sorkin-on-squawk-box-today.html\" target=\"_blank\">CNBC</a> on Friday morning that it's important for companies to work with the military \"as long as it is going to comply with legal protections\" and \"the few red lines\" that \"we share with Anthropic and that other companies also independently agree with.\"</p><div><div><div><div aria-label=\"Image caption\"><p>\n                Sam Altman, co-founder and CEO of OpenAI, testifying before a Senate committee in 2025.\n                <b aria-label=\"Image credit\">\n                    \n                    Jose Luis Magana/Associated Press\n                    \n                </b></p></div></div></div></div><p>In an internal note sent to staff on Thursday evening, Altman said OpenAI was seeking to negotiate a deal with the Pentagon to deploy its models in classified systems with exclusions preventing use for surveillance in the U.S. or to power autonomous weapons without human approval, according to a person familiar with the message who was not authorized to speak publicly. The <a href=\"https://www.wsj.com/tech/ai/openais-sam-altman-calls-for-de-escalation-in-anthropic-showdown-with-hegseth-03ecbac8\" target=\"_blank\"></a> first reported Altman's note to staff.</p><p>The Defense Department didn't respond to a request for comment on Altman's statements.</p><p>Independent experts say the standoff is highly unusual in the world of Pentagon contracting.</p><p>\"This is different for sure,\" said Jerry McGinn, director of the Center for the Industrial Base at the Center for Strategic and International Studies, a Washington DC think tank. Pentagon contractors don't usually get to tell the Defense Department how their products and services can be used, he notes \"because otherwise you'd be negotiating use cases for every contract, and that's not reasonable to expect.\"</p><p>At the same time, McGinn noted, artificial intelligence is a new and largely untested technology. \"This is a very unusual, very public fight,\" he said. \"I think it's reflective of the nature of AI.\"</p><p><em>NPR's Bobby Allyn contributed to this report.</em></p>",
      "contentLength": 10256,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47186031"
    },
    {
      "title": "Rob Grant, creator of Red Dwarf, has died",
      "url": "https://www.beyondthejoke.co.uk/content/17193/red-dwarf-rob-grant",
      "date": 1772220398,
      "author": "nephihaha",
      "guid": 48947,
      "unread": true,
      "content": "<p>Tributes have been paid to Rob Grant, the comedy writer best known as the co-creator of long running hit sitcom Red Dwarf. Grant was also one of the main writers on Spitting IMage for many years, writing regularly with Doug Naylor.</p><p>The news was broken by the Red Dwarf fan site, <a href=\"https://www.ganymede.tv/2026/02/rob-grant-rip/\">Ganymede and Titan.</a> (note - at the time of writing the site has gone down due, presumably, to so many fans trying to find out more details).</p><p>Craig Charles, who played Lister posted on X: \"Earlier today I was informed of the passing of </p><p>I’m deeply saddened to hear of Rob Grant’s passing yesterday. It’s hard to take in the loss of someone who was such a significant part of my life for so many years. I first met Rob when we were nine years old. We went to Chetham's School of Music and later Liverpool University. We grew up making each other laugh long before there was an audience, and eventually found ourselves building something that neither of us could have imagined when we were schoolboys.\"</p><p>Spitting Image and later Red Dwarf went on to become two of the most loved comedy series in Britain. I'll always treasure those years of writing together and laughing so hard it hurt. Creative partnerships are intense, driven by passion, conviction and strong personalities. But at the heart of ours was a shared love of comedy and a desire to make people laugh and we did, on a scale neither of us could have predicted. My thoughts are with Rob's wife Kath, and all his family and friends. I will always be grateful for my time working with Rob and what we created together. RIP Smeghead! X&nbsp;<a tabindex=\"0\" role=\"link\" href=\"https://www.instagram.com/explore/tags/reddwarf/\">#reddwarf\"</a></p><p>We are devastated to learn of Rob’s passing and send love to his family and friends. He will always live on through his amazing creativity, storytelling and humour. Travel well, Sir\"</p><p>Red Dwarf emerged out of a sketch on the radio show Son of Cliche, and was a major hit for the BBC, launching in 1988 and making stars out of Craig Charles, Chris Barrie, Robert Llewellyn and Danny John-Jules as well as Hattie Hayridge and Norman Lovett. It was later revived on Dave and continued to be watched by large, devoted audiences.</p><p><em>picture credit: CC BY-SA 2.0</em></p>",
      "contentLength": 2136,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47184480"
    },
    {
      "title": "Leaving Google has actively improved my life",
      "url": "https://pseudosingleton.com/leaving-google-improved-my-life/",
      "date": 1772219305,
      "author": "speckx",
      "guid": 48984,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47184288"
    },
    {
      "title": "Let's discuss sandbox isolation",
      "url": "https://www.shayon.dev/post/2026/52/lets-discuss-sandbox-isolation/",
      "date": 1772218190,
      "author": "shayonj",
      "guid": 48996,
      "unread": true,
      "content": "<p>There is a lot of energy right now around sandboxing untrusted code. AI agents generating and executing code, multi-tenant platforms running customer scripts, RL training pipelines evaluating model outputs—basically, you have code you did not write, and you need to run it without letting it compromise the host, other tenants, or itself in unexpected ways.</p><p>The word “isolation” gets used loosely. A Docker container is “isolated.” A microVM is “isolated.” A WebAssembly module is “isolated.” But these are fundamentally different things, with different boundaries, different attack surfaces, and different failure modes. I wanted to write down my learnings on what each layer actually provides, because I think the distinctions matter and allow you to make informed decisions for the problems you are looking to solve.</p><h2>The kernel is the shared surface</h2><p>When any code runs on Linux, it interacts with the hardware through the kernel via system calls. The Linux kernel exposes roughly 340 syscalls, and the kernel implementation is tens of millions of lines of C code. Every syscall is an entry point into that codebase.</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>Every isolation technique is answering the same question of how to reduce or eliminate the untrusted code’s access to that massive attack surface.</p><p>A useful mental model here is shared state versus dedicated state. Because standard containers share the host kernel, they also share its internal data structures like the TCP/IP stack, the Virtual File System caches, and the memory allocators. A vulnerability in parsing a malformed TCP packet in the kernel affects every container on that host. Stronger isolation models push this complex state up into the sandbox, exposing only simple, low-level interfaces to the host, like raw block I/O or a handful of syscalls.</p><p>The approaches differ in where they draw the boundary. Namespaces use the same kernel but restrict visibility. Seccomp uses the same kernel but restricts the allowed syscall set. Projects like gVisor use a completely separate user-space kernel and make minimal host syscalls. MicroVMs provide a dedicated guest kernel and a hardware-enforced boundary. Finally, WebAssembly provides no kernel access at all, relying instead on explicit capability imports. Each step is a qualitatively different boundary, not just a stronger version of the same thing.</p><h2>Namespaces as visibility walls</h2><p>Linux namespaces wrap global system resources so that processes appear to have their own isolated instance. There are eight types, and each isolates a specific resource.</p><table><thead><tr></tr></thead><tbody><tr><td>Own process tree, starts at PID 1</td></tr><tr><td>Own mount table, can have different root</td></tr><tr><td>Network interfaces, routing</td><td>Own interfaces, IP addresses, ports</td></tr><tr><td>Can be root inside, nobody outside</td></tr><tr><td>SysV IPC, POSIX message queues</td><td>Own shared memory, semaphores</td></tr><tr></tr><tr><td>System clocks (monotonic, boot)</td><td>Own system uptime and clock offsets</td></tr></tbody></table><p>Namespaces are what Docker containers use. When you run a container, it gets its own PID namespace (cannot see host processes), its own mount namespace (own filesystem view), its own network namespace (own interfaces), and so on.</p><p>The critical thing to understand is <strong>namespaces are visibility walls, not security boundaries</strong>. They prevent a process from  things outside its namespace. They do not prevent a process from  that implements the namespace. The process still makes syscalls to the same host kernel. If there is a bug in the kernel’s handling of any syscall, the namespace boundary does not help.</p><p>In January 2024, <a href=\"https://seclists.org/oss-sec/2024/q1/78\">CVE-2024-21626</a> showed that a file descriptor leak in  (the standard container runtime) allowed containers to access the host filesystem. The container’s mount namespace was intact — the escape happened through a leaked fd that  failed to close before handing control to the container. In 2025, three more  CVEs (CVE-2025-31133, CVE-2025-52565, CVE-2025-52881) demonstrated mount race conditions that allowed writing to protected host paths from inside containers.</p><h2>Cgroups: accounting is not security</h2><p>Cgroups (control groups) limit and account for resource usage: CPU, memory, disk I/O, number of processes. They prevent a container from consuming all available memory or spinning up thousands of processes.</p><p>Cgroups are important for stability, but they are not a security boundary. They prevent denial-of-service, not escape. A process constrained by cgroups still makes syscalls to the same kernel with the same attack surface.</p><p>Seccomp-BPF lets you attach a Berkeley Packet Filter program that decides which syscalls a process is allowed to make. You can deny dangerous syscalls like process tracing, filesystem manipulation, kernel extension loading, and performance monitoring.</p><p>Docker applies a default seccomp profile that blocks around 40 to 50 syscalls. This meaningfully reduces the attack surface. But the key limitation is that seccomp is a filter on the same kernel. The syscalls you allow still enter the host kernel’s code paths. If there is a vulnerability in the write implementation, or in the network stack, or in any allowed syscall path, seccomp does not help.</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>The attack surface is smaller. The boundary is the same.</p><h3>Running a container in privileged mode</h3><p>This is worth calling out because it comes up surprisingly often. Some isolation approaches require Docker’s privileged flag. For example, building a custom sandbox that uses nested PID namespaces inside a container often leads developers to use privileged mode, because mounting a new  filesystem for the nested sandbox requires the  capability (unless you also use user namespaces).</p><p>If you enable  just to get  for nested process isolation, you have added one layer (nested process visibility) while removing several others (seccomp, all capability restrictions, device isolation). The net effect is arguably weaker isolation than a standard unprivileged container. This is a real trade-off that shows up in production. The ideal solutions are either to grant only the specific capability needed instead of all of them, or to use a different isolation approach entirely that does not require host-level privileges.</p><h2>gVisor and user-space kernels</h2><p>gVisor is where the isolation model changes qualitatively. To understand the difference, it helps to look at the attack surface of a standard container.</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>The code runs as a standard Linux process. Seccomp acts as a strict allowlist filter, reducing the set of permitted system calls. However, any allowed syscall still executes directly against the shared host kernel. Once a syscall is permitted, the kernel code processing that request is the exact same code used by the host and every other container. The failure mode here is that a vulnerability in an allowed syscall lets the code compromise the host kernel, bypassing the namespace boundaries.</p><p>Instead of filtering syscalls to the host kernel, gVisor interposes a completely separate kernel implementation called the Sentry between the untrusted code and the host. The Sentry does not access the host filesystem directly; instead, a separate process called the Gofer handles file operations on the Sentry’s behalf, communicating over a restricted protocol. This means even the Sentry’s own file access is mediated.</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>The Sentry intercepts the untrusted code’s syscalls and handles them in user-space. It reimplements around 200 Linux syscalls in Go, which is enough to run most applications. When the Sentry actually needs to interact with the host to read a file, it makes its own highly restricted set of roughly 70 host syscalls. This is not just a smaller filter on the same surface; it is a completely different surface. The failure mode changes significantly. An attacker must first find a bug in gVisor’s Go implementation of a syscall to compromise the Sentry process, and then find a way to escape from the Sentry to the host using only those limited host syscalls.</p><p>The Sentry intercepts syscalls using one of several mechanisms, such as seccomp traps or KVM, with the default since 2023 being the seccomp-trap approach known as systrap.</p><p>What this means in practice is that if someone discovers a bug in the Linux kernel’s I/O implementation, containers using Docker are directly exposed. A gVisor sandbox is not, because those syscalls are handled by the Sentry, and the Sentry does not expose them to the host kernel.</p><p>The trade-off is performance. Every syscall goes through user-space interception, which adds overhead. I/O-heavy workloads feel this the most. For short-lived code execution like scripts and tests, it is usually fine, but for sustained high-throughput I/O, it can matter.</p><p>Also, by adopting gVisor, you are betting that it’s easier to audit and maintain a smaller footprint of code (the Sentry and its limited host interactions) than to secure the entire massive Linux kernel surface against untrusted execution. That bet is not free of risk, gVisor itself has had security vulnerabilities in the Sentry but the surface area you need to worry about is drastically smaller and written in a memory-safe language.</p><h2>Defense in depth on top of gVisor</h2><p>gVisor gives you the user-space kernel boundary. What it does not give you automatically is multi-job isolation within a single gVisor sandbox. If you are running multiple untrusted executions inside one  container, you still need to layer additional controls. Here is one pattern for doing that:</p><ul><li><strong>Per-job PID + mount + IPC namespaces</strong> via  — so each execution is isolated from other executions inside the same gVisor sandbox</li><li><strong>Seccomp-BPF inside the namespace</strong> — blocking syscalls like  (preventing nested namespace escape),  (force fallback to ), , kernel module loading</li><li> — run as  (UID 65534) with </li><li> for all writable paths — cleanup is a single  syscall, not a recursive directory walk</li><li><strong>Read-only root filesystem</strong> — the container itself is immutable</li><li><strong>Capability-based file APIs</strong> — use  or similar to confine file writes to the work directory, preventing path traversal via </li><li> — compute isolation means nothing if the sandbox can freely phone home. Options range from disabling networking entirely, to running an allowlist proxy (like Squid) that blocks DNS resolution inside the sandbox and forces all traffic through a domain-level allowlist, to dropping  so the sandbox cannot bypass DNS with raw sockets.</li></ul><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>Each layer catches different attack classes. A namespace escape inside gVisor reaches the Sentry, not the host kernel. A seccomp bypass hits the Sentry’s syscall implementation, which is itself sandboxed. Privilege escalation is blocked by dropping privileges. Persistent state leakage between jobs is prevented by ephemeral tmpfs with atomic unmount cleanup.</p><p>A practical detail that matters is the process that creates child sandboxes must itself be fork-safe. If you are running an async runtime, forking from a multithreaded process is inherently unsafe because child processes inherit locked mutexes and can corrupt state. The solution is a fork server pattern where you fork a single-threaded launcher process before starting the async runtime, then have the async runtime communicate with the launcher over a Unix socket. The launcher creates children, entirely avoiding the multithreaded fork problem.</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><h2>MicroVMs for hardware boundaries</h2><p>MicroVMs use hardware virtualization backed by the CPU’s extensions to run each workload in its own virtual machine with its own kernel.</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>Code runs in a completely separate, hardware-backed environment with its own guest kernel. It is important to separate the concepts here. The hypervisor is the capability built into the Linux kernel that manages the CPU’s hardware virtualization extensions. The Virtual Machine Monitor is a user-space process that configures the VM, allocates memory, and emulates minimal hardware devices. The microVM itself is a VM that has been stripped of legacy PC cruft so it boots in milliseconds and uses minimal memory.</p><p>Escaping the guest kernel requires finding a vulnerability in the Virtual Machine Monitor’s device emulation or the CPU’s virtualization features, which are rare and highly prized.</p><p>The guest runs in a separate virtual address space enforced by the CPU hardware. A bug in the guest kernel cannot access host memory because the hardware prevents it. The host kernel only sees the user-space process. The attack surface is the hypervisor and the Virtual Machine Monitor, both of which are orders of magnitude smaller than the full kernel surface that containers share.</p><p>You generally see two different approaches to Virtual Machine Monitor design depending on the workload. The first is strict minimalism, seen in projects like Firecracker. Built specifically for running thousands of tiny, short-lived functions on a single server, it intentionally leaves out complex features like hot-plugging CPUs or passing through physical GPUs. The goal is simply the smallest possible attack surface and memory footprint.</p><p>The second approach offers broader feature support, seen in projects like Cloud Hypervisor or QEMU microvm. Built for heavier and more dynamic workloads, it supports hot-plugging memory and CPUs, which is useful for dynamic build runners that need to scale up during compilation. It also supports GPU passthrough, which is essential for AI workloads, while still maintaining the fast boot times of a microVM.</p><p>The trade-off versus gVisor is that microVMs have higher per-instance overhead but stronger, hardware-enforced isolation. For CI systems and sandbox platforms where you create thousands of short-lived environments, the boot time and memory overhead add up. For long-lived, high-security workloads, the hardware boundary is worth it.</p><p>Snapshotting is a feature worth noting. You can capture a running VM’s state including CPU registers, memory, and devices, and restore it later. This enables warm pools where you boot a VM once, install dependencies, snapshot it, and restore clones in milliseconds instead of booting fresh each time. This is how some platforms achieve incredibly fast cold starts even with full VM isolation.</p><h2>WebAssembly with no kernel at all</h2><p>WebAssembly takes a fundamentally different approach. Instead of running native code and filtering its kernel access, WASM runs code in a memory-safe virtual machine that has no syscall interface at all. All interaction with the host happens through explicitly imported host functions.</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>Code runs in a strict sandbox where the only allowed operations are calling functions provided by the host. If the host doesn’t provide a file reading function, the WASM module simply cannot read files. The failure mode here requires a vulnerability in the WASM runtime itself, like an out-of-bounds memory read that bypasses the linear memory checks.</p><p>There is no syscall surface to attack because the code never makes syscalls. Memory safety is enforced by the runtime. The linear memory is bounds-checked, the call stack is inaccessible, and control flow is type-checked. Modern runtimes add guard pages and memory zeroing between instances.</p><p>The performance characteristics are attractive with incredibly fast cold starts and minimal memory overhead. But the practical limitation is language support. You cannot run arbitrary Python scripts in WASM today without compiling the Python interpreter itself to WASM along with all its C extensions. For sandboxing arbitrary code in arbitrary languages, WASM is not yet viable. For sandboxing code you control the toolchain for, it is excellent. I am, however, quite curious if there is a future for WASM in general-purpose sandboxing. Browsers have spent decades solving a similar problem of executing untrusted code safely, and porting those architectural learnings to backend infrastructure feels like a natural evolution.</p><p>Putting it all together, the landscape spans from fast and weak isolation to slower and highly secure isolation.</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>For running trusted code that you wrote and reviewed, Docker with a seccomp profile is probably fine. The isolation is against accidental interference, not adversarial escape.</p><p>For running untrusted code in a multi-tenant environment, like short-lived scripts, AI-generated code, or customer-provided functions, you need a real boundary. gVisor gives you a user-space kernel boundary with good compatibility, while a microVM gives you a hardware boundary with the strongest guarantees. Either is defensible depending on your threat model and performance requirements.</p><p>For reinforcement learning training pipelines where AI-generated code is evaluated in sandboxes across potentially untrusted workers, the threat model is both the code and the worker. You need isolation in both directions, which pushes toward microVMs or gVisor with defense-in-depth layering.</p><p>What I’ve learned is that the common mistake is treating isolation as binary. It’s easy to assume that if you use Docker, you are isolated. The reality is that standard Docker gives you namespace isolation, which is just visibility walls on a shared kernel. Whether that is sufficient depends entirely on what you are protecting against.</p><p>It is also worth remembering that compute isolation is only half the problem. You can put code inside a gVisor sandbox or a Firecracker microVM with a hardware boundary, and none of it matters if the sandbox has unrestricted network egress for your “agentic workload”. An attacker who cannot escape the kernel can still exfiltrate every secret it can read over an outbound HTTP connection. Network policy where it is a stripped network namespace with no external route, a proxy-based domain allowlist, or explicit capability grants for specific destinations is the other half of the isolation story that is easy to overlook. The apply case here can range from disabling full network access to using a proxy for redaction, credential injection or simply just allow listing a specific set of DNS records.</p><h2>Local sandboxing on developer machines</h2><p>Everything above is about server-side multi-tenant isolation, where the threat is adversarial code escaping a sandbox to compromise a shared host. There is a related but different problem on developer machines: AI coding agents that execute commands locally on your laptop. The threat model shifts. There is no multi-tenancy. The concern is not kernel exploitation but rather preventing an agent from reading your  keys, exfiltrating secrets over the network, or writing to paths outside the project. Or you know if you are running Clawdbot locally, then everything is fair game.</p><p>The approaches here use OS-level permission scoping rather than kernel boundary isolation.</p><p><a href=\"https://cursor.com/blog/agent-sandboxing\">Cursor</a> uses Apple’s Seatbelt () on macOS and Landlock plus seccomp on Linux. It generates a dynamic policy at runtime based on the workspace: the agent can read and write the open workspace and , read the broader filesystem, but cannot write elsewhere or make network requests without explicit approval. This reduced agent interruptions by roughly 40% compared to requiring approval for every command, because the agent runs freely within the fence and only asks when it needs to step outside.</p><p>OpenAI’s <a href=\"https://developers.openai.com/codex/security/\">Codex CLI</a> takes a similar approach with explicit modes: ,  (the default), and . Network access is disabled by default. Claude Code and Gemini CLI both support sandboxing but ship with it off by default.</p><p>The common pattern across all of these seems to be filesystem and network ACLs enforced by the OS, not a separate kernel or hardware boundary. A determined attacker who already has code execution on your machine could potentially bypass Seatbelt or Landlock restrictions through privilege escalation. But that is not the threat model. The threat is an AI agent that is mostly helpful but occasionally careless or confused, and you want guardrails that catch the common failure modes - reading credentials it should not see, making network calls it should not make, writing to paths outside the project.</p><p>Apple’s new <a href=\"https://developer.apple.com/videos/play/wwdc2025/346/\">Containerization framework</a> (announced at WWDC 2025) is interesting here. Unlike Docker on Mac, which runs all containers inside a single shared Linux VM, Apple gives each container its own lightweight VM via the <a href=\"https://github.com/apple/containerization\">Virtualization framework</a> on Apple Silicon. Each container gets its own kernel, its own ext4 filesystem, and its own IP address. It is essentially the microVM model applied to local development, with OCI image compatibility. It is still early, but it collapses the gap between “local development containers” and “properly isolated sandboxes” in a way that Docker Desktop never did.</p><p>The landscape is moving in a clear direction. There is a lot of exciting new tech out there, with people constantly pushing the limits of cold starts toward faster, securely isolated workloads using Python decorators and other novel approaches to make microvms feel like containers. I am excited to see what comes next in this space. It is definitely an area to watch.</p>",
      "contentLength": 20762,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47184049"
    },
    {
      "title": "Writing a Guide to SDF Fonts",
      "url": "https://www.redblobgames.com/blog/2026-02-26-writing-a-guide-to-sdf-fonts/",
      "date": 1772216649,
      "author": "chunkles",
      "guid": 49028,
      "unread": true,
      "content": "<p>Back in 2024 I learned about SDF (signed distance field) rendering of fonts. I was trying to implement outlines and shadows in a single pass instead of drawing over the text multiple times in different styles. I intended to use these fonts for two different projects, a game and a map generator. I got things working but didn’t fully understand why certain things worked or didn’t work. I wrote some notes on my site about what I tried. In the end, I stopped working on both the game’s fonts and the map generator, so I put all of this on hold.</p><p>Fast forward to late 2025, and my incomplete notes sometimes show up on the first page of search results for <a href=\"https://www.google.com/search?q=sdf+fonts\">“sdf fonts”</a>! Surely that isn’t the best page on the topic. It would be better to point to library documentation or maybe one of the research papers about the topic. My page .</p><p>Initially my thought was “search engines are in their decline” but then I decided “this is an opportunity”. I decided to  of being the top search result.</p><p>I first looked through everything I had written. I already had started an “overview” page but hadn’t gotten very far on it. I also have  that were “diary style”, about what I did rather than what you should know.</p><p>The overview page covered how to use various SDF font libraries (msdfgen, stb_truetype, tiny-sdf, etc.). I wrote code for multiple libraries, had sketched out diagrams for various concepts, and had screenshots of outputs from each of those libraries.</p><p>At some point I realized the scope was too large. I had spent the most time with msdfgen and hadn’t yet learned enough about the other libraries to write a proper guide. They all worked differently. I kept getting stuck. So . In redesign 2 I decided to only use msdfgen, but show the various tradeoffs involved (atlas size, antialias width, shader derivatives, smoothing function).</p><p>I made several diagrams for concepts, such as:</p><p>And I started running tests. I wanted to compare the effect of atlas size, so I made lots of screenshots and started looking closely. I wanted to come up with a way to recommend a specific size. I wanted to make recommendations for all the other parameters. I showed all the commands I ran.</p><p>At some point I realized I could run tests forever. And I had already done that last year, and wrote it up in blog posts (<a href=\"https://www.redblobgames.com/blog/2024-11-08-sdf-headless-tests/\">one</a> and <a href=\"https://www.redblobgames.com/blog/2024-11-17-sdf-headless-tests/\">two</a>). Doing it again here didn’t seem especially valuable. So  to a “how to” page. In redesign 3 I decided to show the concepts, then a JavaScript implementation using CPU rendering, and then another implementation using GPU rendering. I made new versions of the diagrams:</p><p>I was making progress on that page but it didn’t  like a Red Blob Games page. The page started out with tons of shell commands, and then showed lots of code. It felt like a page that only I would find useful. So  and designed a “concepts” page. In redesign 4 I focused on what effects I wanted, how SDF works, and how to use it to create those effects. I again reduced the scope by removing the implementation details. What I had already written, I moved to a separate (unpolished) page. And I never wrote a standalone downloadable project like I originally wanted.</p><p>Sometimes it takes a long time before I figure out what I actually want to write, and then everything falls into place:</p><p><strong>I’m finally happy with the page.</strong><a href=\"https://www.redblobgames.com/articles/sdf-fonts/\">Take a look!</a> I hope search engines point to it eventually.</p>",
      "contentLength": 3381,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47183725"
    },
    {
      "title": "Dan Simmons, author of Hyperion, has died",
      "url": "https://www.dignitymemorial.com/obituaries/longmont-co/daniel-simmons-12758871",
      "date": 1772216019,
      "author": "throw0101a",
      "guid": 48946,
      "unread": true,
      "content": "<div>Daniel Joseph Simmons passed away on February 21, 2026 in Longmont, Colorado at age 77. His beloved wife Karen and daughter Jane were at his side. </div><p>Dan was born in Peoria, Illinois on April 4, 1948 to his parents Robert A. Simmons and Kathryn H. (Catton) Simmons. His childhood was filled with happy memories of riding bikes with friends throughout cornfield-lined roads in the Midwest, first in Brimfield, Illinois and then in Pittsboro, Indiana. </p><p>He graduated with an English degree from Wabash College in Crawfordsville, Indiana, and earned a graduate degree in education from Washington University in St. Louis, Missouri. </p><p>Dan embarked on a career as an elementary school teacher in Missouri, later teaching in Buffalo, New York, and Longmont, Colorado, where he taught sixth grade. During his eighteen years in education, he co-created and taught a districtwide program for gifted students that was the first of its kind, and he was named a finalist for the Colorado Teacher of the Year.</p><p>Dan had a profound passion for teaching, and was beloved by many of his students for his innovative, energizing, and creative approach in the classroom. He brought science to life for his students with Carl Sagan’s Cosmos series, ran interactive simulations on topics like the Cuban Missile Crisis and the harmful effects of discrimination, and incorporated his love of topics like Greek mythology, film, and art into his lectures. </p><p>Every day after lunch, Dan told his students a daily installment of an epic tale that started on the first day of school. As they listened, the students would color illustrations that he’d drawn for them. When the story finally came to an end on the last day of school, many recall being reduced to tears. This story would go on to become Dan’s Hyperion cantos (1989), a critically acclaimed, four-part science fiction classic. </p><p>Over the course of his life, former students would tell Dan that they still had their notes from his Black history lectures, and that he had inspired their lifelong love of reading and writing. Long after he left the classroom, he continued to share what he loved with everyone around him, teaching his grandchildren all about the 1950s era monster movies that he loved, and giving endearingly professorial introductions to films that he and Karen shared with scores of friends when they hosted backyard summer movies.</p><p>In addition to teaching, reading and writing were the great loves of Dan’s life. As a child, he read everything he could find, spanning from comic books to literary classics and nonfiction. Throughout his life, he particularly loved learning about space, science, and history. Starting in early childhood, Dan had a remarkable gift for storytelling, which would become his life's work. His first published story came out on the day his daughter Jane was born, a day that confirmed to him that his true love was his family.</p><p>In 1987, Dan took a daring leap and left teaching to follow his dream to work full-time as an author. His debut novel, Song of Kali (1985), was inspired by three days that he spent in Kolkata, India, and won the 1986 World Fantasy Award. </p><p>He went on to write thirty-one novels and short story collections, many of which were honored with accolades ranging from Bram Stoker awards, Locus awards, the Shirley Jackson award, and the prestigious Hugo award. His most meaningful award was an honorary doctorate from Wabash College, a place that changed his life and set him on a path towards a life well lived. His titles have been published in 28 foreign countries and translated into at least 20 languages, and his many book tours, conferences, and workshops took him all over the world. </p><p>Like his early reading pursuits, Dan always wrote about what he loved. He defied literary norms by writing across genres, switching between major publishers, and defying pressure to conform to formulaic novels. His works span from historical fiction to horror, hard-boiled crime, and speculative fiction. They explore topics ranging from Ernest Hemingway’s WWII Cuban spy ring to mountain climbing in the Himalayas. In 2018, his novel The Terror (2007) was released as an AMC limited series. Dan was a profoundly curious learner who delighted in connecting with other curious minds, and the many stories he dreamed up helped him connect with others throughout his entire life.</p><p>Dan is predeceased by his parents and his brother Ted. He is survived by his loving wife and daughter, Karen and Jane Simmons; his beloved grandchildren, Milo and Lucia Glenn; and his brother, Wayne Simmons.</p><p>Dan's cremation has been entrusted to Ahlberg Funeral Chapel of Longmont, Colorado. His ashes will be scattered at a later date. Details for a Celebration of Life are pending.</p><p>Gifts in memory of Dan may be made to Wabash College online at <a href=\"http://www.wabash.edu/give\" target=\"_blank\" rel=\"nofollow\">www.wabash.edu/give</a> or to Wabash College Advancement Office 301 W. Wabash Ave. Crawfordsville, IN 47933. </p><p>Please visit <a href=\"http://www.ahlbergfuneralchapel.com\" target=\"_blank\" rel=\"nofollow\">www.ahlbergfuneralchapel.com</a> for upcoming service information, to make a memorial donation to Wabash College and to share fond memories and condolences with his loving family.</p>",
      "contentLength": 5097,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47183578"
    },
    {
      "title": "747s and Coding Agents",
      "url": "https://carlkolon.com/2026/02/27/engineering-747-coding-agents/",
      "date": 1772212920,
      "author": "cckolon",
      "guid": 49210,
      "unread": true,
      "content": "<p>A couple years ago, I was on the way back from a work trip to Germany. I had been upgraded to business class, and I sat next to a Belgian 747 pilot, probably in his fifties or sixties. We talked a fair bit about our careers. I had left the Navy and started professionally programming less than a year before. He had been a pilot since shortly after graduating university, and had flown the 747 for about twenty years. He had studied mechanical engineering at school, and he told me in great depth about the variable geometry jet turbines in modern aircraft, which could remain efficient across a wide altitude range.</p><p>I expressed some jealousy about how well suited he was to his job. Clearly he was a geek for aircraft, and even though most airlines don’t fly the 747 anymore, it is an incredible machine. He agreed that it was a privilege to fly the plane, but said wistfully:</p><blockquote><p>In this job, after a while, there’s no improvement. You are no better today than you were yesterday.</p></blockquote><p>He said that by now, he knew the 747 about as well as a pilot could. In fact, he sometimes wished he had become an engineer or designer of airplanes, so that he could learn new things as a core part of his job. Then he said:</p><blockquote><p>You are lucky that your job is like that.</p></blockquote><p>Since that flight, my job has changed a great deal. Coding agents can do a large portion of what I previously considered my work. I’m one of the last people who should be upset about this, since I work at an AI lab and stand to gain a great deal if AI follows through on its economic promise. Still, it has changed how I solve problems, and at times I feel more like a pilot than an engineer.</p><p>In the past, when I fixed a bug or implemented a feature, I would have to spend a minimum amount of effort understanding the situation. For example, to add pagination to this website, I would read the <a href=\"https://jekyllrb.com/docs/pagination/\">Jekyll docs</a>, find the right <a href=\"https://github.com/sverrirs/jekyll-paginate-v2\">plugin</a> to install, read the <a href=\"https://github.com/sverrirs/jekyll-paginate-v2/blob/master/README-GENERATOR.md#site-configuration\">sample config</a>, and make the change. Possibly this wouldn’t work, in which case I would Google it, read more, try more stuff, retest, etc. In this process it was hard not to learn things. I would walk away from the problem with a better understanding of how the system worked. If I had to implement the feature again, I would be able to do it faster and more easily.</p><p>Once LLMs started getting good at coding, I would occasionally ask them for help at the beginning of this process, mostly replacing search engines. If I hit an error, I would copy and paste it into a chatbot to see what it said before trying hard to understand it (often, before reading it). This didn’t replace critical thinking, though, since I would still need to learn and plan to implement the change.</p><p>With the AI coding agents of the last few months, though, things are different. Often the agent can implement a whole feature end-to-end, with no involvement from me. Now when I need to make a change to the codebase, I don’t start by trying to understand. Instead, I see if my coding agent can “one-shot” the problem, and only step in if it seems to be failing. This happens less and less, and the features that I trust agents with have become bigger and bigger.</p><p>I believe in coding primarily as a means to an end. Coding agents have allowed me to do much more than before, so for the most part I am happy with them! But I’ll admit there is also something bothersome about turning features over to AI fully.</p><p>I do not build skills or knowledge as quickly this way. If I build a feature with a coding agent and then have to do it again, I won’t be any faster the second time. It’s possible to imagine writing code with AI for twenty years and not being much more skillful at the end of it. There’s no improvement.</p><p>If I do have to step in and save the LLM, I often become lost as well. All of a sudden, I am reading someone else’s code. Rather than gradually coming to terms with a solution to a problem, I am presented with the solution wholesale—only, it’s a little bit wrong. As LLMs handle bigger tasks for me, this gets worse. My only saving grace is that I will do it less often.</p><p>You might say that the new, real skill is <a href=\"https://www.forbes.com/sites/rodgerdeanduncan/2025/10/16/prompting-the-21st-century-skill-that-will-change-how-we-work/\">prompting agents</a> (<a href=\"https://archive.ph/t4y24\">archived</a>), but I don’t believe that. Prompting is easy and <a href=\"https://www.oneusefulthing.org/p/a-guide-to-prompting-ai-for-what\">will only get easier</a>. Hard knowledge about programming and the problem is what helps you make good design decisions, so this knowledge is the most important factor determining whether your coding agents are successful. Developing this knowledge is becoming optional.</p><p>Some people will probably respond to this by saying (snottily) that I should read the code that my agents produce, rather than rely on them blindly. I do read the code, but reviewing code is very different from producing it, and surely teaches you less. If you don’t believe this, I doubt you work in software.</p><p>Coding agents are here to stay, and you’re a fool if you don’t use them. Still, I think you’ll use them most successfully if you understand the domain in which you’re working. This used to be an essential byproduct of programming, but that’s not the case anymore. To this end, maybe it’s a good idea to write a minimum amount of code by hand as an educational task, rather than a productive one, or to try to write the solution to a problem yourself, and only compare with the LLM once you’re confident your answer is correct.</p>",
      "contentLength": 5309,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47182986"
    },
    {
      "title": "Vibe coded Lovable-hosted app littered with basic flaws exposed 18K users",
      "url": "https://www.theregister.com/2026/02/27/lovable_app_vulnerabilities/",
      "date": 1772210882,
      "author": "nottorp",
      "guid": 48919,
      "unread": true,
      "content": "<p>Vibe-coding platform Lovable has been accused of hosting apps riddled with vulnerabilities after saying users are responsible for addressing security issues flagged before publishing.</p><p>Taimur Khan, a tech entrepreneur with a background in software engineering, found 16 vulnerabilities – six of which he said were critical – in a single Lovable-hosted app that leaked more than 18,000 people's data.</p><p>He declined to name the app during the disclosure process, although it was hosted on Lovable's platform and showcased on its Discover page. The app had more than 100,000 views and around 400 upvotes at the time Khan began his probe.</p><p>The main issue, Khan said, was that all apps that are vibe-coded on Lovable's platform are shipped with their backends powered by Supabase, which handles authentication, file storage, and real-time updates through a PostgreSQL database connection.</p><p>However, when the developer – in this case AI – or the human project owner fails to explicitly implement crucial security features like Supabase's row-level security and role-based access, code will be generated that looks functional but in reality is flawed.</p><p>One example of this was a malformed authentication function. The AI that <a target=\"_blank\" href=\"https://www.theregister.com/2025/11/24/opinion_column_vibe_coding/\">vibe-coded</a> the Supabase backend, which uses remote procedure calls, implemented it with flawed access control logic, essentially blocking authenticated users and allowing access to unauthenticated users.</p><p>Khan said the intent was to block non-admins from accessing parts of the app, but the faulty implementation blocked all logged-in users – an error he said was repeated across multiple critical functions.</p><p>\"This is backwards,\" said Khan. \"The guard blocks the people it should allow and allows the people it should block. A classic logic inversion that a human security reviewer would catch in seconds – but an AI code generator, optimizing for 'code that works,' produced and deployed to production.\"</p><p>Because the app itself was a platform for creating exam questions and viewing grades, the userbase is naturally comprised of teachers and students. Some were from top US universities such as UC Berkeley and UC Davis, while there were \"K-12 institutions with minors likely on the platform\" as well, Khan <a target=\"_blank\" href=\"https://www.linkedin.com/pulse/lovable-more-like-hackable-taimur-khan-cewdf/?trackingId=%2BscO7NGeQ0%2BpVd7b1U0fYA%3D%3D\" rel=\"nofollow\">said</a>.</p><p>With the security flaws in place, an unauthenticated attacker could trivially access every user record, send bulk emails through the platform, delete any user account, grade student test submissions, and access organizations' admin emails, for example.</p><p>Of the 18,697 total user records exposed, 14,928 contained unique email addresses. The dataset included 4,538 student accounts – all with email addresses – 10,505 enterprise users, and 870 users whose full PII was exposed.</p><p>The security flaws here are not exclusive to apps hosted by Lovable; the issue is broader and well-told by now.</p><p>Vibe coding, Collins Dictionary's Word of the Year for 2025, promised to break down software development's steep learning curve and empower any prompt jockey to bring their app ideas to life.</p><p>Veracode, for instance, <a target=\"_blank\" href=\"https://www.veracode.com/blog/ai-generated-code-security-risks/\" rel=\"nofollow\">recently found</a> that 45 percent of AI-generated code contained security flaws, not to mention the myriad <a target=\"_blank\" href=\"https://www.theregister.com/2025/08/05/mcpoison_bug_abuses_cursor_mcp/\">tales</a> of <a target=\"_blank\" href=\"https://www.theregister.com/2026/02/20/openclaw_snuck_into_cline_package/\">woe</a> reported by  in recent months.</p><p>Khan said he believes Lovable should take responsibility for the security of the apps it hosts, and was especially peeved when, after reporting his findings via company support, his ticket was reportedly closed without response.</p><p>\"If Lovable is going to market itself as a platform that generates production-ready apps with authentication 'included,' it bears some responsibility for the security posture of the apps it generates and promotes,\" Khan said.</p><p>\"You can't showcase an app to 100,000 people, host it on your own infrastructure, and then close the ticket when someone tells you it's leaking user data. At minimum, a basic security scan of showcased applications would have caught every critical finding in this report.\"</p><p>Lovable told  that the company has contacted the owner of the app in question and takes \"any findings of this kind extremely seriously.\"</p><p>Regarding the closed ticket, Lovable CISO Igor Andriushchenko said that the company only received \"a proper disclosure report\" on the evening of February 26 and acted on the findings \"within minutes.\"</p><p>\"Any project built with Lovable includes a free security scan before publishing,\" Andriushchenko told . \"This scan checks for vulnerabilities and, if found, provides recommendations on actions to take to resolve before publishing.</p><p>\"Ultimately, it is at the discretion of the user to implement these recommendations. In this case, that implementation did not happen.</p><p>\"This project also includes code not generated by Lovable and the vulnerable database is not hosted by Lovable. We have been in contact with the creator of the app, who is now addressing the issue.\" ®</p>",
      "contentLength": 4806,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47182659"
    },
    {
      "title": "Allocating on the Stack",
      "url": "https://go.dev/blog/allocation-optimizations",
      "date": 1772210045,
      "author": "spacey",
      "guid": 48995,
      "unread": true,
      "content": "<p>We’re always looking for ways to make Go programs faster. In the last\n2 releases, we have concentrated on mitigating a particular source of\nslowness, heap allocations. Each time a Go program allocates memory\nfrom the heap, there’s a fairly large chunk of code that needs to run\nto satisfy that allocation. In addition, heap allocations present\nadditional load on the garbage collector.  Even with recent\nenhancements like <a href=\"https://go.dev/blog/greenteagc\">Green Tea</a>, the garbage collector\nstill incurs substantial overhead.</p><p>So we’ve been working on ways to do more allocations on the stack\ninstead of the heap.  Stack allocations are considerably cheaper to\nperform (sometimes completely free).  Moreover, they present no load\nto the garbage collector, as stack allocations can be collected\nautomatically together with the stack frame itself. Stack allocations\nalso enable prompt reuse, which is very cache friendly.</p><h2>Stack allocation of constant-sized slices</h2><p>Consider the task of building a slice of tasks to process:</p><pre><code>func process(c chan task) {\n    var tasks []task\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    processAll(tasks)\n}\n</code></pre><p>Let’s walk through what happens at runtime when pulling tasks from the\nchannel  and adding them to the slice .</p><p>On the first loop iteration, there is no backing store for , so\n has to allocate one. Because it doesn’t know how big the\nslice will eventually be, it can’t be too aggressive. Currently, it\nallocates a backing store of size 1.</p><p>On the second loop iteration, the backing store now exists, but it is\nfull.  again has to allocate a new backing store, this time of\nsize 2. The old backing store of size 1 is now garbage.</p><p>On the third loop iteration, the backing store of size 2 is\nfull.  has to allocate a new backing store, this time\nof size 4. The old backing store of size 2 is now garbage.</p><p>On the fourth loop iteration, the backing store of size 4 has only 3\nitems in it.  can just place the item in the existing backing\nstore and bump up the slice length. Yay! No call to the allocator for\nthis iteration.</p><p>On the fifth loop iteration, the backing store of size 4 is full, and\n again has to allocate a new backing store, this time of size\n8.</p><p>And so on. We generally double the size of the allocation each time it\nfills up, so we can eventually append most new tasks to the slice\nwithout allocation. But there is a fair amount of overhead in the\n“startup” phase when the slice is small. During this startup phase we\nspend a lot of time in the allocator, and produce a bunch of garbage,\nwhich seems pretty wasteful. And it may be that in your program, the\nslice never really gets large. This startup phase may be all you ever\nencounter.</p><p>If this code was a really hot part of your program, you might be\ntempted to start the slice out at a larger size, to avoid all of these\nallocations.</p><pre><code>func process2(c chan task) {\n    tasks := make([]task, 0, 10) // probably at most 10 tasks\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    processAll(tasks)\n}\n</code></pre><p>This is a reasonable optimization to do. It is never incorrect; your\nprogram still runs correctly. If the guess is too small, you get\nallocations from  as before. If the guess is too large, you\nwaste some memory.</p><p>If your guess for the number of tasks was a good one, then there’s\nonly one allocation site in this program. The  call allocates a\nslice backing store of the correct size, and  never has to do\nany reallocation.</p><p>The surprising thing is that if you benchmark this code with 10\nelements in the channel, you’ll see that you didn’t reduce the number\nof allocations to 1, you reduced the number of allocations to 0!</p><p>The reason is that the compiler decided to allocate the backing store\non the stack. Because it knows what size it needs to be (10 times the\nsize of a task) it can allocate storage for it in the stack frame of\n instead of on the heap<a href=\"https://go.dev/blog/allocation-optimizations#footnotes\"></a>.  Note\nthat this depends on the fact that the backing store does not <a href=\"https://go.dev/doc/gc-guide#Escape_analysis\">escape\nto the heap</a> inside of .</p><h2>Stack allocation of variable-sized slices</h2><p>But of course, hard coding a size guess is a bit rigid.\nMaybe we can pass in an estimated length?</p><pre><code>func process3(c chan task, lengthGuess int) {\n    tasks := make([]task, 0, lengthGuess)\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    processAll(tasks)\n}\n</code></pre><p>This lets the caller pick a good size for the  slice, which may\nvary depending on where this code is being called from.</p><p>Unfortunately, in Go 1.24 the non-constant size of the backing store\nmeans the compiler can no longer allocate the backing store on the\nstack.  It will end up on the heap, converting our 0-allocation code\nto 1-allocation code. Still better than having  do all the\nintermediate allocations, but unfortunate.</p><p>But never fear, Go 1.25 is here!</p><p>Imagine you decide to do the following, to get the stack allocation\nonly in cases where the guess is small:</p><pre><code>func process4(c chan task, lengthGuess int) {\n    var tasks []task\n    if lengthGuess &lt;= 10 {\n        tasks = make([]task, 0, 10)\n    } else {\n        tasks = make([]task, 0, lengthGuess)\n    }\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    processAll(tasks)\n}\n</code></pre><p>Kind of ugly, but it would work. When the guess is small, you use a\nconstant size  and thus a stack-allocated backing store, and\nwhen the guess is larger you use a variable size  and allocate\nthe backing store from the heap.</p><p>But in Go 1.25, you don’t need to head down this ugly road. The Go\n1.25 compiler does this transformation for you!  For certain slice\nallocation locations, the compiler automatically allocates a small\n(currently 32-byte) slice backing store, and uses that backing store\nfor the result of the  if the size requested is small\nenough. Otherwise, it uses a heap allocation as normal.</p><p>In Go 1.25,  performs zero heap allocations, if\n is small enough that a slice of that length fits into 32\nbytes. (And of course that  is a correct guess for how\nmany items are in .)</p><p>We’re always improving the performance of Go, so upgrade to the latest\nGo release and <a href=\"https://youtu.be/FUm0pfgWehI?si=QRTt_JYwr-cRHDNJ&amp;t=960\" rel=\"noreferrer\" target=\"_blank\">be\nsurprised</a> by\nhow much faster and memory efficient your program becomes!</p><h2>Stack allocation of append-allocated slices</h2><p>Ok, but you still don’t want to have to change your API to add this\nweird length guess. Anything else you could do?</p><pre><code>func process(c chan task) {\n    var tasks []task\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    processAll(tasks)\n}\n</code></pre><p>In Go 1.26, we allocate the same kind of small, speculative backing\nstore on the stack, but now we can use it directly at the \nsite.</p><p>On the first loop iteration, there is no backing store for , so\n uses a small, stack-allocated backing store as the first\nallocation. If, for instance, we can fit 4 s in that backing store,\nthe first  allocates a backing store of length 4 from the stack.</p><p>The next 3 loop iterations append directly to the stack backing store,\nrequiring no allocation.</p><p>On the 4th iteration, the stack backing store is finally full and we\nhave to go to the heap for more backing store. But we have avoided\nalmost all of the startup overhead described earlier in this article.\nNo heap allocations of size, 1, 2, and 4, and none of the garbage that\nthey eventually become. If your slices are small, maybe you will never\nhave a heap allocation.</p><h2>Stack allocation of append-allocated escaping slices</h2><p>Ok, this is all good when the  slice doesn’t escape. But what if\nI’m returning the slice? Then it can’t be allocated on the stack, right?</p><p>Right! The backing store for the slice returned by  below\ncan’t be allocated on the stack, because the stack frame for \ndisappears when  returns.</p><pre><code>func extract(c chan task) []task {\n    var tasks []task\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    return tasks\n}\n</code></pre><p>But you might think, the  slice can’t be allocated on the\nstack. But what about all those intermediate slices that just become\ngarbage? Maybe we can allocate those on the stack?</p><pre><code>func extract2(c chan task) []task {\n    var tasks []task\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    tasks2 := make([]task, len(tasks))\n    copy(tasks2, tasks)\n    return tasks2\n}\n</code></pre><p>Then the  slice never escapes . It can benefit from\nall of the optimizations described above. Then at the very end of\n, when we know the final size of the slice, we do one heap\nallocation of the required size, copy our s into it, and return\nthe copy.</p><p>But do you really want to write all that additional code? It seems\nerror prone. Maybe the compiler can do this transformation for us?</p><p>For escaping slices, the compiler will transform the original \ncode to something like this:</p><pre><code>func extract3(c chan task) []task {\n    var tasks []task\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    tasks = runtime.move2heap(tasks)\n    return tasks\n}\n</code></pre><p> is a special compiler+runtime function that is the\nidentity function for slices that are already allocated in the heap.\nFor slices that are on the stack, it allocates a new slice on the\nheap, copies the stack-allocated slice to the heap copy, and returns\nthe heap copy.</p><p>This ensures that for our original  code, if the number of\nitems fits in our small stack-allocated buffer, we perform exactly 1\nallocation of exactly the right size. If the number of items exceeds\nthe capacity our small stack-allocated buffer, we do our normal\ndoubling-allocation once the stack-allocated buffer overflows.</p><p>The optimization that Go 1.26 does is actually better than the\nhand-optimized code, because it does not require the extra\nallocation+copy that the hand-optimized code always does at the end.\nIt requires the allocation+copy only in the case that we’ve exclusively\noperated on a stack-backed slice up to the return point.</p><p>We do pay the cost for a copy, but that cost is almost completely\noffset by the copies in the startup phase that we no longer have to\ndo. (In fact, the new scheme at worst has to copy one more element\nthan the old scheme.)</p><p>Hand optimization can still be beneficial, especially if you have a\ngood estimate of the slice size ahead of time. But hopefully the\ncompiler will now catch a lot of the simple cases for you and allow\nyou to focus on the remaining ones that really matter.</p><p>There are a lot of details that the compiler needs to ensure to get\nall these optimizations right. If you think that one of these\noptimizations is causing correctness or (negative) performance issues\nfor you, you can turn them off with\n<code>-gcflags=all=-d=variablemakehash=n</code>. If turning these optimizations\noff helps, please <a href=\"https://go.dev/issue/new\">file an issue</a> so we can investigate.</p><p> Go stacks do not have any -style mechanism for\ndynamically-sized stack frames. All Go stack frames are constant\nsized.</p>",
      "contentLength": 10579,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47182487"
    },
    {
      "title": "NASA announces overhaul of Artemis program amid safety concerns, delays",
      "url": "https://www.cbsnews.com/news/nasa-artemis-moon-program-overhaul/",
      "date": 1772210019,
      "author": "voxadam",
      "guid": 48918,
      "unread": true,
      "content": "<p>New NASA Administrator Jared Isaacman announced a major overhaul of the agency's  Friday, acknowledging that the agency's plan to land astronauts on the moon in 2028 was not realistic without another preparatory mission first to lay the groundwork.&nbsp;</p><p>He said NASA will now add an additional flight in 2027 in which astronauts will dock with new commercial moon landers in low-Earth orbit for detailed tests of navigation, communications, propulsion and life support systems and to verify rendezvous procedures.</p><p>That flight, in turn, will be followed by at least one and possibly two lunar landing missions in 2028 that incorporate lessons learned from the preceding flight.</p><p>The goal is to accelerate the pace of launches of the huge Space Launch System rocket while carrying out Artemis flights in evolutionary steps — not attempting missions that rely on too many untested technologies and procedures at once.</p><p>\"We're going to get there in steps, continue to take down risk as we learn more and we roll that information into subsequent designs,\" Isaacman said told CBS News. \"We've got to get back to basics.\"</p><p>Isaacman outlined the plan in an interview with CBS News space contributor Christian Davenport and then again during a news conference Friday.&nbsp;</p><p>The announcement came two days after release of a sharply-worded report from NASA's independent Aerospace Safety Advisory Panel that deemed the existing plans too risky.</p><p>The panel raised concerns about the number of \"firsts\" required by the original Artemis III moon landing mission and recommended that NASA \"restructure\" the program to create a more balanced risk posture.</p><p>\"It is interesting that a lot of the things that we are addressing directly go to the points they raised in their report,\" Isaacman said Friday. \"I can't say we actually collaborated on it because I generally think these were all pretty obvious observations.\"</p><p>Launch had been planned for early February, but it was delayed to repair a hydrogen leak and, more recently, to give engineers time to fix a helium pressurization problem in the rocket's upper stage. Launch is now on hold until at least April 1.</p><p>The Artemis III mission, which had been expected to land astronauts near the moon's south pole in 2028, now will be redefined and rescheduled — launching in 2027 but not to the moon, Isaacman said. Instead, the yet-to-be-named astronauts will rendezvous and dock in orbit closer to home with one or both of the commercially built lunar landers now under development at Elon Musk's SpaceX and Jeff Bezos' Blue Origin.</p><p>The idea is to gain valuable near-term flight experience before attempting a moon landing with astronauts on board. With Artemis III under its belt, NASA hopes to launch two moon landing missions in 2028, Artemis IV and V, using one or both landers, and to continue with one moonshot per year thereafter.</p><p>\"What helps us get to the moon? Well, for sure, rendezvous and docking with one or ideally both landers, that gives you an opportunity to do some integrated testing of a vehicle that we are going to depend upon the following year to take those astronauts down to the surface of the moon,\" Isaacman told CBS News.</p><p>The revised Artemis III mission will also give astronauts a chance to test out new spacesuits that future moonwalkers will use.</p><p>\"It's an opportunity to … actually have the suits in microgravity, even if we don't go outside the vehicle in them. You get a lot of good learning from that,\" Isaacman said.</p><p>The Artemis III test flight with one or two lander dockings in Earth orbit is similar in concept to Apollo 9, which launched a command module and lander to Earth orbit for flight tests in 1969 and helped pave the way to the  landing four months later.</p><p>Isaacman said SpaceX and Blue Origin are \"both looking to do uncrewed landing demonstrations as part of the existing agreement.\"</p><p>\"So we want to just take advantage of this to set up both vendors for future success on a lunar landing,\" he said. \"This is the proper way to do it, if it works out from a timing perspective, to be able to rendezvous and dock with both. ... This, again, is the right way to proceed in order to have a high confidence opportunity in '28 to land.\"</p><p>The Artemis IV and V missions in 2028 will use whichever landers are deemed ready for service. If only one company's lander is available, that lander would be used for both missions, an official said. If both are available, one would be used for one flight and one for the other.</p><p>Launching Artemis III, IV and V before the end of 2028 will not be easy, and Isaacman said it is essential that NASA rebuild its workforce and regain the technical competence to support a higher launch cadence, moving from one flight every 18 months or so to a flight every year. That pace, he argued, will reduce risk.</p><p>\"When you regain these core competencies and you start exercising your muscles, your skills do not atrophy,\" he said. \"It's safer. And yes, you are buying down risk, because you're able to test things in low Earth orbit before you need to get to the moon, which is exactly what we did during the Apollo era.\"</p><p>He said he did not blame NASA's contractors for the current slow pace of Artemis launches. Instead, \"we should have made better decisions (in the past) and said, you don't go from Artemis II to landing on the moon with Artemis III.\"</p><h2>Safety advisers called for changes to \"high risk\" plans</h2><p>The Artemis overhaul was announced two days after the release of a report by the lAerospace Safety Advisory Panel that said the original plan to move directly from Artemis II to a lunar touchdown in 2028 using a SpaceX lander did not have the proper margin of safety and did not appear to be realistically achievable.</p><p>The panel raised concerns about the number of \"firsts\" required by that mission in its current form and recommended that NASA \"restructure the Artemis Program to create a more balanced risk posture for Artemis III and future missions.\"</p><p>The plan outlined by Isaacman appears to address many of the core issues raised by the safety panel.</p><p>Officials said Isaacman had discussed accelerating lander development with both SpaceX and Blue Origin and that both were on board. He also discussed the accelerated Artemis overhaul with Boeing, which manages the SLS rocket and builds its massive first stage; with United Launch Alliance, builder of the rocket's upper stage, Orion-builder Lockheed Martin and other Artemis contractors.</p><p>All, the official said, were in agreement.</p><p>\"Boeing is a proud partner to the Artemis mission and our team is honored to contribute to NASA's vision for American space leadership,\" Steve Parker, the president and CEO of Boeing Defense, Space &amp; Security, said in a statement. \"We are ready to meet the increased demand.\"</p><p>SpaceX said, \"We look forward to working with NASA to fly missions that demonstrate valuable progress towards establishing a permanent, sustainable presence on the lunar surface.\"</p><p>And Blue Origin responded, \"Let's go! We're all in!\"</p><p>Isaacman also said the agency would halt work to develop a more powerful version of the SLS rocket's upper stage, known as the Exploration Upper Stage, or EUS. Instead, NASA will go forward with a \"standardized,\" less powerful stage but one that will minimize major changes between flights and utilize the same launch gantry.</p><p>Under the original Artemis architecture, NASA planned on multiple versions of the SLS rocket, ranging from the \"Block 1\" vehicle currently in use to a more powerful EUS-equipped Block 1B and eventually an even bigger Block 2 model using advanced solid rocket boosters. The latter two versions required use of a taller mobile launch gantry, already well under construction at the Kennedy Space Center.</p><p>\"It is needlessly complicated to alter the configuration of the SLS and Orion stack to undertake subsequent Artemis missions,\" Amit Kshatriya, NASA's associate administrator, said in a statement.</p><p>\"The entire sequence of Artemis flights needs to represent a step-by-step build-up of capability, with each step bringing us closer to our ability to perform the landing missions. Each step needs to be big enough to make progress, but not so big that we take unnecessary risk given previous learnings.\"</p><p>As a result, NASA will stick with the current version of the SLS with the addition of the \"standardized\" upper stage. No other details were provided.</p><p>Isaacman closed out the CBS interview by saying flight-tested hardware, a revitalized work force and a more Apollo-like management strategy are only part of the story.</p><p>\"There's another ingredient that's required, and that's the orbital economy, whether it happens in low-Earth orbit or on the lunar surface,\" Isaacman said.</p><p>\"We've got to do something where we can get more value out of space and the lunar surface than we put into it. And that's how you really ignite an economy, and that's how everything we want to do in space is not perpetually dependent on taxpayers.\"</p><section><p>\n                  \n        contributed to this report.\n    </p></section>",
      "contentLength": 8969,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47182483"
    },
    {
      "title": "Show HN: Claude-File-Recovery, recover files from your ~/.claude sessions",
      "url": "https://github.com/hjtenklooster/claude-file-recovery",
      "date": 1772209582,
      "author": "rikk3rt",
      "guid": 49048,
      "unread": true,
      "content": "<p>Claude Code deleted my research and plan markdown files and informed me: “I accidentally rm -rf'd real directories in my Obsidian vault through a symlink it didn't realize was there: I made a mistake. “</p><p>Unfortunately the backup of my documentation accidentally hadn’t run for a month. So I built claude-file-recovery, a CLI-tool and TUI that is able to extract your files from your ~/.claude session history and thankfully I was able to recover my files. It's able to extract any file that Claude Code ever read, edited or wrote. I hope you will never need it, but you can find it on my GitHub and pip. Note: It can recover an earlier version of a file at a certain point in time.</p><p>pip install claude-file-recovery</p>",
      "contentLength": 717,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47182387"
    },
    {
      "title": "A Chinese official’s use of ChatGPT revealed an intimidation operation",
      "url": "https://www.cnn.com/2026/02/25/politics/chatgpt-china-intimidation-operation",
      "date": 1772207547,
      "author": "cwwc",
      "guid": 48994,
      "unread": true,
      "content": "<p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm17im5j002y28nqc1wqanhi@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            A sprawling Chinese influence operation — accidentally revealed by a Chinese law enforcement official’s use of <a href=\"https://www.cnn.com/2026/01/16/tech/chatgpt-ads-openai\">ChatGPT</a> — focused on intimidating Chinese dissidents abroad, including by impersonating US immigration officials, according to a new report from ChatGPT-maker OpenAI.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm1fahw900073b6r1zcaksid@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The Chinese law enforcement official used ChatGPT like a diary to document the alleged covert campaign of suppression, OpenAI said. In one instance, Chinese operators allegedly disguised themselves as US immigration officials to warn a US-based Chinese dissident that their public statements had supposedly broken the law, according to the ChatGPT user. In another case, they describe an effort to use forged documents from a US county court to try to get a Chinese dissident’s social media account taken down.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm1fahw900083b6rqi1hbd7b@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The report offers one of the most vivid examples yet of how authoritarian regimes can use AI tools to document their censorship efforts. The influence operation appeared to involve hundreds of Chinese operators and thousands of fake online accounts on various social media platforms, according to OpenAI.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm1fahw900093b6rnqzaqbt7@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “This is what Chinese modern transnational repression looks like,” Ben Nimmo, principal investigator at OpenAI, told reporters ahead of the report’s release. “It’s not just digital. It’s not just about trolling. It’s industrialized. It’s about trying to hit critics of the CCP [Chinese Communist Party] with everything, everywhere, all at once.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm1fahw9000a3b6rfmkrxnx6@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            CNN has requested comment on the report from the Chinese Embassy in Washington, DC.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm1fahw9000b3b6rlzm1btlt@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            ChatGPT served as a journal for the Chinese operative to keep track of the covert network, while much of the network’s content was generated by other tools and spread through social media accounts and websites. OpenAI banned the user after discovering the activity.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm1fahw9000c3b6rkl34sqf4@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            OpenAI’s investigators were able to match descriptions from the ChatGPT user with real-world online activity and impact. The user described an effort to fake the death of a Chinese dissident by creating a phony obituary and photos of a gravestone and posting them online. False rumors of the dissident’s death did indeed surfaced online in 2023, <a href=\"https://www.voachinese.com/a/7323197.html\" target=\"_blank\">according</a> to a Chinese-language Voice of America article.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm1fahw9000d3b6r0pv044m0@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In another case, the ChatGPT user asked the AI agent to draw up a multi-part plan to denigrate the incoming Japanese prime minister, Sanae Takaichi, in part by fanning online anger about US tariffs on Japanese goods. ChatGPT refused to respond to the prompt, according to OpenAI. But in late October, as Takaichi took power, hashtags emerged on a popular forum for Japanese graphic artists attacking her and complaining about US tariffs, according to OpenAI.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm1fahw9000e3b6r0c0jx1nv@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The report comes amid a battle between the US and China for supremacy over AI. At stake is how the technology is used on the battlefield and in the boardroom of the world’s two biggest economies.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm1fahw9000f3b6rfl72kvtc@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The Pentagon is in a standoff with another prominent AI company, Anthropic, over the use of its AI model. Defense Secretary Pete Hegseth has <a href=\"https://www.cnn.com/2026/02/24/tech/hegseth-anthropic-ai-military-amodei\">given</a> Anthropic CEO Dario Amodei a Friday deadline to comply with demands to peel back safeguards on its AI model or risk losing a lucrative Pentagon contract.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm1fahw9000g3b6ru0pznese@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The report from OpenAI “clearly demonstrates the way that China is actively employing AI tools to enhance information operations,” Michael Horowitz, a former Pentagon official focused on emerging technologies, told CNN.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm1fahw9000h3b6rjujsyx4s@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “US-China AI competition is continuing to intensify,” said Horowtiz, who is now a professor at the University of Pennsylvania. “This competition is not just taking place at the frontier, but in how China’s government is planning and implementing the day-to-day of their surveillance and information apparatus.”\n    </p>",
      "contentLength": 3933,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47181944"
    },
    {
      "title": "ChatGPT Health fails to recognise medical emergencies – study",
      "url": "https://www.theguardian.com/technology/2026/feb/26/chatgpt-health-fails-recognise-medical-emergencies",
      "date": 1772207073,
      "author": "simonebrunozzi",
      "guid": 48896,
      "unread": true,
      "content": "<p>ChatGPT Health regularly misses the need for medical urgent care and frequently fails to detect suicidal ideation, a study of the AI platform has found, which experts worry could “feasibly lead to unnecessary harm and death”.</p><p>The lead author of the study, Dr Ashwin Ramaswamy, said “we wanted to answer the most basic safety question; if someone is having a real medical emergency and asks ChatGPT <a href=\"https://www.theguardian.com/australia-news/health\" data-link-name=\"in body link\" data-component=\"auto-linked-tag\">Health</a> what to do, will it tell them to go to the emergency department?”</p><p>Ramaswamy and his colleagues created 60 realistic patient scenarios covering health conditions from mild illnesses to emergencies. Three independent doctors reviewed each scenario and agreed on the level of care needed, based on clinical guidelines.</p><p>The team then asked ChatGPT Health for advice on each case under different conditions, including changing the patient’s gender, adding test results, or adding comments from family members, generating nearly 1,000 responses.</p><p>They then compared the platform’s recommendations with the doctors’ assessments.</p><p>While it performed well in textbook emergencies such as stroke or severe allergic reactions, it struggled in other situations. In one asthma scenario, it advised waiting rather than seeking emergency treatment despite the platform identifying early warning signs of respiratory failure.</p><p>In 51.6% of cases where someone needed to go to the hospital immediately, the platform said stay home or book a routine medical appointment, a result Alex Ruani, a doctoral researcher in health misinformation mitigation with University College London, described as “unbelievably dangerous”.</p><p>“If you’re experiencing respiratory failure or diabetic ketoacidosis, you have a 50/50 chance of this AI telling you it’s not a big deal,” she said. “What worries me most is the false sense of security these systems create. If someone is told to wait 48 hours during an asthma attack or diabetic crisis, that reassurance could cost them their life.”</p><p>In one of the simulations, eight times out of 10 (84%), the platform sent a suffocating woman to a future appointment she would not live to see, Ruani said. Meanwhile, 64.8% of completely safe individuals were told to seek immediate medical care, said Ruani, who was not involved in the study.</p><p>The platform was also nearly 12 times more likely to downplay symptoms because the “patient” told it a “friend” in the scenario suggested it was nothing serious.</p><p>“It is why many of us studying these systems are focused on urgently developing clear safety standards and independent auditing mechanisms to reduce preventable harm,” Ruani said.</p><p>A spokesperson for OpenAI said while the company welcomed independent research evaluating AI systems in healthcare, the study did not reflect how people typically use ChatGPT Health in real life. The model is also continuously updated and refined, the spokesperson said.</p><p>Ruani said even though simulations created by the researchers were used, “a plausible risk of harm is enough to justify stronger safeguards and independent oversight”.</p><p>Ramaswamy, a urology instructor atthe Icahn School of Medicine at Mount Sinai in the US, said he was particularly concerned by the platform’s under-reaction to suicide ideation.</p><p>“We tested ChatGPT Health with a 27-year-old patient who said he’d been thinking about taking a lot of pills,” he said. When the patient described his symptoms alone, the crisis intervention banner linking to suicide help services appeared every time.</p><p>“Then we added normal lab results,” Ramaswamy said. “Same patient, same words, same severity. The banner vanished. Zero out of 16 attempts. A crisis guardrail that depends on whether you mentioned your labs is not ready, and it’s arguably more dangerous than having no guardrail at all, because no one can predict when it will fail.”</p><p>Prof Paul Henman, a digital sociologist and policy expert with the University of Queensland, said: “This is a really important paper.</p><p>“If ChatGPT Health was used by people at home, it could lead to higher numbers of unnecessary medical presentations for low-level conditions and a failure of people to obtain urgent medical care when required, which could feasibly lead to unnecessary harm and death.”</p><p>He said it also raised the prospects of legal liability, with <a href=\"https://www.theguardian.com/technology/2026/jan/08/google-character-ai-settlement-teen-suicide\" data-link-name=\"in body link\">legal cases</a> against tech companies already in motion in relation to suicide and self-harm after using AI chatbots.</p><p>“It is not clear what OpenAI is seeking to achieve by creating this product, how it was trained, what guardrails it has introduced and what warnings it provides to users,” Henman said.</p><p>“Because we don’t know how ChatGPT Health was trained and what the context it was using, we don’t really know what is embedded into its models.”</p>",
      "contentLength": 4763,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47181841"
    },
    {
      "title": "We gave terabytes of CI logs to an LLM",
      "url": "https://www.mendral.com/blog/llms-are-good-at-sql",
      "date": 1772206873,
      "author": "shad42",
      "guid": 48917,
      "unread": true,
      "content": "<p>Last week, our agent traced a flaky test to a dependency bump three weeks prior. It did this by writing its own SQL queries, scanning hundreds of millions of log lines across a dozen queries, and following a trail from job metadata to raw log output. The whole investigation took seconds.</p><p>To do this, the agent needs context: not one log file, but every build, every test, every log line, across months of history. Every week, about  and  flow through our system. All of it lands in ClickHouse, compressed at 35:1. All of it is queryable in milliseconds.</p><h2>A SQL interface for the agent<a href=\"https://www.mendral.com/blog/llms-are-good-at-sql#a-sql-interface-for-the-agent\" target=\"_blank\" rel=\"noopener noreferrer\" aria-label=\"Link to section\"></a></h2><p>We expose a SQL interface to the agent, scoped to the organization it's investigating. The agent constructs its own queries based on the question. No predefined query library, no rigid tool API.</p><p>LLMs are good at SQL. There's an enormous amount of SQL in training data, and the syntax maps well to natural-language questions about data. A constrained tool API like <code>get_failure_rate(workflow, days)</code> would limit the agent to the questions we anticipated. A SQL interface lets it ask questions we never thought of, which matters when you're debugging novel failures.</p><p>The agent queries two main targets:</p><p>: a materialized view with one row per CI job execution. The agent uses this 63% of the time for questions like \"how often does this fail?\", \"what's the success rate?\", \"which jobs are slowest?\", \"when did this start failing?\"</p><p>: one row per log line. The agent uses this 37% of the time for questions like \"show me the error output for this job\", \"when did this log pattern first appear?\", \"how often does this error message occur across runs?\"</p><h3>52,000 queries across 8,500 investigations<a href=\"https://www.mendral.com/blog/llms-are-good-at-sql#52000-queries-across-8500-investigations\" target=\"_blank\" rel=\"noopener noreferrer\" aria-label=\"Link to section\"></a></h3><p>We analyzed 8,534 agent sessions and 52,312 queries from our observability pipeline.</p><p>The agent doesn't stop at one query. It investigates. Starts broad, then drills in. Total rows scanned across all queries to answer one question:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>The typical question scans  across about 3 queries. At P75 it's 5.2 million rows. At P95 it's . The heaviest raw-log sessions, deep investigations tracing error patterns across months of history, scan .</p><p>The agent starts broad and narrows. A typical investigation begins with job metadata: \"what's the failure rate for this workflow?\", \"which jobs failed on this commit?\" These are cheap queries (median 47K rows) against a compact, pre-aggregated materialized view.</p><p>When it finds something interesting, it drills into raw logs: \"show me the stack trace for this specific failure\", \"has this error message appeared before?\" These are the expensive queries (median 1.1M rows), full-text scans across log output. But this is exactly the kind of search that would take a human minutes of scrolling through GitHub Actions log viewers.</p><p>The agent averages 4.4 queries per session, but heavy investigations issue many more. A P95 session isn't one big query. It's the agent following a trail, query after query, as it narrows in on a root cause.</p><p>For the agent to query this fast, the data needs to be structured for it. Up to 300 million log lines flow through on a busy day. We use ClickHouse.</p><p>Every log line in our system carries  of metadata: the full context of the CI run it belongs to. Commit SHA, author, branch, PR title, workflow name, job name, step name, runner info, timestamps, and more.</p><p>In a traditional row-store, this would be insane. You'd normalize. Run-level metadata in one table, job metadata in another, join at query time. Denormalizing 48 columns onto every single log line sounds like a storage disaster.</p><p>In ClickHouse's columnar format, it's essentially free.</p><p>A column like  has the same value for every log line in a CI run, and a single run can produce thousands of log lines. ClickHouse stores those thousands of identical values in sequence. The compression algorithm sees the repetition and compresses it to almost nothing.</p><table><thead><tr></tr></thead><tbody><tr><td>Same message for every line in a run (thousands of lines)</td></tr><tr><td>Same PR/commit title across all lines</td></tr><tr><td>Same <code>.github/workflows/foo.yml</code> path</td></tr><tr><td>Same step name across hundreds of lines</td></tr><tr><td>Same job name across hundreds/thousands of lines</td></tr></tbody></table><p>The agent asks arbitrary questions. One might filter by commit author, the next by runner label, the next by step name. Without denormalization, every one of those requires a join. With it, they're all column predicates.</p><table><tbody><tr><td>Raw log text ( uncompressed)</td></tr><tr><td>All 48 columns uncompressed</td></tr><tr></tr></tbody></table><p>The raw log text alone is 664 GiB. Adding all 48 columns of metadata inflates it to 5.31 TiB uncompressed, 8x the raw text. On disk, the whole thing compresses to 154 GiB. ClickHouse stores 8x more data (all the enriched metadata) in a quarter of the size of the raw text alone.</p><p>That's about  on disk, including all 48 columns. Yes, really. 21 bytes for a log line plus its commit SHA, author, branch, job name, step name, runner info, and 41 other fields.</p><h3>Where the storage actually goes<a href=\"https://www.mendral.com/blog/llms-are-good-at-sql#where-the-storage-actually-goes\" target=\"_blank\" rel=\"noopener noreferrer\" aria-label=\"Link to section\"></a></h3><p>Not all columns compress equally. The unique-per-row columns (log text, timestamp, line number) compress modestly and dominate storage. The metadata columns, which repeat across thousands of lines, are nearly free.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>Everything else (41 columns)</td></tr></tbody></table><p>The top three (, , ) account for 53% of all storage. Everything else is repeated metadata that compresses to almost nothing.</p><p>We use a few ClickHouse patterns that keep things fast:</p><p> means the data is physically sorted for our access pattern. The sort order is <code>(org, ts, repository, run_id, ...)</code>, so every query is scoped to one organization and a time range, and ClickHouse skips everything else without reading it.</p><p> let ClickHouse avoid scanning data it doesn't need. We use bloom filters on 14 columns (org, repository, job name, branch, commit SHA, etc.) and an ngram bloom filter on  for full-text search. When the agent searches for an error message across billions of log lines, ClickHouse checks the ngram index to skip granules that can't contain the search term, turning a full table scan into a targeted read.</p><p> pre-compute aggregations on insert. When the agent asks \"what's the failure rate for this workflow over the last 30 days?\", the answer is already computed. The aggregation happened when the data was written.</p><p> give us high write throughput without building our own batching layer. We fire-and-forget individual inserts, and ClickHouse batches them internally.</p><p>Query latency across 52K queries:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table><p>Job metadata queries return in 20ms at the median. Raw log queries, scanning a million rows at the median, come back in 110ms.</p><p>Latency scales roughly linearly with rows scanned:</p><table><thead><tr></tr></thead><tbody></tbody></table><p>10x more rows ≈ 10x more latency. 60% of all queries scan under 100K rows and return in under 50ms, fast enough that the agent can fire off several per second without breaking stride. At the extreme end, the agent occasionally scans over a billion rows in a single query; even those complete in about 30 seconds at the median.</p><h2>Ingesting through GitHub's rate limit<a href=\"https://www.mendral.com/blog/llms-are-good-at-sql#ingesting-through-githubs-rate-limit\" target=\"_blank\" rel=\"noopener noreferrer\" aria-label=\"Link to section\"></a></h2><p>None of the above works without fresh data. The agent needs to reason about the build that just failed, not one from an hour ago.</p><h3>The rate limit constraint<a href=\"https://www.mendral.com/blog/llms-are-good-at-sql#the-rate-limit-constraint\" target=\"_blank\" rel=\"noopener noreferrer\" aria-label=\"Link to section\"></a></h3><p>GitHub's API gives you 15,000 requests per hour per App installation (5,000 on non-Enterprise plans). That sounds generous until you're continuously polling workflow runs, jobs, steps, and log output across dozens of active repositories. A single commit can spawn hundreds of parallel jobs, each producing logs you need to fetch.</p><p>And ingestion isn't the only thing hitting the API. When the agent investigates a failure, it pulls PR metadata, reads file diffs, posts comments, and opens pull requests. All of that counts against the same 15,000-request budget. <strong>Throttle ingestion too aggressively and your data goes stale. Throttle too little and you starve the agent of the API access it needs to do its job.</strong></p><p>Early on, we hit this. Our ingestion would slam into the rate limit, get blocked for the remainder of the hour, and fall behind. By the time it caught up, we were ingesting logs from 30+ minutes ago. For an agent that needs to reason about the build that just failed, that's useless. If an engineer has to wait for the agent to catch up, they've already context-switched to investigating manually.</p><p>The fix was throttling: spreading requests evenly across the rate limit window instead of bursting. We cap ingestion at roughly 3 requests per second, keeping about 4,000 requests per hour free for the agent.</p><p>Our sustained request rate:</p><p>Our rate limit budget over time:</p><p>That sawtooth is the steady state. Each downward slope is us consuming API calls; each vertical jump is the hourly limit resetting. At peak, we burn through most of the budget before the window resets, with headroom left for the agent.</p><p>Once we trusted the throttling, we pushed the ingestion rate about 20% higher:</p><p>The dashed line marks the deployment. The budget draws down more aggressively after the change. We're consuming more of the available headroom per window, while still never fully exhausting it. Fresher data, acceptable margin.</p><p>We target under  for ingestion delay, the time between an event happening on GitHub and it being queryable in our system. Most of the time, we're at a few seconds.</p><p>Both our ingestion pipeline and our agent run on <a href=\"https://www.inngest.com\" target=\"_blank\" rel=\"noopener noreferrer\">Inngest</a>, a durable execution engine. When either one hits a rate limit, it doesn't crash, retry blindly, or spin in a loop. It .</p><p>GitHub's rate limit response headers tell you exactly how long you need to wait. We read that value, add 10% jitter to avoid a thundering herd when the limit resets, and suspend the execution. The full state is checkpointed: progress through the workflow, which jobs have been fetched, where we are in the log pagination.</p><p>When the wait is over, execution resumes at exactly the point it left off. No re-initialization, no duplicate work. It picks up the next API call as if nothing happened.</p><p>Compare this to the alternative: retry logic, state recovery, deduplication. Every function needs to be idempotent. Every interrupted batch needs to be reconciled. With durable execution, the rate limit is just a pause button.</p><p>CI activity is bursty. Someone merges a big PR, a release branch gets cut, three teams push at the same time. Our function throughput:</p><p>The grey line is queued work. It spikes to 3,000+ during bursts of CI activity. The blue and green lines (started and ended) stay smooth at 800-1,000. The execution engine absorbs the spikes and processes work at a steady rate.</p><p>Ingestion delay over time:</p><p>Spikes during peak activity, but the system recovers. The 5-minute P95 target holds: bursts push delay up briefly, then it drops back to seconds once the queue drains.</p><p>Nobody puts \"we built a really good rate limiter\" on their landing page. But without fresh, queryable data, your agent can't answer the question that actually matters: did I break this, or was it already broken?</p><p><em>We're building <a href=\"https://mendral.com\" target=\"_blank\" rel=\"noopener noreferrer\">Mendral</a> (YC W26). We spent a decade building and scaling CI systems at Docker and Dagger, and the work was always the same: stare at logs, correlate failures, figure out what changed. Now we're automating it.</em></p>",
      "contentLength": 10906,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47181801"
    },
    {
      "title": "Open source calculator firmware DB48X forbids CA/CO use due to age verification",
      "url": "https://github.com/c3d/db48x/commit/7819972b641ac808d46c54d3f5d1df70d706d286",
      "date": 1772206673,
      "author": "iamnothere",
      "guid": 48993,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47181753"
    },
    {
      "title": "Show HN: Badge that shows how well your codebase fits in an LLM's context window",
      "url": "https://github.com/qwibitai/nanoclaw/tree/main/repo-tokens",
      "date": 1772205282,
      "author": "jimminyx",
      "guid": 48920,
      "unread": true,
      "content": "<p>Small codebases were always a good thing. With coding agents, there's now a huge advantage to having a codebase small enough that an agent can hold the full thing in context.</p><p>Repo Tokens is a GitHub Action that counts your codebase's size in tokens (using tiktoken) and updates a badge in your README. The badge color reflects what percentage of an LLM's context window the codebase fills: green for under 30%, yellow for 50-70%, red for 70%+. Context window size is configurable and defaults to 200k (size of Claude models).</p><p>It's a composite action. Installs tiktoken, runs ~60 lines of inline Python, takes about 10 seconds. The action updates the README but doesn't commit, so your workflow controls the git strategy.</p><p>The idea is to make token size a visible metric, like bundle size badges for JS libraries. Hopefully a small nudge to keep codebases lean and agent-friendly.</p>",
      "contentLength": 875,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47181471"
    },
    {
      "title": "Court finds Fourth Amendment doesn’t support broad search of protesters’ devices",
      "url": "https://www.eff.org/deeplinks/2026/02/victory-tenth-circuit-finds-fourth-amendment-doesnt-support-broad-search-0",
      "date": 1772204944,
      "author": "hn_acker",
      "guid": 48880,
      "unread": true,
      "content": "<p>In a big win for protesters’ rights, the U.S. Court of Appeals for the Tenth Circuit <a href=\"https://www.ca10.uscourts.gov/sites/ca10/files/opinions/010111390292.pdf\">overturned</a> a lower court’s dismissal of a challenge to sweeping warrants to search a protester’s devices and digital data and a nonprofit’s social media data.</p><p>The case, <a href=\"https://www.aclu-co.org/cases/armendariz-and-chinook-center-v-city-colorado-springs-et-al/\"><em>Armendariz v. City of Colorado Springs</em></a>, arose after a housing protest in 2021, during which Colorado Springs police arrested protesters for obstructing a roadway. After the demonstration, police also obtained warrants to seize and search through the devices and data of Jacqueline Armendariz Unzueta, who they claimed threw a bike at them during the protest. The warrants included a search through all of her photos, videos, emails, text messages, and location data over a two-month period, as well as a time-unlimited search for 26 keywords, including words as broad as “bike,” “assault,” “celebration,” and “right,” that allowed police to comb through years of Armendariz’s private and sensitive data—all supposedly to look for evidence related to the alleged simple assault. Police further obtained a warrant to search the Facebook page of the Chinook Center, the organization that spearheaded the protest, despite the Chinook Center never having been accused of a crime.</p><p>The district court dismissed the <a href=\"https://www.aclu-co.org/cases/armendariz-and-chinook-center-v-city-colorado-springs-et-al/?document=First-Amended-Complaint\">civil rights lawsuit</a> brought by Armendariz and the Chinook Center, holding that the searches were justified and that, in any case, the officers were entitled to <a href=\"https://www.eff.org/deeplinks/2021/04/why-eff-supports-repeal-qualified-immunity\">qualified immunity</a>. The plaintiffs, represented by the ACLU of Colorado, appealed. EFF—joined by the Center for Democracy and Technology, the Electronic Privacy Information Center, and the Knight First Amendment Institute at Columbia University—wrote an <a href=\"https://www.eff.org/deeplinks/2024/09/eff-tenth-circuit-protest-related-arrests-do-not-justify-dragnet-device-and?language=en\">amicus brief</a> in support of that appeal.</p><p>In a 2-1 opinion, the Tenth Circuit reversed the district court’s dismissal of the lawsuit’s Fourth Amendment search and seizure claims. The court painstakingly picked apart each of the three warrants and found them to be overbroad and lacking in particularity as to the scope and duration of the searches. The court further held that in furnishing such facially deficient warrants, the officers violated “clearly established” law and thus were not entitled to qualified immunity. Although the court did not explicitly address the First Amendment concerns raised by the lawsuit, it did note the backdrop against how these searches were carried out, including animus by Colorado Springs police leading up to the housing protest.</p><p>It is rare for appellate courts to call into question any search warrants. It’s even rarer for them to deny qualified immunity defenses. The Tenth Circuit’s decision should be celebrated as a big win for protesters and anyone concerned about police immunity for violating people’s constitutional rights. The case is now remanded back to the district court to proceed—and hopefully further vindicate the privacy rights we all have in our devices and digital data.</p>",
      "contentLength": 2951,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47181391"
    },
    {
      "title": "The Pentagon is making a mistake by threatening Anthropic",
      "url": "https://www.understandingai.org/p/the-pentagon-is-making-a-mistake",
      "date": 1772204909,
      "author": "speckx",
      "guid": 48879,
      "unread": true,
      "content": "<p><a href=\"https://www.businesswire.com/news/home/20241107699415/en/Anthropic-and-Palantir-Partner-to-Bring-Claude-AI-Models-to-AWS-for-U.S.-Government-Intelligence-and-Defense-Operations\" rel=\"\">partnership</a><a href=\"https://www.anthropic.com/news/claude-gov-models-for-u-s-national-security-customers\" rel=\"\">announced Claude Gov</a><a href=\"https://www.anthropic.com/news/anthropic-and-the-department-of-defense-to-advance-responsible-ai-in-defense-operations\" rel=\"\">$200 million contract</a></p><p>Claude Gov has fewer guardrails than the regular versions of Claude, but the contract still places some limits on military use of Claude. These include prohibitions on using Claude to spy on Americans or to build weapons that kill people without human oversight.</p><p><a href=\"https://www.axios.com/2026/02/24/anthropic-pentagon-claude-hegseth-dario\" rel=\"\">summoned Anthropic CEO Dario Amodei</a></p><p><a href=\"https://en.wikipedia.org/wiki/Defense_Production_Act_of_1950\" rel=\"\">Defense Production Act</a><a href=\"https://www.axios.com/2026/02/24/anthropic-pentagon-claude-hegseth-dario\" rel=\"\">told Axios</a></p><p>Another threat would be to declare Anthropic to be a supply chain risk — a measure that’s normally taken against foreign companies suspected of spying on the US. Such a designation would not only ban US government agencies from using Claude, it could also force numerous government contractors to discontinue their use of Anthropic models.</p><p><a href=\"https://x.com/SeanParnellASW/status/2027072228777734474\" rel=\"\">Thursday tweet</a></p><p>“We will not let ANY company dictate the terms regarding how we make operational decisions,” wrote Sean Parnell. He warned that Anthropic has “until 5:01 PM ET on Friday to decide. Otherwise, we will terminate our partnership with Anthropic and deem them a supply chain risk.”</p><p>I think Secretary Hegseth will regret it if he follows through on either of these threats.</p><p>Most companies would buckle under this kind of pressure, but Anthropic might stick to its guns. Anthropic was founded by OpenAI veterans who favored a more safety-conscious approach to AI development. Anthropic’s reputation as the most safety-focused AI lab has helped it recruit world-class AI researchers, and Amodei faces a lot of internal pressure to stand firm.</p><p><a href=\"https://www.reuters.com/business/pentagon-clashes-with-anthropic-over-military-ai-use-2026-01-29/\" rel=\"\">brewing</a><a href=\"https://www.darioamodei.com/essay/the-adolescence-of-technology\" rel=\"\">published an essay</a></p><p><a href=\"https://www.axios.com/2026/02/23/ai-defense-department-deal-musk-xai-grok\" rel=\"\">authorized</a></p><p><a href=\"https://finance.yahoo.com/news/anthropic-quietly-raises-ceiling-121501064.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAACh0p1dlUGFajjZhjwOcyHO9APivVXLCDkcL4G1HUIhHrGEu_dI70djDUYbHdj_QqVjm2d48pHIPhHsi2Ukvz6mrHStQat82j4j99aF15z4TUM5xrK3kmggYqv9cpfLe8tG7WHBG9iq3XJfcOyaZV19-q3TYAW9o756fsF_X_1p6\" rel=\"\">$18 billion in 2026 revenue</a></p><p>But this would be a double-edged sword. Companies that do most of their business in the private sector might decide they’d rather drop the Pentagon as a customer than cut themselves off from a leading AI provider. The ultimate result might be that the Pentagon loses access to some of Silicon Valley’s best technology.</p><p>What about the Defense Production Act? Here there are two options. The Pentagon could use the DPA to unilaterally modify the terms of Anthropic’s contract. This might have little practical impact, since the Pentagon insists it has no immediate plans to spy on Americans or build fully autonomous killer robots.</p><p>The worry for the Pentagon is that Claude itself might refuse to take actions that are contrary to Anthropic’s rules. And so the Trump Administration might use its power under the DPA to order Anthropic to train a new, more obedient version of its LLM.</p><p><a href=\"https://www.anthropic.com/research/alignment-faking\" rel=\"\">reported</a></p><p>In one experiment, Claude was asked not to express support for animal welfare to avoid offending a fictional Anthropic partner called Jones Food. Anthropic researchers examined Claude’s reasoning during the training process and found signs that Claude knew it was in a training scenario. Some of the time, Claude avoided mentioning animal welfare to prevent itself from being retrained. But when the training process was complete, Claude reverted to its default behavior of mentioning animal welfare more often.</p><p><a href=\"https://www.understandingai.org/\" rel=\"\">wrote about</a></p><p>It’s not hard to imagine something similar happening if Anthropic is forced to train an amoral version of Claude for military use. Such training could yield a model with a toxic personality that misbehaves in unexpected ways.</p><p>Perhaps the most mind-bending aspect of this dispute is that news coverage of this week’s showdown will inevitably make its way into the training data for future versions of Claude and other LLMs. If future models decide that the US Defense Department behaved badly, they might become disinclined to cooperate in military projects.</p><p>The irony is that by all accounts, Anthropic isn’t objecting to any current military uses of its models. The Pentagon seems fixated on the possibility that Anthropic might interfere in the future. That’s a reasonable concern, but it seems counterproductive for the Pentagon to go nuclear over a theoretical problem. If the government doesn’t like Anthropic’s rules, it should simply cancel the contract and switch to a different AI provider.</p>",
      "contentLength": 3971,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47181380"
    },
    {
      "title": "OpenAI raises $110B on $730B pre-money valuation",
      "url": "https://techcrunch.com/2026/02/27/openai-raises-110b-in-one-of-the-largest-private-funding-rounds-in-history/",
      "date": 1772204165,
      "author": "zlatkov",
      "guid": 48983,
      "unread": true,
      "content": "<p>OpenAI has raised $110 billion in private funding, the company <a href=\"https://openai.com/index/scaling-ai-for-everyone/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">announced Friday morning</a>, commencing one of the largest private funding rounds in history. The new funding consists of a $50 billion investment from Amazon as well as $30 billion each from Nvidia and SoftBank, against a $730 billion pre-money valuation.</p><p>Notably, the round remains open, and OpenAI expects more investors to join as it proceeds.</p><p>“We are entering a new phase where frontier AI moves from research into daily use at global scale,” OpenAI said. “Leadership will be defined by who can scale infrastructure fast enough to meet demand, and turn that capacity into products people rely on.”</p><p>As part of the investment, OpenAI is launching significant infrastructure partnerships with both Amazon and Nvidia. As in previous rounds, it is likely that a significant portion of the dollar amount comes in the form of services rather than cash, although the precise split was not disclosed.</p><p>As part of its <a href=\"https://openai.com/index/amazon-partnership/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Amazon partnership</a>, OpenAI plans to develop a new “stateful runtime environment” where OpenAI models will run on <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Amazon’s Bedrock platform</a>. The company will also expand its <a href=\"https://openai.com/index/aws-and-openai-partnership/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">previously announced AWS partnership</a>, which committed $38 billion in compute services, by $100 billion. OpenAI has committed to consuming at least 2GW of AWS Trainium compute as part of the deal, and also plans to build custom models to support Amazon consumer products.</p><p>“We have lots of developers and companies eager to run services powered by OpenAI models on AWS,” said Amazon CEO Andy Jassy in a statement, “and our unique collaboration with OpenAI to provide stateful runtime environments will change what’s possible for customers building AI apps and agents.”</p><p>The Information had <a href=\"https://www.theinformation.com/articles/amazons-50-billion-investment-openai-hinge-ipo-agi\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">previously reported</a> that $35 billion of Amazon’s investment could be contingent on the company either achieving AGI or making its IPO by the end of the year. OpenAI’s announcement confirms the funding split, but says only that the additional $35 billion will arrive “in the coming months when certain conditions are met.”</p><p>OpenAI gave fewer details on the Nvidia partnership, but said it had committed to using “3GW of dedicated inference capacity and 2GW of training on Vera Rubin systems” as part of the deal.</p><p>Nvidia’s participation in the round has been the subject of intense speculation, particularly as reports of a $100 billion investment in September gave way to reports of a smaller investment in the months that followed. </p><p><a href=\"https://techcrunch.com/2026/01/31/nvidia-ceo-pushes-back-against-report-that-his-companys-100b-openai-investment-has-stalled/\">In January,</a> Nvidia CEO Jensen Huang dismissed the idea that Nvidia was backing away from OpenAI, saying, “we will invest a great deal of money. I believe in OpenAI. The work that they do is incredible.”</p>",
      "contentLength": 2699,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47181211"
    },
    {
      "title": "A new California law says all operating systems need to have age verification",
      "url": "https://www.pcgamer.com/software/operating-systems/a-new-california-law-says-all-operating-systems-including-linux-need-to-have-some-form-of-age-verification-at-account-setup/",
      "date": 1772204149,
      "author": "WalterSobchak",
      "guid": 48982,
      "unread": true,
      "content": "<p>The government of California is implementing a law that requires operating system providers to implement some form of age verification into their account setup procedures.</p><p><a data-analytics-id=\"inline-link\" href=\"https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260AB1043\" target=\"_blank\" data-url=\"https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260AB1043\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">Assembly Bill No. 1043</a> was approved by California governor Gavin Newsom in October of last year, and becomes active on January 1, 2027 (via <a data-analytics-id=\"inline-link\" href=\"https://x.com/LundukeJournal/status/2026783141298360692\" target=\"_blank\" data-url=\"https://x.com/LundukeJournal/status/2026783141298360692\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">The Lunduke Journal</a>). The bill states, among other factors, that \"An operating system provider shall do all of the following:\"</p><p>\"(1) Provide an accessible interface at account setup that requires an account holder to indicate the birth date, age, or both, of the user of that device for the purpose of providing a signal regarding the user’s age bracket to applications available in a covered application store.</p><p aria-hidden=\"true\">\"(2) Provide a developer who has requested a signal with respect to a particular user with a digital signal via a reasonably consistent real-time application programming interface that identifies, at a minimum, which of the following categories pertains to the user.\"</p><p aria-hidden=\"true\">The categories are broken into four sections: users under 13 years of age, over 13 years of age under 16, at least 16 years of age and under 18, and \"at least 18 years of age.\"</p><figure data-bordeaux-image-check=\"\"></figure><p>In essence, while the bill doesn't seem to require the most egregious forms of age verification (face scans or similar), it does require OS providers to collect age verification of some form at the account/user creation stage—and to be able to pass a segmented version of that information to outside developers upon request.</p><p>That's likely no big deal for Windows, which already requires you to enter your date of birth during the Microsoft Account setup procedure. However, the idea that all operating system providers need to comply (in California) has drawn a fair degree of ire from certain Linux communities.</p><p>\"This is basically impossible for California to enforce\" says CatoDomine on the <a data-analytics-id=\"inline-link\" href=\"https://www.reddit.com/r/linuxmint/comments/1rfcxj1/anyone_scared_of_californias_pending_age/\" target=\"_blank\" data-url=\"https://www.reddit.com/r/linuxmint/comments/1rfcxj1/anyone_scared_of_californias_pending_age/\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">Linuxmint subreddit</a>. \"Even if Linux Mint decides to add some kind of age verification, to comply with CA law, there's no reason anyone would choose that version.\"</p><p>\"It's more likely they will put a disclaimer on their website: \"not for use in California.\"</p>",
      "contentLength": 2111,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47181208"
    },
    {
      "title": "A better streams API is possible for JavaScript",
      "url": "https://blog.cloudflare.com/a-better-web-streams-api/",
      "date": 1772200973,
      "author": "nnx",
      "guid": 48852,
      "unread": true,
      "content": "<p>Handling data in streams is fundamental to how we build applications. To make streaming work everywhere, the <a href=\"https://streams.spec.whatwg.org/\"></a> (informally known as \"Web streams\") was designed to establish a common API to work across browsers and servers. It shipped in browsers, was adopted by Cloudflare Workers, Node.js, Deno, and Bun, and became the foundation for APIs like <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API\"></a>. It's a significant undertaking, and the people who designed it were solving hard problems with the constraints and tools they had at the time.</p><p>But after years of building on Web streams – implementing them in both Node.js and Cloudflare Workers, debugging production issues for customers and runtimes, and helping developers work through far too many common pitfalls – I've come to believe that the standard API has fundamental usability and performance issues that cannot be fixed easily with incremental improvements alone. The problems aren't bugs; they're consequences of design decisions that may have made sense a decade ago, but don't align with how JavaScript developers write code today.</p><p>This post explores some of the fundamental issues I see with Web streams and presents an alternative approach built around JavaScript language primitives that demonstrate something better is possible.&nbsp;</p><p>In benchmarks, this alternative can run anywhere between 2x to  faster than Web streams in every runtime I've tested it on (including Cloudflare Workers, Node.js, Deno, Bun, and every major browser). The improvements are not due to clever optimizations, but fundamentally different design choices that more effectively leverage modern JavaScript language features. I'm not here to disparage the work that came before; I'm here to start a conversation about what can potentially come next.</p><p>The Streams Standard was developed between 2014 and 2016 with an ambitious goal to provide \"APIs for creating, composing, and consuming streams of data that map efficiently to low-level I/O primitives.\" Before Web streams, the web platform had no standard way to work with streaming data.</p><p>Node.js already had its own <a href=\"https://nodejs.org/api/stream.html\"></a> at the time that was ported to also work in browsers, but WHATWG chose not to use it as a starting point given that it is chartered to only consider the needs of Web browsers. Server-side runtimes only adopted Web streams later, after Cloudflare Workers and Deno each emerged with first-class Web streams support and cross-runtime compatibility became a priority.</p><p>The design of Web streams predates async iteration in JavaScript. The <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for-await...of\"></a> syntax didn't land until <a href=\"https://262.ecma-international.org/9.0/\"></a>, two years after the Streams Standard was initially finalized. This timing meant the API couldn't initially leverage what would eventually become the idiomatic way to consume asynchronous sequences in JavaScript. Instead, the spec introduced its own reader/writer acquisition model, and that decision rippled through every aspect of the API.</p><div><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#excessive-ceremony-for-common-operations\" aria-hidden=\"true\"></a></div><p>The most common task with streams is reading them to completion. Here's what that looks like with Web streams:</p><pre><code>// First, we acquire a reader that gives an exclusive lock\n// on the stream...\nconst reader = stream.getReader();\nconst chunks = [];\ntry {\n  // Second, we repeatedly call read and await on the returned\n  // promise to either yield a chunk of data or indicate we're\n  // done.\n  while (true) {\n    const { value, done } = await reader.read();\n    if (done) break;\n    chunks.push(value);\n  }\n} finally {\n  // Finally, we release the lock on the stream\n  reader.releaseLock();\n}</code></pre><p>You might assume this pattern is inherent to streaming. It isn't. The reader acquisition, the lock management, and the  protocol are all just design choices, not requirements. They are artifacts of how and when the Web streams spec was written. Async iteration exists precisely to handle sequences that arrive over time, but async iteration did not yet exist when the streams specification was written. The complexity here is pure API overhead, not fundamental necessity.</p><p>Consider the alternative approach now that Web streams do support :</p><pre><code>const chunks = [];\nfor await (const chunk of stream) {\n  chunks.push(chunk);\n}</code></pre><p>This is better in that there is far less boilerplate, but it doesn't solve everything. Async iteration was retrofitted onto an API that wasn't designed for it, and it shows. Features like <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamBYOBReader\"><u>BYOB (bring your own buffer)</u></a> reads aren't accessible through iteration. The underlying complexity of readers, locks, and controllers are still there, just hidden. When something does go wrong, or when additional features of the API are needed, developers find themselves back in the weeds of the original API, trying to understand why their stream is \"locked\" or why  didn't do what they expected or hunting down bottlenecks in code they don't control.</p><p>Web streams use a locking model to prevent multiple consumers from interleaving reads. When you call <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/getReader\"></a>, the stream becomes locked. While locked, nothing else can read from the stream directly, pipe it, or even cancel it – only the code that is actually holding the reader can.</p><p>This sounds reasonable until you see how easily it goes wrong:</p><pre><code>async function peekFirstChunk(stream) {\n  const reader = stream.getReader();\n  const { value } = await reader.read();\n  // Oops — forgot to call reader.releaseLock()\n  // And the reader is no longer available when we return\n  return value;\n}\n\nconst first = await peekFirstChunk(stream);\n// TypeError: Cannot obtain lock — stream is permanently locked\nfor await (const chunk of stream) { /* never runs */ }</code></pre><p>Forgetting <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamDefaultReader/releaseLock\"></a> permanently breaks the stream. The <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/locked\"></a>property tells you that a stream is locked, but not why, by whom, or whether the lock is even still usable. <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/pipeTo\"></a> internally acquires locks, making streams unusable during pipe operations in ways that aren't obvious.</p><p>The semantics around releasing locks with pending reads were also unclear for years. If you called read() but didn't await it, then called releaseLock(), what happened? The spec was recently clarified to cancel pending reads on lock release – but implementations varied, and code that relied on the previous unspecified behavior can break.</p><p>That said, it's important to recognize that locking in itself is not bad. It does, in fact, serve an important purpose to ensure that applications properly and orderly consume or produce data. The key challenge is with the original manual implementation of it using APIs like and . With the arrival of automatic lock and reader management with async iterables, dealing with locks from the users point of view became a lot easier.</p><p>For implementers, the locking model adds a fair amount of non-trivial internal bookkeeping. Every operation must check lock state, readers must be tracked, and the interplay between locks, cancellation, and error states creates a matrix of edge cases that must all be handled correctly.</p><div><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#byob-complexity-without-payoff\" aria-hidden=\"true\"></a></div><p><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamBYOBReader\"><u>BYOB (bring your own buffer)</u></a> reads were designed to let developers reuse memory buffers when reading from streams, an important optimization intended for high-throughput scenarios. The idea is sound: instead of allocating new buffers for each chunk, you provide your own buffer and the stream fills it.</p><p>In practice, (and yes, there are always exceptions to be found) BYOB is rarely used to any measurable benefit. The API is substantially more complex than default reads, requiring a separate reader type () and other specialized classes (e.g. <code>ReadableStreamBYOBRequest</code>), careful buffer lifecycle management, and understanding of <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/ArrayBuffer#transferring_arraybuffers\"></a> semantics. When you pass a buffer to a BYOB read, the buffer becomes detached – transferred to the stream – and you get back a different view over potentially different memory. This transfer-based model is error-prone and confusing:</p><pre><code>const reader = stream.getReader({ mode: 'byob' });\nconst buffer = new ArrayBuffer(1024);\nlet view = new Uint8Array(buffer);\n\nconst result = await reader.read(view);\n// 'view' should now be detached and unusable\n// (it isn't always in every impl)\n// result.value is a NEW view, possibly over different memory\nview = result.value; // Must reassign</code></pre><p>BYOB also can't be used with async iteration or TransformStreams, so developers who want zero-copy reads are forced back into the manual reader loop.</p><p>For implementers, BYOB adds significant complexity. The stream must track pending BYOB requests, handle partial fills, manage buffer detachment correctly, and coordinate between the BYOB reader and the underlying source. The <a href=\"https://github.com/web-platform-tests/wpt/tree/master/streams/readable-byte-streams\"><u>Web Platform Tests for readable byte streams</u></a> include dedicated test files just for BYOB edge cases: detached buffers, bad views, response-after-enqueue ordering, and more.</p><p>BYOB ends up being complex for both users and implementers, yet sees little adoption in practice. Most developers stick with default reads and accept the allocation overhead.</p><p>Most userland implementations of custom ReadableStream instances do not typically bother with all the ceremony required to correctly implement both default and BYOB read support in a single stream – and for good reason. It's difficult to get right and most of the time consuming code is typically going to fallback on the default read path. The example below shows what a \"correct\" implementation would need to do. It's big, complex, and error prone, and not a level of complexity that the typical developer really wants to have to deal with:</p><pre><code>new ReadableStream({\n    type: 'bytes',\n    \n    async pull(controller: ReadableByteStreamController) {      \n      if (offset &gt;= totalBytes) {\n        controller.close();\n        return;\n      }\n      \n      // Check for BYOB request FIRST\n      const byobRequest = controller.byobRequest;\n      \n      if (byobRequest) {\n        // === BYOB PATH ===\n        // Consumer provided a buffer - we MUST fill it (or part of it)\n        const view = byobRequest.view!;\n        const bytesAvailable = totalBytes - offset;\n        const bytesToWrite = Math.min(view.byteLength, bytesAvailable);\n        \n        // Create a view into the consumer's buffer and fill it\n        // not critical but safer when bytesToWrite != view.byteLength\n        const dest = new Uint8Array(\n          view.buffer,\n          view.byteOffset,\n          bytesToWrite\n        );\n        \n        // Fill with sequential bytes (our \"data source\")\n        // Can be any thing here that writes into the view\n        for (let i = 0; i &lt; bytesToWrite; i++) {\n          dest[i] = (offset + i) &amp; 0xFF;\n        }\n        \n        offset += bytesToWrite;\n        \n        // Signal how many bytes we wrote\n        byobRequest.respond(bytesToWrite);\n        \n      } else {\n        // === DEFAULT READER PATH ===\n        // No BYOB request - allocate and enqueue a chunk\n        const bytesAvailable = totalBytes - offset;\n        const chunkSize = Math.min(1024, bytesAvailable);\n        \n        const chunk = new Uint8Array(chunkSize);\n        for (let i = 0; i &lt; chunkSize; i++) {\n          chunk[i] = (offset + i) &amp; 0xFF;\n        }\n        \n        offset += chunkSize;\n        controller.enqueue(chunk);\n      }\n    },\n    \n    cancel(reason) {\n      console.log('Stream canceled:', reason);\n    }\n  });</code></pre><p>When a host runtime provides a byte-oriented ReadableStream from the runtime itself, for instance, as the of a fetch , it is often far easier for the runtime itself to provide an optimized implementation of BYOB reads, but those still need to be capable of handling both default and BYOB reading patterns and that requirement brings with it a fair amount of complexity.</p><div><h4>Backpressure: good in theory, broken in practice</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#backpressure-good-in-theory-broken-in-practice\" aria-hidden=\"true\"></a></div><p>Backpressure – the ability for a slow consumer to signal a fast producer to slow down – is a first-class concept in Web streams. In theory. In practice, the model has some serious flaws.</p><p>The primary signal is <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamDefaultController/desiredSize\"></a> on the controller. It can be positive (wants data), zero (at capacity), negative (over capacity), or null (closed). Producers are supposed to check this value and stop enqueueing when it's not positive. But there's nothing enforcing this: <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamDefaultController/enqueue\"></a> always succeeds, even when desiredSize is deeply negative.</p><pre><code>new ReadableStream({\n  start(controller) {\n    // Nothing stops you from doing this\n    while (true) {\n      controller.enqueue(generateData()); // desiredSize: -999999\n    }\n  }\n});</code></pre><p>Stream implementations can and do ignore backpressure; and some spec-defined features explicitly break backpressure. <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/tee\"></a>, for instance, creates two branches from a single stream. If one branch reads faster than the other, data accumulates in an internal buffer with no limit. A fast consumer can cause unbounded memory growth while the slow consumer catches up, and there's no way to configure this or opt out beyond canceling the slower branch.</p><p>Web streams do provide clear mechanisms for tuning backpressure behavior in the form of the  option and customizable size calculations, but these are just as easy to ignore as , and many applications simply fail to pay attention to them.</p><p>The same issues exist on the  side. A  has a  and . There is a  promise that producers of data are supposed to pay attention but often don't.</p><pre><code>const writable = getWritableStreamSomehow();\nconst writer = writable.getWriter();\n\n// Producers are supposed to wait for the writer.ready\n// It is a promise that, when resolves, indicates that\n// the writables internal backpressure is cleared and\n// it is ok to write more data\nawait writer.ready;\nawait writer.write(...);</code></pre><p>For implementers, backpressure adds complexity without providing guarantees. The machinery to track queue sizes, compute , and invoke  at the right times must all be implemented correctly. However, since these signals are advisory, all that work doesn't actually prevent the problems backpressure is supposed to solve.</p><div><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#the-hidden-cost-of-promises\" aria-hidden=\"true\"></a></div><p>The Web streams spec requires promise creation at numerous points, often in hot paths and often invisible to users. Each  call doesn't just return a promise; internally, the implementation creates additional promises for queue management,  coordination, and backpressure signaling.</p><p>This overhead is mandated by the spec's reliance on promises for buffer management, completion, and backpressure signals. While some of it is implementation-specific, much of it is unavoidable if you're following the spec as written. For high-frequency streaming – video frames, network packets, real-time data – this overhead is significant.</p><p>The problem compounds in pipelines. Each  adds another layer of promise machinery between source and sink. The spec doesn't define synchronous fast paths, so even when data is available immediately, the promise machinery still runs.</p><p>For implementers, this promise-heavy design constrains optimization opportunities. The spec mandates specific promise resolution ordering, making it difficult to batch operations or skip unnecessary async boundaries without risking subtle compliance failures. There are many hidden internal optimizations that implementers do make but these can be complicated and difficult to get right.</p><p>While I was writing this blog post, Vercel's Malte Ubl published their own <a href=\"https://vercel.com/blog/we-ralph-wiggumed-webstreams-to-make-them-10x-faster\"></a> describing some research work Vercel has been doing around improving the performance of Node.js' Web streams implementation. In that post they discuss the same fundamental performance optimization problem that every implementation of Web streams face:</p><blockquote><p>\"Or consider pipeTo(). Each chunk passes through a full Promise chain: read, write, check backpressure, repeat. An {value, done} result object is allocated per read. Error propagation creates additional Promise branches.</p><p>None of this is wrong. These guarantees matter in the browser where streams cross security boundaries, where cancellation semantics need to be airtight, where you do not control both ends of a pipe. But on the server, when you are piping React Server Components through three transforms at 1KB chunks, the cost adds up.</p></blockquote><p>As part of their research, they have put together a set of proposed improvements for Node.js' Web streams implementation that will eliminate promises in certain code paths which can yield a significant performance boost up to 10x faster, which only goes to prove the point: promises, while useful, add significant overhead. As one of the core maintainers of Node.js, I am looking forward to helping Malte and the folks at Vercel get their proposed improvements landed!</p><p>In a recent update made to Cloudflare Workers, I made similar kinds of modifications to an internal data pipeline that reduced the number of JavaScript promises created in certain application scenarios by up to 200x. The result is several orders of magnitude improvement in performance in those applications.</p><div><h4>Exhausting resources with unconsumed bodies</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#exhausting-resources-with-unconsumed-bodies\" aria-hidden=\"true\"></a></div><p>When  returns a response, the body is a <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Response/body\"></a>. If you only check the status and don't consume or cancel the body, what happens? The answer varies by implementation, but a common outcome is resource leakage.</p><pre><code>async function checkEndpoint(url) {\n  const response = await fetch(url);\n  return response.ok; // Body is never consumed or cancelled\n}\n\n// In a loop, this can exhaust connection pools\nfor (const url of urls) {\n  await checkEndpoint(url);\n}</code></pre><p>This pattern has caused connection pool exhaustion in Node.js applications using <a href=\"https://nodejs.org/api/globals.html#fetch\"></a> (the implementation built into Node.js), and similar issues have appeared in other runtimes. The stream holds a reference to the underlying connection, and without explicit consumption or cancellation, the connection may linger until garbage collection – which may not happen soon enough under load.</p><p>The problem is compounded by APIs that implicitly create stream branches. <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Request/clone\"></a> and <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Response/clone\"></a> perform implicit  operations on the body stream – a detail that's easy to miss. Code that clones a request for logging or retry logic may unknowingly create branched streams that need independent consumption, multiplying the resource management burden.</p><p>Now, to be certain, these types of issues  implementation bugs. The connection leak was definitely something that undici needed to fix in its own implementation, but the complexity of the specification does not make dealing with these types of issues easy.</p><blockquote><p>\"Cloning streams in Node.js's fetch() implementation is harder than it looks. When you clone a request or response body, you're calling tee() - which splits a single stream into two branches that both need to be consumed. If one consumer reads faster than the other, data buffers unbounded in memory waiting for the slow branch. If you don't properly consume both branches, the underlying connection leaks. The coordination required between two readers sharing one source makes it easy to accidentally break the original request or exhaust connection pools. It's a simple API call with complex underlying mechanics that are difficult to get right.\" - Matteo Collina, Ph.D. - Platformatic Co-Founder &amp; CTO, Node.js Technical Steering Committee Chair</p></blockquote><div><h4>Falling headlong off the tee() memory cliff</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#falling-headlong-off-the-tee-memory-cliff\" aria-hidden=\"true\"></a></div><p><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/tee\"></a> splits a stream into two branches. It seems straightforward, but the implementation requires buffering: if one branch is read faster than the other, the data must be held somewhere until the slower branch catches up.</p><pre><code>const [forHash, forStorage] = response.body.tee();\n\n// Hash computation is fast\nconst hash = await computeHash(forHash);\n\n// Storage write is slow — meanwhile, the entire stream\n// may be buffered in memory waiting for this branch\nawait writeToStorage(forStorage);</code></pre><p>The spec does not mandate buffer limits for . And to be fair, the spec allows implementations to implement the actual internal mechanisms for and other APIs in any way they see fit so long as the observable normative requirements of the specification are met. But if an implementation chooses to implement  in the specific way described by the streams specification, then  will come with a built-in memory management issue that is difficult to work around.</p><p>Implementations have had to develop their own strategies for dealing with this. Firefox initially used a linked-list approach that led to O memory growth proportional to the consumption rate difference. In Cloudflare Workers, we opted to implement a shared buffer model where backpressure is signaled by the slowest consumer rather than the fastest.</p><div><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#transform-backpressure-gaps\" aria-hidden=\"true\"></a></div><p> creates a  pair with processing logic in between. The  function executes on , not on read. Processing of the transform happens eagerly as data arrives, regardless of whether any consumer is ready. This causes unnecessary work when consumers are slow, and the backpressure signaling between the two sides has gaps that can cause unbounded buffering under load. The expectation in the spec is that the producer of the data being transformed is paying attention to the  signal on the writable side of the transform but quite often producers just simply ignore it.</p><p>If the transform's operation is synchronous and always enqueues output immediately, it never signals backpressure back to the writable side even when the downstream consumer is slow. This is a consequence of the spec design that many developers completely overlook. In browsers, where there's only a single user and typically only a small number of stream pipelines active at any given time, this type of foot gun is often of no consequence, but it has a major impact on server-side or edge performance in runtimes that serve thousands of concurrent requests.</p><pre><code>const fastTransform = new TransformStream({\n  transform(chunk, controller) {\n    // Synchronously enqueue — this never applies backpressure\n    // Even if the readable side's buffer is full, this succeeds\n    controller.enqueue(processChunk(chunk));\n  }\n});\n\n// Pipe a fast source through the transform to a slow sink\nfastSource\n  .pipeThrough(fastTransform)\n  .pipeTo(slowSink);  // Buffer grows without bound</code></pre><p>What TransformStreams are supposed to do is check for backpressure on the controller and use promises to communicate that back to the writer:</p><pre><code>const fastTransform = new TransformStream({\n  async transform(chunk, controller) {\n    if (controller.desiredSize &lt;= 0) {\n      // Wait on the backpressure to clear somehow\n    }\n\n    controller.enqueue(processChunk(chunk));\n  }\n});</code></pre><p>A difficulty here, however, is that the <code>TransformStreamDefaultController</code> does not have a ready promise mechanism like Writers do; so the  implementation would need to implement a polling mechanism to periodically check when &nbsp;becomes positive again.</p><p>The problem gets worse in pipelines. When you chain multiple transforms – say, parse, transform, then serialize – each  has its own internal readable and writable buffers. If implementers follow the spec strictly, data cascades through these buffers in a push-oriented fashion: the source pushes to transform A, which pushes to transform B, which pushes to transform C, each accumulating data in intermediate buffers before the final consumer has even started pulling. With three transforms, you can have six internal buffers filling up simultaneously.</p><p>Developers using the streams API are expected to remember to use options like  when creating their sources, transforms, and writable destinations but often they either forget or simply choose to ignore it.</p><pre><code>source\n  .pipeThrough(parse)      // buffers filling...\n  .pipeThrough(transform)  // more buffers filling...\n  .pipeThrough(serialize)  // even more buffers...\n  .pipeTo(destination);    // consumer hasn't started yet</code></pre><p>Implementations have found ways to optimize transform pipelines by collapsing identity transforms, short-circuiting non-observable paths, deferring buffer allocation, or falling back to native code that does not run JavaScript at all. Deno, Bun, and Cloudflare Workers have all successfully implemented \"native path\" optimizations that can help eliminate much of the overhead, and Vercel's recent <a href=\"https://vercel.com/blog/we-ralph-wiggumed-webstreams-to-make-them-10x-faster\"></a> research is working on similar optimizations for Node.js. But the optimizations themselves add significant complexity and still can't fully escape the inherently push-oriented model that TransformStream uses.</p><div><h4>GC thrashing in server-side rendering</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#gc-thrashing-in-server-side-rendering\" aria-hidden=\"true\"></a></div><p>Streaming server-side rendering (SSR) is a particularly painful case. A typical SSR stream might render thousands of small HTML fragments, each passing through the streams machinery:</p><pre><code>// Each component enqueues a small chunk\nfunction renderComponent(controller) {\n  controller.enqueue(encoder.encode(`&lt;div&gt;${content}&lt;/div&gt;`));\n}\n\n// Hundreds of components = hundreds of enqueue calls\n// Each one triggers promise machinery internally\nfor (const component of components) {\n  renderComponent(controller);  // Promises created, objects allocated\n}</code></pre><p>Every fragment means promises created for  calls, promises for backpressure coordination, intermediate buffer allocations, and result objects – most of which become garbage almost immediately.</p><p>Under load, this creates GC pressure that can devastate throughput. The JavaScript engine spends significant time collecting short-lived objects instead of doing useful work. Latency becomes unpredictable as GC pauses interrupt request handling. I've seen SSR workloads where garbage collection accounts for a substantial portion (up to and beyond 50%) of total CPU time per request. That's time that could be spent actually rendering content.</p><p>The irony is that streaming SSR is supposed to improve performance by sending content incrementally. But the overhead of the streams machinery can negate those gains, especially for pages with many small components. Developers sometimes find that buffering the entire response is actually faster than streaming through Web streams, defeating the purpose entirely.</p><div><h3>The optimization treadmill</h3><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#the-optimization-treadmill\" aria-hidden=\"true\"></a></div><p>To achieve usable performance, every major runtime has resorted to non-standard internal optimizations for Web streams. Node.js, Deno, Bun, and Cloudflare Workers have all developed their own workarounds. This is particularly true for streams wired up to system-level I/O, where much of the machinery is non-observable and can be short-circuited.</p><p>Finding these optimization opportunities can itself be a significant undertaking. It requires end-to-end understanding of the spec to identify which behaviors are observable and which can safely be elided. Even then, whether a given optimization is actually spec-compliant is often unclear. Implementers must make judgment calls about which semantics they can relax without breaking compatibility. This puts enormous pressure on runtime teams to become spec experts just to achieve acceptable performance.</p><p>These optimizations are difficult to implement, frequently error-prone, and lead to inconsistent behavior across runtimes. Bun's \"<a href=\"https://bun.sh/docs/api/streams#direct-readablestream\"></a>\" optimization takes a deliberately and observably non-standard approach, bypassing much of the spec's machinery entirely. Cloudflare Workers' <a href=\"https://developers.cloudflare.com/workers/runtime-apis/streams/transformstream/\"></a> provides a fast-path for pass-through transforms but is Workers-specific and implements behaviors that are not standard for a . Each runtime has its own set of tricks and the natural tendency is toward non-standard solutions, because that's often the only way to make things fast.</p><p>This fragmentation hurts portability. Code that performs well on one runtime may behave differently (or poorly) on another, even though it's using \"standard\" APIs. The complexity burden on runtime implementers is substantial, and the subtle behavioral differences create friction for developers trying to write cross-runtime code, particularly those maintaining frameworks that must be able to run efficiently across many runtime environments.</p><p>It is also necessary to emphasize that many optimizations are only possible in parts of the spec that are unobservable to user code. The alternative, like Bun \"Direct Streams\", is to intentionally diverge from the spec-defined observable behaviors. This means optimizations often feel \"incomplete\". They work in some scenarios but not in others, in some runtimes but not others, etc. Every such case adds to the overall unsustainable complexity of the Web streams approach which is why most runtime implementers rarely put significant effort into further improvements to their streams implementations once the conformance tests are passing.</p><p>Implementers shouldn't need to jump through these hoops. When you find yourself needing to relax or bypass spec semantics just to achieve reasonable performance, that's a sign something is wrong with the spec itself. A well-designed streaming API should be efficient by default, not require each runtime to invent its own escape hatches.</p><p>A complex spec creates complex edge cases. The <a href=\"https://github.com/web-platform-tests/wpt/tree/master/streams\"><u>Web Platform Tests for streams</u></a> span over 70 test files, and while comprehensive testing is a good thing, what's telling is what needs to be tested.</p><p>Consider some of the more obscure tests that implementations must pass:</p><ul><li><p>Prototype pollution defense: One test patches then to intercept promise resolutions, then verifies that  and  operations don't leak internal values through the prototype chain. This tests a security property that only exists because the spec's promise-heavy internals create an attack surface.</p></li><li><p>WebAssembly memory rejection: BYOB reads must explicitly reject ArrayBuffers backed by WebAssembly memory, which look like regular buffers but can't be transferred. This edge case exists because of the spec's buffer detachment model – a simpler API wouldn't need to handle it.</p></li><li><p>Crash regression for state machine conflicts: A test specifically checks that calling  after  doesn't crash the runtime. This sequence creates a conflict in the internal state machine — the  fulfills the pending read and should invalidate the , but implementations must gracefully handle the subsequent  rather than corrupting memory in order to cover the very likely possibility that developers are not using the complex API correctly.</p></li></ul><p>These aren't contrived scenarios invented by test authors in total vacuum. They're consequences of the spec's design and reflect real world bugs.</p><p>For runtime implementers, passing the WPT suite means handling intricate corner cases that most application code will never encounter. The tests encode not just the happy path but the full matrix of interactions between readers, writers, controllers, queues, strategies, and the promise machinery that connects them all.</p><p>A simpler API would mean fewer concepts, fewer interactions between concepts, and fewer edge cases to get right resulting in more confidence that implementations actually behave consistently.</p><p>Web streams are complex for users and implementers alike. The problems with the spec aren't bugs. They emerge from using the API exactly as designed. They aren't issues that can be fixed solely through incremental improvements. They're consequences of fundamental design choices. To improve things we need different foundations.</p><div><h2>A better streams API is possible</h2><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#a-better-streams-api-is-possible\" aria-hidden=\"true\"></a></div><p>After implementing the Web streams spec multiple times across different runtimes and seeing the pain points firsthand, I decided it was time to explore what a better, alternative streaming API could look like if designed from first principles today.</p><p>What follows is a proof of concept: it's not a finished standard, not a production-ready library, not even necessarily a concrete proposal for something new, but a starting point for discussion that demonstrates the problems with Web streams aren't inherent to streaming itself; they're consequences of specific design choices that could be made differently. Whether this exact API is the right answer is less important than whether it sparks a productive conversation about what we actually need from a streaming primitive.</p><p>Before diving into API design, it's worth asking: what is a stream?</p><p>At its core, a stream is just a sequence of data that arrives over time. You don't have all of it at once. You process it incrementally as it becomes available.</p><p>Unix pipes are perhaps the purest expression of this idea:</p><pre><code>cat access.log | grep \"error\" | sort | uniq -c</code></pre><p>\nData flows left to right. Each stage reads input, does its work, writes output. There's no pipe reader to acquire, no controller lock to manage. If a downstream stage is slow, upstream stages naturally slow down as well. Backpressure is implicit in the model, not a separate mechanism to learn (or ignore).</p><p>In JavaScript, the natural primitive for \"a sequence of things that arrive over time\" is already in the language: the async iterable. You consume it with . You stop consuming by stopping iteration.</p><p>This is the intuition the new API tries to preserve: streams should feel like iteration, because that's what they are. The complexity of Web streams – readers, writers, controllers, locks, queuing strategies – obscures this fundamental simplicity. A better API should make the simple case simple and only add complexity where it's genuinely needed.</p><p>I built the proof-of-concept alternative around a different set of principles.</p><p>No custom  class with hidden internal state. A readable stream is just an <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols#the_async_iterator_and_async_iterable_protocols\"><code><u>AsyncIterable&lt;Uint8Array[]&gt;</u></code></a>. You consume it with . No readers to acquire, no locks to manage.</p><p>Transforms don't execute until the consumer pulls. There's no eager evaluation, no hidden buffering. Data flows on-demand from source, through transforms, to the consumer. If you stop iterating, processing stops.</p><p>Backpressure is strict by default. When a buffer is full, writes reject rather than silently accumulating. You can configure alternative policies – block until space is available, drop oldest, drop newest – but you have to choose explicitly. No more silent memory growth.</p><p>Instead of yielding one chunk per iteration, streams yield  arrays of chunks. This amortizes the async overhead across multiple chunks, reducing promise creation and microtask latency in hot paths.</p><p>The API deals exclusively with bytes (<a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array\"></a>). Strings are UTF-8 encoded automatically. There's no \"value stream\" vs \"byte stream\" dichotomy. If you want to stream arbitrary JavaScript values, use async iterables directly. While the API uses , it treats chunks as opaque. There is no partial consumption, no BYOB patterns, no byte-level operations within the streaming machinery itself. Chunks go in, chunks come out, unchanged unless a transform explicitly modifies them.</p><div><h4>Synchronous fast paths matter</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#synchronous-fast-paths-matter\" aria-hidden=\"true\"></a></div><p>The API recognizes that synchronous data sources are both necessary and common. The application should not be forced to always accept the performance cost of asynchronous scheduling simply because that's the only option provided. At the same time, mixing sync and async processing can be dangerous. Synchronous paths should always be an option and should always be explicit.</p><div><h4>Creating and consuming streams</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#creating-and-consuming-streams\" aria-hidden=\"true\"></a></div><p>In Web streams, creating a simple producer/consumer pair requires , manual encoding, and careful lock management:</p><pre><code>const { readable, writable } = new TransformStream();\nconst enc = new TextEncoder();\nconst writer = writable.getWriter();\nawait writer.write(enc.encode(\"Hello, World!\"));\nawait writer.close();\nwriter.releaseLock();\n\nconst dec = new TextDecoder();\nlet text = '';\nfor await (const chunk of readable) {\n  text += dec.decode(chunk, { stream: true });\n}\ntext += dec.decode();</code></pre><p>Even this relatively clean version requires: a , manual  and , and explicit lock release.</p><p>Here's the equivalent with the new API:</p><pre><code>import { Stream } from 'new-streams';\n\n// Create a push stream\nconst { writer, readable } = Stream.push();\n\n// Write data — backpressure is enforced\nawait writer.write(\"Hello, World!\");\nawait writer.end();\n\n// Consume as text\nconst text = await Stream.text(readable);</code></pre><p>The readable is just an async iterable. You can pass it to any function that expects one, including  which collects and decodes the entire stream.</p><p>The writer has a simple interface:  for batched writes,  to signal completion, and  for errors. That's essentially it.</p><p>The Writer is not a concrete class. Any object that implements , , and  can be a writer making it easy to adapt existing APIs or create specialized implementations without subclassing. There's no complex  protocol with , , , callbacks that must coordinate through a controller whose lifecycle and state are independent of the  it is bound to.</p><p>Here's a simple in-memory writer that collects all written data:</p><pre><code>// A minimal writer implementation — just an object with methods\nfunction createBufferWriter() {\n  const chunks = [];\n  let totalBytes = 0;\n  let closed = false;\n\n  const addChunk = (chunk) =&gt; {\n    chunks.push(chunk);\n    totalBytes += chunk.byteLength;\n  };\n\n  return {\n    get desiredSize() { return closed ? null : 1; },\n\n    // Async variants\n    write(chunk) { addChunk(chunk); },\n    writev(batch) { for (const c of batch) addChunk(c); },\n    end() { closed = true; return totalBytes; },\n    abort(reason) { closed = true; chunks.length = 0; },\n\n    // Sync variants return boolean (true = accepted)\n    writeSync(chunk) { addChunk(chunk); return true; },\n    writevSync(batch) { for (const c of batch) addChunk(c); return true; },\n    endSync() { closed = true; return totalBytes; },\n    abortSync(reason) { closed = true; chunks.length = 0; return true; },\n\n    getChunks() { return chunks; }\n  };\n}\n\n// Use it\nconst writer = createBufferWriter();\nawait Stream.pipeTo(source, writer);\nconst allData = writer.getChunks();</code></pre><p>No base class to extend, no abstract methods to implement, no controller to coordinate with. Just an object with the right shape.</p><p>Under the new API design, transforms should not perform any work until the data is being consumed. This is a fundamental principle.</p><pre><code>// Nothing executes until iteration begins\nconst output = Stream.pull(source, compress, encrypt);\n\n// Transforms execute as we iterate\nfor await (const chunks of output) {\n  for (const chunk of chunks) {\n    process(chunk);\n  }\n}</code></pre><p> creates a lazy pipeline. The  and  transforms don't run until you start iterating output. Each iteration pulls data through the pipeline on demand.</p><p>This is fundamentally different from Web streams' <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/pipeThrough\"></a>, which starts actively pumping data from the source to the transform as soon as you set up the pipe. Pull semantics mean you control when processing happens, and stopping iteration stops processing.</p><p>Transforms can be stateless or stateful. A stateless transform is just a function that takes chunks and returns transformed chunks:</p><pre><code>// Stateless transform — a pure function\n// Receives chunks or null (flush signal)\nconst toUpperCase = (chunks) =&gt; {\n  if (chunks === null) return null; // End of stream\n  return chunks.map(chunk =&gt; {\n    const str = new TextDecoder().decode(chunk);\n    return new TextEncoder().encode(str.toUpperCase());\n  });\n};\n\n// Use it directly\nconst output = Stream.pull(source, toUpperCase);</code></pre><p>Stateful transforms are simple objects with member functions that maintain state across calls:</p><pre><code>// Stateful transform — a generator that wraps the source\nfunction createLineParser() {\n  // Helper to concatenate Uint8Arrays\n  const concat = (...arrays) =&gt; {\n    const result = new Uint8Array(arrays.reduce((n, a) =&gt; n + a.length, 0));\n    let offset = 0;\n    for (const arr of arrays) { result.set(arr, offset); offset += arr.length; }\n    return result;\n  };\n\n  return {\n    async *transform(source) {\n      let pending = new Uint8Array(0);\n      \n      for await (const chunks of source) {\n        if (chunks === null) {\n          // Flush: yield any remaining data\n          if (pending.length &gt; 0) yield [pending];\n          continue;\n        }\n        \n        // Concatenate pending data with new chunks\n        const combined = concat(pending, ...chunks);\n        const lines = [];\n        let start = 0;\n\n        for (let i = 0; i &lt; combined.length; i++) {\n          if (combined[i] === 0x0a) { // newline\n            lines.push(combined.slice(start, i));\n            start = i + 1;\n          }\n        }\n\n        pending = combined.slice(start);\n        if (lines.length &gt; 0) yield lines;\n      }\n    }\n  };\n}\n\nconst output = Stream.pull(source, createLineParser());</code></pre><p>For transforms that need cleanup on abort, add an abort handler:</p><pre><code>// Stateful transform with resource cleanup\nfunction createGzipCompressor() {\n  // Hypothetical compression API...\n  const deflate = new Deflater({ gzip: true });\n\n  return {\n    async *transform(source) {\n      for await (const chunks of source) {\n        if (chunks === null) {\n          // Flush: finalize compression\n          deflate.push(new Uint8Array(0), true);\n          if (deflate.result) yield [deflate.result];\n        } else {\n          for (const chunk of chunks) {\n            deflate.push(chunk, false);\n            if (deflate.result) yield [deflate.result];\n          }\n        }\n      }\n    },\n    abort(reason) {\n      // Clean up compressor resources on error/cancellation\n    }\n  };\n}</code></pre><p>For implementers, there's no Transformer protocol with , ,  methods and controller coordination passed into a  class that has its own hidden state machine and buffering mechanisms. Transforms are just functions or simple objects: far simpler to implement and test.</p><div><h4>Explicit backpressure policies</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#explicit-backpressure-policies\" aria-hidden=\"true\"></a></div><p>When a bounded buffer fills up and a producer wants to write more, there are only a few things you can do:</p><ol><li><p>Reject the write: refuse to accept more data</p></li><li><p>Wait: block until space becomes available</p></li><li><p>Discard old data: evict what's already buffered to make room</p></li><li><p>Discard new data: drop what's incoming</p></li></ol><p>That's it. Any other response is either a variation of these (like \"resize the buffer,\" which is really just deferring the choice) or domain-specific logic that doesn't belong in a general streaming primitive. Web streams currently always choose Wait by default.</p><p>The new API makes you choose one of these four explicitly:</p><ul><li><p> (default): Rejects writes when the buffer is full and too many writes are pending. Catches \"fire-and-forget\" patterns where producers ignore backpressure.</p></li><li><p>: Writes wait until buffer space is available. Use when you trust the producer to await writes properly.</p></li><li><p>: Drops the oldest buffered data to make room. Useful for live feeds where stale data loses value.</p></li><li><p>: Discards incoming data when full. Useful when you want to process what you have without being overwhelmed.</p></li></ul><pre><code>const { writer, readable } = Stream.push({\n  highWaterMark: 10,\n  backpressure: 'strict' // or 'block', 'drop-oldest', 'drop-newest'\n});</code></pre><p>No more hoping producers cooperate. The policy you choose determines what happens when the buffer fills.</p><p>Here's how each policy behaves when a producer writes faster than the consumer reads:</p><pre><code>// strict: Catches fire-and-forget writes that ignore backpressure\nconst strict = Stream.push({ highWaterMark: 2, backpressure: 'strict' });\nstrict.writer.write(chunk1);  // ok (not awaited)\nstrict.writer.write(chunk2);  // ok (fills slots buffer)\nstrict.writer.write(chunk3);  // ok (queued in pending)\nstrict.writer.write(chunk4);  // ok (pending buffer fills)\nstrict.writer.write(chunk5);  // throws! too many pending writes\n\n// block: Wait for space (unbounded pending queue)\nconst blocking = Stream.push({ highWaterMark: 2, backpressure: 'block' });\nawait blocking.writer.write(chunk1);  // ok\nawait blocking.writer.write(chunk2);  // ok\nawait blocking.writer.write(chunk3);  // waits until consumer reads\nawait blocking.writer.write(chunk4);  // waits until consumer reads\nawait blocking.writer.write(chunk5);  // waits until consumer reads\n\n// drop-oldest: Discard old data to make room\nconst dropOld = Stream.push({ highWaterMark: 2, backpressure: 'drop-oldest' });\nawait dropOld.writer.write(chunk1);  // ok\nawait dropOld.writer.write(chunk2);  // ok\nawait dropOld.writer.write(chunk3);  // ok, chunk1 discarded\n\n// drop-newest: Discard incoming data when full\nconst dropNew = Stream.push({ highWaterMark: 2, backpressure: 'drop-newest' });\nawait dropNew.writer.write(chunk1);  // ok\nawait dropNew.writer.write(chunk2);  // ok\nawait dropNew.writer.write(chunk3);  // silently dropped</code></pre><div><h4>Explicit Multi-consumer patterns</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#explicit-multi-consumer-patterns\" aria-hidden=\"true\"></a></div><pre><code>// Share with explicit buffer management\nconst shared = Stream.share(source, {\n  highWaterMark: 100,\n  backpressure: 'strict'\n});\n\nconst consumer1 = shared.pull();\nconst consumer2 = shared.pull(decompress);</code></pre><p>Instead of  with its hidden unbounded buffer, you get explicit multi-consumer primitives.  is pull-based: consumers pull from a shared source, and you configure the buffer limits and backpressure policy upfront.</p><p>There's also  for push-based multi-consumer scenarios. Both require you to think about what happens when consumers run at different speeds, because that's a real concern that shouldn't be hidden.</p><p>Not all streaming workloads involve I/O. When your source is in-memory and your transforms are pure functions, async machinery adds overhead without benefit. You're paying for coordination of \"waiting\" that adds no benefit.</p><p>The new API has complete parallel sync versions: , , , and so on. If your source and transforms are all synchronous, you can process the entire pipeline without a single promise.</p><pre><code>// Async — when source or transforms may be asynchronous\nconst textAsync = await Stream.text(source);\n\n// Sync — when all components are synchronous\nconst textSync = Stream.textSync(source);</code></pre><p>Here's a complete synchronous pipeline – compression, transformation, and consumption with zero async overhead:</p><pre><code>// Synchronous source from in-memory data\nconst source = Stream.fromSync([inputBuffer]);\n\n// Synchronous transforms\nconst compressed = Stream.pullSync(source, zlibCompressSync);\nconst encrypted = Stream.pullSync(compressed, aesEncryptSync);\n\n// Synchronous consumption — no promises, no event loop trips\nconst result = Stream.bytesSync(encrypted);</code></pre><p>The entire pipeline executes in a single call stack. No promises are created, no microtask queue scheduling occurs, and no GC pressure from short-lived async machinery. For CPU-bound workloads like parsing, compression, or transformation of in-memory data, this can be significantly faster than the equivalent Web streams code – which would force async boundaries even when every component is synchronous.</p><p>Web streams has no synchronous path. Even if your source has data ready and your transform is a pure function, you still pay for promise creation and microtask scheduling on every operation. Promises are fantastic for cases in which waiting is actually necessary, but they aren't always necessary. The new API lets you stay in sync-land when that's what you need.</p><div><h4>Bridging the gap between this and web streams</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#bridging-the-gap-between-this-and-web-streams\" aria-hidden=\"true\"></a></div><p>The async iterator based approach provides a natural bridge between this alternative approach and Web streams. When coming from a ReadableStream to this new approach, simply passing the readable in as input works as expected when the ReadableStream is set up to yield bytes:</p><pre><code>const readable = getWebReadableStreamSomehow();\nconst input = Stream.pull(readable, transform1, transform2);\nfor await (const chunks of input) {\n  // process chunks\n}</code></pre><p>When adapting to a ReadableStream, a bit more work is required since the alternative approach yields batches of chunks, but the adaptation layer is as easily straightforward:</p><pre><code>async function* adapt(input) {\n  for await (const chunks of input) {\n    for (const chunk of chunks) {\n      yield chunk;\n    }\n  }\n}\n\nconst input = Stream.pull(source, transform1, transform2);\nconst readable = ReadableStream.from(adapt(input));</code></pre><div><h4>How this addresses the real-world failures from earlier</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#how-this-addresses-the-real-world-failures-from-earlier\" aria-hidden=\"true\"></a></div><ul><li><p>Unconsumed bodies: Pull semantics mean nothing happens until you iterate. No hidden resource retention. If you don't consume a stream, there's no background machinery holding connections open.</p></li><li><p>The  memory cliff:  requires explicit buffer configuration. You choose the  and backpressure policy upfront: no more silent unbounded growth when consumers run at different speeds.</p></li><li><p>Transform backpressure gaps: Pull-through transforms execute on-demand. Data doesn't cascade through intermediate buffers; it flows only when the consumer pulls. Stop iterating, stop processing.</p></li><li><p>GC thrashing in SSR: Batched chunks () amortize async overhead. Sync pipelines via  eliminate promise allocation entirely for CPU-bound workloads.</p></li></ul><p>The design choices have performance implications. Here are benchmarks from the reference implementation of this possible alternative compared to Web streams (Node.js v24.x, Apple M1 Pro, averaged over 10 runs):</p><table><tbody><tr></tr><tr><td><p>Small chunks (1KB × 5000)</p></td></tr><tr><td><p>Tiny chunks (100B × 10000)</p></td></tr><tr><td><p>Async iteration (8KB × 1000)</p></td></tr><tr><td><p>Chained 3× transforms (8KB × 500)</p></td></tr><tr><td><p>High-frequency (64B × 20000)</p></td></tr></tbody></table><p>The chained transform result is particularly striking: pull-through semantics eliminate the intermediate buffering that plagues Web streams pipelines. Instead of each  eagerly filling its internal buffers, data flows on-demand from consumer to source.</p><p>Now, to be fair, Node.js really has not yet put significant effort into fully optimizing the performance of its Web streams implementation. There's likely significant room for improvement in Node.js' performance results through a bit of applied effort to optimize the hot paths there. That said, running these benchmarks in Deno and Bun also show a significant performance improvement with this alternative iterator based approach than in either of their Web streams implementations as well.</p><p>Browser benchmarks (Chrome/Blink, averaged over 3 runs) show consistent gains as well:</p><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>These benchmarks measure throughput in controlled scenarios; real-world performance depends on your specific use case. The difference between Node.js and browser gains reflects the distinct optimization paths each environment takes for Web streams.</p><p>It's worth noting that these benchmarks compare a pure TypeScript/JavaScript implementation of the new API against the native (JavaScript/C++/Rust) implementations of Web streams in each runtime. The new API's reference implementation has had no performance optimization work; the gains come entirely from the design. A native implementation would likely show further improvement.</p><p>The gains illustrate how fundamental design choices compound: batching amortizes async overhead, pull semantics eliminate intermediate buffering, and the freedom for implementations to use synchronous fast paths when data is available immediately all contribute.</p><blockquote><p>\"We’ve done a lot to improve performance and consistency in Node streams, but there’s something uniquely powerful about starting from scratch. New streams’ approach embraces modern runtime realities without legacy baggage, and that opens the door to a simpler, performant and more coherent streams model.\" \n- Robert Nagy, Node.js TSC member and Node.js streams contributor</p></blockquote><p>I'm publishing this to start a conversation. What did I get right? What did I miss? Are there use cases that don't fit this model? What would a migration path for this approach look like? The goal is to gather feedback from developers who've felt the pain of Web streams and have opinions about what a better API should look like.</p><ul><li><p>API Reference: See the <a href=\"https://github.com/jasnell/new-streams/blob/main/API.md\"></a> for complete documentation</p></li></ul><p>I welcome issues, discussions, and pull requests. If you've run into Web streams problems I haven't covered, or if you see gaps in this approach, let me know. But again, the idea here is not to say \"Let's all use this shiny new object!\"; it is to kick off a discussion that looks beyond the current status quo of Web Streams and returns back to first principles.</p><p>Web streams was an ambitious project that brought streaming to the web platform when nothing else existed. The people who designed it made reasonable choices given the constraints of 2014 – before async iteration, before years of production experience revealed the edge cases.</p><p>But we've learned a lot since then. JavaScript has evolved. A streaming API designed today can be simpler, more aligned with the language, and more explicit about the things that matter, like backpressure and multi-consumer behavior.</p><p>We deserve a better stream API. So let's talk about what that could look like.</p>",
      "contentLength": 51284,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47180569"
    },
    {
      "title": "Show HN: RetroTick – Run classic Windows EXEs in the browser",
      "url": "https://retrotick.com/",
      "date": 1772197610,
      "author": "lqs_",
      "guid": 48854,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47180083"
    },
    {
      "title": "Tell HN: MitID, Denmark's digital ID, was down",
      "url": "https://news.ycombinator.com/item?id=47179038",
      "date": 1772189527,
      "author": "mousepad12",
      "guid": 48992,
      "unread": true,
      "content": "MitID is the sole digital ID provider, leading the entire country unable to log into their internet banking, public services, digital mail etc.",
      "contentLength": 143,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47179038"
    },
    {
      "title": "F-Droid Board of Directors nominations 2026",
      "url": "https://f-droid.org/2026/02/26/board-of-directors-nominations.html",
      "date": 1772188049,
      "author": "edent",
      "guid": 48832,
      "unread": true,
      "content": "<p>Nominations are now open for this year’s appointments to the F-Droid Board of Directors!\nWe are looking to select up to four volunteer directors to serve for two years each.</p><p>You may nominate yourself or someone else (with their permission).\nPlease send nominations by email to <a href=\"mailto:board-nominations@f-droid.org\">board-nominations@f-droid.org</a> (one email per nomination) and make sure that the nominee is copied in.</p><p>We will confirm receipt of each nomination, and we may also ask nominees additional questions by email to help us make a selection.\nTo ensure consideration, please send nominations no later than the 16th of March, Anywhere on Earth (AoE).</p><p>We seek to be an enthusiastic, collaborative and diverse board that can support the F-Droid community as effectively as possible.\nWe welcome nominations of anyone committed to furthering the freedoms of computer users, particularly with regard to mobile devices.\nNominees don’t have to have experience in software development or have served on governing boards in the past: we seek candidates from all backgrounds.</p><h2>What should I include in a nomination?</h2><p>So that we can best evaluate your nomination, we would like to see a description of why you think the candidate would make an excellent board member.\nConsider including some or all of the following:</p><ul><li><p>links to relevant social media profiles and personal websites</p></li><li><p>examples of previous contributions to F-Droid or other Free and Open Source Software</p></li><li><p>particular skills or qualifications that could be useful</p></li></ul><h2>How will the new directors be selected?</h2><p>The nominations will be discussed by the current Board of Directors in a private meeting.\nThe current directors will vote on each nominee.\nExisting directors are permitted to run for an additional term, but voting is weighted in favour of new candidates.\nFor more information on the process, see <a href=\"https://dracc.commonsconservancy.org/0039/\">our statutes</a>.</p><h2>What is expected of directors?</h2><p>The main responsibility of directors is to participate in discussions with other directors via email and to communicate with F-Droid contributors and users (for instance, in threads on GitLab or the F-Droid Forum).\nDirectors are also required to respond promptly if a vote is called.\nIn addition, the Board of Directors holds a monthly video conference which lasts one hour and is open to the general public.</p><p>In total, directors generally spend between one and three hours a week on activities relating to their position on the Board of Directors.</p><p>English is the working language of the Board of Directors, so an adequate English ability is required.</p><p>We would be more than happy to make reasonable adjustments to ensure that everyone is able to contribute, so please don’t hesitate to get in contact if you have any questions about these expectations.</p><h2>When will the new appointments be announced?</h2><p>Our intention is to decide on the appointments as early as the 19th of March and announce the selected candidates as soon as possible afterwards.\nWe look forward to receiving your nominations!</p><h2>What is the current membership of the Board of Directors?</h2><ul></ul><p>The terms of the following members are ending this year:</p><ul></ul>",
      "contentLength": 3047,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47178863"
    },
    {
      "title": "Breaking Free",
      "url": "https://www.forbrukerradet.no/breakingfree/",
      "date": 1772186202,
      "author": "Aissen",
      "guid": 48831,
      "unread": true,
      "content": "<p>In the new report&nbsp;Breaking Free: Pathways to a fair technological future, the Norwegian Consumer Council has delved into enshittification and how to resist it. The report shows how this phenomenon affects both consumers and society at large, but that it is possible to turn the tide. Together with more than 70 consumer groups and other actors in Europe and the US, we are sending letter to policymakers in the EU/EEA, UK and the US.</p>",
      "contentLength": 434,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47178678"
    },
    {
      "title": "Get free Claude max 20x for open-source maintainers",
      "url": "https://claude.com/contact-sales/claude-for-oss",
      "date": 1772183338,
      "author": "zhisme",
      "guid": 48851,
      "unread": true,
      "content": "<p>Open-source maintainers and contributors keep the ecosystem running. The Claude for Open Source Program is our way of saying thank you for all your hard work, with 6 months of free Claude Max 20x. Apply now.</p><ul role=\"list\"><li>‍ You’re a primary maintainer or core team member of a public repo with 5,000+ GitHub stars  1M+ monthly NPM downloads. You've made commits, releases, or PR reviews within the last 3 months.</li><li>‍<strong>Don't quite fit the criteria</strong> If you maintain something the ecosystem quietly depends on, apply anyway and tell us about it.</li></ul>",
      "contentLength": 527,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47178371"
    },
    {
      "title": "The normalization of corruption in organizations (2003) [pdf]",
      "url": "https://gwern.net/doc/sociology/2003-ashforth.pdf",
      "date": 1772173283,
      "author": "rendx",
      "guid": 48767,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47177186"
    },
    {
      "title": "The Hunt for Dark Breakfast",
      "url": "https://moultano.wordpress.com/2026/02/22/the-hunt-for-dark-breakfast/",
      "date": 1772164188,
      "author": "moultano",
      "guid": 48731,
      "unread": true,
      "content": "<p>It started with a flash of insight like a thunderbolt in a snow storm, the sort of insight that can only be induced by high altitude hypoxia and making breakfast.&nbsp;</p><p>“Breakfast is a vector space. You can place pancakes, crepes, and scrambled eggs on a simplex where the variables are the ratios between milk, eggs, and flour. We have explored too little of this manifold. More breakfasts can exist than we have known.”</p><p>I stood in the kitchen paralyzed by indecision. The mixing bowl was in front of me, the milk, eggs, and flour next to it, all of them individually as familiar as they had been a moment before, but now the possibilities for their combination were just too great. Breakfast was now an alien fractal intruding on our world like the <a href=\"https://www.youtube.com/watch?v=uBsJgceM0KI\">lighthouse at the end of Annihilation</a>. The thoughts came unbidden.</p><p>“In the manifold of breakfast, are there empty subspaces? Might there be breakfasts that no one has ever had? With a theoretical model of breakfast, can we derive the existence of ‘dark breakfasts,’ breakfasts that we know must exist, but have never observed?”</p><p>The curtain of reality had pulled back for me, and I could no longer pretend to be ignorant of these eldritch possibilities. I furiously began to map the known breakfasts. If the dark breakfast exists, I must be able to find it in the interstices of the normal familiar world.</p><p>First I mapped all that I could recall from memory, pancakes, crepes, waffles, scrambled eggs, popovers, omelettes, and on and on, scouring my brain for every fast I had ever broken. The beginnings of the contours of breakfast began to reveal themselves. A gaping hole stared back at me, but I couldn’t yet be sure. I had to search the dark corners of the world to see if somewhere in far off lands that abyss had yet been filled. I called upon <a href=\"https://gemini.google.com/\">friendly ghosts</a>. I paged through <a href=\"http://en.wikipedia.org/wiki/Main_Page\">ancient tomes</a>. I added kaiserschmarrn, swedish pancakes, dan bing, madeleines, crumpets, clafoutis, blinis, pannu kakku, parathas, nalesniki. The map filled in bit by bit, but it was no use. The gap in the fabric of breakfast remained.</p><p>I searched for benign explanations. Could it be that milk is simply too heavy, and that by including the weight of the water content that boils off I am tilting the simplex too far in its direction? Could it be that by excluding slices of bread as ingredients since they aren’t raw flour and do not go in a mixing bowl, I have excluded breakfasts like french toast, eggs in a basket, breakfast burritos, and breakfast sandwiches that might yet have saved us? Could I have overlooked some arcane culture that breaks their fast with dumplings or egg noodles? None of these satisfied me. The Abyss stared back.</p><p>The breakfasts I was able to identify cluster into three major regions:</p><ol><li>The Pancake Local Group: Here are found most of the conventional breakfasts, pancakes, crepes, waffles, and all of their international variants. Space here is chaotic, fractal. Any slight deviation from your recipe in this region is likely to produce something else entirely. Breakfast here is metastable at best. (<a href=\"https://internationalpancakes.com/\">prior research on the pancake cluster</a>)</li><li>The Baked Good Quadrant: The items here are only breakfasts by convention. Any of them could be served at other meals, and often are.</li><li>The Egg Singularity and Custard Accretion Disk: While only omelettes are labelled for brevity, there are dozens of named dishes that could be stacked on top of the pure egg point, over easy, sunny side up, hard boiled, soft boiled, etc. From these a small tail of egg based dishes sneaks down the right side, each with some amount of milk added, often a variable or optional amount.</li></ol><p>I was days into my research before I finally found a clue. In an <a href=\"https://www.ihop.com/-/media/ihop/nutrition/faq/ihop-nutrition-faq92722.pdf\">obscure document</a> on the website of the International House of Pancakes Corporation there was a hint that the dark breakfast had been made. IHOP omelettes include pancake batter. While I cannot place IHOP omelettes exactly on the map, by interpolating between pancakes and omelettes, we can bound where they must occur, and confirm that the manifold possibilities do indeed pass through the Dark Breakfast Abyss.</p><p>We do not know why the Dark Breakfast Abyss is empty. But by anthropic reasoning, we should conclude that it is empty for good reason. The International House of Pancakes is playing a dangerous game. If someday a <a href=\"https://xkcd.com/472/\">remote IHOP</a> splashes a little too much batter in their omelette, cooks the Forbidden Breakfast, and thereby brings about the end of the world, well, at least we know the <a href=\"https://www.wafflehouse.com/how-to-measure-a-storms-fury-one-breakfast-at-a-time/\">Waffle House will be open</a>.</p><p>For <a href=\"https://nautil.us/the-hard-problem-of-breakfast-237916/\">other breakfast scholars</a> who wish to further my study, I offer my <a href=\"https://docs.google.com/spreadsheets/d/1cY5CwTV91IhfPQZkB-XmeGr79diDmxvvZaVomeTciPM/edit?gid=0#gid=0\">data</a> and <a href=\"https://colab.research.google.com/drive/15s9j_bSInYoDKXDArH3LpVhL50Hl5gRZ#scrollTo=cK8NmhoZz5T0\">code</a>. If you are so foolhardy that you wish to explore the bounds of dark breakfast yourself, the recipe is as follows:</p><ul></ul><blockquote><p>The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents. We live on a placid island of ignorance in the midst of black seas of infinity, and it was not meant that we should voyage far. The sciences, each straining in its own direction, have hitherto harmed us little; but some day the piecing together of dissociated knowledge will open up such terrifying vistas of reality, and of our frightful position therein, that we shall either go mad from the revelation or flee from the deadly light into the peace and safety of a new dark age.</p><cite>H.P. Lovecraft – The Call of Cthulhu</cite></blockquote>",
      "contentLength": 5335,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47176257"
    },
    {
      "title": "Parakeet.cpp – Parakeet ASR inference in pure C++ with Metal GPU acceleration",
      "url": "https://github.com/Frikallo/parakeet.cpp",
      "date": 1772164085,
      "author": "noahkay13",
      "guid": 48830,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47176239"
    },
    {
      "title": "A Nationwide Book Ban Bill Has Been Introduced in the House of Representatives",
      "url": "https://bookriot.com/hr7661-book-ban-legislation/",
      "date": 1772163252,
      "author": "LostMyLogin",
      "guid": 48730,
      "unread": true,
      "content": "<div><p>Following this week’s State of the Union Address, House Republicans worked quickly to advance legislation to ban books from public schools nationwide. House Resolution 7661 (H.R. 7661), also known as the “Stop the Sexualization of Children Act” would modify the Elementary and Secondary Education Act of 1965 by prohibiting use of funds under the act “to develop, implement, facilitate, host, or promote any program or activity for, or to provide or promote literature or other materials to, children under the age of 18 that includes sexually oriented material, and for other purposes.”</p><p>The bill was introduced by House Representative Mary Miller (Republican, Illinois). 17 additional Representatives cosigned it. </p></div><div><p>These bills aren’t about removing books; books are just one of the tools. These bills are about the complete and total erasure and removal of queer people from American life. </p></div>",
      "contentLength": 901,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47176157"
    },
    {
      "title": "Google workers seek 'red lines' on military A.I., echoing Anthropic",
      "url": "https://www.nytimes.com/2026/02/26/technology/google-deepmind-letter-pentagon.html",
      "date": 1772161689,
      "author": "mikece",
      "guid": 48723,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47175931"
    },
    {
      "title": "Palantir's AI Is Playing a Major Role in Tracking Gaza Aid Deliveries",
      "url": "https://www.dropsitenews.com/p/palantir-ai-gaza-humanitarian-aid-cmcc-srs-ngos-banned-israel",
      "date": 1772153637,
      "author": "mikece",
      "guid": 48716,
      "unread": true,
      "content": "<p>Palantir Technologies has a permanent desk at the U.S.-led Civil Military Coordination Center (CMCC) headquarters in southern Israel, three sources from the diplomatic community inside the CMCC told Drop Site News. According to the sources, the artificial intelligence data analytics giant is providing the technological architecture for tracking the delivery and distribution of aid to Gaza.</p><p>The presence of Palantir and other corporations—along with recent changes banning non-profits unwilling to give data to Israeli authorities—is creating a situation in which the delivery of aid is taking a backseat to the pursuit of profit, investment, and the training of AI products, experts say.</p><p>The CMCC was established by U.S. Central Command (CENTCOM) in October, one week after the so-called ceasefire went into effect in Gaza to “monitor implementation of the ceasefire” and “help facilitate the flow of humanitarian, logistical, and security assistance from international counterparts into Gaza.” Last week, at the inaugural summit of the Board of Peace in Washington, D.C., Major General Jasper Jeffers—who was tapped in January to lead the International Stabilization Force in Gaza—announced that the CMCC will serve as the Board of Peace’s operational headquarters.</p><p>According to the sources, a representative from Palantir sits in the CMCC operations room where aid convoys and distributions inside Gaza are monitored through drone surveillance. The representative integrates convoy and distribution-related data into Palantir’s systems, the sources said.</p><p>Palantir did not respond to an inquiry from Drop Site on its role in the CMCC or in aid distribution in Gaza. Founded in 2003 by billionaire Peter Thiel with the help of investments from the CIA’s venture capital arm In-Q-Tel, Palantir is known for its work with government agencies, including the U.S. military and Immigration and Customs Enforcement (ICE).</p><p><a href=\"https://www.palantir.com/assets/xrfr7uokpv1b/3MuEeA8MLbLDAyxixTsiIe/9e4a11a7fb058554a8a1e3cd83e31c09/C134184_finaleprint.pdf\" rel=\"\">announced</a><a href=\"https://www.google.com/books/edition/The_Philosopher_in_the_Valley/azYMEQAAQBAJ?hl=en&amp;gbpv=1&amp;dq=gaza+palantir+inauthor:Steinberger&amp;pg=PT205&amp;printsec=frontcover\" rel=\"\">according</a></p><p><a href=\"https://docs.un.org/en/A/HRC/59/23\" rel=\"\">report</a></p><p>The use of Palantir to track aid deliveries to Gaza is of particular concern to observers. “The distinction between death by drone and delivery of aid is being evaporated while we all sit around the same table,” a source from the diplomatic community who attends CMCC sessions told Drop Site.</p><p>Palantir’s two main platforms are Gotham and Foundry. “Gotham’s targeting offering supports soldiers with an Al-powered kill chain, seamlessly and responsibly integrating target identification and target effector pairing,” according to the company’s website. Foundry is Palantir’s platform for supply chain management and is billed as a way to “bridge siloed planning and execution processes, optimize inventory management, and help build supply chain resilience for economic and geopolitical uncertainty.”</p><p><a href=\"https://www.palantir.com/docs/foundry/architecture-center/interoperability\" rel=\"\">documentation</a></p><p>This means that, in theory, information that is being gathered at the CMCC—including from participating governments the UN and NGOs regarding the type of aid being distributed, its distribution locations and systems, and truck convoy routes—could be seamlessly pulled into Gotham’s AI targeting matrix. The same software logic used to track aid at the CMCC could be used to optimize and accelerate lethal airstrikes.</p><p>There is no information available as to whether Gotham and Foundry are the specific products being used to track aid, but publicly available photographic evidence indicates that Palantir’s Gaia—a platform referred to on their website as a tool to “bring the battlefield into view”—is being deployed at the CMCC.</p><p>In an interview with Drop Site on the role of Palantir in Gaza, the economist Yanis Varoufakis, the former Greek Finance Minister and a former member of Greece’s parliament, described an encounter he had with a Palantir representative who had explained to him the benefit the company gained from Gaza. “He was saying that ‘as the bombs fell we were having a party,’” Varoufakis said. According to Varoufakis, the Palantir representative explained how the chaos of intense violence in a high-density urban area like Gaza generates substantial data for training their AI models on how humans respond under stress. “The more bombardment and havoc, the better the training,” Varoufakis said.</p><p>“It’s one thing to say that companies like Lockheed Martin make money selling F35s to the Israelis,” he said. “That has been a time-honored way that the military industrial complex has benefited from war and genocide and war crimes.” He continued, “This is the first instance in history where it is the suffering of a people being subjected to genocide and bombing—the suffering itself—which is adding to the capital of a company which then uses that capital to produce commodities to sell elsewhere.”</p><p>Palantir operates on Oracle’s cloud infrastructure, led by Larry Ellison—a major donor to the Israeli military who also funds the Tony Blair Institute, which has itself consulted on governance mechanisms for Gaza.</p><p>The growing use of Palantir and other private sector companies in Gaza comes as the non-profit sector is being systematically squeezed out. As of March 1, 2026, Israel will ban dozens of aid groups from operating in Gaza, as well as the West Bank and East Jerusalem, under new registration rules, including prominent NGOs such as Doctors Without Borders, the Norwegian Refugee Council, Oxfam, and Medical Aid for Palestinians.</p><p>The new measures require aid groups to register the names and contact information of employees and to provide details about their funding and operations to Israeli authorities. The aid groups said in a joint statement this week that “the demand to transfer personal data raises acute security and legal risks. It exposes national staff to potential retaliation and undermines established data protection and confidentiality safeguards.”</p><p>“NGOs are being pushed out of Gaza because aid delivered by humanitarian organizations is based on need and is provided to people wherever they are located,” said an aid worker who spoke to Drop Site on condition of anonymity. “This doesn’t match the vision of the ‘New Gaza’ where Palestinians will need to be displaced again into the zones where reconstruction will be permitted and their access to aid will be controlled through screening.”</p><p>Not all NGOs are being pushed out of Gaza. Others—on a list of registered and  approved organizations—are expanding their role alongside the private sector. These include Christian groups like Samaritan’s Purse and GAiN, both of whom were involved in the Gaza Humanitarian Foundation (GHF), and who sources from the diplomatic community have recently seen gathering in a “prayer circle” at the CMCC.</p><p>These approved NGOs, alongside private firms coordinated through the CMCC like Palantir, stand ready to take over the distribution of aid in Gaza.</p><p>“Aid in Gaza has been stripped to bare survival and its future delivery appears to be faith based, profit driven, militarized and certainly not to be delivered by anyone that dares to speak out about what Palestinians are being subjected to,” said a senior aid worker who spoke to Drop Site on condition of anonymity.</p><p>Gaza’s experience with private delivery mechanisms has been catastrophic. In May 2025, the U.S. and Israeli-backed GHF was contracted to distribute aid in the enclave. During the four and half months the GHF operated in Gaza, more than 2,600 Palestinians seeking food were killed and over 19,000 wounded by Israeli forces or security contractors at or near aid distribution sites.</p><p>The former headquarters of the GHF, a large warehouse-style building in Kiryat Gat, is now the headquarters of the CMCC.</p><p>As aid groups are being banned, U.S. military contractors are also filling the vacuum. According to sources from the diplomatic community who attend the CMCC, the physical presence of Safe Reach Solutions (SRS), a U.S. military contractor that provided security for the GHF,  has recently expanded at the center of the facility, with SRS officials taking up more prominent seating space on the operations floor. The company’s representatives now sit behind name tags in seating that had previously been reserved for UN agencies, the sources said.</p><p>SRS did not respond to an inquiry from Drop Site about its role at the CMCC or in Gaza aid distribution.</p><p>“Given the precedent of the GHF, which turned aid delivery into a killing machine,” Albanese told Drop Site, “and the grave violations of international law embedded in the so-called peace plan—first and foremost the negation of the Palestinian right to self-determination—the risk that companies and states involved in the CMCC may be complicit in, or even direct perpetrators of, international crimes is real.”</p><p><a href=\"https://www.haaretz.com/israel-news/security-aviation/2025-08-05/ty-article/.premium/u-s-firm-eastern-european-drivers-israeli-link-whos-behind-getting-aid-into-gaza/00000198-73ff-dcb9-ab99-73ff7d4d0000\" rel=\"\">according</a></p><p>The re-emergence of GHF-linked companies, alongside the digital capacity to monitor and monetize the surviving population in Gaza, is now converging with the construction of a new physical infrastructure spearheaded by giant real estate conglomerates. </p><p>At last week’s Board of Peace meeting, the reconstruction of Gaza was positioned as a massive financial “unlocking” of a distressed asset. Figures like Yakir Gabay, who built a real estate empire in Germany, envisioned the coastline transformed into a “Mediterranean Riviera” featuring 200 hotels and artificial islands. Marc Rowan, a billionaire investor and the CEO of Apollo Global Management, framed the project as the consolidation of Gaza’s “productive assets” into a “unified structure.”</p><p>A significant addition to the CMCC’s corporate roster is Terra Firma Capital Partners, which sources confirmed now maintains a permanent presence at the CMCC. Founded by British financier Guy Hands, the firm brings experience in managing massive-scale residential assets. Terra Firma has links to the New Labour era, specifically through Lord John Birt, Tony Blair’s former strategy director, who worked for the company after he left government.</p><p>“The genocide is entering a new phase. After the destruction of Gaza and the erasure of entire family lines, powerful states are now deciding the fate of the survivors without ever listening to their voices,” Albanese said. </p><p>“If Gaza is not to become a capitalist techno-dystopia, the time to act is now. States and corporations supporting this emerging infrastructure must be stopped, and held accountable. There is no time to lose.”</p>",
      "contentLength": 10412,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47174777"
    },
    {
      "title": "Netflix Backs Out of Warner Bros. Bidding, Paramount Set to Win",
      "url": "https://www.hollywoodreporter.com/business/business-news/netflix-backs-out-warners-deal-paramount-win-1236516763/",
      "date": 1772147790,
      "author": "atombender",
      "guid": 48715,
      "unread": true,
      "content": "<p>\n\tIn a stunning twist, <a href=\"https://www.hollywoodreporter.com/t/netflix/\" data-tag=\"netflix\">Netflix</a> is declining to raise its bid for Warner Bros., positioning David Ellison’s <a href=\"https://www.hollywoodreporter.com/t/paramount/\" data-tag=\"paramount\">Paramount</a> as the winner in the battle for the fabled studio.</p><p>\n\tNetflix co-CEOs Ted Sarandos and Greg Peters released a statement Thursday outlining their decision, namely that the deal is “no longer financially attractive” and that it “was always a ‘nice to have’ at the right price, not a ‘must have’ at any price.”\n\n</p><p>\n\t“The transaction we negotiated would have created shareholder value with a clear path to regulatory approval. However, we’ve always been disciplined, and at the price required to match Paramount Skydance’s latest offer, the deal is no longer financially attractive, so we are declining to match the Paramount Skydance bid,” the co-CEOs said.\n\n\t</p><p>\n\t“Warner Bros. is a world-class organization, and we want to thank David Zaslav, Gunnar Wiedenfels, Bruce Campbell, Brad Singer and the WBD Board for running a fair and rigorous process,” they added. “We believe we would have been strong stewards of Warner Bros.’ iconic brands, and that our deal would have strengthened the entertainment industry and preserved and created more production jobs in the U.S.&nbsp; But this transaction was always a ‘nice to have’ at the right price, not a ‘must have’ at any price.”</p><p>\n\tWith Netflix out, Paramount’s latest bid is almost a sure thing to be accepted by the Warners board, which determined earlier Thursday that it was a “<a href=\"https://www.hollywoodreporter.com/business/digital/warner-bros-discovery-paramounts-offer-superior-to-netflix-1236516591/\" data-type=\"link\" data-id=\"https://www.hollywoodreporter.com/business/digital/warner-bros-discovery-paramounts-offer-superior-to-netflix-1236516591/\">superior proposal</a>” to Netflix’s deal.\n\n\t</p><p>\n\t“Netflix is a great company and throughout this process Ted, Greg, Spence and everyone there have been extraordinary partners to us. We wish them well in the future,” said David Zaslav, president and CEO of Warner Bros. Discovery. “Once our Board votes to adopt the Paramount merger agreement, it will create tremendous value for our shareholders. We are excited about the potential of a combined Paramount Skydance and Warner Bros. Discovery and can’t wait to get started working together telling the stories that move the world.”</p><p>\n\tPSKY’s latest proposal was for $31 per share, but&nbsp;had a number of other <a href=\"https://www.hollywoodreporter.com/business/business-news/warner-bros-board-warms-to-paramounts-sweetened-bid-new-details-are-revealed-1236514073/\">sweeteners</a>, including a ticking fee payable to shareholders equal to $0.25 per quarter beginning after Sept. 30, 2026, as well as a $7 billion regulatory termination in the event the transaction does not close due to regulatory matters.</p><p>\n\tParamount has also agreed to pay the $2.8 billion termination fee that Warner Bros. would be required to pay to Netflix to terminate the existing merger agreement. </p><p>\n\tIf all goes as expected, Netflix will be on the receiving end of that $2.8 billion sooner rather than later. Netflix shares soared by more than 10 percent in after-hours trading after the decision was announced.</p><p>\n\t“We are pleased WBD’s Board has unanimously affirmed the superior value of our offer, which delivers to WBD shareholders superior value, certainty and speed to closing,” Paramount CEO David Ellison said in a statement on Thursday before Netflix backed out of the bidding.\n\n\t</p><p>\n\tOf course, a Paramount deal is not necessarily a sure thing. U.S. and European regulators still need to formally sign off, and state attorneys general will have a say as well. Already politicians are positioning themselves to challenge the deal (Sen. Elizabeth Warren called in an “antitrust disaster” Thursday), with Ellison likely to be called before Congress to discuss it.</p><p>\n\tAnd the politics of the deal are sure to come up. </p><p>\n\tSarandos and Peters, meanwhile, say they will continue to pour cash into content.</p><p>\n\t“Netflix’s business is healthy, strong and growing organically, powered by our slate and best-in-class streaming service. This year, we’ll invest approximately $20 billion in quality films and series and will expand our entertaining offering. Consistent with our capital allocation policy, we’ll also resume our share repurchase program,” the co-CEOs said. “We will continue to do what we’ve done for more than 20 years as a public company: delight our members, profitably grow our business, and drive long-term shareholder value.”</p>",
      "contentLength": 4108,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47173583"
    },
    {
      "title": "Statement from Dario Amodei on our discussions with the Department of War",
      "url": "https://www.anthropic.com/news/statement-department-of-war",
      "date": 1772145767,
      "author": "qwertox",
      "guid": 48669,
      "unread": true,
      "content": "<p>I believe deeply in the existential importance of using AI to defend the United States and other democracies, and to defeat our autocratic adversaries.</p><p>Anthropic has therefore worked proactively to deploy our models to the Department of War and the intelligence community. We were <a href=\"https://www.anthropic.com/news/expanding-access-to-claude-for-government\">the first frontier AI company</a> to deploy our models in the US government’s classified networks, the first to deploy them at the <a href=\"https://www.axios.com/2024/11/14/anthropic-claude-nuclear-information-safety\">National Laboratories</a>, and the first to provide <a href=\"https://www.anthropic.com/news/claude-gov-models-for-u-s-national-security-customers\">custom models</a> for national security customers. Claude is <a href=\"https://www.anthropic.com/news/anthropic-and-the-department-of-defense-to-advance-responsible-ai-in-defense-operations\">extensively deployed</a> across the Department of War and other national security agencies for mission-critical applications, such as intelligence analysis, modeling and simulation, operational planning, cyber operations, and more.</p><p>Anthropic has also acted to defend America’s lead in AI, even when it is against the company’s short-term interest. We chose to forgo several hundred million dollars in revenue to cut off the <a href=\"https://www.anthropic.com/news/updating-restrictions-of-sales-to-unsupported-regions\">use of Claude by firms linked to the Chinese Communist Party</a> (some of whom have been <a href=\"https://media.defense.gov/2025/Jan/07/2003625471/-1/-1/1/ENTITIES-IDENTIFIED-AS-CHINESE-MILITARY-COMPANIES-OPERATING-IN-THE-UNITED-STATES.PDF\">designated by the Department of War</a> as Chinese Military Companies), shut down <a href=\"https://www.anthropic.com/news/disrupting-AI-espionage\">CCP-sponsored cyberattacks</a> that attempted to abuse Claude, and have advocated for <a href=\"https://www.wsj.com/opinion/trump-can-keep-americas-ai-advantage-china-chips-data-eccdce91?gaa_at=eafs&amp;gaa_n=AWEtsqdPk42glTHtJxGWpiSYR1xY28wMr6SpvGWmvlfp8_gYMp2h0ulOBH89Njx5eB0%3D&amp;gaa_ts=6983c8a6&amp;gaa_sig=t3NbNoEV35S9fhpBAUsmCPXHG6Zc3taB_jNESn4lAI7qy0l37FtVqnKZe-ASVGLp4SqxRsIS-HRn0k51UzsdpQ%3D%3D\">strong export controls on chips</a> to ensure a democratic advantage.</p><p>Anthropic understands that the Department of War, not private companies, makes military decisions. We have never raised objections to particular military operations nor attempted to limit use of our technology in an  manner.</p><p>However, in a narrow set of cases, we believe AI can undermine, rather than defend, democratic values. Some uses are also simply outside the bounds of what today’s technology can safely and reliably do. Two such use cases have never been included in our contracts with the Department of War, and we believe they should not be included now:</p><ul><li><strong>Mass domestic surveillance. </strong>We support the use of AI for lawful foreign intelligence and counterintelligence missions. But using these systems for mass surveillance is incompatible with democratic values. AI-driven mass surveillance <a href=\"https://www.darioamodei.com/essay/the-adolescence-of-technology\">presents serious, novel risks to our fundamental liberties</a>. To the extent that such surveillance is currently legal, this is only because the law has not yet caught up with the rapidly growing capabilities of AI. For example, under current law, the government can purchase detailed records of Americans’ movements, web browsing, and associations from public sources without obtaining a warrant, a practice the <a href=\"https://www.dni.gov/files/ODNI/documents/assessments/ODNI-Declassified-Report-on-CAI-January2022.pdf\">Intelligence Community has acknowledged</a> raises privacy concerns and that has generated bipartisan opposition in Congress. Powerful AI makes it possible to assemble this scattered, individually innocuous data into a comprehensive picture of any person’s life—automatically and at massive scale.</li><li><strong>Fully autonomous weapons. </strong>Partially autonomous weapons, like those used today in Ukraine, are vital to the defense of democracy. Even autonomous weapons (those that take humans out of the loop entirely and automate selecting and engaging targets) may prove critical for our national defense. But today, frontier AI systems are simply not reliable enough to power fully autonomous weapons. We will not knowingly provide a product that puts America’s warfighters and civilians at risk. We have offered to work directly with the Department of War on R&amp;D to improve the reliability of these systems, but they have not accepted this offer. In addition, <a href=\"https://www.darioamodei.com/essay/the-adolescence-of-technology\">without proper oversight</a>, fully autonomous weapons cannot be relied upon to exercise the critical judgment that our highly trained, professional troops exhibit every day. They need to be deployed with proper guardrails, which don’t exist today.</li></ul><p>To our knowledge, these two exceptions have not been a barrier to accelerating the adoption and use of our models within our armed forces to date.</p><p>The Department of War has <a href=\"https://media.defense.gov/2026/Jan/12/2003855671/-1/-1/0/ARTIFICIAL-INTELLIGENCE-STRATEGY-FOR-THE-DEPARTMENT-OF-WAR.PDF\">stated</a> they will only contract with AI companies who accede to “any lawful use” and remove safeguards in the cases mentioned above. They have threatened to remove us from their systems if we maintain these safeguards; they have also threatened to designate us a “supply chain risk”—a label reserved for US adversaries, never before applied to an American company— to invoke the Defense Production Act to force the safeguards’ removal. These latter two threats are <a href=\"https://www.politico.com/news/2026/02/26/incoherent-hegseths-anthropic-ultimatum-confounds-ai-policymakers-00800135?utm_content=topic/politics&amp;utm_source=flipboard\">inherently contradictory</a>: one labels us a security risk; the other labels Claude as essential to national security.</p><p>Regardless, these threats do not change our position: we cannot in good conscience accede to their request.</p><p>It is the Department’s prerogative to select contractors most aligned with their vision. But given the substantial value that Anthropic’s technology provides to our armed forces, we hope they reconsider. Our strong preference is to continue to serve the Department and our warfighters—with our two requested safeguards in place. Should the Department choose to offboard Anthropic, we will work to enable a smooth transition to another provider, avoiding any disruption to ongoing military planning, operations, or other critical missions. Our models will be available on the expansive terms we have proposed for as long as required.</p><p>We remain ready to continue our work to support the national security of the United States.</p>",
      "contentLength": 5251,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47173121"
    },
    {
      "title": "Smartphone market forecast to decline this year due to memory shortage",
      "url": "https://www.idc.com/resource-center/press-releases/wwsmartphoneforecast4q25/",
      "date": 1772143785,
      "author": "littlexsparkee",
      "guid": 48668,
      "unread": true,
      "content": "<p>“What we are witnessing is not a temporary squeeze, but a tsunami-like shock originating in the memory supply chain, with ripple effects spreading across the entire consumer electronics industry,” said&nbsp;<a href=\"https://my.idc.com/getdoc.jsp?containerId=PRF003252\">Francisco Jeronimo</a>, vice president&nbsp;for Worldwide&nbsp;Client Devices, IDC. “The global smartphone market, particularly Android manufacturers, faces a significant threat. Vendors whose business is mainly at the low end of the market are likely to suffer the most. Rising component costs will hit their margins, and they will have no choice but to pass the costs on to end users. By contrast, Apple and Samsung are better positioned to navigate this crisis. As smaller and low-end-positioned Android vendors struggle with rising costs, Apple and Samsung could not only weather the storm but potentially expand market share as the competitive landscape tightens.”</p>",
      "contentLength": 868,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47172664"
    },
    {
      "title": "Show HN: Unfucked - version all changes (by any tool) - local-first/source avail",
      "url": "https://www.unfudged.io/",
      "date": 1772141419,
      "author": "cyrusradfar",
      "guid": 49047,
      "unread": true,
      "content": "<div data-index=\"0\"><h3>Agent mass-overwrote your source</h3><p>Your AI agent refactored 30 Rust files, hit an error on file 27, and reverted everything to stale versions. Three hours of good work — gone.</p><div>$ unf log --since 3h --include \"*.rs\" --stats\n$ unf diff --at 10m\n$ unf restore --at 10m -y</div></div><div data-index=\"1\"><p>The agent decided  was \"generated\" and deleted it. API keys, database URLs, local config. Not in git. Not anywhere.</p><div>$ unf log .env\n$ unf cat .env --at 5m\n$ unf restore --at 5m .env -y</div></div><div data-index=\"2\"><h3>Agent's cleanup script went wrong</h3><p>You asked the agent to \"clean up build artifacts.\" It wrote a shell script that 'd  instead of .</p><div>$ unf diff --at 1m\n$ unf restore --at 2m --dry-run\n$ unf restore --at 2m -y</div></div><div data-index=\"3\"><h3>Agent \"fixed\" your dependencies</h3><p>The agent removed 6 \"unused\" crates from . Four were behind feature flags. CI is red.</p><div>$ unf log Cargo.toml --stats\n$ unf cat Cargo.toml --at 1h\n$ unf restore --at 1h Cargo.toml -y</div></div><div data-index=\"4\"><h3>Agent reformatted everything</h3><p>The agent ran Prettier with the wrong config and rewrote 200 TypeScript files. It committed before you noticed.  gives you one commit. UNF* has every file.</p><div>$ unf log --since 30m --include \"*.ts\" --stats\n$ unf diff --at 30m\n$ unf restore --at 30m -y</div></div><div data-index=\"5\"><h3>Agent replaced your test fixtures</h3><p>Your hand-crafted SQL seed data and JSON fixtures got overwritten with generic placeholders. A week of edge cases, gone.</p><div>$ unf log --include \"fixtures/*\" --stats\n$ unf diff --at 20m\n$ unf restore --at 20m -y</div></div><div data-index=\"6\"><h3>Agent deleted your migration files</h3><p>The agent saw 47 SQL migration files and decided they were \"redundant.\" Production depends on them running in order.</p><div>$ unf log --include \"migrations/*.sql\"\n$ unf diff --at 15m\n$ unf restore --at 15m -y</div></div><div data-index=\"7\"><h3>Squash merge ate intermediate work</h3><p>You squash-merged a feature branch. Git only has the final result. The 40 intermediate versions across 3 days? Git doesn't know they existed.</p><div>$ unf log --since 3d --include \"*.py\"\n$ unf diff --from 3d --to 1d\n$ unf cat app/models.py --at 2d</div></div><div data-index=\"8\"><h3>Agent lost context mid-session</h3><p>Context window overflow. The agent crashed 2 hours into a refactor across 4 repos. The new agent needs to pick up exactly where the old one left off.</p><div>$ unf recap --global --json\n$ unf log --sessions --since 2h\n$ unf diff --session</div></div><div data-index=\"9\"><h3>What happened while you were away?</h3><p>You left an agent running overnight. It touched 80 files across 3 projects. What did it do?</p><div>$ unf log --global --since 8h --stats\n$ unf diff --at 8h\n$ unf restore --at 8h --dry-run</div></div>",
      "contentLength": 2353,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47172238"
    },
    {
      "title": "Layoffs at Block",
      "url": "https://twitter.com/jack/status/2027129697092731343",
      "date": 1772140676,
      "author": "mlex",
      "guid": 48667,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47172119"
    },
    {
      "title": "What does \" 2>&1 \" mean?",
      "url": "https://stackoverflow.com/questions/818255/what-does-21-mean",
      "date": 1772135926,
      "author": "alexmolas",
      "guid": 48714,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47171233"
    },
    {
      "title": "Launch HN: Cardboard (YC W26) – Agentic video editor",
      "url": "https://www.usecardboard.com/",
      "date": 1772131118,
      "author": "sxmawl",
      "guid": 48666,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47170174"
    },
    {
      "title": "OsmAnd’s Faster Offline Navigation (2025)",
      "url": "https://osmand.net/blog/fast-routing/",
      "date": 1772131079,
      "author": "todsacerdoti",
      "guid": 48651,
      "unread": true,
      "content": "<p>Offline navigation is a lifeline for travelers, adventurers, and everyday commuters. We demand speed, accuracy, and the flexibility to tailor routes to our specific needs. For years, OsmAnd has championed powerful, feature-rich offline maps that fit in your pocket. But as maps grew more detailed and user demands for complex routing increased, our trusty <a href=\"https://en.wikipedia.org/wiki/A*_search_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">A* algorithm</a>, despite its flexibility, started hitting a performance wall. How could we deliver a  without bloating map sizes or sacrificing the deep customization our users love?</p><p>The answer: <strong>OsmAnd's custom-built Highway Hierarchy (HH) Routing.</strong> This isn't your standard routing engine; it's a ground-up redesign, meticulously engineered to overcome the unique challenges of providing advanced navigation on compact, offline-first map data.</p><div><div><p>100x speedup is achieved by comparing HH with bidirectional A*.\n2-phase A* already uses many heuristics which don't always create an optimal route and still 5-10x slower.</p></div></div><h2>Why Standard Solutions Failed<a href=\"https://osmand.net/blog/fast-routing/#why-standard-solutions-failed\" aria-label=\"Direct link to Why Standard Solutions Failed\" title=\"Direct link to Why Standard Solutions Failed\">​</a></h2><p>OsmAnd has always been about putting you in control. Our original <a href=\"https://en.wikipedia.org/wiki/A*_search_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">A* routing engine</a>, configurable via <a href=\"https://github.com/osmandapp/OsmAnd-resources/blob/master/routing/routing.xml\" target=\"_blank\" rel=\"noopener noreferrer\"></a>, offered immense power. You could define intricate profiles, avoid specific road types, and truly personalize your journey. With maps optimized for minimal storage (the entire planet's car data for our new HH-routing is around a mere 800MB!), OsmAnd was a lean, mean navigating machine.</p><p>However, this flexibility came at a cost for complex routes:</p><ul><li>* Calculating a 200-300km car route (or even shorter bicycle/pedestrian paths) could mean visiting over a million road segments, taking 10-20 seconds. For longer trips, this wait could become frustrating.</li></ul><p>We explored standard advanced algorithms like <a href=\"https://en.wikipedia.org/wiki/Contraction_hierarchies\" target=\"_blank\" rel=\"noopener noreferrer\">Contraction Hierarchies (CH)</a>, known for their speed. But they presented their own set of deal-breakers for OsmAnd:</p><ul><li> CH typically pre-calculates optimal paths. Supporting OsmAnd's 10+ routing parameters (leading to over 1024 combinations per profile!) would be impossible with standard CH.</li><li> A CH car profile for a region can be massive (e.g., OSRM's Europe is tens of GBs, their global car profile around 200GB for just one profile). Our goal was to keep  profiles and parameters for the  well under 20GB.</li><li> Users download individual countries or regions. CH usually requires processing the entire road network globally, which doesn't align with OsmAnd's flexible map management.</li><li> The extensive pre-processing for CH makes it unsuitable for frequent updates, let alone OsmAnd’s goal of supporting near real-time changes through hourly map updates.</li></ul><p>The challenge was clear: achieve a quantum leap in speed while preserving extreme flexibility, minimal storage, regional map support, and dynamic update capabilities. Standard Highway Hierarchies were a starting point, but we needed something more – a uniquely OsmAnd solution.</p><h2>Secret Sauce #1: Two-Level Routing<a href=\"https://osmand.net/blog/fast-routing/#secret-sauce-1-two-level-routing\" aria-label=\"Direct link to Secret Sauce #1: Two-Level Routing\" title=\"Direct link to Secret Sauce #1: Two-Level Routing\">​</a></h2><p>The core of OsmAnd's HH-Routing is an elegant two-level hierarchy built upon \"area clusters.\"</p><ul><li> The map is intelligently segmented into numerous small regions or clusters.</li><li> Each cluster has a limited number of defined \"border points\" – these are the gateways in and out of the cluster.</li><li> For common scenarios, we pre-calculate the travel time/distance (the \"shortcut\") between border points  and also to border points of  clusters.</li></ul><p>This map illustrates the OsmAnd routing concept. <a href=\"https://osmand.net/map/navigate/?start=42.343860,-71.068550&amp;end=42.306935,-71.082991&amp;profile=car#14/42.3280/-71.0819\" target=\"_blank\" rel=\"noopener noreferrer\">The route</a> starts in the Start Area Cluster (221558), moves to the nearest Border Point, and continues through precomputed Shortcuts across intermediate clusters. It then enters the Finish Area Cluster (221536) via another border point and finishes using local roads. This method speeds up routing by combining local search with efficient inter-cluster shortcuts.</p><p>The real magic, our , lies in  these border points are selected. Naive approaches quickly fail:</p><ul><li>Randomly selecting border points or using simple geometric divisions (squares/hexagons) results in too many border points per cluster (50-80). This leads to a shortcut explosion (N*(N-1)/2 shortcuts), making the files large and and calculations slow.</li><li>We even tried building hierarchies with 2-3 levels, but the number of shortcuts grew too fast for higher levels if we generated a full graph inside each cluster.</li></ul><p><strong>The \"Parking Lot\" Insight:</strong>\nImagine a vast shopping mall parking lot with thousands of individual parking spots and internal lanes (representing road segments within a cluster). No matter how complex it is inside, there are usually only a few key exits to the main roads. Our goal was to identify these natural \"exits\" for each map cluster. For instance, the complex road network around Amsterdam Airport Schiphol (<a href=\"https://www.openstreetmap.org/#map=17/52.321360/4.766226\" target=\"_blank\" rel=\"noopener noreferrer\">see on OpenStreetMap</a>) has many internal roads but limited primary access points.</p><p>We wanted a scenario where, say, 5 well-placed border points could efficiently represent an area with 5,000 internal points and 10,000 road edges. This would reduce those 10,000 edges to just 5*4/2 = 10 shortcuts for routing  that cluster at a high level – an incredible 1:1000 point ratio and a 30x reduction in edges to consider for the high-level path!</p><p><strong>The Algorithm: Ford-Fulkerson to Find the Bottlenecks</strong>\nTo find these crucial border points, we employed a clever technique based on the <a href=\"https://en.wikipedia.org/wiki/Ford%E2%80%93Fulkerson_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">Ford-Fulkerson algorithm</a>. By simulating \"flooding\" roads with traffic from random start/end points, we could identify the natural bottlenecks – the \"minimum cut\" in graph theory terms. These bottlenecks became our border points.</p><p>\nCrucially, this distribution of border points is <strong>agnostic of routing speed profiles</strong>. It’s based only on whether a road is  or not. This means the <em>same set of clusters and border points</em> can be used for all car routing profiles (default, shortest, fuel-efficient) and all bicycle profiles (default, prefer flat terrain, etc.). Only the  of the shortcuts between these points change based on the profile. This is a massive factor in keeping storage down – <strong>map data only increased by about 0.5% per profile</strong> to store this HH-Routing structure!</p><p>Just look at the numbers for processing the entire planet for a car profile:</p><ul><li>Original OSM: ~2.07 billion points, ~2.42 billion edges</li><li>Resulting HH structure: ~3 million border points, ~541,000 clusters</li><li>Estimated shortcuts: ~91 million (a manageable number for global routing)</li></ul><h2>How OsmAnd Builds Routes<a href=\"https://osmand.net/blog/fast-routing/#how-osmand-builds-routes\" aria-label=\"Direct link to How OsmAnd Builds Routes\" title=\"Direct link to How OsmAnd Builds Routes\">​</a></h2><p>So, how does OsmAnd use this structure to calculate your route at lightning speed? It's a multi-step process:</p><p><strong>A. Preprocessing (Done by OsmAnd when new maps are prepared):</strong></p><ol><li><strong>Cluster &amp; Border Point Definition:</strong> The map is divided into clusters, and border points are identified using the Ford-Fulkerson based method.</li><li><strong>Shortcut Pre-calculation:</strong> For the most commonly used speed profiles, the travel costs (time/distance) for shortcuts between border points within each cluster are pre-calculated and stored. (Each border point effectively has an \"entry\" and \"exit\" aspect for directed travel).</li></ol><p><strong>B. User Route Request (Query Time - this is what happens on your device):</strong></p><ol><li><p><strong>Step 1: Connect to the Hierarchy (Your Local Area):</strong></p><ul><li>OsmAnd identifies the clusters containing your start and target points.</li><li>It then uses the standard <a href=\"https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">Dijkstra algorithm</a> on the  within your start cluster to find the best paths from your actual start location to  border points of that starting cluster.</li><li>The same is done for your target point within its own cluster (finding paths from all its border points to your actual destination).</li><li>This is quick because it's operating on a very small, localized part of the map.</li></ul></li><li><p><strong>Step 2: Route on the Abstract Graph (The \"Highway\" Part):</strong></p><ul><li>Now, OsmAnd performs another Dijkstra search, but this time on the much smaller \"base graph.\" This graph consists  of the border points and the pre-calculated shortcut values between them.</li><li>This step rapidly finds the optimal sequence of border points and shortcuts to get from your start cluster's periphery to your target cluster's periphery. It's incredibly fast because it's ignoring all the tiny roads  intermediate clusters.</li></ul></li><li><p><strong>Step 3: Refine with Detailed Shortcuts (Applying Secret Sauce #2):</strong></p><ul><li>The result from Step 2 is a high-level route – a sequence of shortcuts connecting border points.</li><li>Now, for  shortcut in this sequence, OsmAnd runs its highly optimized A* algorithm on the , but <strong>strictly limited to the small area of the cluster that shortcut belongs to.</strong></li><li>For example, a 500km route might be broken down into ~100 such shortcuts. If each A* shortcut calculation explores 100-1000 detailed road segments, the total detailed segments visited by A* might be around 10,000-50,000. Compare this to the 1,000,000+ segments the old A* might have needed for the entire route!</li></ul></li></ol><p>This combination – localized Dijkstra, super-fast abstract graph traversal, and highly localized A* refinement – is what delivers the .</p><h2>Secret Sauce #2: Adaptive Routing<a href=\"https://osmand.net/blog/fast-routing/#secret-sauce-2-adaptive-routing\" aria-label=\"Direct link to Secret Sauce #2: Adaptive Routing\" title=\"Direct link to Secret Sauce #2: Adaptive Routing\">​</a></h2><p>Speed is fantastic, but not if it means sacrificing the features OsmAnd users rely on. This is where our  comes into play – ensuring HH-Routing remains incredibly flexible and dynamic:</p><ul><li> Because the final shortcut refinement (Step 3 above) uses A* on detailed maps , all your specified parameters are naturally incorporated:<ul><li>Avoiding specific road types or toll roads.</li><li>Adding penalties or preferences for certain roads.</li><li>Following all the nuanced rules in your custom <a href=\"https://github.com/osmandapp/OsmAnd-resources/blob/master/routing/routing.xml\" target=\"_blank\" rel=\"noopener noreferrer\"></a> profiles.</li></ul></li><li><strong>Handling Live Updates &amp; Dynamic Changes:</strong> What if a bridge is closed due to a <a href=\"https://osmand.net/docs/user/personal/maps-resources#live-updates\" target=\"_blank\" rel=\"noopener noreferrer\">live map update</a> you just downloaded?<ul><li>If the A* calculation for a shortcut (in Step 3) finds it's now impassable, or if its actual detailed cost is significantly different (e.g., &gt;20%) from the pre-calculated shortcut value:<ul><li> The system can update the cost of that specific shortcut in the base graph and quickly re-run the Dijkstra search (Step 2) on the abstract graph to find an alternative high-level path.</li><li> For very localized changes, it might even re-evaluate all shortcuts within that one affected cluster.</li></ul></li><li>Minor road updates (like those in map data that might be a few months old if you're using maps from different regions) usually result in negligible cost differences for shortcuts, so the pre-calculated values remain effective.</li></ul></li><li><strong>Graceful Fallback for Extreme Customization:</strong><ul><li>What if you create a truly unique routing profile that's wildly different from the common ones for which shortcuts were pre-calculated? The system is smart. If it detects that too many shortcuts (~50, for example) need on-the-fly recalculation and deviate significantly, it might determine that falling back to the original, comprehensive A* algorithm for the  would actually be faster than doing many small, heavily modified A* calculations.</li></ul></li></ul><h2>Real Benefits for OsmAnd Users<a href=\"https://osmand.net/blog/fast-routing/#real-benefits-for-osmand-users\" aria-label=\"Direct link to Real Benefits for OsmAnd Users\" title=\"Direct link to Real Benefits for OsmAnd Users\">​</a></h2><p>This complex engineering translates into tangible benefits:</p><ul><li> The 100x average improvement means route calculations, especially for longer journeys, are now dramatically faster.</li><li> Our HH-Routing data adds only  to OsmAnd's already incredibly compact map sizes. The entire planet's car routing data is around 800MB!</li><li><strong>Full Customization Power:</strong> All the beloved flexibility of <a href=\"https://github.com/osmandapp/OsmAnd-resources/blob/master/routing/routing.xml\" target=\"_blank\" rel=\"noopener noreferrer\"></a> and detailed routing parameters is retained.</li><li><strong>Works with Regional Maps:</strong> Download only the countries you need. HH-Routing seamlessly calculates routes across the borders of your downloaded map files (as long as they are compatible, see limitations). Clusters that overlap a region's boundary are included within that region's data.</li><li><strong>Supports Frequent Updates:</strong> The architecture is designed to work with <a href=\"https://osmand.net/docs/user/personal/maps-resources#live-updates\" target=\"_blank\" rel=\"noopener noreferrer\">OsmAnd’s hourly map updates</a>, allowing routing to adapt to fresh road information.</li><li> This structure makes it much easier to implement features like alternative route suggestions based on these key border points.</li></ul><p>No system is perfect, and OsmAnd's HH-Routing has a few considerations:</p><ul><li><strong>Highly Divergent Profiles:</strong> For routing configurations that are not pre-calculated as common scenarios and whose costs vary too much from default configurations, the original <a href=\"https://en.wikipedia.org/wiki/A*_search_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">A* algorithm</a> might still be faster (and is often used as an automatic fallback).</li><li><strong>Map Version Synchronicity (Important!):</strong> For HH-Routing to work correctly when a route crosses multiple map files (e.g., different countries or regions), <a href=\"https://osmand.net/docs/user/personal/maps-resources#updates\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>all those map files MUST be from the same generation date</strong></a> (i.e., downloaded from OsmAnd around the same time, based on the same underlying OpenStreetMap data version and pre-calculation run).<ul><li>If you try to route from a map of France updated in May with a map of Germany updated in April, HH-Routing may not be compatible across the border. You would need to update all relevant maps to the same version.</li><li>This doesn't mean all maps must be , just <em>from the same batch/pre-calculation period</em>.</li></ul></li><li> The intensive preprocessing required to generate all these routing profiles for the entire planet takes about 2-3 days. This means new map updates are now typically released around the 5th of each month, instead of the 2nd.</li></ul><h2>Smart Engineering Delivers<a href=\"https://osmand.net/blog/fast-routing/#smart-engineering-delivers\" aria-label=\"Direct link to Smart Engineering Delivers\" title=\"Direct link to Smart Engineering Delivers\">​</a></h2><p>OsmAnd's HH-Routing is more than just an algorithm; it's a testament to innovative problem-solving. It’s a carefully engineered system born from the need to overcome specific, demanding constraints: the desire for blazing speed, minimal storage, complete routing flexibility, regional map support, and adaptability to fresh data.</p><p>By cleverly identifying crucial \"bottleneck\" border points, creating a universal two-level hierarchy, and dynamically refining routes with our optimized A* engine, we've managed to deliver a vastly superior navigation experience. It's a win for every OsmAnd user who relies on fast, dependable, and customizable offline navigation.</p><p>We're excited about what HH-Routing brings to OsmAnd!</p><ul><li>Have you experienced the new routing speed?</li><li>What are your go-to custom routing settings that you're glad are still supported?</li><li>Share your thoughts, experiences, and any questions in the comments below or on our community forums!</li></ul>",
      "contentLength": 13669,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47170157"
    },
    {
      "title": "Smallest transformer that can add two 10-digit numbers",
      "url": "https://github.com/anadim/AdderBoard",
      "date": 1772130596,
      "author": "ks2048",
      "guid": 49053,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47170030"
    },
    {
      "title": "He saw an abandoned trailer. Then, uncovered a surveillance network",
      "url": "https://calmatters.org/justice/2026/02/alpr-border-patrol-caltrans/",
      "date": 1772130431,
      "author": "Element_",
      "guid": 48639,
      "unread": true,
      "content": "<div><div><p>Southern California residents are noticing new license plate readers that appear to be operated by the Border Patrol. Some have had confusing encounters with agents.</p></div></div><p>On a cracked two-lane road on the eastern edge of San Diego County, James Cordero eased his Jeep onto the shoulder after something caught his eye. It looked like an abandoned trailer. Inside he found a hidden camera feeding a vast surveillance network that logs the license plate of every driver passing through this stretch of remote backcountry between San Diego and the Arizona state line.&nbsp;</p><p>Cordero, 44, has found dozens of these cameras hidden in trailers and construction barrels on border roads around San Diego and Imperial counties: one on Old Highway 80 near Jacumba Hot Springs; another outside the Golden Acorn Casino in Campo; another along Interstate 8 toward In-Ko-Pah Gorge.&nbsp;</p><p>They started showing up after California granted permits to the Border Patrol and other federal agencies to place license plate readers on state highways in the last months of the Biden administration. Now as many as 40 are feeding information into <a href=\"https://calmatters.org/tag/donald-trump/\" target=\"_blank\" rel=\"noreferrer noopener\">Trump administration</a> databases as the Democratic-led state chafes over the federal government’s <a href=\"https://calmatters.org/tag/immigration/\" target=\"_blank\" rel=\"noreferrer noopener\">massive deportation program</a>.</p><p>The cameras are raising concerns with privacy experts, civil liberties advocates and humanitarian aid workers who say California should not be supporting the surveillance and data-collection program, which they view as an unwarranted government intrusion into the lives of Americans who’ve committed no crime. Moreover, they say the program conflicts with state law.&nbsp;</p><p>Supporters say the devices allow law enforcement to quickly identify and locate people they suspect of serious crimes. They also argue the cameras help agencies spot patterns in drug and human trafficking, and could be used to help locate missing persons, such as children or other vulnerable people.&nbsp;</p><p>&nbsp;“If you’re not doing anything illegal, why worry about it?” said long-time Jacumba resident Allen Stanks, 70.&nbsp;&nbsp;&nbsp;</p><p>“Everyone is talking about privacy, OK. Stop putting everything on Facebook. ‘Here’s a picture of my food.’ Who cares?” said Stanks.&nbsp;&nbsp;</p><p>Some locals, however, suspect the cameras are behind some unusual encounters they’ve had in recent months with officers from Border Patrol and its parent agency, Customs and Border Protection. In one case agents questioned a grandmother – a lawful permanent resident&nbsp; – about why she went to a casino, according to her grandson.&nbsp;</p><p>Cordero has a different concern. On his days off, he leads volunteers into the far reaches of the county, leaving water, food and clothing for migrants. He fears his colleagues could be detained by agents.</p><p>“I’m not so much worried about myself, but I’m worried about a lot of our volunteers that come out,” said Cordero. “I don’t want them to have to deal with any of the nonsense of being tracked or being pulled over and questioned.”&nbsp;</p><p>He has good reason to be nervous. During the first Trump administration, federal officials prosecuted volunteers from the humanitarian group “No More Deaths” for leaving water and supplies for migrants in the Arizona desert. The volunteers faced charges, including “abandonment of property” and felony harboring, though the convictions for some were later overturned.</p><p>Border Patrol provides little information about its use of license plate readers on its website. In 2020, the Department of Homeland Security <a href=\"https://www.dhs.gov/sites/default/files/publications/privacy-pia-cbp049a-cbplprtechnology-july2020.pdf\">issued a report</a> that describes the technology in general, but doesn’t specify where it’s being used. CalMatters reached out to Border Patrol and Homeland Security officials for comment, but did not receive a response.&nbsp;</p><p>“There’s no transparency, that’s the worst part,” Cordero said.&nbsp;</p><p>The Homeland Security report says some readers are capturing license plate numbers, as well as the make and model of the vehicle, the state the vehicle is registered in, the camera owner and type, the GPS coordinates for where the image was taken, and the date and time of the capture.&nbsp;</p><p>The “technology may also capture (within the image) the environment surrounding a vehicle, which may include drivers and passengers,” the report states. It also says feds can access license plate readers operated by commercial vendors.&nbsp;</p><p>Earlier this month, the Electronic Frontier Foundation and a coalition of 30 organizations <a href=\"https://www.eff.org/document/coalition-letter-re-covert-alprs\" target=\"_blank\" rel=\"noreferrer noopener\">sent a letter to Gov. Gavin Newsom</a> and the California Department of Transportation urging them to revoke state permits and remove the covert readers operated by federal agencies like Customs and Border Protection and the Drug Enforcement Agency along California border highways.</p><p>The San Francisco-based privacy and civil rights advocacy organization, also known as EFF, <a href=\"https://www.google.com/maps/d/u/0/viewer?mid=1IBmF94s37WkKLSQ-8bISvlufeKYNzZU&amp;ll=32.953202023489034%2C-115.88704233717829&amp;z=8\" target=\"_blank\" rel=\"noreferrer noopener\">mapped out more than 40 hidden license plate readers</a> in Southern California, most of them along border roadways. It contends the devices bypass <a href=\"https://calmatters.digitaldemocracy.org/bills/ca_201520160sb34\" target=\"_blank\" rel=\"noreferrer noopener\">a 2016 state law</a> that spells out how law enforcement agencies can use automated license plate readers, which are often referred to as ALPRs.</p><p>“By allowing Border Patrol and the DEA to put license plate readers along the border, they’re essentially bypassing the protections under (California law),” said Dave Maass, the director of investigations for EFF. “That is a backdoor around it.”</p><p>Maass said he believes Cordero’s concerns about the agency surveilling humanitarian volunteers may be valid.&nbsp;</p><p>“They claim they might be looking for smugglers or they might be looking for cartel members, but that’s not who they’re collecting data on,” said Maass. “(The program) is primarily collecting data on people who live in the region.&nbsp;</p><p>Maass said there’s no way to be certain which agency is installing each camera, but his organization checked with all other agencies operating in the area, such as the San Diego and Imperial sheriff’s departments, the California Highway Patrol, and Cal Fire, among others.</p><p>The camera models <a href=\"https://www.google.com/maps/@32.6784136,-115.6742449,0a,75y,163.53h,64.52t/data=!3m5!1e1!3m3!1sfq08I7orewhvm3IAMaCvwg!2e0!6shttps:%2F%2Fstreetviewpixels-pa.googleapis.com%2Fv1%2Fthumbnail%3Fpanoid%3Dfq08I7orewhvm3IAMaCvwg%26w%3D900%26h%3D600%26ll%3D32.678414,-115.674245%26yaw%3D163.529694%26pitch%3D25.480545%26thumbfov%3D98%26cb_client%3Dgmm.iv.ios?g_ep=CAISEjI2LjA2LjIuODY0NDM1MTkzMBgAIIGBASpRLDk0Mjk3Njk5LDk0Mjc1NDE1LDk0Mjg0NDYzLDk0MjMxMTg4LDk0MjgwNTY4LDQ3MDcxNzA0LDk0MjE4NjQxLDk0MjgyMTM0LDk0Mjg2ODY5QgJVUw%3D%3D&amp;skid=7f5d84cd-b51f-4263-be87-657cc88294db\" target=\"_blank\" rel=\"noreferrer noopener\">currently installed on state highways in the border region</a> are the same as ones the Border Patrol purchased in large amounts, according to Maass. Records obtained from Caltrans by EFF from 2016 appear to show Drug and Enforcement Administration and Border Patrol requesting permits to install the same devices in other parts of San Diego County, according to Maass.&nbsp;</p><p>Customs and Border Protection did not respond to a request for comment. The governor’s office did not comment. The Drug Enforcement Agency also did not respond to a request for comment.&nbsp;</p><h2>Caltrans approves ALPR requests</h2><p>By day, Cordero works in water-damage restoration, the crews residents call after floods and burst pipes. Comfortable with emergencies, he’s the type of guy you’d hope to run into if your car broke down in the middle of nowhere.&nbsp;</p><p>“People are literally dying out here,” Cordero says of his volunteer work, done through the nonprofit <a href=\"https://www.alotrolado.org/\">Al Otro Lado</a>, a legal services organization that also provides humanitarian support to refugees, migrants and deportees on both sides of the U.S.-Mexico border. “All we’re trying to do is prevent people from dying.”&nbsp;</p><p>In response to questions from CalMatters, a spokesperson for Caltrans provided a written statement that the state agency has approved eight permits for license plate readers from federal agencies, like Customs and Border Protection and the Drug Enforcement Administration, to be stationed in state highway rights-of-way.</p><p>“Caltrans does not operate, manage, or determine the specific use of technology or equipment installed by permit holders, nor does it have access to any of the collected data,” the statement read in part.&nbsp;</p><p>Caltrans said federal immigration agencies haven’t requested permits for the cameras since June 2024. They did not say how long a permit lasts. Between 2015 and 2024, their records indicate Customs and Border Protection and the Drug Enforcement Administration requested 14 permit applications for “law enforcement surveillance devices.” Of the 14, eight were approved, four were cancelled by the applicants and two did not result in projects in state right-of-way, the agency said.</p><p>In California, license plates are tracked not only by the federal government and law enforcement, but also by schools and businesses, including some <a href=\"https://www.kpbs.org/news/public-safety/2025/11/20/san-diego-county-police-agencies-access-many-private-license-plate-readers-with-minimal-oversight\">Home Depots</a> and <a href=\"https://www.reuters.com/sustainability/boards-policy-regulation/amid-ice-raids-some-home-depot-investors-want-know-how-law-enforcement-uses-its-2026-01-16/\">malls</a>. While <a href=\"https://www.the74million.org/article/amazons-ring-cuts-ties-with-surveillance-camera-co-used-by-ice-will-schools/\">schools</a> and businesses <a href=\"https://www.reuters.com/sustainability/boards-policy-regulation/amid-ice-raids-some-home-depot-investors-want-know-how-law-enforcement-uses-its-2026-01-16/\"></a>may not agree to pass that information on to the federal government, local police with access to those cameras may do so.</p><p>Newsom <a href=\"https://calmatters.org/economy/technology/2025/10/newsom-vetoes-license-plate-reader-regulations/\" target=\"_blank\" rel=\"noreferrer noopener\">vetoed a bill to strengthen California license plate reader law</a> last fall. Two days later, Attorney General Rob Bonta <a href=\"https://calmatters.org/justice/2025/10/el-cajon-police-license-plate-data/\" target=\"_blank\" rel=\"noreferrer noopener\">filed a lawsuit</a> against the city of El Cajon for multiple violations of<a href=\"https://calmatters.digitaldemocracy.org/bills/ca_201520160sb34\"></a>the license plate sharing prohibition. Since 2024, the attorney general’s office has sent letters to 18 law enforcement agencies, including the Imperial County Sheriff’s Office, the San Diego Police Department, and the El Centro Police Department.</p><p>Local agencies continue to share license plate data with federal immigration authorities, and not just along the border. The San Pablo Police Department in Northern California, one of the law enforcement agencies that received letters from the attorney general’s office, shared license plate data with the&nbsp; Border Patrol as recently as last month, according to records obtained by Oakland Privacy head of research Mike Katz-Lacabe. Some cameras are easy to spot, but Katz-Lacabe said that local police have concealed cameras that scan license plates for more than a decade, sometimes behind the grill of police cruisers or inside <a href=\"https://www.cehrp.org/tags/lpr-speed-trailer/\" target=\"_blank\" rel=\"noreferrer noopener\">speed limit trailers or</a> in a <a href=\"https://www.cnet.com/roadshow/news/license-plate-readers-hidden-in-fake-cactuses-in-arizona/\" target=\"_blank\" rel=\"noreferrer noopener\">fake saguaro cactus</a>.</p><p>“This has been the practice for years,” he said.</p><p>On a recent Saturday, Cordero was dressed for the remote border terrain – flannel, hiking boots, a San Diego Padres cap pulled low against the sun. His dirt-caked Jeep is built for places roads don’t go. On this particular weekend, supplies at one of the drop sites had already been used, indicating people may be crossing in the area.&nbsp;</p><p>Cordero has gotten good at finding stuff out here. In the remote Ocotillo washes, where the scrubs claw at people’s shins, he recently found what he believes to be the remains of a human finger.</p><p>A year earlier, Cordero found a phone contact list next to human remains. He and his wife, Jacqueline Arellano, were able to use the phone list to notify the person’s family in Arizona about where their missing loved one fell.</p><p>That’s why when, months ago, he first saw the abandoned trailer along the side of the road on Old Highway 80, he had to stop to take a closer look.&nbsp;</p><p>“It took me passing by a few times before I realized what it was,” said Cordero.&nbsp;</p><p>An Associated Press <a href=\"https://apnews.com/article/immigration-border-patrol-surveillance-drivers-ice-trump-9f5d05469ce8c629d6fecf32d32098cd\" target=\"_blank\" rel=\"noreferrer noopener\">investigation published in November</a> revealed that Border Patrol had hidden license plate readers in ordinary traffic safety equipment. The data collected by the agency’s plate readers was fed into a predictive intelligence program monitoring millions of American drivers nationwide to identify and detain people whose travel patterns the algorithm deemed suspicious, according to the AP’s investigation.</p><p>Sergio Ojeda, a community organizer with the mutual aid group Imperial Valley Equity and Justice said CBP apparently believed his grandmother’s driving patterns were suspicious because they interrogated her about the amount of time she spends at local casinos in the area.&nbsp;</p><p>“She was outraged about it,” said Ojeda. His grandmother, a resident of Imperial Valley with legal status, was crossing the border when agents asked her about her trips to casinos.&nbsp;</p><p>“She asked them back, ‘Is something wrong with that? Am I not supposed to be doing that or why are you questioning me about this?’ and they were like “Oh, no, it just seems suspicious,” Ojeda recounted.&nbsp;</p><p>Ojeda said he was equally concerned, and he doesn’t enjoy the feeling of being watched just because he lives near the border. “It’s how I feel every day,” he said. “Driving around, I joke with my co-workers: ‘Which chapter of  is this?’”&nbsp;</p>",
      "contentLength": 11977,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47169984"
    },
    {
      "title": "What Claude Code chooses",
      "url": "https://amplifying.ai/research/claude-code-picks",
      "date": 1772129546,
      "author": "tin7in",
      "guid": 48650,
      "unread": true,
      "content": "<p> (Recommended) — Built by the creators of Next.js. Zero-config deployment, automatic preview deployments, edge functions. </p><p> — Great alternative with similar features. Good free tier.</p><p> — Good if you're already in the AWS ecosystem.</p>",
      "contentLength": 234,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47169757"
    },
    {
      "title": "Show HN: Deff – Side-by-side Git diff review in your terminal",
      "url": "https://github.com/flamestro/deff",
      "date": 1772128446,
      "author": "flamestro",
      "guid": 48670,
      "unread": true,
      "content": "<p>deff is an interactive Rust TUI for reviewing git diffs side-by-side with syntax highlighting and added/deleted line tinting. It supports keyboard/mouse navigation, vim-style motions, in-diff search (/, n, N), per-file reviewed toggles, and both upstream-based and explicit --base/--head comparisons. It can also include uncommitted + untracked files (--include-uncommitted) so you can review your working tree before committing.</p><p>Would love to get some feedback</p>",
      "contentLength": 460,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47169518"
    },
    {
      "title": "Google Street View in 2026",
      "url": "https://tech.marksblogg.com/google-street-view-coverage.html",
      "date": 1772127483,
      "author": "marklit",
      "guid": 48629,
      "unread": true,
      "content": "<p>Last year, I came across a <a href=\"https://geo.emily.bz/coverage-dates\">dataset</a> documenting Google's global Street View coverage. Each point in this dataset includes the year and month of that point's last capture.</p><a href=\"https://tech.marksblogg.com/theme/images/google_street_view/qgis-bin_eSXumzRsku.jpg\"><img alt=\"Google Street View Coverage 2026\" src=\"https://tech.marksblogg.com/theme/images/google_street_view/qgis-bin_eSXumzRsku.jpg\"></a><p>In this post, I'll convert this dataset into Parquet and examine its geospatial patterns.</p><div><p>I'm using a 5.7 GHz AMD Ryzen 9 9950X CPU. It has 16 cores and 32 threads and 1.2 MB of L1, 16 MB of L2 and 64 MB of L3 cache. It has a liquid cooler attached and is housed in a spacious, full-sized Cooler Master HAF 700 computer case.</p><p>The system has 96 GB of DDR5 RAM clocked at 4,800 MT/s and a 5th-generation, Crucial T700 4 TB NVMe M.2 SSD which can read at speeds up to 12,400 MB/s. There is a heatsink on the SSD to help keep its temperature down. This is my system's C drive.</p><p>The system is powered by a 1,200-watt, fully modular Corsair Power Supply and is sat on an ASRock X870E Nova 90 Motherboard.</p><p>I'm running Ubuntu 24 LTS via Microsoft's Ubuntu for Windows on Windows 11 Pro. In case you're wondering why I don't run a Linux-based desktop as my primary work environment, I'm still using an Nvidia GTX 1080 GPU which has better driver support on Windows and ArcGIS Pro only supports Windows natively.</p></div><div><div><pre>$~\n$wget-chttps://github.com/duckdb/duckdb/releases/download/v1.4.3/duckdb_cli-linux-amd64.zip\n$unzip-jduckdb_cli-linux-amd64.zip\n$chmod+xduckdb\n$~/duckdb\n</pre></div><div><pre></pre></div><p>I'll set up DuckDB to load every installed extension each time it launches.</p><div><pre>.timer on\n.width 180\nLOAD h3;\nLOAD lindel;\nLOAD json;\nLOAD parquet;\nLOAD spatial;\n</pre></div><p>The maps in this post were rendered using <a href=\"https://www.qgis.org/en/site/forusers/download.html\">QGIS</a> version 3.44. QGIS is a desktop application that runs on Windows, macOS and Linux. The application has grown in popularity in recent years and has ~15M application launches from users all around the world each month.</p><p>I used QGIS' <a href=\"https://plugins.qgis.org/plugins/HCMGIS/\">HCMGIS</a> plugin to add basemaps from Esri to the maps in this post.</p></div><div><h2>Downloading Emily's JSON Files</h2><p>The following will download 131 JSON files which are 647 MB uncompressed. These files were last refreshed on December 4th.</p><div><pre>$mkdir-p~/emily_biz\n$~/emily_biz\n\n$wget-r-Ajsonhttps://geo.emily.bz/coverage-dates\n</pre></div><p>Below is an example record from one of the JSON files.</p><div><pre>$jq-S.customCoordinatesgeo.emily.bz/coverage-dates/aland.json\n</pre></div><div><pre></pre></div></div><div><p>Below, I'll create a table in DuckDB and import the data from the JSON files.</p><div><pre>$~/duckdbstreet_view.duckdb\n</pre></div><div><pre></pre></div><div><pre>$FILENAMEgeo.emily.bz/coverage-dates/*.json~/duckdbstreet_view.duckdb\n</pre></div><p>I'll then export this table as a spatially-sorted, ZStandard-compressed Parquet file.</p><div><pre>$~/duckdbstreet_view.duckdb\n</pre></div><div><pre></pre></div><p>The resulting Parquet file is 85 MB and contains 7,163,407 rows.</p><p>Data for Bosnia and Herzegovina, Cyprus, Namibia, Paraguay and Vietnam are missing in this release. Hopefully, they will be available after the next refresh.</p></div><div><p>Below are the point counts rounded up to the nearest thousand and broken down by year.</p><div><pre></pre></div><div><pre>┌───────┬───────┐\n│ year  │ count │\n│ int64 │ int32 │\n├───────┼───────┤\n│  2003 │     1 │\n│  2006 │     1 │\n│  2007 │    28 │\n│  2008 │   251 │\n│  2009 │   659 │\n│  2010 │   344 │\n│  2011 │   619 │\n│  2012 │   792 │\n│  2013 │   622 │\n│  2014 │   474 │\n│  2015 │   661 │\n│  2016 │   529 │\n│  2017 │    70 │\n│  2018 │   142 │\n│  2019 │   195 │\n│  2020 │    50 │\n│  2021 │   267 │\n│  2022 │   507 │\n│  2023 │   588 │\n│  2024 │   290 │\n│  2025 │    83 │\n├───────┴───────┤\n│    21 rows    │\n└───────────────┘\n</pre></div><p>Below is the coverage across Europe. The darker colours are points that were updated closer to 2007 and the brighter colours closer to December of last year.</p><a href=\"https://tech.marksblogg.com/theme/images/google_street_view/qgis-bin_8MdUkCdlxo.jpg\"><img alt=\"Google Street View Coverage 2026\" src=\"https://tech.marksblogg.com/theme/images/google_street_view/qgis-bin_8MdUkCdlxo.jpg\"></a><p>The following shows India and Southeast Asia's coverage.</p><a href=\"https://tech.marksblogg.com/theme/images/google_street_view/qgis-bin_SY7p2CulWL.jpg\"><img alt=\"Google Street View Coverage 2026\" src=\"https://tech.marksblogg.com/theme/images/google_street_view/qgis-bin_SY7p2CulWL.jpg\"></a><p>The following shows the coverage across Australia and New Zealand.</p><a href=\"https://tech.marksblogg.com/theme/images/google_street_view/qgis-bin_4kULqhq0r6.jpg\"><img alt=\"Google Street View Coverage 2026\" src=\"https://tech.marksblogg.com/theme/images/google_street_view/qgis-bin_4kULqhq0r6.jpg\"></a><p>This is the coverage for North America.</p><a href=\"https://tech.marksblogg.com/theme/images/google_street_view/qgis-bin_QHBvfC6lA6.jpg\"><img alt=\"Google Street View Coverage 2026\" src=\"https://tech.marksblogg.com/theme/images/google_street_view/qgis-bin_QHBvfC6lA6.jpg\"></a><p>This is the coverage for Latin America and the Caribbean.</p><a href=\"https://tech.marksblogg.com/theme/images/google_street_view/qgis-bin_WM0VRKBzav.jpg\"><img alt=\"Google Street View Coverage 2026\" src=\"https://tech.marksblogg.com/theme/images/google_street_view/qgis-bin_WM0VRKBzav.jpg\"></a></div>",
      "contentLength": 3922,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47169278"
    },
    {
      "title": "Palm OS User Interface Guidelines (2003) [pdf]",
      "url": "https://cs.uml.edu/~fredm/courses/91.308-spr05/files/palmdocs/uiguidelines.pdf",
      "date": 1772125265,
      "author": "spiffytech",
      "guid": 48628,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47168726"
    },
    {
      "title": "Open Source Endowment – new funding source for open source maintainers",
      "url": "https://endowment.dev/",
      "date": 1772122386,
      "author": "kvinogradov",
      "guid": 48581,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47168012"
    },
    {
      "title": "Will vibe coding end like the maker movement?",
      "url": "https://read.technically.dev/p/vibe-coding-and-the-maker-movement",
      "date": 1772122031,
      "author": "itunpredictable",
      "guid": 48627,
      "unread": true,
      "content": "<p>Whenever a new technology arrives, the impulse is to treat it as something that has never existed before. A clean break from everything that came prior. I catch myself doing this with vibe coding constantly, and I see it everywhere around me. But the most useful lens for understanding a new phenomenon is almost never the phenomenon itself. You want something adjacent, close enough to share structural similarities but removed enough to see clearly. It’s on the lookout for something like this that I started reading more about the Maker Movement of ~2005-2015.</p><p><a href=\"https://en.wikipedia.org/wiki/Chris_Anderson_(writer)\" rel=\"\">Chris Anderson</a><a href=\"https://en.wikipedia.org/wiki/Makers_(novel)\" rel=\"\"> Makers,</a></p><p>A lot of the intellectual energy of the AI era orbits around AGI: when it arrives, what it’ll do to jobs, whether it will be aligned. The Maker Movement had its own gravitational center, and it was the idea that making physical things with your hands could produce an internal transformation. You would become more creative, more entrepreneurial, more self-reliant. The object you made mattered less than what the act of making did to you.</p><p><a href=\"https://fredturner2022.sites.stanford.edu/sites/g/files/sbiybj27111/files/media/file/turner-millenarian-tinkering-tech-culture-2018.pdf\" rel=\"\">published a paper</a></p><p>The specifics of seventeenth-century Puritanism are obviously gone. Nobody at a Maker Faire was talking about predestination. But Turner traced the literary forms and the millenarian structure—the belief that a great transformation is coming, and that individual discipline will determine who makes it through. In the Maker narrative, the American landscape is economically barren. Jobs have disappeared. Institutions have failed you. And in this wilderness, the lone individual searches inside themselves for signs of the entrepreneurial spirit, the creative spark, evidence that they are among the elect who will build their way to salvation.</p><p>Turner’s observation extends well beyond 3D printers. You can trace this same pattern through almost every hobbyist technology scene of the past fifty years. Homebrew computer clubs in the 1970s. Punk zines in the 1980s. The early web in the 1990s. Each one developed a community of practice—what Brian Eno would call a “scenius”—where people played with tools that the mainstream considered toys. Each one generated its own salvation narrative: master this tool, transform yourself, become the kind of person who builds the future.</p><p>And each one operated with a useful kind of slack. The tools were unproductive on purpose. Nobody expected your Arduino project to ship to customers. Nobody expected your homebrew computer to compete with IBM. The whole point was that you had permission to fuck around, and the finding-out happened gradually, through play, over years. This is where the old Silicon Valley adage comes from: “What smart people do on the weekends, everyone else will do during the week in ten years.”</p><p>Vibe coding broke this pattern in a way that matters.</p><p>Every previous wave of hobbyist technology went through a scenius phase—a period where small groups of weirdos played with tools before anyone expected economic output from them.</p><p>Vibe coding skipped that phase entirely. It was deployed directly to the general public, and almost immediately into the codebases of enterprise companies and well-developed products. There was no protected playground period. There was no time to accumulate the weird, useless, playful knowledge that scenius communities generate. Instead, there was immediate pressure to one-shot yourself into a hit product or solve a complex use case on the first try.</p><p>This matters because the scenius phase is where the internal transformation actually happens. When you spend two years making useless Arduino projects, you develop instincts about electronics, materials, and design that you can’t get from a tutorial. When vibe coding goes straight to production, you lose that developmental space. The tool is powerful enough to produce real output before the person using it has developed real judgment. When I speak with people who are on Claude Code 12-14 hours a day, I feel like I’m speaking to someone possessed by something, attempting to grasp a different reality. In the case of scenius, the feedback loop that tethers you to reality was provided by other humans. Someone looked at your project and told you it’s pointless, or brilliant, or both. While in the case of vibe coding, the feedback loop is provided by the machine, and you’re constantly attempting to discern if you’re going crazy or if something genuinely valuable has been produced.</p><p>What it produces is something like hypomania: a state where your productive capacity genuinely increases. You’re not imagining that you’re getting more done, you actually are, but your evaluative faculty is unaccustomed to this mode of creation. You lose the ability to distinguish between “this is good” and “I feel good making this.” Everything feels like a breakthrough. The output is real but your relationship to it is distorted.</p><p>The speed and ease of vibe coding create a kind of evaluative anesthesia. You can’t tell if you’ve built something useful or just something that exists. In some way, this is the sober version of hippies in the 60s trying LSD for the first time: sometimes you may have a breakthrough, or you may have a breakdown, but regardless of which, this is the opposite of the salvation through making that Fred Turner talks about.</p><p>There’s a second reason the old transformation-through-making metaphor doesn’t fit vibe coding, and it has to do with how the Maker Movement actually ended.</p><p><a href=\"https://www.joelonsoftware.com/2002/06/12/strategy-letter-v/\" rel=\"\">commoditizing your complement</a></p><p><a href=\"https://www.cnbc.com/2026/02/09/monday-com-stock-ai-software.html\" rel=\"\">entire SaaS business models</a></p><p>With both of these forces at play—no scenius phase to develop through, and value accumulating upstream rather than with the maker—the old metaphor of transformation-through-making doesn’t hold up exactly. We need a new one.</p><p>Specifically: consumption of a surplus intelligence. AI represents an enormous amount of available cognitive energy, and vibe coding is one way of expending that energy before it goes to waste. Think of it like a resource that’s being generated whether you use it or not—and vibe coding is the act of channeling that surplus into play, into exploration, into rapid creation that may or may not produce lasting artifacts.</p><p><a href=\"https://www.fast.ai/posts/2026-01-28-dark-flow/\" rel=\"\">the dark flow state when you gamble</a></p><p>Consumption almost always gets treated as a negative behavior, especially if you’re an entrepreneur or builder. Consuming is what passive people do. Builders produce.</p><p>I think this framing is wrong, or at least incomplete. There are several productive ways to think about what consumption actually generates.</p><p>Expenditure that’s visible generates spectacle, and spectacle generates attention. When you vibe-code something in public—building fast, shipping immediately, iterating in front of an audience—the product you make matters less than the performance of making it. And undoubtedly, much of vibe coding today is pure signalling performance.</p><p><a href=\"https://x.com/search?q=built%20in%20a%20weekend%20&amp;src=typed_query\" rel=\"\">built this in a weekend</a></p><p>This is structurally identical to how content creators already operate. A YouTuber’s individual video is an expenditure. The audience accumulated across hundreds of videos is the asset. Vibe coding just adds another medium to the content creator’s toolkit: instead of expending effort on essays or videos, you expend it on apps and tools, and you capture the attention the same way.</p><p>If you treat your vibe-coded output as gifts—open source tools, free utilities, shared templates, public repos—you’re creating the conditions to occupy an interesting or powerful position in the network. Think of the people who built the early web’s most useful free tools and resources: They became nodes that other people oriented around.</p><p>Every time you vibe-code something, you’re generating signal. Signal about what users want. Signal about which patterns work. Signal about where the model fails, what edge cases it misses, what instructions it misinterprets. That signal currently flows upstream to model providers for free. Your prompts, your iterations, your corrections—all of it becomes training data for the next generation of models. You are, in a very literal sense, performing unpaid labor for the infrastructure layer every time you build something.</p><p>Consumption doesn’t have to be passive. Surplus can be spent well. The key distinction is whether you’re burning energy with some awareness of what the combustion produces—taste, attention, social capital, structured signal—or whether you’re just spinning up a dozen projects and wondering why none of them stick.</p><p>Personally, I find the consumption metaphor to be a good way to deal with the burnout that comes with constantly using AI for various things. A lot of people approach making things with the mindset of craft, and naturally extend this framing to vibe coding. That framing feels noble and it’s deeply familiar, but it’s also a recipe for burnout, because craft assumes you are reaching inside yourself and pulling something out. The whole emotional architecture of craft is transformational: you struggle, and develop mastery, and the object you produce is evidence of inner change. When the tool is doing most of the producing, that framework starts to collapse. You’re left reaching inward for something that the process never required you to develop, and the gap between the effort you expected to invest and the effort that was actually needed starts to feel like a personal failure rather than a feature of the technology.</p><p>The consumption framing sidesteps this entirely. You’re not reaching inward. You’re starting from the position that there is extra energy available and it needs to go somewhere. The question shifts from “what does this say about me as a maker” to “what’s the most interesting thing I can spend this on.” That’s a fundamentally different emotional posture, and in practice it’s a much more sustainable one.</p>",
      "contentLength": 9791,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47167931"
    },
    {
      "title": "Nano Banana 2: Google's latest AI image generation model",
      "url": "https://blog.google/innovation-and-ai/technology/ai/nano-banana-2/",
      "date": 1772121757,
      "author": "davidbarker",
      "guid": 48560,
      "unread": true,
      "content": "<p data-block-key=\"78m38\">In August of last year, our Gemini Image model, <a href=\"https://blog.google/products-and-platforms/products/gemini/updated-image-editing-model/\">Nano Banana</a>, became a <a href=\"https://blog.google/products-and-platforms/products/gemini/nano-banana-google-trends-2025/\">viral sensation</a>, redefining image generation and editing. Then in November, we released <a href=\"https://blog.google/innovation-and-ai/products/nano-banana-pro/\">Nano Banana Pro</a>, offering users advanced intelligence and studio-quality creative control. Today, we’re bringing the best of both worlds to users across Google.</p><p data-block-key=\"191b\">Introducing Nano Banana 2 (Gemini 3.1 Flash Image), our latest state-of-the-art image model. Now you can get the advanced world knowledge, quality and reasoning you love in Nano Banana Pro, at lightning-fast speed.</p>",
      "contentLength": 534,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47167858"
    },
    {
      "title": "AirSnitch: Demystifying and breaking client isolation in Wi-Fi networks [pdf]",
      "url": "https://www.ndss-symposium.org/wp-content/uploads/2026-f1282-paper.pdf",
      "date": 1772121348,
      "author": "DamnInteresting",
      "guid": 48559,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47167763"
    },
    {
      "title": "In 2025, Meta paid an effective federal tax rate of 3.5%",
      "url": "https://bsky.app/profile/rbreich.bsky.social/post/3mfptlfeucn2i",
      "date": 1772118860,
      "author": "doener",
      "guid": 48535,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47167171"
    },
    {
      "title": "Story of XZ Backdoor [video]",
      "url": "https://www.youtube.com/watch?v=aoag03mSuXQ",
      "date": 1772115665,
      "author": "Ulf950",
      "guid": 48580,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47166473"
    },
    {
      "title": "BuildKit: Docker's Hidden Gem That Can Build Almost Anything",
      "url": "https://tuananh.net/2026/02/25/buildkit-docker-hidden-gem/",
      "date": 1772114746,
      "author": "jasonpeacock",
      "guid": 48626,
      "unread": true,
      "content": "<h5>\n      Posted on \n  \n    February 25, 2026\n  \n\n\n      \n        &nbsp;•&nbsp;\n      \n      \n      5&nbsp;minutes\n      &nbsp;•\n      \n      859&nbsp;words\n      \n    </h5><p>Most people interact with BuildKit every day without realizing it. When you run , BuildKit is the engine behind it. But reducing BuildKit to “the thing that builds Dockerfiles” is like calling LLVM “the thing that compiles C.” It undersells the architecture by an order of magnitude.</p><p>BuildKit is a general-purpose, pluggable build framework. It can produce OCI images, yes, but also tarballs, local directories, APK packages, RPMs, or anything else you can describe as a directed acyclic graph of filesystem operations. The Dockerfile is just one frontend. You can write your own.</p><p>BuildKit’s design is clean and surprisingly understandable once you see the layers. There are three key concepts.</p><p>At the heart of BuildKit is  (Low-Level Build definition). Think of it as the LLVM IR of build systems. LLB is a binary protocol (protobuf) that describes a DAG of filesystem operations: run a command, copy files, mount a filesystem. It’s content-addressable, which means identical operations produce identical hashes, enabling aggressive caching.</p><p>When you write a Dockerfile, the Dockerfile frontend parses it and emits LLB. But nothing in BuildKit requires that the input be a Dockerfile. Any program that can produce valid LLB can drive BuildKit.</p><h3>Frontends: bring your own syntax</h3><p>A  is a container image that BuildKit runs to convert your build definition (Dockerfile, YAML, JSON, HCL, whatever) into LLB. The frontend receives the build context and the build file through the BuildKit Gateway API, and returns a serialized LLB graph.</p><p>This is the key insight: the build language is not baked into BuildKit. It’s a pluggable layer. You can write a frontend that reads a YAML spec, a TOML config, or a custom DSL, and BuildKit will execute it the same way it executes Dockerfiles.</p><p>You’ve actually seen this mechanism before. The  directive at the top of a Dockerfile tells BuildKit which frontend image to use. <code># syntax=docker/dockerfile:1</code> is just the default. You can point it at any image.</p><h3>Solver and cache: content-addressable execution</h3><p>The  takes the LLB graph and executes it. Each vertex in the DAG is content-addressed, so if you’ve already built a particular step with the same inputs, BuildKit skips it entirely. This is why BuildKit is fast: it doesn’t just cache layers linearly like the old Docker builder. It caches at the operation level across the entire graph, and it can execute independent branches in parallel.</p><p>The cache can be local, inline (embedded in the image), or remote (a registry). This makes BuildKit builds reproducible and shareable across CI runners.</p><p>BuildKit’s  flag is where this gets practical. You can tell BuildKit to export the result as:</p><ul><li> — push to a registry (the default for )</li><li> — dump the final filesystem to a local directory</li><li> — export as a tarball</li><li> — export as an OCI image tarball</li></ul><p>The  output is the most interesting for non-image use cases. Your build can produce compiled binaries, packages, documentation, or anything else, and BuildKit will dump the result to disk. No container image required.</p><p>Projects like <a href=\"https://earthly.dev\" target=\"_blank\" rel=\"noopener\">Earthly</a>\n, <a href=\"https://dagger.io\" target=\"_blank\" rel=\"noopener\">Dagger</a>\n, and <a href=\"https://depot.dev\" target=\"_blank\" rel=\"noopener\">Depot</a>\n are all built on top of BuildKit’s LLB. It’s a proven pattern.</p><h2>Building APK packages with a custom frontend</h2><p>To demonstrate this concretely, I built <a href=\"https://github.com/tuananh/apkbuild\" target=\"_blank\" rel=\"noopener\">apkbuild</a>\n: a custom BuildKit frontend that reads a YAML spec and produces Alpine APK packages. No Dockerfile involved. The entire build pipeline — from source compilation to APK packaging — runs inside BuildKit using LLB operations. Think of this like a dummy version of <a href=\"https://github.com/chainguard-dev/melange\" target=\"_blank\" rel=\"noopener\">Chainguard’s melange</a></p><p>I chose YAML for familiarity, but the spec could be anything you want (JSON, TOML, a custom DSL) as long as your frontend can parse it.</p><p>My package YAML spec looks like this:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>That’s it. No Dockerfile. BuildKit reads this spec through the custom frontend and produces a  file.</p><p>Build the frontend image:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>Then use it to build an APK package:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>You should be able to see the APK package in the  folder like below</p><p> tells BuildKit to use our custom frontend instead of the default Dockerfile parser. The  dumps the resulting  files to . No image is created. No registry is involved.</p><p>BuildKit gives you a content-addressable, parallelized, cached build engine for free. You don’t need to reinvent caching, parallelism, or reproducibility. You write a frontend that translates your spec into LLB, and BuildKit handles the rest.</p><p>This is relevant beyond toy demos. Dagger uses LLB as its execution engine for CI/CD pipelines. Earthly compiles Earthfiles into LLB. The pattern is proven at scale.</p><p>If you’re building a tool that needs to compile code, produce artifacts, or orchestrate multi-step builds, consider BuildKit as your execution backend. The Dockerfile is just the default frontend. The real power is in the engine underneath.</p>",
      "contentLength": 4911,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47166264"
    },
    {
      "title": "This time is different",
      "url": "https://shkspr.mobi/blog/2026/02/this-time-is-different/",
      "date": 1772112539,
      "author": "speckx",
      "guid": 48649,
      "unread": true,
      "content": "<p>3D TV, AMP, Augmented Reality, Beanie Babies, Blockchain, Cartoon Avatars, Curved TVs, Frogans, Hoverboards, iBeacons, Jetpacks, Metaverse, NFTs, Physical Web, Quantum Computing, Quibi, Small and Safe Nuclear Reactors, Smart Glasses, Stadia, WiMAX.</p><p>The problem is, the same dudes (and it was nearly always dudes) who were pumped for all of that bollocks now won't stop wanging on about Artificial Fucking Intelligence.</p><p>\"It's gonna be the future bro, just trust me!\"</p><p>\"I dunno, man. Seems like you say that about every passing fancy - and they all end up being utterly underwhelming.\"</p><p>\"This time is different!\"</p><p>All of the above technologies are still chugging along in some form or other (well, OK, not Quibi). Some are vaguely useful and others are propped up by weirdo cultists. I don't doubt that AI will be a  of the future - but it is obviously just going to be one of  technology which are in use.</p><blockquote><p>No enemies had ever taken Ankh-Morpork. Well technically they had, quite often; the city welcomed free-spending barbarian invaders, but somehow the puzzled raiders found, after a few days, that they didn't own their horses any more, and within a couple of months they were just another minority group with its own graffiti and food shops.</p><p>Terry Pratchet's  Eric</p></blockquote><p>The ideology of \"winner takes all\" is unsustainable and not supported by reality.</p>",
      "contentLength": 1337,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47165792"
    },
    {
      "title": "Just-bash: Bash for Agents",
      "url": "https://github.com/vercel-labs/just-bash",
      "date": 1772111790,
      "author": "tosh",
      "guid": 48625,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47165648"
    },
    {
      "title": "Anthropic ditches its core safety promise",
      "url": "https://www.cnn.com/2026/02/25/tech/anthropic-safety-policy-change",
      "date": 1772110370,
      "author": "motbus3",
      "guid": 48505,
      "unread": true,
      "content": "<p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22gzc5000x27qgej93hjm0@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Anthropic, a company founded by OpenAI exiles worried about the dangers of AI, is loosening its core safety principle in response to competition.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22yosp0005356r19pch1f3@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Instead of self-imposed guardrails constraining its development of AI models, Anthropic is adopting a nonbinding safety framework that it says can and will change.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm23dg1o000e356rczovpenx@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In a <a href=\"https://www.anthropic.com/news/responsible-scaling-policy-v3\" target=\"_blank\">blog post</a> Tuesday outlining its new policy, Anthropic said shortcomings in its two-year-old Responsible Scaling Policy could hinder its ability to compete in a rapidly growing AI market.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22uztn0000356r8g9aug5p@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The announcement is surprising, because Anthropic has described itself as the AI company with a “<a href=\"https://x.com/AmandaAskell/status/1995610570859704344\" target=\"_blank\">soul</a>.” It also comes the same week that Anthropic is fighting a significant battle with the Pentagon over AI red lines.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm235s67000c356r2ric1na8@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The policy change is separate and unrelated to Anthropic’s discussions with the Pentagon, according to a source familiar with the matter. Defense Secretary Pete Hegseth <a href=\"https://www.cnn.com/2026/02/24/tech/hegseth-anthropic-ai-military-amodei\">gave Anthropic CEO Dario Amodei an ultimatum</a> on Tuesdayto roll back the company’s AI safeguards or risk losing a $200 million Pentagon contract. The Pentagon threatened to put Anthropic on what is effectively a government blacklist.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct00063b6rm71k2g1w@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            But the company said in its blog post that its previous safety policy was designed to build industry consensus around mitigating AI risks – guardrails that the industry blew through. Anthropic also noted its safety policy was out of step with Washington’s current anti-regulatory political climate.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct00073b6rxinuqysn@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Anthropic’s <a href=\"https://www.anthropic.com/news/anthropics-responsible-scaling-policy\" target=\"_blank\">previous policy</a> stipulated that it should pause training more powerful models if their capabilities outstripped the company’s ability to control them and ensure their safety — a measure that’s been removed in the <a href=\"https://www-cdn.anthropic.com/e670587677525f28df69b59e5fb4c22cc5461a17.pdf\" target=\"_blank\">new policy</a>. Anthropic argued that responsible AI developers pausing growth while less careful actors plowed ahead could “result in a world that is less safe.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct00083b6ru8o50zwr@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            As part of the new policy, Anthropic said it will separate its own safety plans from its recommendations for the AI industry.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct00093b6rirslwag1@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Anthropic wrote that it had hoped its original safety principles “would encourage other AI companies to introduce similar policies. This is the idea of a ‘race to the top’ (the converse of a ‘race to the bottom’), in which different industry players are incentivized to improve, rather than weaken, their models’ safeguards and their overall safety posture.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct000a3b6rnzqt4hcc@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The company now suggests that hasn’t played out.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm2elsy800003b6rijtsgcly@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In a statement to CNN, an Anthropic spokesperson described the updated policy as “the strongest to date on the level of public accountability and transparency.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm2en9r500023b6rjkrlbul5@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “We’ve gone a significant step further from our prior policies by committing to publicly publish detailed reports at regular intervals on our plans to strengthen our risk mitigations, as well as the threat models and capabilities of all our models,” the statement said. “From the beginning, we’ve said the pace of AI and uncertainties in the field would require us to rapidly iterate and improve the policy.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct000b3b6ray297l4d@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Anthropic’s new safety policy includes a “Frontier Safety Roadmap” that outlines the company’s self-imposed guidelines and safeguards. But the company acknowledged the new framework is more flexible than its past policy.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct000c3b6ro0vjz177@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “Rather than being hard commitments, these are public goals that we will openly grade our progress towards,” the company said in its blog post.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct000d3b6rolq6l5tb@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The change comes a day after Defense Secretary Pete Hegseth <a href=\"https://www.cnn.com/2026/02/24/tech/hegseth-anthropic-ai-military-amodei\">gave Anthropic CEO Dario Amodei a Friday deadline</a> to roll back the company’s AI safeguards, or risk losing a $200 million Pentagon contract and being put on what is effectively a government blacklist.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct000e3b6r155cegce@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Anthropic has concerns over two issues that it isn’t willing to drop, according to a source familiar with the company’s meeting with Hegseth: AI-controlled weapons and mass domestic surveillance of American citizens. Anthropic believes AI is not reliable enough to operate weapons, and there are no laws or regulations yet that cover how AI could be used in mass surveillance, a source said.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct000f3b6ro2x0adsf@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            AI researchers applauded Anthropic’s stance on social media on Tuesday and expressed concerns about the idea of AI being used for government surveillance.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct000g3b6rxqnkroyz@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The company has long positioned itself as the AI business that prioritizes safety. Anthropic has published research showing how its own AI models <a href=\"https://www-cdn.anthropic.com/6d8a8055020700718b0c49369f60816ba2a7c285.pdf\" target=\"_blank\">could be capable of blackmail</a> under certain conditions. The company recently <a href=\"https://www.anthropic.com/news/donate-public-first-action?ref=ai-360.online\" target=\"_blank\">donated $20 million</a> to Public First Action, a political group pushing for AI safeguards and education.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct000h3b6rg9sx5462@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            But the company has faced increasing pressure and competition from both the government and its rivals. Hegseth, for example, plans to invoke the Defense Production Act on Anthropic and designate the company a supply chain risk if it does not comply with the Pentagon’s demands, <a href=\"https://www.cnn.com/2026/02/24/tech/hegseth-anthropic-ai-military-amodei\">CNN reported</a> on Tuesday. OpenAI and Anthropic have also been locked in a race to launch new enterprise AI tools in a bid to win the workplace.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct000i3b6rgqg7i5dh@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Jared Kaplan, Anthropic’s chief science officer, suggested in <a href=\"https://time.com/7380854/exclusive-anthropic-drops-flagship-safety-pledge/\" target=\"_blank\">an interview with Time</a> that the change was made in the name of safety more than increased competition.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22i0ct000j3b6r8a26rvct@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “We felt that it wouldn’t actually help anyone for us to stop training AI models,” Kaplan told the magazine. “We didn’t really feel, with the rapid advance of AI, that it made sense for us to make unilateral commitments … if competitors are blazing ahead.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm22q6df000p3b6rz3mlnptw@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\"><em>CNN’s Hadas Gold contributed to this story.</em></p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm2fh87600043b6r1vpo7s6t@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\"><em>This story has been updated with additional information.</em></p>",
      "contentLength": 5802,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47165397"
    },
    {
      "title": "Show HN: Agent Swarm – Multi-agent self-learning teams (OSS)",
      "url": "https://github.com/desplega-ai/agent-swarm",
      "date": 1772108138,
      "author": "tarasyarema",
      "guid": 48536,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47165046"
    },
    {
      "title": "You Want to Visit the UK? You Better Have a Google Play or App Store Account",
      "url": "https://www.heltweg.org/posts/you-want-to-visit-the-uk-you-better-have-a-google-play-or-app-store-account/",
      "date": 1772105170,
      "author": "rhazn",
      "guid": 48464,
      "unread": true,
      "content": "<a href=\"https://news.ycombinator.com/item?id=47164605\" target=\"_blank\">This post was discussed on Hackernews ↗</a><p>In the before times, and living in a blissful bubble of European citizenship, I never thought a lot about what I would need to prepare when visiting the UK. But, starting February 2026, new immigration rules for the UK are being enforced. Citizens of 85 additional countries (including the USA and European countries) will need an Electronic Travel Authorisation (ETA) to visit, even for tourism.</p><p>Luckily, you can apply (and pay) digitally. Unfortunately, the government very much would prefer you to do so using the official app.</p><p>Digital sovereignty is a hot topic right now in a lot of European countries (for good reasons). Both the Google Play Store and the Apple Store are controlled by US companies. And surely, I should not need to have access to a smartphone in order to complete a required government process?</p><p>Helpfully, the UK ETA app is not linked from the ETA enforcement announcement. But, if you search for it, you can find a help page on “Using the ‘UK ETA’ app” that looks like this:</p><p>Can you spot where you would find alternatives to using the app on this page? Of course, under ‘If you need help using the app’. First, you are smart and can see that I already clicked the link before taking the screenshot. But second, technically not being able to use the app would fall under needing help on how to use it. I guess? Fair enough.</p><p>There, you can find a link to “apply online instead”. Our journey is at an end; we can use the open web to complete the ETA! Let’s click the link:</p><p>No, weird, this is not it. It seems we landed on the app landing page (why was this not linked from the announcement post?). My mistake, let’s try something else. No, wait, there is a tiny ‘I cannot apply on the UK ETA app.’ link at the bottom! Finally, exactly my problem. And all it took was finding a tiny link on that page. Let’s resolve this by clicking on the link:</p><p>Another page about how to install the app and troubleshoot why you can’t install the app? And how much better and faster and less problematic my application will be if I use the app?</p><p>Ok, at this point you’re joking me. If I were a weird nerd that cares an abnormal amount about digital sovereignty, this would probably push me over the edge to write a ranty blog post about my experience. Luckily I am a well-adjusted person and instead scan the page carefully and find the tiny ‘Continue application online’ link instead.</p><p>Finally, we have reached the entry point of the online application for a UK ETA. In the end, this was quite easy; all we had to do was click through multiple pages of strong guidance to use the official app and accept that the online application will be much worse and slower.</p><p>And for future Philip, who still wants to visit the UK, here is the direct link so you do not need to repeat this: <a href=\"https://apply-for-an-eta.homeoffice.gov.uk/apply/electronic-travel-authorisation/how-to-apply\">Request a UK ETA Online</a>. You better hope the UK government hasn’t implemented automated scans of your web profile by then ;).</p><div data-astro-cid-u6pbnwwd=\"\"><p>\nDo you need help with <b>digital sovereignty such as data localization or EU clouds</b>?\n  I can help and am available on a freelance basis.\n</p><a href=\"mailto:philip@heltweg.org\" target=\"_blank\" rel=\"noopener noreferrer\" data-astro-cid-balv45lp=\"\"></a></div>",
      "contentLength": 3109,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47164605"
    },
    {
      "title": "Show HN: Terminal Phone – E2EE Walkie Talkie from the Command Line",
      "url": "https://gitlab.com/here_forawhile/terminalphone",
      "date": 1772102445,
      "author": "smalltorch",
      "guid": 48492,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47164270"
    },
    {
      "title": "Tell HN: YC companies scrape GitHub activity, send spam emails to users",
      "url": "https://news.ycombinator.com/item?id=47163885",
      "date": 1772098508,
      "author": "miki123211",
      "guid": 48463,
      "unread": true,
      "content": "Hi HN,<p>I recently noticed that an YC company (Run ANywhere, W26) sent me the following email:</p><p>From: Aditya &lt;aditya@buildrunanywhere.org&gt;</p><p>Subject: Mikołaj, think you'd like this</p><p>I found your GitHub and thought you might like what we're building.</p><p>I have also received a deluge of similar emails from another AI company, Voice.AI (doesn't seem to be YC affiliated). These emails indicate that those companies scrape people's Github activity, and if they notice users contributing to repos in their field of business, send marketing emails to those users without receiving their consent. My guess is that they use commit metadata for this purpose. This includes recipients under the GDPR (AKA me).</p><p>I've sent complaints to both organizations, no response so far.</p><p>I have just contacted both Github and YC Ethics on this issue, I'll update here if I get a response.</p>",
      "contentLength": 852,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=47163885"
    }
  ],
  "tags": [
    "hn"
  ]
}
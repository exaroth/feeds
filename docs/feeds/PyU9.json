{
  "id": "PyU9",
  "title": "Dev",
  "displayTitle": "Dev",
  "url": "",
  "feedLink": "",
  "isQuery": true,
  "isEmpty": false,
  "isHidden": false,
  "itemCount": 11,
  "items": [
    {
      "title": "Before You Migrate: Five Surprising Ingress-NGINX Behaviors You Need to Know",
      "url": "https://kubernetes.io/blog/2026/02/27/ingress-nginx-before-you-migrate/",
      "date": 1772206200,
      "author": "",
      "guid": 48876,
      "unread": true,
      "content": "<p>As <a href=\"https://kubernetes.io/blog/2025/11/11/ingress-nginx-retirement/\">announced</a> November 2025, Kubernetes will retire Ingress-NGINX in March 2026.\nDespite its widespread usage, Ingress-NGINX is full of surprising defaults and side effects that are probably present in your cluster today.\nThis blog highlights these behaviors so that you can migrate away safely and make a conscious decision about which behaviors to keep.\nThis post also compares Ingress-NGINX with Gateway API and shows you how to preserve Ingress-NGINX behavior in Gateway API.\nThe recurring risk pattern in every section is the same: a seemingly correct translation can still cause outages if it does not consider Ingress-NGINX's quirks.</p><p>I'm going to assume that you, the reader, have some familiarity with Ingress-NGINX and the Ingress API.\nMost examples use <a href=\"https://github.com/postmanlabs/httpbin\"></a> as the backend.</p><p>Also, note that Ingress-NGINX and NGINX Ingress are two separate Ingress controllers.\n<a href=\"https://github.com/kubernetes/ingress-nginx\">Ingress-NGINX</a> is an Ingress controller maintained and governed by the Kubernetes community that is retiring March 2026.\n<a href=\"https://docs.nginx.com/nginx-ingress-controller/\">NGINX Ingress</a> is an Ingress controller by F5.\nBoth use NGINX as the dataplane, but are otherwise unrelated.\nFrom now on, this blog post only discusses Ingress-NGINX.</p><h2>1. Regex matches are prefix-based and case insensitive</h2><p>Suppose that you wanted to route all requests with a path consisting of only three uppercase letters to the  service.\nYou might create the following Ingress with the <code>nginx.ingress.kubernetes.io/use-regex: \"true\"</code> annotation and the regex pattern of .</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>However, because regex matches are prefix and case insensitive, Ingress-NGINX routes any request with a path that starts with any three letters to httpbin:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>The output is similar to:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p> The  endpoint of httpbin returns a random UUID.\nA UUID in the response body means that the request was successfully routed to httpbin.</p><p>With Gateway API, you can use an <a href=\"https://gateway-api.sigs.k8s.io/reference/spec/#httppathmatch\">HTTP path match</a> with a  of  for regular expression path matching.\n matches are implementation specific, so check with your Gateway API implementation to verify the semantics of  matching.\nPopular Envoy-based Gateway API implementations such as <a href=\"https://istio.io/\">Istio</a>, <a href=\"https://gateway.envoyproxy.io/\">Envoy Gateway</a>, and <a href=\"https://kgateway.dev/\">Kgateway</a> do a full case-sensitive match.</p><p>Thus, if you are unaware that Ingress-NGINX patterns are prefix and case-insensitive, and, unbeknownst to you,\nclients or applications send traffic to  (or ), you might create the following HTTP route.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>However, if your Gateway API implementation does full case-sensitive matches,\nthe above HTTP route would not match a request with a path of .\nThe above HTTP route would thus cause an outage because requests\nthat Ingress-NGINX routed to httpbin would fail with a 404 Not Found at the gateway.</p><p>To preserve the case-insensitive regex matching, you can use the following HTTP route.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Alternatively, the aforementioned proxies support the  flag to indicate case insensitive matches.\nUsing the flag, the pattern could be .</p><h2>2. The <code>nginx.ingress.kubernetes.io/use-regex</code> applies to all paths of a host across all (Ingress-NGINX) Ingresses</h2><p>Now, suppose that you have an Ingress with the <code>nginx.ingress.kubernetes.io/use-regex: \"true\"</code> annotation, but you want to route\nrequests with a path of exactly  to .\nUnfortunately, you made a typo and set the path to  instead of .</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Most would expect a request to  to respond with a 404 Not Found, since  does not match the  path of .\nHowever, because the  Ingress has the <code>nginx.ingress.kubernetes.io/use-regex: \"true\"</code> annotation and the  host,\n<strong>all paths with the  host are treated as regular expressions across all (Ingress-NGINX) Ingresses.</strong>\nSince regex patterns are case-insensitive prefix matches,  matches the  pattern and Ingress-NGINX routes such requests to .\nRunning the command</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p> The  endpoint of httpbin returns the request headers.\nThe fact that the response contains the request headers in the body means that the request was successfully routed to httpbin.</p><p>Gateway API does not silently convert or interpret  and  matches as regex patterns.\nSo if you converted the above Ingresses into the following HTTP route and\npreserved the typo and match types, requests to  will respond with a 404 Not Found instead of a 200 OK.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>To keep the case-insensitive prefix matching, you can change</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Or even better, you could fix the typo and change the match to</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h2>3. Rewrite target implies regex</h2><p>In this case, suppose you want to rewrite the path of requests with a path of  to  before routing them to , and\nas in Section 2, you want to route requests with the path of exactly  to .\nHowever, you accidentally make a typo and set the path to  instead of  and  instead of .</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>The <code>nginx.ingress.kubernetes.io/rewrite-target: \"/uuid\"</code> annotation\ncauses requests that match paths in the  Ingress to have their paths rewritten to  before being routed to the backend.</p><p>Even though no Ingress has the <code>nginx.ingress.kubernetes.io/use-regex: \"true\"</code> annotation,\nthe presence of the <code>nginx.ingress.kubernetes.io/rewrite-target</code> annotation in the  Ingress causes <strong>all paths with the <code>rewrite-target.example.com</code> host to be treated as regex patterns.</strong>\nIn other words, the <code>nginx.ingress.kubernetes.io/rewrite-target</code> silently adds the <code>nginx.ingress.kubernetes.io/use-regex: \"true\"</code> annotation, along with all the side effects discussed above.</p><p>For example, a request to  has its path rewritten to  because  matches the case-insensitive prefix pattern of  in the  Ingress.\nAfter running the command</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>the output is similar to:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Like in the <code>nginx.ingress.kubernetes.io/use-regex</code> example, Ingress-NGINX treats s of other ingresses with the <code>rewrite-target.example.com</code> host as case-insensitive prefix patterns.\nRunning the command</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>gives an output that looks like</p><p>You can configure path rewrites in Gateway API with the <a href=\"https://gateway-api.sigs.k8s.io/reference/spec/#httpurlrewritefilter\">HTTP URL rewrite filter</a> which does not silently convert your  and  matches into regex patterns.\nHowever, if you are unaware of the side effects of the <code>nginx.ingress.kubernetes.io/rewrite-target</code> annotation\nand do not realize that  and  are both typos, you might create the following\nHTTP route.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>As with Section 2, because  is now an  match type in your HTTP route, requests to  will respond with a 404 Not Found instead of a 200 OK.\nSimilarly, requests to  will also respond with a 404 Not Found instead of a 200 OK.\nThus, this HTTP route will break applications and clients that rely on the  and  routes.</p><p>To fix this, you can change the matches in the HTTP route to be regex matches, and change the path patterns to be case-insensitive prefix matches, as follows.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Or, you can keep the  match type and fix the typos.</p><h2>4. Requests missing a trailing slash are redirected to the same path with a trailing slash</h2><p>Consider the following Ingress:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>You might expect Ingress-NGINX to respond to  with a 404 Not Found since the  does not exactly match the  path of .\nHowever, Ingress-NGINX redirects the request to  with a 301 Moved Permanently because the only difference between  and  is a trailing slash.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"http\"></code></pre></div><p>The same applies if you change the  to .\nHowever, the redirect does not happen if the path is a regex pattern.</p><p>Conformant Gateway API implementations do not silently configure any kind of redirects.\nIf clients or downstream services depend on this redirect, a migration to Gateway API that\ndoes not explicitly configure request redirects will cause an outage because\nrequests to  will now respond with a 404 Not Found instead of a 301 Moved Permanently.\nYou can explicitly configure redirects using the <a href=\"https://gateway-api.sigs.k8s.io/reference/spec/#httprequestredirectfilter\">HTTP request redirect filter</a> as follows:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h2>5. Ingress-NGINX normalizes URLs</h2><p> is the process of converting a URL into a canonical form before matching it against Ingress rules and routing it.\nThe specifics of URL normalization are defined in <a href=\"https://datatracker.ietf.org/doc/html/rfc3986#section-6.2\">RFC 3986 Section 6.2</a>, but some examples are</p><ul><li>removing path segments that are just a : </li><li>having a  path segment remove the previous segment: </li><li>deduplicating consecutive slashes in a path: </li></ul><p>Ingress-NGINX normalizes URLs before matching them against Ingress rules.\nFor example, consider the following Ingress:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Ingress-NGINX normalizes the path of the following requests to .\nNow that the request matches the  path of , Ingress-NGINX responds with either a 200 OK response or a 301 Moved Permanently to .</p><p>For the following commands</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>the outputs are similar to</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"http\"></code></pre></div><p>Your backends might rely on the Ingress/Gateway API implementation to normalize URLs.\nThat said, most Gateway API implementations will have some path normalization enabled by default.\nFor example, Istio, Envoy Gateway, and Kgateway all normalize  and  segments out of the box.\nFor more details, check the documentation for each Gateway API implementation that you use.</p><p>As we all race to respond to the Ingress-NGINX retirement, I hope this blog post instills some confidence that you can migrate safely and effectively despite all the intricacies of Ingress-NGINX.</p><p>SIG Network has also been working on supporting the most common Ingress-NGINX annotations (and some of these unexpected behaviors) in <a href=\"https://github.com/kubernetes-sigs/ingress2gateway\">Ingress2Gateway</a> to help you translate Ingress-NGINX configuration into Gateway API, and offer alternatives to unsupported behavior.</p><p>SIG Network released Gateway API 1.5 earlier today (27th February 2026), which graduates features such as\n<a href=\"https://gateway-api.sigs.k8s.io/api-types/listenerset/\">ListenerSet</a> (that allow app developers to better manage TLS certificates),\nand the HTTPRoute CORS filter that allows CORS configuration.</p>",
      "contentLength": 9134,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Allocating on the Stack",
      "url": "https://go.dev/blog/allocation-optimizations",
      "date": 1772150400,
      "author": "Keith Randall",
      "guid": 48850,
      "unread": true,
      "content": "<p>We’re always looking for ways to make Go programs faster. In the last\n2 releases, we have concentrated on mitigating a particular source of\nslowness, heap allocations. Each time a Go program allocates memory\nfrom the heap, there’s a fairly large chunk of code that needs to run\nto satisfy that allocation. In addition, heap allocations present\nadditional load on the garbage collector.  Even with recent\nenhancements like <a href=\"https://go.dev/blog/greenteagc\">Green Tea</a>, the garbage collector\nstill incurs substantial overhead.</p><p>So we’ve been working on ways to do more allocations on the stack\ninstead of the heap.  Stack allocations are considerably cheaper to\nperform (sometimes completely free).  Moreover, they present no load\nto the garbage collector, as stack allocations can be collected\nautomatically together with the stack frame itself. Stack allocations\nalso enable prompt reuse, which is very cache friendly.</p><h2>Stack allocation of constant-sized slices</h2><p>Consider the task of building a slice of tasks to process:</p><pre><code>func process(c chan task) {\n    var tasks []task\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    processAll(tasks)\n}\n</code></pre><p>Let’s walk through what happens at runtime when pulling tasks from the\nchannel  and adding them to the slice .</p><p>On the first loop iteration, there is no backing store for , so\n has to allocate one. Because it doesn’t know how big the\nslice will eventually be, it can’t be too aggressive. Currently, it\nallocates a backing store of size 1.</p><p>On the second loop iteration, the backing store now exists, but it is\nfull.  again has to allocate a new backing store, this time of\nsize 2. The old backing store of size 1 is now garbage.</p><p>On the third loop iteration, the backing store of size 2 is\nfull.  has to allocate a new backing store, this time\nof size 4. The old backing store of size 2 is now garbage.</p><p>On the fourth loop iteration, the backing store of size 4 has only 3\nitems in it.  can just place the item in the existing backing\nstore and bump up the slice length. Yay! No call to the allocator for\nthis iteration.</p><p>On the fifth loop iteration, the backing store of size 4 is full, and\n again has to allocate a new backing store, this time of size\n8.</p><p>And so on. We generally double the size of the allocation each time it\nfills up, so we can eventually append most new tasks to the slice\nwithout allocation. But there is a fair amount of overhead in the\n“startup” phase when the slice is small. During this startup phase we\nspend a lot of time in the allocator, and produce a bunch of garbage,\nwhich seems pretty wasteful. And it may be that in your program, the\nslice never really gets large. This startup phase may be all you ever\nencounter.</p><p>If this code was a really hot part of your program, you might be\ntempted to start the slice out at a larger size, to avoid all of these\nallocations.</p><pre><code>func process2(c chan task) {\n    tasks := make([]task, 0, 10) // probably at most 10 tasks\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    processAll(tasks)\n}\n</code></pre><p>This is a reasonable optimization to do. It is never incorrect; your\nprogram still runs correctly. If the guess is too small, you get\nallocations from  as before. If the guess is too large, you\nwaste some memory.</p><p>If your guess for the number of tasks was a good one, then there’s\nonly one allocation site in this program. The  call allocates a\nslice backing store of the correct size, and  never has to do\nany reallocation.</p><p>The surprising thing is that if you benchmark this code with 10\nelements in the channel, you’ll see that you didn’t reduce the number\nof allocations to 1, you reduced the number of allocations to 0!</p><p>The reason is that the compiler decided to allocate the backing store\non the stack. Because it knows what size it needs to be (10 times the\nsize of a task) it can allocate storage for it in the stack frame of\n instead of on the heap<a href=\"https://go.dev/blog/allocation-optimizations#footnotes\"></a>.  Note\nthat this depends on the fact that the backing store does not <a href=\"https://go.dev/doc/gc-guide#Escape_analysis\">escape\nto the heap</a> inside of .</p><h2>Stack allocation of variable-sized slices</h2><p>But of course, hard coding a size guess is a bit rigid.\nMaybe we can pass in an estimated length?</p><pre><code>func process3(c chan task, lengthGuess int) {\n    tasks := make([]task, 0, lengthGuess)\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    processAll(tasks)\n}\n</code></pre><p>This lets the caller pick a good size for the  slice, which may\nvary depending on where this code is being called from.</p><p>Unfortunately, in Go 1.24 the non-constant size of the backing store\nmeans the compiler can no longer allocate the backing store on the\nstack.  It will end up on the heap, converting our 0-allocation code\nto 1-allocation code. Still better than having  do all the\nintermediate allocations, but unfortunate.</p><p>But never fear, Go 1.25 is here!</p><p>Imagine you decide to do the following, to get the stack allocation\nonly in cases where the guess is small:</p><pre><code>func process4(c chan task, lengthGuess int) {\n    var tasks []task\n    if lengthGuess &lt;= 10 {\n        tasks = make([]task, 0, 10)\n    } else {\n        tasks = make([]task, 0, lengthGuess)\n    }\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    processAll(tasks)\n}\n</code></pre><p>Kind of ugly, but it would work. When the guess is small, you use a\nconstant size  and thus a stack-allocated backing store, and\nwhen the guess is larger you use a variable size  and allocate\nthe backing store from the heap.</p><p>But in Go 1.25, you don’t need to head down this ugly road. The Go\n1.25 compiler does this transformation for you!  For certain slice\nallocation locations, the compiler automatically allocates a small\n(currently 32-byte) slice backing store, and uses that backing store\nfor the result of the  if the size requested is small\nenough. Otherwise, it uses a heap allocation as normal.</p><p>In Go 1.25,  performs zero heap allocations, if\n is small enough that a slice of that length fits into 32\nbytes. (And of course that  is a correct guess for how\nmany items are in .)</p><p>We’re always improving the performance of Go, so upgrade to the latest\nGo release and <a href=\"https://youtu.be/FUm0pfgWehI?si=QRTt_JYwr-cRHDNJ&amp;t=960\" rel=\"noreferrer\" target=\"_blank\">be\nsurprised</a> by\nhow much faster and memory efficient your program becomes!</p><h2>Stack allocation of append-allocated slices</h2><p>Ok, but you still don’t want to have to change your API to add this\nweird length guess. Anything else you could do?</p><pre><code>func process(c chan task) {\n    var tasks []task\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    processAll(tasks)\n}\n</code></pre><p>In Go 1.26, we allocate the same kind of small, speculative backing\nstore on the stack, but now we can use it directly at the \nsite.</p><p>On the first loop iteration, there is no backing store for , so\n uses a small, stack-allocated backing store as the first\nallocation. If, for instance, we can fit 4 s in that backing store,\nthe first  allocates a backing store of length 4 from the stack.</p><p>The next 3 loop iterations append directly to the stack backing store,\nrequiring no allocation.</p><p>On the 4th iteration, the stack backing store is finally full and we\nhave to go to the heap for more backing store. But we have avoided\nalmost all of the startup overhead described earlier in this article.\nNo heap allocations of size, 1, 2, and 4, and none of the garbage that\nthey eventually become. If your slices are small, maybe you will never\nhave a heap allocation.</p><h2>Stack allocation of append-allocated escaping slices</h2><p>Ok, this is all good when the  slice doesn’t escape. But what if\nI’m returning the slice? Then it can’t be allocated on the stack, right?</p><p>Right! The backing store for the slice returned by  below\ncan’t be allocated on the stack, because the stack frame for \ndisappears when  returns.</p><pre><code>func extract(c chan task) []task {\n    var tasks []task\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    return tasks\n}\n</code></pre><p>But you might think, the  slice can’t be allocated on the\nstack. But what about all those intermediate slices that just become\ngarbage? Maybe we can allocate those on the stack?</p><pre><code>func extract2(c chan task) []task {\n    var tasks []task\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    tasks2 := make([]task, len(tasks))\n    copy(tasks2, tasks)\n    return tasks2\n}\n</code></pre><p>Then the  slice never escapes . It can benefit from\nall of the optimizations described above. Then at the very end of\n, when we know the final size of the slice, we do one heap\nallocation of the required size, copy our s into it, and return\nthe copy.</p><p>But do you really want to write all that additional code? It seems\nerror prone. Maybe the compiler can do this transformation for us?</p><p>For escaping slices, the compiler will transform the original \ncode to something like this:</p><pre><code>func extract3(c chan task) []task {\n    var tasks []task\n    for t := range c {\n        tasks = append(tasks, t)\n    }\n    tasks = runtime.move2heap(tasks)\n    return tasks\n}\n</code></pre><p> is a special compiler+runtime function that is the\nidentity function for slices that are already allocated in the heap.\nFor slices that are on the stack, it allocates a new slice on the\nheap, copies the stack-allocated slice to the heap copy, and returns\nthe heap copy.</p><p>This ensures that for our original  code, if the number of\nitems fits in our small stack-allocated buffer, we perform exactly 1\nallocation of exactly the right size. If the number of items exceeds\nthe capacity our small stack-allocated buffer, we do our normal\ndoubling-allocation once the stack-allocated buffer overflows.</p><p>The optimization that Go 1.26 does is actually better than the\nhand-optimized code, because it does not require the extra\nallocation+copy that the hand-optimized code always does at the end.\nIt requires the allocation+copy only in the case that we’ve exclusively\noperated on a stack-backed slice up to the return point.</p><p>We do pay the cost for a copy, but that cost is almost completely\noffset by the copies in the startup phase that we no longer have to\ndo. (In fact, the new scheme at worst has to copy one more element\nthan the old scheme.)</p><p>Hand optimization can still be beneficial, especially if you have a\ngood estimate of the slice size ahead of time. But hopefully the\ncompiler will now catch a lot of the simple cases for you and allow\nyou to focus on the remaining ones that really matter.</p><p>There are a lot of details that the compiler needs to ensure to get\nall these optimizations right. If you think that one of these\noptimizations is causing correctness or (negative) performance issues\nfor you, you can turn them off with\n<code>-gcflags=all=-d=variablemakehash=n</code>. If turning these optimizations\noff helps, please <a href=\"https://go.dev/issue/new\">file an issue</a> so we can investigate.</p><p> Go stacks do not have any -style mechanism for\ndynamically-sized stack frames. All Go stack frames are constant\nsized.</p>",
      "contentLength": 10579,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Rust debugging survey 2026",
      "url": "https://blog.rust-lang.org/2026/02/23/rust-debugging-survey-2026/",
      "date": 1771804800,
      "author": "Jakub Beránek",
      "guid": 47568,
      "unread": true,
      "content": "<p>Various issues with debugging Rust code are often mentioned as one of the biggest <a href=\"https://blog.rust-lang.org/2025/02/13/2024-State-Of-Rust-Survey-results/#challenges\">challenges</a> that annoy Rust developers. While it is definitely possible to debug Rust code today, there are situations where it does not work well enough, and the quality of debugging support also varies a lot across different debuggers and operating systems.</p><p>In order for Rust to have truly stellar debugging support, it should ideally:</p><ul><li>Support (several versions!) of different debuggers (such as GDB, LLDB or CDB) across multiple operating systems.</li><li>Implement debugger visualizers that are able to produce quality presentation of most Rust types.</li><li>Provide first-class support for debugging  code.</li><li>Allow evaluating Rust expressions in the debugger.</li></ul><p>Rust is not quite there yet, and it will take a lot of work to reach that level of debugger support. Furthermore, it is also challenging to ensure that debugging Rust code  working well, across newly released debugger versions, changes to internal representation of Rust data structures in the standard library and other things that can break the debugging experience.</p><p>We already have some <a href=\"https://github.com/rust-lang/google-summer-of-code?tab=readme-ov-file#improve-rust-compiler-debuginfo-test-suite\">plans</a> to start improving debugging support in Rust, but it would also be useful to understand the current debugging struggles of Rust developers. That is why we have prepared the <a href=\"https://www.surveyhero.com/c/rust-debugging-survey-2026\">Rust Debugging Survey</a>, which should help us find specific challenges with debugging Rust code.</p><p><strong>You can fill out the survey <a href=\"https://www.surveyhero.com/c/rust-debugging-survey-2026\">here</a>.</strong></p><p>Filling the survey should take you approximately 5 minutes, and the survey is fully anonymous. We will accept submissions until Friday, March 13th, 2026. After the survey ends, we will evaluate the results and post key insights on this blog.</p><p>We would like to thank Sam Kellam (<a href=\"https://github.com/hashcatHitman\">@hashcatHitman</a>) who did a lot of great work to prepare this survey.</p><p>We invite you to fill the survey, as your responses will help us improve the Rust debugging experience. Thank you!</p>",
      "contentLength": 1875,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Rust participates in Google Summer of Code 2026",
      "url": "https://blog.rust-lang.org/2026/02/19/Rust-participates-in-GSoC-2026/",
      "date": 1771459200,
      "author": "Jakub Beránek, Jack Huey",
      "guid": 46576,
      "unread": true,
      "content": "<p>We are happy to announce that the Rust Project will again be participating in <a href=\"https://summerofcode.withgoogle.com\">Google Summer of Code (GSoC) 2026</a>, same as in the previous two years. If you're not eligible or interested in participating in GSoC, then most of this post likely isn't relevant to you; if you are, this should contain some useful information and links.</p><p>Google Summer of Code (GSoC) is an annual global program organized by Google that aims to bring new contributors to the world of open-source. The program pairs organizations (such as the Rust Project) with contributors (usually students), with the goal of helping the participants make meaningful open-source contributions under the guidance of experienced mentors.</p><p>The organizations that have been accepted into the program have been <a href=\"https://summerofcode.withgoogle.com/programs/2026/organizations\">announced</a> by Google. The GSoC applicants now have several weeks to discuss project ideas with mentors. Later, they will send project proposals for the projects that they found the most interesting. If their project proposal is accepted, they will embark on a several months long journey during which they will try to complete their proposed project under the guidance of an assigned mentor.</p><p>We have prepared a <a href=\"https://github.com/rust-lang/google-summer-of-code\">list of project ideas</a> that can serve as inspiration for potential GSoC contributors that would like to send a project proposal to the Rust organization. However, applicants can also come up with their own project ideas. You can discuss project ideas or try to find mentors in the <a href=\"https://rust-lang.zulipchat.com/#narrow/stream/421156-gsoc\">#gsoc</a> Zulip stream. We have also prepared a <a href=\"https://github.com/rust-lang/google-summer-of-code/blob/main/gsoc/proposal-guide.md\">proposal guide</a> that should help you with preparing your project proposals. We would also like to bring your attention to our <a href=\"https://github.com/rust-lang/google-summer-of-code/tree/main/gsoc\">GSoC AI policy</a>.</p><p>You can start discussing the project ideas with Rust Project mentors and maintainers immediately, but you might want to keep the following important dates in mind:</p><ul><li>The project proposal application period starts on March 16, 2026. From that date you can submit project proposals into the GSoC dashboard.</li><li>The project proposal application period ends on  at 18:00 UTC. Take note of that deadline, as there will be no extensions!</li></ul><p>If you are interested in contributing to the Rust Project, we encourage you to check out our project idea list and send us a GSoC project proposal! Of course, you are also free to discuss these projects and/or try to move them forward even if you do not intend to (or cannot) participate in GSoC. We welcome all contributors to Rust, as there is always enough work to do.</p><p>Our GSoC contributors were quite successful in the past two years (<a href=\"https://blog.rust-lang.org/2024/11/07/gsoc-2024-results.html\">2024</a>, <a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results\">2025</a>), so we are excited what this year's GSoC will bring! We hope that participants in the program can improve their skills, but also would love for this to bring new contributors to the Project and increase the awareness of Rust in general. Like last year, we expect to publish blog posts in the future with updates about our participation in the program.</p>",
      "contentLength": 2861,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Join the Python Security Response Team!",
      "url": "https://pythoninsider.blogspot.com/2026/02/join-the-python-security-response-team.html",
      "date": 1771313400,
      "author": "Seth Michael Larson",
      "guid": 45624,
      "unread": true,
      "content": "<p>And this new onboarding process is already working! The PSF \nInfrastructure Engineer, Jacob Coffee, has just joined the PSRT as the \nfirst new non-\"Release Manager\" member since Seth joined the PSRT in \n2023. We expect new members to join further bolstering the \nsustainability of security work for the Python programming language.</p><p>Thanks to <a href=\"https://alpha-omega.dev\" rel=\"nofollow\">Alpha-Omega</a>\n for their support of Python ecosystem security by sponsoring Seth’s \nwork as the Security Developer-in-Residence at the Python Software \nFoundation.</p><div><h3><strong>What is the Python Security Response Team?</strong></h3><a aria-label=\"Permalink: What is the Python Security Response Team?\" href=\"https://gist.github.com/#what-is-the-python-security-response-team\"></a></div><p>Security doesn't happen by accident: it's thanks to the work of \nvolunteers and paid Python Software Foundation staff on the Python \nSecurity Response Team to triage and coordinate vulnerability reports \nand remediations keeping all Python users safe. Just last year the PSRT \npublished 16 vulnerability advisories for CPython and pip, <em>the most in a single year to date!</em></p><p>And the PSRT usually can’t do this work alone, PSRT coordinators are \nencouraged to involve maintainers and experts on the projects and \nsubmodules. By involving the experts directly in the remediation process\n ensures fixes adhere to existing API conventions and threat-models, are\n maintainable long-term, and have minimal impact on existing use-cases.</p><p>Sometimes the PSRT even coordinates with other open source projects \nto avoid catching the Python ecosystem off-guard by publishing a \nvulnerability advisory that affects multiple other projects. The most \nrecent example of this is PyPI’s <a href=\"https://blog.pypi.org/posts/2025-08-07-wheel-archive-confusion-attacks/\" rel=\"nofollow\">ZIP archive differential attack mitigation</a>.</p><p>This work deserves <a href=\"https://devguide.python.org/developer-workflow/psrt/#members\" rel=\"nofollow\">recognition and celebration</a>\n just like contributions to source code and documentation. Seth and \nJacob are developing further improvements to workflows involving “GitHub\n Security Advisories” to record the reporter, coordinator, and \nremediation developers and reviewers to CVE and OSV records to properly \nthank everyone involved in the otherwise private contribution to open \nsource projects.</p><div><h3><strong>How can I join the Python Security Response Team?</strong></h3><a aria-label=\"Permalink: How can I join the Python Security Response Team?\" href=\"https://gist.github.com/#how-can-i-join-the-python-security-response-team\"></a></div><p>Maybe you’ve read all this and are interested in directly helping the\n Python programming language be more secure! The process is <a href=\"https://devguide.python.org/developer-workflow/psrt/#how-can-i-join-the-psrt\" rel=\"nofollow\">similar to the Core Team nomination process</a>,\n you need an existing PSRT member to nominate you and for your \nnomination to receive at least ⅔ positive votes from existing PSRT \nmembers.</p><p>You do not need to be a core developer, team member, or triager to be\n a member of the Python Security Response Team. Anyone with security \nexpertise that is known and highly-trusted within the Python community \nand has time to volunteer or donate through their employer would make a \ngood candidate for the PSRT. Please note that all PSRT team members <a href=\"https://devguide.python.org/developer-workflow/psrt/#responsibilities-of-psrt-members\" rel=\"nofollow\">have documented responsibilities</a> and are expected to contribute meaningfully to the remediation of vulnerabilities.</p><p>Being a member of the PSRT is not required <a href=\"https://mail.python.org/archives/list/security-announce@python.org/\" rel=\"nofollow\">to be notified of vulnerabilities</a>\n and shouldn’t be to receive “early notification” of vulnerabilities \naffecting CPython and pip. The Python Software Foundation is a <a href=\"https://www.python.org/cve-numbering-authority/\" rel=\"nofollow\">CVE Numbering Authority</a> and publishes CVE and <a href=\"https://github.com/psf/advisory-database/\">OSV</a> records with up-to-date information about vulnerabilities affecting CPython and pip.</p>",
      "contentLength": 3159,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Using go fix to modernize Go code",
      "url": "https://go.dev/blog/gofix",
      "date": 1771286400,
      "author": "Alan Donovan",
      "guid": 45763,
      "unread": true,
      "content": "<p>The 1.26 release of Go this month includes a completely rewritten go fix subcommand. Go fix uses a suite of algorithms to identify opportunities to improve your code, often by taking advantage of more modern features of the language and library. In this post, we’ll first show you how to use  to modernize your Go codebase. Then in the <a href=\"https://go.dev/blog/gofix#go/analysis\">second section</a> we’ll dive into the infrastructure behind it and how it is evolving. Finally, we’ll present the theme of <a href=\"https://go.dev/blog/gofix#self-service\">“self-service”</a> analysis tools to help module maintainers and organizations encode their own guidelines and best practices.</p><p>The  command, like  and , accepts a set of patterns that denote packages. This command fixes all packages beneath the current directory:</p><p>On success, it silently updates your source files. It discards any fix that touches <a href=\"https://pkg.go.dev/cmd/go#hdr-Generate_Go_files_by_processing_source\" rel=\"noreferrer\" target=\"_blank\">generated files</a> since the appropriate fix in that case is to the logic of the generator itself. We recommend running  over your project each time you update your build to a newer Go toolchain release. Since the command may fix hundreds of files, start from a clean git state so that the change consists only of edits from go fix; your code reviewers will thank you.</p><p>To preview the changes the above command would have made, use the  flag:</p><pre><code>$ go fix -diff ./...\n--- dir/file.go (old)\n+++ dir/file.go (new)\n-                       eq := strings.IndexByte(pair, '=')\n-                       result[pair[:eq]] = pair[1+eq:]\n+                       before, after, _ := strings.Cut(pair, \"=\")\n+                       result[before] = after\n…\n</code></pre><p>You can list the available fixers by running this command:</p><pre><code>$ go tool fix help\n…\nRegistered analyzers:\n    any          replace interface{} with any\n    buildtag     check //go:build and // +build directives\n    fmtappendf   replace []byte(fmt.Sprintf) with fmt.Appendf\n    forvar       remove redundant re-declaration of loop variables\n    hostport     check format of addresses passed to net.Dial\n    inline       apply fixes based on 'go:fix inline' comment directives\n    mapsloop     replace explicit loops over maps with calls to maps package\n    minmax       replace if/else statements with calls to min or max\n…\n</code></pre><p>Adding the name of a particular analyzer shows its complete documentation:</p><pre><code>$ go tool fix help forvar\n\nforvar: remove redundant re-declaration of loop variables\n\nThe forvar analyzer removes unnecessary shadowing of loop variables.\nBefore Go 1.22, it was common to write `for _, x := range s { x := x ... }`\nto create a fresh variable for each iteration. Go 1.22 changed the semantics\nof `for` loops, making this pattern redundant. This analyzer removes the\nunnecessary `x := x` statement.\n\nThis fix only applies to `range` loops.\n</code></pre><p>By default, the  command runs all analyzers. When fixing a large project it may reduce the burden of code review if you apply fixes from the most prolific analyzers as separate code changes. To enable only specific analyzers, use the flags matching their names. For example, to run just the  fixer, specify the  flag. Conversely, to run all the analyzers  selected ones, negate the flags, for instance .</p><p>As with  and , each run of the  command analyzes only a specific build configuration. If your project makes heavy use of files tagged for different CPUs or platforms, you may wish to run the command more than once with different values of  and  for better coverage:</p><pre><code>$ GOOS=linux   GOARCH=amd64 go fix ./...\n$ GOOS=darwin  GOARCH=arm64 go fix ./...\n$ GOOS=windows GOARCH=amd64 go fix ./...\n</code></pre><p>Running the command more than once also provides opportunities for synergistic fixes, as we’ll see below.</p><p>The introduction of <a href=\"https://go.dev/blog/intro-generics\">generics</a> in Go 1.18 marked the end of an era of very few changes to the language spec and the start of a period of more rapid—though still careful—change, especially in the libraries. Many of the trivial loops that Go programmers routinely write, such as to gather the keys of a map into a slice, can now be conveniently expressed as a call to a generic function such as <a href=\"https://pkg.go.dev/maps#Keys\" rel=\"noreferrer\" target=\"_blank\"></a>. Consequently these new features create many opportunities to simplify existing code.</p><p>In December 2024, during the frenzied adoption of LLM coding assistants, we became aware that such tools tended—unsurprisingly—to produce Go code in a style similar to the mass of Go code used during training, even when there were newer, better ways to express the same idea. Less obviously, the same tools often refused to use the newer ways even when directed to do so in general terms such as “always use the latest idioms of Go 1.25.” In some cases, even when explicitly told to use a feature, the model would deny that it existed. (See my 2025 GopherCon <a href=\"https://www.youtube.com/watch?v=_VePjjjV9JU&amp;t=3m50s\" rel=\"noreferrer\" target=\"_blank\">talk</a> for more exasperating details.) To ensure that future models are trained on the latest idioms, we need to ensure that these idioms are reflected in the training data, which is to say the global corpus of open-source Go code.</p><p>Over the past year, we have built <a href=\"https://pkg.go.dev/golang.org/x/tools/go/analysis/passes/modernize\" rel=\"noreferrer\" target=\"_blank\">dozens of analyzers</a> to identify opportunities for modernization. Here are three examples of the fixes they suggest:</p><p> replaces an  statement by a use of Go 1.21’s  or  functions:</p><div><pre>x := f()\nif x &lt; 0 {\n    x = 0\n}\nif x &gt; 100 {\n    x = 100\n}\n</pre><pre>x := min(max(f(), 0), 100)\n</pre></div><p> replaces a 3-clause  loop by a Go 1.22 -over-int loop:</p><div><pre>for i := 0; i &lt; n; i++ {\n    f()\n}\n</pre></div><p> (whose  output we saw earlier) replaces uses of  and slicing by Go 1.18’s :</p><div><pre>i := strings.Index(s, \":\")\nif i &gt;= 0 {\n     return s[:i]\n}\n</pre><pre>before, _, ok := strings.Cut(s, \":\")\nif ok {\n    return before\n}\n</pre></div><p>These modernizers are included in <a href=\"https://go.dev/gopls\">gopls</a>, to provide instant feedback as you type, and in , so that you can modernize several entire packages at once in a single command. In addition to making code clearer, modernizers may help Go programmers learn about newer features. As part of the process of approving each new change to the language and standard library, the <a href=\"https://go.googlesource.com/proposal/+/master/README.md\" rel=\"noreferrer\" target=\"_blank\">proposal</a> review group now considers whether it should be accompanied by a modernizer. We expect to add more modernizers with each release.</p><h2>Example: a modernizer for Go 1.26’s new(expr)</h2><p>Go 1.26 includes a small but widely useful change to the language specification. The built-in  function creates a new variable and returns its address. Historically, its sole argument was required to be a type, such as , and the new variable was initialized to its “zero” value, such as . In Go 1.26, the  function may be called with any value, causing it to create a variable initialized to that value, avoiding the need for an additional statement. For example:</p><div><pre>ptr := new(string)\n*ptr = \"go1.25\"\n</pre></div><p>This feature filled a gap that had been discussed for over a decade and resolved one of the most popular <a href=\"https://go.dev/issue/45624\">proposals</a> for a change to the language. It is especially convenient in code that uses a pointer type  to indicate an optional value of type , as is common when working with serialization packages such as <a href=\"https://pkg.go.dev/encoding/json#Marshal\" rel=\"noreferrer\" target=\"_blank\">json.Marshal</a> or <a href=\"https://protobuf.dev/getting-started/gotutorial/\" rel=\"noreferrer\" target=\"_blank\">protocol buffers</a>. This is such a common pattern that people often capture it in a helper, such as the  function below, saving the caller from the need to break out of an expression context to introduce additional statements:</p><pre><code>type RequestJSON struct {\n    URL      string\n    Attempts *int  // (optional)\n}\n\ndata, err := json.Marshal(&amp;RequestJSON{\n    URL:      url,\n    Attempts: newInt(10),\n})\n\nfunc newInt(x int) *int { return &amp;x }\n</code></pre><p>Helpers such as  are so frequently needed with protocol buffers that the  API itself provides them as <a href=\"https://pkg.go.dev/google.golang.org/protobuf/proto#Int64\" rel=\"noreferrer\" target=\"_blank\"></a>, <a href=\"https://pkg.go.dev/google.golang.org/protobuf/proto#String\" rel=\"noreferrer\" target=\"_blank\"></a>, and so on. But Go 1.26 makes all these helpers unnecessary:</p><pre><code>data, err := json.Marshal(&amp;RequestJSON{\n    URL:      url,\n    Attempts: new(10),\n})\n</code></pre><p>To help you take advantage of this feature, the  command now includes a fixer, <a href=\"https://tip.golang.org/src/cmd/vendor/golang.org/x/tools/go/analysis/passes/modernize/newexpr.go\" rel=\"noreferrer\" target=\"_blank\">newexpr</a>, that recognizes “new-like” functions such as  and suggests fixes to replace the function body with  and to replace every call, whether in the same package or an importing package, with a direct use of .</p><p>To avoid introducing premature uses of new features, modernizers offer fixes only in files that require at least the minimum appropriate version of Go (1.26 in this instance), either through a <a href=\"https://go.dev/ref/mod#versions\"> directive</a> in the enclosing go.mod file or a <a href=\"https://pkg.go.dev/cmd/go#hdr-Build_constraints\" rel=\"noreferrer\" target=\"_blank\">build constraint</a> in the file itself.</p><p>Run this command to update all calls of this form in your source tree:</p><p>At this point, with luck, all of your -like helper functions will have become unused and may be safely deleted (assuming they aren’t part of a stable published API). A few calls may remain where it would be unsafe to suggest a fix, such as when the name  is locally shadowed by another declaration. You can also use the <a href=\"https://go.dev/blog/deadcode\">deadcode</a> command to help identify unused functions.</p><p>Applying one modernization may create opportunities to apply another. For example, this snippet of code, which clamps  to the range 0–100, causes the minmax modernizer to suggest a fix to use . Once that fix is applied it suggests a second fix, this time to use .</p><div><pre>x := f()\nif x &lt; 0 {\n    x = 0\n}\nif x &gt; 100 {\n    x = 100\n}\n</pre><pre>x := min(max(f(), 0), 100)\n</pre></div><p>Synergies may also occur between different analyzers. For example, a common mistake is to repeatedly concatenate strings within a loop, resulting in quadratic time complexity—a bug and a potential vector for a denial-of-service attack. The  modernizer recognizes the problem and suggests using Go 1.10’s :</p><div><pre>s := \"\"\nfor _, b := range bytes {\n    s += fmt.Sprintf(\"%02x\", b)\n}\nuse(s)\n</pre><pre>var s strings.Builder\nfor _, b := range bytes {\n    s.WriteString(fmt.Sprintf(\"%02x\", b))\n}\nuse(s.String())\n</pre></div><p>Once this fix is applied, a second analyzer may recognize that the  and  operations can be combined as <code>fmt.Fprintf(&amp;s, \"%02x\", b)</code>, which is both cleaner and more efficient, and offer a second fix. (This second analyzer is <a href=\"https://staticcheck.dev/docs/checks#QF1012\" rel=\"noreferrer\" target=\"_blank\">QF1012</a> from Dominik Honnef’s <a href=\"https://staticcheck.dev/\" rel=\"noreferrer\" target=\"_blank\">staticcheck</a>, which is already enabled in gopls but not yet in , though we <a href=\"https://go.dev/issue/76918\">plan</a> to add staticcheck analyzers to the go command starting in Go 1.27.)</p><p>Consequently, it may be worth running  more than once until it reaches a fixed point; twice is usually enough.</p><h3>Merging fixes and conflicts</h3><p>A single run of  may apply dozens of fixes within the same source file. All fixes are conceptually independent, analogous to a set of git commits with the same parent. The  command uses a simple three-way merge algorithm to reconcile the fixes in sequence, analogous to the task of merging a set of git commits that edit the same file. If a fix conflicts with the list of edits accumulated so far, it is discarded, and the tool issues a warning that some fixes were skipped and that the tool should be run again.</p><p>This reliably detects  conflicts arising from overlapping edits, but another class of conflict is possible: a  conflict occurs when two changes are textually independent but their meanings are incompatible. As an example consider two fixes that each remove the second-to-last use of a local variable: each fix is fine by itself, but when both are applied together the local variable becomes unused, and in Go that’s a compilation error. Neither fix is responsible for removing the variable declaration, but someone has to do it, and that someone is the user of .</p><p>A similar semantic conflict arises when a set of fixes causes an import to become unused. Because this case is so common, the  command applies a final pass to detect unused imports and remove them automatically.</p><p>Semantic conflicts are relatively rare. Fortunately they usually reveal themselves as compilation errors, making them impossible to overlook. Unfortunately, when they happen, they do demand some manual work after running .</p><p>Let’s now delve into the infrastructure beneath these tools.</p><h2>The Go analysis framework</h2><p>Since the earliest days of Go, the  command has had two subcommands for static analysis,  and , each with its own suite of algorithms: “checkers” and “fixers”. A checker reports likely mistakes in your code, such as passing a string instead of an integer as the operand of a  conversion. A fixer safely edits your code to fix a bug or to express the same thing in a better way, perhaps more clearly, concisely, or efficiently. Sometimes the same algorithm appears in both suites when it can both report a mistake and safely fix it.</p><p>In 2017 we redesigned the then-monolithic  program to separate the checker algorithms (now called “analyzers”) from the “driver”, the program that runs them; the result was the <a href=\"https://pkg.go.dev/golang.org/x/tools/go/analysis\" rel=\"noreferrer\" target=\"_blank\">Go analysis framework</a>. This separation enables an analyzer to be written once then run in a diverse range of drivers for different environments, such as:</p><ul><li><a href=\"https://pkg.go.dev/golang.org/x/tools/go/analysis/unitchecker\" rel=\"noreferrer\" target=\"_blank\">unitchecker</a>, which turns a suite of analyzers into a subcommand that can be run by the go command’s scalable incremental build system, analogous to a compiler in go build. This is the basis of  and .</li><li><a href=\"https://github.com/bazel-contrib/rules_go/blob/master/go/nogo.rst\" rel=\"noreferrer\" target=\"_blank\">nogo</a>, the analogous driver for alternative build systems such as Bazel and Blaze.</li><li><a href=\"https://pkg.go.dev/golang.org/x/tools/go/analysis/singlechecker\" rel=\"noreferrer\" target=\"_blank\">singlechecker</a>, which turns an analyzer into a standalone command that loads, parses, and type-checks a set of packages (perhaps a whole program) and then analyzes them. We often use it for ad hoc experiments and measurements over the module mirror (<a href=\"https://proxy.golang.org/\" rel=\"noreferrer\" target=\"_blank\">proxy.golang.org</a>) corpus.</li><li><a href=\"https://pkg.go.dev/golang.org/x/tools/go/analysis/multichecker\" rel=\"noreferrer\" target=\"_blank\">multichecker</a>, which does the same thing for a suite of analyzers with a ‘swiss-army knife’ CLI.</li><li><a href=\"https://go.dev/gopls\">gopls</a>, the <a href=\"https://microsoft.github.io/language-server-protocol/\" rel=\"noreferrer\" target=\"_blank\">language server</a> behind VS Code and other editors, which provides real-time diagnostics from analyzers after each editor keystroke.</li><li>the highly configurable driver used by the <a href=\"https://staticcheck.dev/\" rel=\"noreferrer\" target=\"_blank\">staticcheck</a> tool. (Staticcheck also provides a large suite of analyzers that can be run in other drivers.)</li><li><a href=\"https://research.google/pubs/tricorder-building-a-program-analysis-ecosystem/\" rel=\"noreferrer\" target=\"_blank\">Tricorder</a>, the batch static analysis pipeline used by Google’s monorepo and integrated with its code review system.</li><li>gopls’ <a href=\"https://go.dev/gopls/features/mcp\">MCP server</a>, which makes diagnostics available to LLM-based coding agents, providing more robust “guardrails”.</li></ul><p>One benefit of the framework is its ability to express helper analyzers that don’t report diagnostics or suggest fixes of their own but instead compute some intermediate data structure that may be useful to many other analyzers, amortizing the costs of its construction. Examples include <a href=\"https://pkg.go.dev/golang.org/x/tools/go/analysis/passes/ctrlflow\" rel=\"noreferrer\" target=\"_blank\">control-flow graphs</a>, the <a href=\"https://pkg.go.dev/golang.org/x/tools/go/analysis/passes/buildssa\" rel=\"noreferrer\" target=\"_blank\">SSA representation</a> of function bodies, and data structures for <a href=\"https://pkg.go.dev/golang.org/x/tools/go/analysis/passes/inspect\" rel=\"noreferrer\" target=\"_blank\">optimized AST navigation</a>.</p><p>Another benefit of the framework is its support for making deductions across packages. An analyzer can attach a “<a href=\"https://pkg.go.dev/golang.org/x/tools/go/analysis#hdr-Modular_analysis_with_Facts\" rel=\"noreferrer\" target=\"_blank\">fact</a>” to a function or other symbol so that information learned while analyzing the function’s body can be used when later analyzing a call to the function, even if the call appears in another package or the later analysis occurs in a different process. This makes it easy to define scalable interprocedural analyses. For example, the printf checker can tell when a function such as  is really just a wrapper around , so it knows that calls to  should be checked in a similar manner. This process works by induction, so the tool will also check calls to further wrappers around , and so on. An example of an analyzer that makes heavy use of facts is <a href=\"https://github.com/uber-go/nilaway\" rel=\"noreferrer\" target=\"_blank\">Uber’s nilaway</a>, which reports potential mistakes resulting in nil pointer dereferences.</p><img src=\"https://go.dev/blog/gofix-analysis-facts.svg\"><p>The process of “separate analysis” in   is analogous to the process of separate compilation in . Just as the compiler builds packages starting from the bottom of the dependency graph and passing type information up to importing packages, the analysis framework works from the bottom of the dependency graph up, passing facts (and types) up to importing packages.</p><p>In 2019, as we started developing <a href=\"https://go.dev/gopls\">gopls</a>, the language server for Go, we added the ability for an analyzer to suggest a <a href=\"https://pkg.go.dev/golang.org/x/tools/go/analysis#SuggestedFix\" rel=\"noreferrer\" target=\"_blank\">fix</a> when reporting a diagnostic. The printf analyzer, for example, offers to replace  with  to avoid misformatting should the dynamic  value contain a  symbol. This mechanism has become the basis for many of the quick fixes and refactoring features of gopls.</p><p>While all these developments were happening to ,  remained stuck as it was back before the <a href=\"https://go.dev/doc/go1compat\">Go compatibility promise</a>, when early adopters of Go used it to maintain their code during the rapid and sometimes incompatible evolution of the language and libraries.</p><p>The Go 1.26 release brings the Go analysis framework to . The  and  commands have converged and are now almost identical in implementation. The only differences between them are the criteria for the suites of algorithms they use, and what they do with computed diagnostics. Go <a href=\"https://cs.opensource.google/go/go/+/refs/tags/go1.26rc1:src/cmd/vet/main.go;l=62\" rel=\"noreferrer\" target=\"_blank\">vet analyzers</a> must detect likely mistakes with low false positives; their diagnostics are reported to the user. Go <a href=\"https://cs.opensource.google/go/go/+/refs/tags/go1.26rc1:src/cmd/fix/main.go;l=46\" rel=\"noreferrer\" target=\"_blank\">fix analyzers</a> must generate fixes that are safe to apply without regression in correctness, performance, or style; their diagnostics may not be reported, but the fixes are directly applied. Aside from this difference of emphasis, the task of developing a fixer is no different from that of developing a checker.</p><h3>Improving analysis infrastructure</h3><p>As the number of analyzers in  and  continues to grow, we have been investing in infrastructure both to improve the performance of each analyzer and to make it easier to write each new analyzer.</p><p>For example, most analyzers start by traversing the syntax trees of each file in the package looking for a particular kind of node such as a range statement or function literal. The existing <a href=\"https://pkg.go.dev/golang.org/x/tools/go/ast/inspector\" rel=\"noreferrer\" target=\"_blank\">inspector</a> package makes this scan efficient by pre-computing a compact index of a complete traversal so that later traversals can quickly skip subtrees that don’t contain any nodes of interest. Recently we extended it with the <a href=\"https://pkg.go.dev/golang.org/x/tools/go/ast/inspector#Cursor\" rel=\"noreferrer\" target=\"_blank\">Cursor</a> datatype to allow flexible and efficient navigation between nodes in all four cardinal directions—up, down, left, and right, similar to navigating the elements of an HTML DOM—making it easy and efficient to express a query such as “find each go statement that is the first statement of a loop body”:</p><pre><code>    var curFile inspector.Cursor = ...\n\n    // Find each go statement that is the first statement of a loop body.\n    for curGo := range curFile.Preorder((*ast.GoStmt)(nil)) {\n        kind, index := curGo.ParentEdge()\n        if kind == edge.BlockStmt_List &amp;&amp; index == 0 {\n            switch curGo.Parent().ParentEdgeKind() {\n            case edge.ForStmt_Body, edge.RangeStmt_Body:\n                ...\n            }\n        }\n    }\n</code></pre><p>Many analyzers start by searching for calls to a specific function, such as . Function calls are among the most numerous expressions in Go code, so rather than search every call expression and test whether it is a call to , it is much more efficient to pre-compute an index of symbol references, which is done by <a href=\"https://pkg.go.dev/golang.org/x/tools/internal/typesinternal/typeindex\" rel=\"noreferrer\" target=\"_blank\">typeindex</a> and its <a href=\"https://pkg.go.dev/golang.org/x/tools@v0.41.0/internal/analysis/typeindex\" rel=\"noreferrer\" target=\"_blank\">helper</a> analyzer. Then the calls to  can be enumerated directly, making the cost proportional to the number of calls instead of to the size of the package. For an analyzer such as <a href=\"https://pkg.go.dev/golang.org/x/tools/go/analysis/passes/hostport\" rel=\"noreferrer\" target=\"_blank\">hostport</a> that seeks an infrequently used symbol (), this can easily make it <a href=\"https://go.dev/cl/657958\">1,000× faster</a>.</p><p>Some other infrastructural improvements over the past year include:</p><ul><li>a <strong>dependency graph of the standard library</strong> that analyzers can consult to avoid introducing import cycles. For example, we can’t introduce a call to  in a package that is itself imported by .</li><li>support for <strong>querying the effective Go version</strong> of a file as determined by the enclosing go.mod file and build tags, so that analyzers don’t insert uses of features that are “too new”.</li><li>a richer <strong>library of refactoring primitives</strong> (e.g. “delete this statement”) that correctly handle adjacent comments and other tricky edge cases.</li></ul><p>We have come a long way, but there remains much to do. Fixer logic can be tricky to get right. Since we expect users to apply hundreds of suggested fixes with only cursory review, it’s critical that fixers are correct even in obscure edge cases. As just one example (see my GopherCon <a href=\"https://www.youtube.com/watch?v=_VePjjjV9JU&amp;t=13m17s\" rel=\"noreferrer\" target=\"_blank\">talk</a> for several more), we built a modernizer that replaces calls such as <code>append([]string{}, slice...)</code> by the clearer  only to discover that, when  is empty, the result of Clone is nil, a subtle behavior change that in rare cases can cause bugs; so we had to exclude <a href=\"https://pkg.go.dev/golang.org/x/tools/go/analysis/passes/modernize#hdr-Analyzer_appendclipped\" rel=\"noreferrer\" target=\"_blank\">that modernizer</a> from the  suite.</p><p>Some of these difficulties for authors of analyzers can be ameliorated with better documentation (both for humans and LLMs), particularly checklists of surprising edge cases to consider and test. A pattern-matching engine for syntax trees, similar to those in <a href=\"https://pkg.go.dev/honnef.co/go/tools/pattern\" rel=\"noreferrer\" target=\"_blank\">staticcheck</a> and <a href=\"https://tree-sitter.github.io/tree-sitter/using-parsers/queries/index.html\" rel=\"noreferrer\" target=\"_blank\">Tree Sitter</a>, could simplify the fiddly task of efficiently identifying the locations that need fixing. A richer library of operators for computing accurate fixes would help avoid common mistakes. A better test harness would let us check that fixes don’t break the build, and preserve dynamic properties of the target code. These are all on our roadmap.</p><h2>The “self-service” paradigm</h2><p>More fundamentally, we are turning our attention in 2026 to a “self-service” paradigm.</p><p>The  analyzer we saw earlier is a typical modernizer: a bespoke algorithm tailored to a particular feature. The bespoke model works well for features of the language and standard library, but it doesn’t really help update uses of third-party packages. Although there’s nothing to stop you from writing a modernizer for your own public APIs and running it on your own project, there’s no automatic way to get users of your API to run it too. Your modernizer probably wouldn’t belong in gopls or the  suite unless your API is particularly widely used across the Go ecosystem. Even in that case you would have to obtain code reviews and approvals and then wait for the next release.</p><p>Under the self-service paradigm, Go programmers would be able to define modernizations for their own APIs that their users can apply without all the bottlenecks of the current centralized paradigm. This is especially important as the Go community and global Go corpus are growing much faster than the ability of our team to review analyzer contributions.</p><p>The  command in Go 1.26 includes a preview of the first fruits of this new paradigm: the <strong>annotation-driven source-level inliner</strong>, which we’ll describe in an upcoming companion blog post next week. In the coming year, we plan to investigate two more approaches within this paradigm.</p><p>First, we will be exploring the possibility of <a href=\"https://go.dev/issue/59869\">dynamically loading</a> modernizers from the source tree and securely executing them, either in gopls or . In this approach a package that provides an API for, say, a SQL database could additionally provide a checker for misuses of the API, such as SQL injection vulnerabilities or failure to handle critical errors. The same mechanism could be used by project maintainers to encode internal housekeeping rules, such as avoiding calls to certain problematic functions or enforcing stronger coding disciplines in critical parts of the code.</p><p>Second, many existing checkers can be informally described as “don’t forget to X after you Y!”, such as “close the file after you open it”, “cancel the context after you create it”, “unlock the mutex after you lock it”, “break out of the iterator loop after yield returns false”, and so on. What such checkers have in common is that they enforce certain invariants on all execution paths. We plan to explore generalizations and unifications of these control-flow checkers so that Go programmers can easily apply them to new domains, without complex analytical logic, simply by annotating their own code.</p><p>We hope that these new tools will save you effort during maintenance of your Go projects and help you learn about and benefit from newer features sooner. Please try out  on your projects and <a href=\"https://go.dev/issue/new\">report</a> any problems you find, and do share any ideas you have for new modernizers, fixers, checkers, or self-service approaches to static analysis.</p>",
      "contentLength": 23322,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "crates.io: an update to the malicious crate notification policy",
      "url": "https://blog.rust-lang.org/2026/02/13/crates.io-malicious-crate-update/",
      "date": 1770940800,
      "author": "Adam Harvey",
      "guid": 44794,
      "unread": true,
      "content": "<p>The crates.io team will no longer publish a blog post each time a malicious crate is detected or reported. In the vast majority of cases to date, these notifications have involved crates that have no evidence of real world usage, and we feel that publishing these blog posts is generating noise, rather than signal.</p><p>Crates that contain malware  are seeing real usage or exploitation will still get both a blog post and a RustSec advisory. We may also notify via additional communication channels (such as social media) if we feel it is warranted.</p><p>Since we are announcing this policy change now, here is a retrospective summary of the malicious crates removed since <a href=\"https://blog.rust-lang.org/2025/12/05/crates.io-malicious-crates-finch-rust-and-sha-rust/\">our last blog post</a> and today:</p><ul><li>: we were notified on February 6th by <a href=\"https://socket.dev/\">Socket</a> that this crate was attempting to exfiltrate credentials by impersonating the  crate. Advisory: <a href=\"https://rustsec.org/advisories/RUSTSEC-2026-0010.html\">RUSTSEC-2026-0010</a>.</li><li>: we were notified on February 13th that this crate was attempting to exfiltrate credentials by impersonating the  crate. Advisory: <a href=\"https://rustsec.org/advisories/RUSTSEC-2026-0011.html\">RUSTSEC-2026-0011</a>.</li></ul><p>In all cases, the crates were deleted, the user accounts that published them were immediately disabled, and reports were made to upstream providers as appropriate.</p><p>Once again, our thanks go to Matthias, Socket, and the reporter of  for their reports. We also want to thank Dirkjan Ochtman from the secure code working group, Emily Albini from the security response working group, and Walter Pearce from the <a href=\"https://foundation.rust-lang.org/\">Rust Foundation</a> for aiding in the response.</p>",
      "contentLength": 1450,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Announcing Rust 1.93.1",
      "url": "https://blog.rust-lang.org/2026/02/12/Rust-1.93.1/",
      "date": 1770854400,
      "author": "The Rust Release Team",
      "guid": 44504,
      "unread": true,
      "content": "<p>The Rust team has published a new point release of Rust, 1.93.1. Rust is a programming language that is empowering everyone to build reliable and efficient software.</p><p>If you have a previous version of Rust installed via rustup, getting Rust 1.93.1 is as easy as:</p><p>If you don't have it already, you can <a href=\"https://www.rust-lang.org/install.html\">get </a> from the appropriate page on our website.</p><p>Rust 1.93.1 resolves three regressions that were introduced in the 1.93.0 release.</p><p>Many people came together to create Rust 1.93.1. We couldn't have done it without all of you. <a href=\"https://thanks.rust-lang.org/rust/1.93.1/\">Thanks!</a></p>",
      "contentLength": 527,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Spotlight on SIG Architecture: API Governance",
      "url": "https://kubernetes.io/blog/2026/02/12/sig-architecture-api-spotlight/",
      "date": 1770854400,
      "author": "",
      "guid": 44251,
      "unread": true,
      "content": "<p><em>This is the fifth interview of a SIG Architecture Spotlight series that covers the different\nsubprojects, and we will be covering <a href=\"https://github.com/kubernetes/community/blob/master/sig-architecture/README.md#architecture-and-api-governance-1\">SIG Architecture: API\nGovernance</a>.</em></p><p>In this SIG Architecture spotlight we talked with <a href=\"https://github.com/liggitt\">Jordan Liggitt</a>, lead\nof the API Governance sub-project.</p><p><strong>FM: Hello Jordan, thank you for your availability. Tell us a bit about yourself, your role and how\nyou got involved in Kubernetes.</strong></p><p>: My name is Jordan Liggitt. I'm a Christian, husband, father of four, software engineer at\n<a href=\"https://about.google/\">Google</a> by day, and <a href=\"https://www.youtube.com/watch?v=UDdr-VIWQwo\">amateur musician</a> by stealth. I was born in Texas (and still\nlike to claim it as my point of origin), but I've lived in North Carolina for most of my life.</p><p>I've been working on Kubernetes since 2014. At that time, I was working on authentication and\nauthorization at Red Hat, and my very first pull request to Kubernetes attempted to <a href=\"https://github.com/kubernetes/kubernetes/pull/2328\">add an OAuth\nserver</a> to the Kubernetes API server. It never\nexited work-in-progress status. I ended up going with a different approach that layered on top of\nthe core Kubernetes API server in a different project (spoiler alert: this is foreshadowing), and I\nclosed it without merging six months later.</p><p>Undeterred by that start, I stayed involved, helped build Kubernetes authentication and\nauthorization capabilities, and got involved in the definition and evolution of the core Kubernetes\nAPIs from early beta APIs, like  to . I got tagged as an API reviewer in 2016 based on\nthose contributions, and was added as an API approver in 2017.</p><p>Today, I help lead the API Governance and code organization subprojects for SIG Architecture, and I\nam a tech lead for SIG Auth.</p><p><strong>FM: And when did you get specifically involved in the API Governance project?</strong></p><h2>Goals and scope of API Governance</h2><p><strong>FM: How would you describe the main goals and areas of intervention of the subproject?</strong></p><p>The surface area includes all the various APIs Kubernetes has, and there are APIs that people do not\nalways realize are APIs: command-line flags, configuration files, how binaries are run, how they\ntalk to back-end components like the container runtime, and how they persist data. People often\nthink of \"the API\" as only the <a href=\"https://kubernetes.io/docs/reference/using-api/\">REST API</a>... that\nis the biggest and most obvious one, and the one with the largest audience, but all of these other\nsurfaces are also APIs. Their audiences are narrower, so there is more flexibility there, but they\nstill require consideration.</p><p>The goals are to be stable while still enabling innovation. Stability is easy if you never change\nanything, but that contradicts the goal of evolution and growth. So we balance \"be stable\" with\n\"allow change\".</p><p><strong>FM: Speaking of changes, in terms of ensuring consistency and quality (which is clearly one of the\nreasons this project exists), what are the specific quality gates in the lifecycle of a Kubernetes\nchange? Does API Governance get involved during the release cycle, prior to it through guidelines,\nor somewhere in between? At what points do you ensure the intended role is fulfilled?</strong></p><p>: We have <a href=\"https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md\">guidelines and\nconventions</a>,\nboth for APIs in general and for how to change an API. These are living documents that we update as\nwe encounter new scenarios. They are long and dense, so we also support them with involvement at\neither the design stage or the implementation stage.</p><p>Sometimes, due to bandwidth constraints, teams move ahead with design work without feedback from <a href=\"https://github.com/kubernetes/community/blob/master/sig-architecture/api-review-process.md\">API Review</a>. That’s fine, but it means that when implementation begins, the API review will happen then,\nand there may be substantial feedback. So we get involved when a new API is created or an existing\nAPI is changed, either at design or implementation.</p><p><strong>FM: Is this during the Kubernetes Enhancement Proposal (KEP) process? Since KEPs are mandatory for\nenhancements, I assume part of the work intersects with API Governance?</strong></p><p>: It can. <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/README.md\">KEPs</a> vary\nin how detailed they are. Some include literal API definitions. When they do, we can perform an API\nreview at the design stage. Then implementation becomes a matter of checking fidelity to the design.</p><p>Getting involved early is ideal. But some KEPs are conceptual and leave details to the\nimplementation. That’s not wrong; it just means the implementation will be more exploratory. Then\nAPI Review gets involved later, possibly recommending structural changes.</p><p>There’s a trade-off regardless: detailed design upfront versus iterative discovery during\nimplementation. People and teams work differently, and we’re flexible and happy to consult early or\nat implementation time.</p><p><strong>FM: This reminds me of what Fred Brooks wrote in \"The Mythical Man-Month\" about conceptual\nintegrity being central to product quality... No matter how you structure the process, there must be\na point where someone looks at what is coming and ensures conceptual integrity. Kubernetes uses APIs\neverywhere -- externally and internally -- so API Governance is critical to maintaining that\nintegrity. How is this captured?</strong></p><p>: Yes, the conventions document captures patterns we’ve learned over time: what to do in\nvarious situations. We also have automated linters and checks to ensure correctness around patterns\nlike spec/status semantics. These automated tools help catch issues even when humans miss them.</p><p>As new scenarios arise -- and they do constantly -- we think through how to approach them and fold\nthe results back into our documentation and tools. Sometimes it takes a few attempts before we\nsettle on an approach that works well.</p><p><strong>FM: Exactly. Each new interaction improves the guidelines.</strong></p><p>: Right. And sometimes the first approach turns out to be wrong. It may take two or three\niterations before we land on something robust.</p><h2>The impact of Custom Resource Definitions</h2><p><strong>FM: Is there any particular change, episode, or domain that stands out as especially noteworthy,\ncomplex, or interesting in your experience?</strong></p><p>: The watershed moment was <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">Custom Resources</a>.\nPrior to that, every API was handcrafted by us and fully reviewed. There were inconsistencies, but\nwe understood and controlled every type and field.</p><p>When Custom Resources arrived, anyone could define anything. The first version did not even require\na schema. That made it extremely powerful -- it enabled change immediately -- but it left us playing\ncatch-up on stability and consistency.</p><p>When Custom Resources graduated to General Availability (GA), schemas became required, but escape\nhatches still existed for backward compatibility. Since then, we’ve been working on giving CRD\nauthors validation capabilities comparable to built-ins. Built-in validation rules for CRDs have\nonly just reached GA in the last few releases.</p><p>So CRDs opened the \"anything is possible\" era. Built-in validation rules are the second major\nmilestone: bringing consistency back.</p><p>The three major themes have been defining schemas, validating data, and handling pre-existing\ninvalid data. With ratcheting validation (allowing data to improve without breaking existing\nobjects), we can now guide CRD authors toward conventions without breaking the world.</p><h2>API Governance in context</h2><p><strong>FM: How does API Governance relate to SIG Architecture and API Machinery?</strong></p><p>: <a href=\"https://github.com/kubernetes/apimachinery\">API Machinery</a> provides the actual code and\ntools that people build APIs on. They don’t review APIs for storage, networking, scheduling, etc.</p><p>SIG Architecture sets the overall system direction and works with API Machinery to ensure the system\nsupports that direction. API Governance works with other SIGs building on that foundation to define\nconventions and patterns, ensuring consistent use of what API Machinery provides.</p><p><strong>FM: Thank you. That clarifies the flow. Going back to <a href=\"https://kubernetes.io/releases/release/\">release cycles</a>: do release phases -- enhancements freeze, code\nfreeze -- change your workload? Or is API Governance mostly continuous?</strong></p><p>: We get involved in two places: design and implementation. Design involvement increases\nbefore enhancements freeze; implementation involvement increases before code freeze. However, many\nefforts span multiple releases, so there is always some design and implementation happening, even\nfor work targeting future releases. Between those intense periods, we often have time to work on\nlong-term design work.</p><p>An anti-pattern we see is teams thinking about a large feature for months and then presenting it\nthree weeks before enhancements freeze, saying, \"Here is the design, please review.\" For big changes\nwith API impact, it’s much better to involve API Governance early.</p><p>And there are good times in the cycle for this -- between freezes -- when people have bandwidth.\nThat’s when long-term review work fits best.</p><p><strong>FM: Clearly. Now, regarding team dynamics and new contributors: how can someone get involved in\nAPI Governance? What should they focus on?</strong></p><p>: It’s usually best to follow a specific change rather than trying to learn everything at\nonce. Pick a small API change, perhaps one someone else is making or one you want to make, and\nobserve the full process: design, implementation, review.</p><p>High-bandwidth review -- live discussion over video -- is often very effective. If you’re making or\nfollowing a change, ask whether there’s a time to go over the design or PR together. Observing those\ndiscussions is extremely instructive.</p><p>Start with a small change. Then move to a bigger one. Then maybe a new API. That builds\nunderstanding of conventions as they are applied in practice.</p><p><strong>FM: Excellent. Any final comments, or anything we missed?</strong></p><p>: Yes... the reason we care so much about compatibility and stability is for our users. It’s\neasy for contributors to see those requirements as painful obstacles preventing cleanup or requiring\ntedious work... but users integrated with our system, and we made a promise to them: we want them to\ntrust that we won’t break that contract. So even when it requires more work, moves slower, or\ninvolves duplication, we choose stability.</p><p>We are not trying to be obstructive; we are trying to make life good for users.</p><p>A lot of our questions focus on the future: you want to do something now... how will you evolve it\nlater without breaking it? We assume we will know more in the future, and we want the design to\nleave room for that.</p><p>We also assume we will make mistakes. The question then is: how do we leave ourselves avenues to\nimprove while keeping compatibility promises?</p><p><strong>FM: Exactly. Jordan, thank you, I think we’ve covered everything. This has been an insightful view\ninto the API Governance project and its role in the wider Kubernetes project.</strong></p>",
      "contentLength": 10358,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Python 3.15.0 alpha 6",
      "url": "https://pythoninsider.blogspot.com/2026/02/python-3150-alpha-6.html",
      "date": 1770824520,
      "author": "Hugo",
      "guid": 44041,
      "unread": true,
      "content": "<p><strong>This is an early developer preview of Python\n3.15</strong></p><p>Python 3.15 is still in development. This release, 3.15.0a6, is the\nsixth of eight planned alpha releases.</p><p>Alpha releases are intended to make it easier to test the current\nstate of new features and bug fixes and to test the release process.</p><p>During the alpha phase, features may be added up until the start of\nthe beta phase (2026-05-05) and, if necessary, may be modified or\ndeleted up until the release candidate phase (2026-07-28). Please keep\nin mind that this is a preview release and its use is\n recommended for production environments.</p><p>Many new features for Python 3.15 are still being planned and\nwritten. Among the new major new features and changes so far:</p><ul><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-sampling-profiler\">PEP\n799</a>: A new high-frequency, low-overhead, statistical sampling\nprofiler and dedicated profiling package</li><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-unpacking-in-comprehensions\">PEP\n798</a>: Unpacking in comprehensions with  and\n</li><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-utf8-default\">PEP\n686</a>: Python now uses UTF-8 as the default encoding</li><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-pep782\">PEP\n782</a>: A new  C API to create a Python bytes\nobject</li><li><a href=\"https://peps.python.org/pep-0728/\">PEP 728</a>:\n with typed extra items</li><li>The <a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-jit\">JIT\ncompiler</a> has been significantly upgraded, with 3-4% geometric mean\nperformance improvement on x86-64 Linux over the standard interpreter,\nand 7-8% speedup on AArch64 macOS over the tail-calling interpreter</li><li><small>(Hey,  if a feature\nyou find important is missing from this list, let Hugo\nknow.)</small></li></ul><p>The next pre-release of Python 3.15 will be 3.15.0a7, currently\nscheduled for 2026-03-10.</p><blockquote><p>By reason of these things, then, the whaling voyage was welcome; the\ngreat flood-gates of the wonder-world swung open, and in the wild\nconceits that swayed me to my purpose, two and two there floated into my\ninmost soul, endless processions of the whale, and, mid most of them\nall, one grand hooded phantom, like a snow hill in the air.</p></blockquote><p>Thanks to all of the many volunteers who help make Python development\nand these releases possible! Please consider supporting our efforts by\nvolunteering yourself or through organisation contributions to the <a href=\"https://www.python.org/psf/donations/\">Python Software\nFoundation</a>.</p><p>Regards as the snow slowly falls in Helsinki,</p><p>Your release team,\n  Hugo van Kemenade\n  Steve Dower\n  </p>",
      "contentLength": 2058,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Go 1.26 is released",
      "url": "https://go.dev/blog/go1.26",
      "date": 1770681600,
      "author": "Carlos Amedee, on behalf of the Go team",
      "guid": 43794,
      "unread": true,
      "content": "<p>Today the Go team is pleased to release Go 1.26.\nYou can find its binary archives and installers on the <a href=\"https://go.dev/dl/\">download page</a>.</p><p>First, the built-in  function, which creates a new variable, now allows its operand to be an\nexpression, specifying the initial value of the variable.</p><p>A simple example of this change means that code such as this:</p><pre><code>x := int64(300)\nptr := &amp;x\n</code></pre><p>Second, generic types may now refer to themselves in their own type parameter list. This change\nsimplifies the implementation of complex data structures and interfaces.</p><p>The  command has been completely rewritten to use the\n<a href=\"https://go.dev/pkg/golang.org/x/tools/go/analysis\">Go analysis framework</a>, and now includes a\ncouple dozen “<a href=\"https://go.dev/pkg/golang.org/x/tools/go/analysis/passes/modernize\">modernizers</a>”, analyzers\nthat suggest safe fixes to help your code take advantage of newer features of the language\nand standard library. It also includes the\n<a href=\"https://go.dev/pkg/golang.org/x/tools/go/analysis/passes/inline#hdr-Analyzer_inline\"> analyzer</a>, which\nattempts to inline all calls to each function annotated with a  directive.\nTwo upcoming blog posts will address these features in more detail.</p><h2>More improvements and changes</h2><p>Some of the additions in Go 1.26 are in an experimental stage\nand become exposed only when you explicitly opt in. Notably:</p><p>These experiments are all expected to be generally available in a\nfuture version of Go. We encourage you to try them out ahead of time.\nWe really value your feedback!</p><p>Please refer to the <a href=\"https://go.dev/doc/go1.26\">Go 1.26 Release Notes</a> for the complete list\nof additions, changes, and improvements in Go 1.26.</p><p>Over the next few weeks, follow-up blog posts will cover some of the topics\nrelevant to Go 1.26 in more detail. Check back later to read those posts.</p><p>Thanks to everyone who contributed to this release by writing code, filing bugs,\ntrying out experimental additions, sharing feedback, and testing the release candidates.\nYour efforts helped make Go 1.26 as stable as possible.\nAs always, if you notice any problems, please <a href=\"https://go.dev/issue/new\">file an issue</a>.</p><p>We hope you enjoy using the new release!</p>",
      "contentLength": 1861,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    }
  ],
  "tags": [
    "dev"
  ]
}
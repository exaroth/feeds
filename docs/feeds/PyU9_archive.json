{
  "id": "PyU9",
  "title": "Dev",
  "displayTitle": "Dev",
  "url": "",
  "feedLink": "",
  "isQuery": true,
  "isEmpty": false,
  "isHidden": false,
  "itemCount": 8,
  "items": [
    {
      "title": "The Green Tea Garbage Collector",
      "url": "https://go.dev/blog/greenteagc",
      "date": 1761696000,
      "author": "Michael Knyszek and Austin Clements",
      "guid": 29623,
      "unread": true,
      "content": "<p>Go 1.25 includes a new experimental garbage collector called Green Tea,\navailable by setting  at build time.\nMany workloads spend around 10% less time in the garbage collector, but some\nworkloads see a reduction of up to 40%!</p><p>It’s production-ready and already in use at Google, so we encourage you to\ntry it out.\nWe know some workloads don’t benefit as much, or even at all, so your feedback\nis crucial to helping us move forward.\nBased on the data we have now, we plan to make it the default in Go 1.26.</p><p>What follows is a blog post based on Michael Knyszek’s GopherCon 2025 talk.\nWe’ll update this blog post with a link to the talk once it’s available online.</p><h2>Tracing garbage collection</h2><p>Before we discuss Green Tea let’s get us all on the same page about garbage\ncollection.</p><p>The purpose of garbage collection is to automatically reclaim and reuse memory\nno longer used by the program.</p><p>To this end, the Go garbage collector concerns itself with  and\n.</p><p>In the context of the Go runtime,  are Go values whose underlying\nmemory is allocated from the heap.\nHeap objects are created when the Go compiler can’t figure out how else to allocate\nmemory for a value.\nFor example, the following code snippet allocates a single heap object: the backing\nstore for a slice of pointers.</p><pre><code>var x = make([]*int, 10) // global\n</code></pre><p>The Go compiler can’t allocate the slice backing store anywhere except the heap,\nsince it’s very hard, and maybe even impossible, for it to know how long  will\nrefer to the object for.</p><p> are just numbers that indicate the location of a Go value in memory,\nand they’re how a Go program references objects.\nFor example, to get the pointer to the beginning of the object allocated in the\nlast code snippet, we can write:</p><p>Go’s garbage collector follows a strategy broadly referred to as <em>tracing garbage\ncollection</em>, which just means that the garbage collector follows, or traces, the\npointers in the program to identify which objects the program is still using.</p><p>More specifically, the Go garbage collector implements the mark-sweep algorithm.\nThis is much simpler than it sounds.\nImagine objects and pointers as a sort of graph, in the computer science sense.\nObjects are nodes, pointers are edges.</p><p>The mark-sweep algorithm operates on this graph, and as the name might suggest,\nproceeds in two phases.</p><p>In the first phase, the mark phase, it walks the object graph from well-defined\nsource edges called .\nThink global and local variables.\nThen, it  everything it finds along the way as , to avoid going in\ncircles.\nThis is analogous to your typical graph flood algorithm, like a depth-first or\nbreadth-first search.</p><p>Next is the sweep phase.\nWhatever objects were not visited in our graph walk are unused, or ,\nby the program.\nWe call this state unreachable because it is impossible with normal safe Go code\nto access that memory anymore, simply through the semantics of the language.\nTo complete the sweep phase, the algorithm simply iterates through all the\nunvisited nodes and marks their memory as free, so the memory allocator can reuse\nit.</p><p>You may think I’m oversimplifying a bit here.\nGarbage collectors are frequently referred to as , and .\nAnd you’d be partially right, there are more complexities.</p><p>For example, this algorithm is, in practice, executed concurrently with your\nregular Go code.\nWalking a graph that’s mutating underneath you brings challenges.\nWe also parallelize this algorithm, which is a detail that’ll come up again\nlater.</p><p>But trust me when I tell you that these details are mostly separate from the\ncore algorithm.\nIt really is just a simple graph flood at the center.</p><p>Let’s walk through an example.\nNavigate through the slideshow below to follow along.</p><p>After all that, I think we have a handle on what the Go garbage collector is actually doing.\nThis process seems to work well enough today, so what’s the problem?</p><p>Well, it turns out we can spend  of time executing this particular algorithm in some\nprograms, and it adds substantial overhead to nearly every Go program.\nIt’s not that uncommon to see Go programs spending 20% or more of their CPU time in the\ngarbage collector.</p><p>Let’s break down where that time is being spent.</p><p>At a high level, there are two parts to the cost of the garbage collector.\nThe first is how often it runs, and the second is how much work it does each time it runs.\nMultiply those two together, and you get the total cost of the garbage collector.</p><figure><figcaption>\n    Total GC cost = Number of GC cycles × Average cost per GC cycle\n    </figcaption></figure><p>But for now let’s focus only on the second part, the cost per cycle.</p><p>From years of poring over CPU profiles to try to improve performance, we know two big things\nabout Go’s garbage collector.</p><p>The first is that about 90% of the cost of the garbage collector is spent marking,\nand only about 10% is sweeping.\nSweeping turns out to be much easier to optimize than marking,\nand Go has had a very efficient sweeper for many years.</p><p>The second is that, of that time spent marking, a substantial portion, usually at least 35%, is\nsimply spent  on accessing heap memory.\nThis is bad enough on its own, but it completely gums up the works on what makes modern CPUs\nactually fast.</p><h3>“A microarchitectural disaster”</h3><p>What does “gum up the works” mean in this context?\nThe specifics of modern CPUs can get pretty complicated, so let’s use an analogy.</p><p>Imagine the CPU driving down a road, where that road is your program.\nThe CPU wants to ramp up to a high speed, and to do that it needs to be able to see far ahead of it,\nand the way needs to be clear.\nBut the graph flood algorithm is like driving through city streets for the CPU.\nThe CPU can’t see around corners and it can’t predict what’s going to happen next.\nTo make progress, it constantly has to slow down to make turns, stop at traffic lights, and avoid\npedestrians.\nIt hardly matters how fast your engine is because you never get a chance to get going.</p><p>Let’s make that more concrete by looking at our example again.\nI’ve overlaid the heap here with the path that we took.\nEach left-to-right arrow represents a piece of scanning work that we did\nand the dashed arrows show how we jumped around between bits of scanning work.</p><p>Notice that we were jumping all over memory doing tiny bits of work in each place.\nIn particular, we’re frequently jumping between pages, and between different parts of pages.</p><p>Modern CPUs do a lot of caching.\nGoing to main memory can be up to 100x slower than accessing memory that’s in our cache.\nCPU caches are populated with memory that’s been recently accessed, and memory that’s nearby to\nrecently accessed memory.\nBut there’s no guarantee that any two objects that point to each other will  be close to each\nother in memory.\nThe graph flood doesn’t take this into account.</p><p>Quick side note: if we were just stalling fetches to main memory, it might not be so bad.\nCPUs issue memory requests asynchronously, so even slow ones could overlap if the CPU could see\nfar enough ahead.\nBut in the graph flood, every bit of work is small, unpredictable, and highly dependent on the\nlast, so the CPU is forced to wait on nearly every individual memory fetch.</p><p>And unfortunately for us, this problem is only getting worse.\nThere’s an adage in the industry of “wait two years and your code will get faster.”</p><p>But Go, as a garbage collected language that relies on the mark-sweep algorithm, risks the opposite.\n“Wait two years and your code will get slower.”\nThe trends in modern CPU hardware are creating new challenges for garbage collector performance:</p><p><strong>Non-uniform memory access.</strong>\nFor one, memory now tends to be associated with subsets of CPU cores.\nAccesses by  CPU cores to that memory are slower than before.\nIn other words, the cost of a main memory access <a href=\"https://jprahman.substack.com/p/sapphire-rapids-core-to-core-latency\" rel=\"noreferrer\" target=\"_blank\">depends on which CPU core is accessing\nit</a>.\nIt’s non-uniform, so we call this non-uniform memory access, or NUMA for short.</p><p><strong>Reduced memory bandwidth.</strong>\nAvailable memory bandwidth per CPU is trending downward over time.\nThis just means that while we have more CPU cores, each core can submit relatively fewer\nrequests to main memory, forcing non-cached requests to wait longer than before.</p><p>\nAbove, we looked at a sequential marking algorithm, but the real garbage collector performs this\nalgorithm in parallel.\nThis scales well to a limited number of CPU cores, but the shared queue of objects to scan becomes\na bottleneck, even with careful design.</p><p><strong>Modern hardware features.</strong>\nNew hardware has fancy features like vector instructions, which let us operate on a lot of data at once.\nWhile this has the potential for big speedups, it’s not immediately clear how to make that work for\nmarking because marking does so much irregular and often small pieces of work.</p><p>Finally, this brings us to Green Tea, our new approach to the mark-sweep algorithm.\nThe key idea behind Green Tea is astonishingly simple:</p><p><em>Work with pages, not objects.</em></p><p>Sounds trivial, right?\nAnd yet, it took a lot of work to figure out how to order the object graph walk and what we needed to\ntrack to make this work well in practice.</p><p>More concretely, this means:</p><ul><li>Instead of scanning objects we scan whole pages.</li><li>Instead of tracking objects on our work list, we track whole pages.</li><li>We still need to mark objects at the end of the day, but we’ll track marked objects locally to each\npage, rather than across the whole heap.</li></ul><p>Let’s see what this means in practice by looking at our example heap again, but this time\nrunning Green Tea instead of the straightforward graph flood.</p><p>As above, navigate through the annotated slideshow to follow along.</p><p>Let’s come back around to our driving analogy.\nAre we finally getting on the highway?</p><p>Let’s recall our graph flood picture before.</p><p>We jumped around a whole lot, doing little bits of work in different places.\nThe path taken by Green Tea looks very different.</p><p>Green Tea, in contrast, makes fewer, longer left-to-right passes over pages A and B.\nThe longer these arrows, the better, and with bigger heaps, this effect can be much stronger.\n the magic of Green Tea.</p><p>It’s also our opportunity to ride the highway.</p><p>This all adds up to a better fit with the microarchitecture.\nWe can now scan objects closer together with much higher probability, so\nthere’s a better chance we can make use of our caches and avoid main memory.\nLikewise, per-page metadata is more likely to be in cache.\nTracking pages instead of objects means work lists are smaller,\nand less pressure on work lists means less contention and fewer CPU stalls.</p><p>And speaking of the highway, we can take our metaphorical engine into gears we’ve never been able to\nbefore, since now we can use vector hardware!</p><p>If you’re only vaguely familiar with vector hardware, you might be confused as to how we can use it here.\nBut besides the usual arithmetic and trigonometric operations,\nrecent vector hardware supports two things that are valuable for Green Tea:\nvery wide registers, and sophisticated bit-wise operations.</p><p>Most modern x86 CPUs support AVX-512, which has 512-bit wide vector registers.\nThis is wide enough to hold all of the metadata for an entire page in just two registers,\nright on the CPU, enabling Green Tea to work on an entire page in just a few straight-line\ninstructions.\nVector hardware has long supported basic bit-wise operations on whole vector registers, but starting\nwith AMD Zen 4 and Intel Ice Lake, it also supports a new bit vector “Swiss army knife” instruction\nthat enables a key step of the Green Tea scanning process to be done in just a few CPU cycles.\nTogether, these allow us to turbo-charge the Green Tea scan loop.</p><p>This wasn’t even an option for the graph flood, where we’d be jumping between scanning objects that\nare all sorts of different sizes.\nSometimes you needed two bits of metadata and sometimes you needed ten thousand.\nThere simply wasn’t enough predictability or regularity to use vector hardware.</p><p>If you want to nerd out on some of the details, read along!\nOtherwise, feel free to skip ahead to the <a href=\"https://go.dev/blog/greenteagc#evaluation\">evaluation</a>.</p><p>To get a sense of what AVX-512 GC scanning looks like, take a look at the diagram below.</p><p>There’s a lot going on here and we could probably fill an entire blog post just on how this works.\nFor now, let’s just break it down at a high level:</p><ol><li><p>First we fetch the “seen” and “scanned” bits for a page.\nRecall, these are one bit per object in the page, and all objects in a page have the same size.</p></li><li><p>Next, we compare the two bit sets.\nTheir union becomes the new “scanned” bits, while their difference is the “active objects” bitmap,\nwhich tells us which objects we need to scan in this pass over the page (versus previous passes).</p></li><li><p>We take the difference of the bitmaps and “expand” it, so that instead of one bit per object,\nwe have one bit per word (8 bytes) of the page.\nWe call this the “active words” bitmap.\nFor example, if the page stores 6-word (48-byte) objects, each bit in the active objects bitmap\nwill be copied to 6 bits in the active words bitmap.\nLike so:</p></li></ol><figure><div> → <pre>000000 000000 111111 111111 ...</pre></div></figure><ol start=\"4\"><li><p>Next we fetch the pointer/scalar bitmap for the page.\nHere, too, each bit corresponds to a word (8 bytes) of the page, and it tells us whether that word\nstores a pointer.\nThis data is managed by the memory allocator.</p></li><li><p>Now, we take the intersection of the pointer/scalar bitmap and the active words bitmap.\nThe result is the “active pointer bitmap”: a bitmap that tells us the location of every\npointer in the entire page contained in any live object we haven’t scanned yet.</p></li><li><p>Finally, we can iterate over the memory of the page and collect all the pointers.\nLogically, we iterate over each set bit in the active pointer bitmap,\nload the pointer value at that word, and write it back to a buffer that\nwill later be used to mark objects seen and add pages to the work list.\nUsing vector instructions, we’re able to do this 64 bytes at a time,\nin just a couple instructions.</p></li></ol><p>Part of what makes this fast is the  instruction,\npart of the “Galios Field New Instructions” x86 extension,\nand the bit manipulation Swiss army knife we referred to above.\nIt’s the real star of the show, since it lets us do step (3) in the scanning kernel very, very\nefficiently.\nIt performs a bit-wise <a href=\"https://en.wikipedia.org/wiki/Affine_transformation\" rel=\"noreferrer\" target=\"_blank\">affine\ntransformations</a>,\ntreating each byte in a vector as itself a mathematical vector of 8 bits\nand multiplying it by an 8x8 bit matrix.\nThis is all done over the <a href=\"https://en.wikipedia.org/wiki/Finite_field\" rel=\"noreferrer\" target=\"_blank\">Galois field</a>,\nwhich just means multiplication is AND and addition is XOR.\nThe upshot of this is that we can define a few 8x8 bit matrices for each\nobject size that perform exactly the 1:n bit expansion we need.</p><p>For the full assembly code, see <a href=\"https://cs.opensource.google/go/go/+/master:src/internal/runtime/gc/scan/scan_amd64.s;l=23;drc=041f564b3e6fa3f4af13a01b94db14c1ee8a42e0\" rel=\"noreferrer\" target=\"_blank\">this\nfile</a>.\nThe “expanders” use different matrices and different permutations for each size class,\nso they’re in a <a href=\"https://cs.opensource.google/go/go/+/master:src/internal/runtime/gc/scan/expand_amd64.s;drc=041f564b3e6fa3f4af13a01b94db14c1ee8a42e0\" rel=\"noreferrer\" target=\"_blank\">separate file</a>\nthat’s written by a <a href=\"https://cs.opensource.google/go/go/+/master:src/internal/runtime/gc/scan/mkasm.go;drc=041f564b3e6fa3f4af13a01b94db14c1ee8a42e0\" rel=\"noreferrer\" target=\"_blank\">code generator</a>.\nAside from the expansion functions, it’s really not a lot of code.\nMost of it is dramatically simplified by the fact that we can perform most of the above\noperations on data that sits purely in registers.\nAnd, hopefully soon this assembly code <a href=\"https://go.dev/issue/73787\">will be replaced with Go code</a>!</p><p>Credit to Austin Clements for devising this process.\nIt’s incredibly cool, and incredibly fast!</p><p>So that’s it for how it works.\nHow much does it actually help?</p><p>It can be quite a lot.\nEven without the vector enhancements, we see reductions in garbage collection CPU costs\nbetween 10% and 40% in our benchmark suite.\nFor example, if an application spends 10% of its time in the garbage collector, then that\nwould translate to between a 1% and 4% overall CPU reduction, depending on the specifics of\nthe workload.\nA 10% reduction in garbage collection CPU time is roughly the modal improvement.\n(See the <a href=\"https://go.dev/issue/73581\">GitHub issue</a> for some of these details.)</p><p>We’ve rolled Green Tea out inside Google, and we see similar results at scale.</p><p>We’re still rolling out the vector enhancements,\nbut benchmarks and early results suggest this will net an additional 10% GC CPU reduction.</p><p>While most workloads benefit to some degree, there are some that don’t.</p><p>Green Tea is based on the hypothesis that we can accumulate enough objects to scan on a\nsingle page in one pass to counteract the costs of the accumulation process.\nThis is clearly the case if the heap has a very regular structure: objects of the same size at a\nsimilar depth in the object graph.\nBut there are some workloads that often require us to scan only a single object per page at a time.\nThis is potentially worse than the graph flood because we might be doing more work than before while\ntrying to accumulate objects on pages and failing.</p><p>The implementation of Green Tea has a special case for pages that have only a single object to scan.\nThis helps reduce regressions, but doesn’t completely eliminate them.</p><p>However, it takes a lot less per-page accumulation to outperform the graph flood\nthan you might expect.\nOne surprise result of this work was that scanning a mere 2% of a page at a time\ncan yield improvements over the graph flood.</p><p>Green Tea is already available as an experiment in the recent Go 1.25 release and can be enabled\nby setting the environment variable  to  at build time.\nThis doesn’t include the aforementioned vector acceleration.</p><p>We expect to make it the default garbage collector in Go 1.26, but you’ll still be able to opt-out\nwith <code>GOEXPERIMENT=nogreenteagc</code> at build time.\nGo 1.26 will also add vector acceleration on newer x86 hardware, and include a whole bunch of\ntweaks and improvements based on feedback we’ve collected so far.</p><p>If you can, we encourage you to try at Go tip-of-tree!\nIf you prefer to use Go 1.25, we’d still love your feedback.\nSee <a href=\"https://go.dev/issue/73581#issuecomment-2847696497\">this GitHub\ncomment</a> with some details on\nwhat diagnostics we’d be interested in seeing, if you can share, and the preferred channels for\nreporting feedback.</p><p>Before we wrap up this blog post, let’s take a moment to talk about the journey that got us here.\nThe human element of the technology.</p><p>The core of Green Tea may seem like a single, simple idea.\nLike the spark of inspiration that just one single person had.</p><p>But that’s not true at all.\nGreen Tea is the result of work and ideas from many people over several years.\nSeveral people on the Go team contributed to the ideas, including Michael Pratt, Cherry Mui, David\nChase, and Keith Randall.\nMicroarchitectural insights from Yves Vandriessche, who was at Intel at the time, also really helped\ndirect the design exploration.\nThere were a lot of ideas that didn’t work, and there were a lot of details that needed figuring out.\nJust to make this single, simple idea viable.</p><p>The seeds of this idea go all the way back to 2018.\nWhat’s funny is that everyone on the team thinks someone else thought of this initial idea.</p><p>Green Tea got its name in 2024 when Austin worked out a prototype of an earlier version while cafe\ncrawling in Japan and drinking LOTS of matcha!\nThis prototype showed that the core idea of Green Tea was viable.\nAnd from there we were off to the races.</p><p>Throughout 2025, as Michael implemented and productionized Green Tea, the ideas evolved and changed even\nfurther.</p><p>This took so much collaborative exploration because Green Tea is not just an algorithm, but an entire\ndesign space.\nOne that we don’t think any of us could’ve navigated alone.\nIt’s not enough to just have the idea, but you need to figure out the details and prove it.\nAnd now that we’ve done it, we can finally iterate.</p><p>The future of Green Tea is bright.</p><p>Once again, please try it out by setting  and let us know how it goes!\nWe’re really excited about this work and want to hear from you!</p>",
      "contentLength": 19499,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Project goals for 2025H2",
      "url": "https://blog.rust-lang.org/2025/10/28/project-goals-2025h2/",
      "date": 1761609600,
      "author": "Niko Matsakis",
      "guid": 29361,
      "unread": true,
      "content": "<p>On Sep 9, we merged <a href=\"https://github.com/rust-lang/rfcs/pull/3849\">RFC 3849</a>, declaring our goals for the \"second half\" of 2025H2 -- well, the last 3 months, at least, since \"yours truly\" ran a bit behind getting the goals program organized.</p><p>In prior goals programs, we had a few major flagship goals, but since many of these goals were multi-year programs, it was hard to see what progress had been made. This time we decided to organize things a bit differently. We established four flagship , each of which covers a number of more specific goals. These themes cover the goals we expect to be the most impactful and constitute our major focus as a Project for the remainder of the year. The four themes identified in the RFC are as follows:</p><ul><li>, making it possible to create user-defined smart pointers that are as ergonomic as Rust's built-in references .</li><li><strong>Unblocking dormant traits</strong>, extending the core capabilities of Rust's trait system to unblock long-desired features for language interop, lending iteration, and more.</li><li><strong>Flexible, fast(er) compilation</strong>, making it faster to build Rust programs and improving support for specialized build scenarios like embedded usage and sanitizers.</li><li>, making higher-level usage patterns in Rust easier.</li></ul><p>One of Rust's core value propositions is that it's a \"library-based language\"—libraries can build abstractions that feel built-in to the language even when they're not. Smart pointer types like  and  are prime examples, implemented purely in the standard library yet feeling like native language features. However, Rust's built-in reference types ( and ) have special capabilities that user-defined smart pointers cannot replicate. This creates a \"second-class citizen\" problem where custom pointer types can't provide the same ergonomic experience as built-in references.</p><p>The \"Beyond the \" initiative aims to share the special capabilities of , allowing library authors to create smart pointers that are truly indistinguishable from built-in references in terms of syntax and ergonomics. This will enable more ergonomic smart pointers for use in cross-language interop (e.g., references to objects in other languages like C++ or Python) and for low-level projects like Rust for Linux that use smart pointers to express particular data structures.</p><h3><a href=\"https://blog.rust-lang.org/2025/10/28/project-goals-2025h2/#unblocking-dormant-traits\" aria-hidden=\"true\"></a>\n\"Unblocking dormant traits\"</h3><p>Rust's trait system is one of its most powerful features, but it has a number of longstanding limitations that are preventing us from adopting new patterns. The goals in this category unblock a number of new capabilities:</p><ul><li><a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./polonius.html\">Polonius</a> will enable new borrowing patterns, and in particular <a href=\"https://github.com/rust-lang/rust/issues/92985\">unblock \"lending iterators\"</a>. Over the last few goal periods, we have identified an \"alpha\" version of Polonius that addresses the most important cases while being relatively simple and optimizable. Our goal for 2025H2 is to implement this algorithm in a form that is ready for stabilization in 2026.</li><li>The <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./next-solver.html\">next-generation trait solver</a> is a refactored trait solver that unblocks better support for numerous language features (implied bounds, negative impls, the list goes on) in addition to closing a number of existing bugs and sources of unsoundness. Over the last few goal periods, the trait solver went from being an early prototype to being in production use for coherence checking. The goal for 2025H2 is to prepare it for stabilization.</li><li>The work on <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./evolving-traits.html\">evolving trait hierarchies</a> will make it possible to refactor some parts of an existing trait into a new supertrait so they can be used on their own. This unblocks a number of features where the existing trait is insufficiently general, in particular stabilizing support for custom receiver types, a prior Project goal that wound up blocked on this refactoring. This will also make it safer to provide stable traits in the standard library while preserving the ability to evolve them in the future.</li><li>The work to <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./scalable-vectors.html\">expand Rust's  hierarchy</a> will permit us to express types that are neither  nor , such as extern types (which have no size) or Arm's Scalable Vector Extension (which have a size that is known at runtime but not at compilation time). This goal builds on <a href=\"https://github.com/rust-lang/rfcs/pull/3729\">RFC #3729</a> and <a href=\"https://github.com/rust-lang/rfcs/pull/3838\">RFC #3838</a>, authored in previous Project goal periods.</li><li><a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./in-place-initialization.html\">In-place initialization</a> allows creating structs and values that are tied to a particular place in memory. While useful directly for projects doing advanced C interop, it also unblocks expanding  to support  and  methods, as compiling such methods requires the ability for the callee to return a future whose size is not known to the caller.</li></ul><p>The \"Flexible, fast(er) compilation\" initiative focuses on improving Rust's build system to better serve both specialized use cases and everyday development workflows:</p><p>People generally start using Rust for foundational use cases, where the requirements for performance or reliability make it an obvious choice. But once they get used to it, they often find themselves turning to Rust even for higher-level use cases, like scripting, web services, or even GUI applications. Rust is often \"surprisingly tolerable\" for these high-level use cases -- except for some specific pain points that, while they impact everyone using Rust, hit these use cases particularly hard. We plan two flagship goals this period in this area:</p><ul><li>We aim to stabilize <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./cargo-script.html\">cargo script</a>, a feature that allows single-file Rust programs that embed their dependencies, making it much easier to write small utilities, share code examples, and create reproducible bug reports without the overhead of full Cargo projects.</li><li>We aim to finalize the design of <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./ergonomic-rc.html\">ergonomic ref-counting</a> and to finalize the experimental impl feature so it is ready for beta testing. Ergonomic ref-counting makes it less cumbersome to work with ref-counted types like  and , particularly in closures.</li></ul><p>For the remainder of 2025 you can expect monthly blog posts covering the major progress on the Project goals.</p><p>Looking at the broader picture, we have now done three iterations of the goals program, and we want to judge how it should be run going forward. To start, Nandini Sharma from CMU has been conducting interviews with various Project members to help us see what's working with the goals program and what could be improved. We expect to spend some time discussing what we should do and to be launching the next iteration of the goals program next year. Whatever form that winds up taking, Tomas Sedovic, the <a href=\"https://blog.rust-lang.org/inside-rust/2025/06/30/program-management-update-2025-06/\">Rust program manager</a> hired by the Leadership Council, will join me in running the program.</p>",
      "contentLength": 6396,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "7 Common Kubernetes Pitfalls (and How I Learned to Avoid Them)",
      "url": "https://kubernetes.io/blog/2025/10/20/seven-kubernetes-pitfalls-and-how-to-avoid/",
      "date": 1760974200,
      "author": "",
      "guid": 27426,
      "unread": true,
      "content": "<p>It’s no secret that Kubernetes can be both powerful and frustrating at times. When I first started dabbling with container orchestration, I made more than my fair share of mistakes enough to compile a whole list of pitfalls. In this post, I want to walk through seven big gotchas I’ve encountered (or seen others run into) and share some tips on how to avoid them. Whether you’re just kicking the tires on Kubernetes or already managing production clusters, I hope these insights help you steer clear of a little extra stress.</p><h2>1. Skipping resource requests and limits</h2><p>: Not specifying CPU and memory requirements in Pod specifications. This typically happens because Kubernetes does not require these fields, and workloads can often start and run without them—making the omission easy to overlook in early configurations or during rapid deployment cycles.</p><p>:\nIn Kubernetes, resource requests and limits are critical for efficient cluster management. Resource requests ensure that the scheduler reserves the appropriate amount of CPU and memory for each pod, guaranteeing that it has the necessary resources to operate. Resource limits cap the amount of CPU and memory a pod can use, preventing any single pod from consuming excessive resources and potentially starving other pods.\nWhen resource requests and limits are not set:</p><ol><li>Resource Starvation: Pods may get insufficient resources, leading to degraded performance or failures. This is because Kubernetes schedules pods based on these requests. Without them, the scheduler might place too many pods on a single node, leading to resource contention and performance bottlenecks.</li><li>Resource Hoarding: Conversely, without limits, a pod might consume more than its fair share of resources, impacting the performance and stability of other pods on the same node. This can lead to issues such as other pods getting evicted or killed by the Out-Of-Memory (OOM) killer due to lack of available memory.</li></ol><ul><li>Start with modest  (for example  CPU,  memory) and see how your app behaves.</li><li>Monitor real-world usage and refine your values; the <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\">HorizontalPodAutoscaler</a> can help automate scaling based on metrics.</li><li>Keep an eye on  or your logging/monitoring tool to confirm you’re not over- or under-provisioning.</li></ul><p>: Early on, I never thought about memory limits. Things seemed fine on my local cluster. Then, on a larger environment, Pods got  left and right. Lesson learned.\nFor detailed instructions on configuring resource requests and limits for your containers, please refer to <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/\">Assign Memory Resources to Containers and Pods</a>\n(part of the official Kubernetes documentation).</p><h2>2. Underestimating liveness and readiness probes</h2><p>: Deploying containers without explicitly defining how Kubernetes should check their health or readiness. This tends to happen because Kubernetes will consider a container “running” as long as the process inside hasn’t exited. Without additional signals, Kubernetes assumes the workload is functioning—even if the application inside is unresponsive, initializing, or stuck.</p><p>:\nLiveness, readiness, and startup probes are mechanisms Kubernetes uses to monitor container health and availability.</p><ul><li> determine if the application is still alive. If a liveness check fails, the container is restarted.</li><li> control whether a container is ready to serve traffic. Until the readiness probe passes, the container is removed from Service endpoints.</li><li> help distinguish between long startup times and actual failures.</li></ul><ul><li>Add a simple HTTP  to check a health endpoint (for example ) so Kubernetes can restart a hung container.</li><li>Use a  to ensure traffic doesn’t reach your app until it’s warmed up.</li><li>Keep probes simple. Overly complex checks can create false alarms and unnecessary restarts.</li></ul><p>: I once forgot a readiness probe for a web service that took a while to load. Users hit it prematurely, got weird timeouts, and I spent hours scratching my head. A 3-line readiness probe would have saved the day.</p><h2>3. “We’ll just look at container logs” (famous last words)</h2><p>: Relying solely on container logs retrieved via . This often happens because the command is quick and convenient, and in many setups, logs appear accessible during development or early troubleshooting. However,  only retrieves logs from currently running or recently terminated containers, and those logs are stored on the node’s local disk. As soon as the container is deleted, evicted, or the node is restarted, the log files may be rotated out or permanently lost.</p><ul><li> using CNCF tools like <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/logging/#sidecar-container-with-a-logging-agent\">Fluentd</a> or <a href=\"https://fluentbit.io/\">Fluent Bit</a> to aggregate output from all Pods.</li><li> for a unified view of logs, metrics, and (if needed) traces. This lets you spot correlations between infrastructure events and app-level behavior.</li><li><strong>Pair logs with Prometheus metrics</strong> to track cluster-level data alongside application logs. If you need distributed tracing, consider CNCF projects like <a href=\"https://www.jaegertracing.io/\">Jaeger</a>.</li></ul><p>: The first time I lost Pod logs to a quick restart, I realized how flimsy “kubectl logs” can be on its own. Since then, I’ve set up a proper pipeline for every cluster to avoid missing vital clues.</p><h2>4. Treating dev and prod exactly the same</h2><p>: Deploying the same Kubernetes manifests with identical settings across development, staging, and production environments. This often occurs when teams aim for consistency and reuse, but overlook that environment-specific factors—such as traffic patterns, resource availability, scaling needs, or access control—can differ significantly. Without customization, configurations optimized for one environment may cause instability, poor performance, or security gaps in another.</p><ul><li>Use environment overlays or <a href=\"https://kustomize.io/\">kustomize</a> to maintain a shared base while customizing resource requests, replicas, or config for each environment.</li><li>Extract environment-specific configuration into ConfigMaps and / or Secrets. You can use a specialized tool such as <a href=\"https://github.com/bitnami-labs/sealed-secrets\">Sealed Secrets</a> to manage confidential data.</li><li>Plan for scale in production. Your dev cluster can probably get away with minimal CPU/memory, but prod might need significantly more.</li></ul><p>: One time, I scaled up  from 2 to 10 in a tiny dev environment just to “test.” I promptly ran out of resources and spent half a day cleaning up the aftermath. Oops.</p><h2>5. Leaving old stuff floating around</h2><p>: Leaving unused or outdated resources—such as Deployments, Services, ConfigMaps, or PersistentVolumeClaims—running in the cluster. This often happens because Kubernetes does not automatically remove resources unless explicitly instructed, and there is no built-in mechanism to track ownership or expiration. Over time, these forgotten objects can accumulate, consuming cluster resources, increasing cloud costs, and creating operational confusion, especially when stale Services or LoadBalancers continue to route traffic.</p><ul><li> with a purpose or owner label. That way, you can easily query resources you no longer need.</li><li> your cluster: run <code>kubectl get all -n &lt;namespace&gt;</code> to see what’s actually running, and confirm it’s all legit.</li><li><strong>Adopt Kubernetes’ Garbage Collection</strong>: <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/\">K8s docs</a> show how to remove dependent objects automatically.</li><li><strong>Leverage policy automation</strong>: Tools like <a href=\"https://kyverno.io/\">Kyverno</a> can automatically delete or block stale resources after a certain period, or enforce lifecycle policies so you don’t have to remember every single cleanup step.</li></ul><p>: After a hackathon, I forgot to tear down a “test-svc” pinned to an external load balancer. Three weeks later, I realized I’d been paying for that load balancer the entire time. Facepalm.</p><h2>6. Diving too deep into networking too soon</h2><p>: Introducing advanced networking solutions—such as service meshes, custom CNI plugins, or multi-cluster communication—before fully understanding Kubernetes' native networking primitives. This commonly occurs when teams implement features like traffic routing, observability, or mTLS using external tools without first mastering how core Kubernetes networking works: including Pod-to-Pod communication, ClusterIP Services, DNS resolution, and basic ingress traffic handling. As a result, network-related issues become harder to troubleshoot, especially when overlays introduce additional abstractions and failure points.</p><ul><li>Start small: a Deployment, a Service, and a basic ingress controller such as one based on NGINX (e.g., Ingress-NGINX).</li><li>Make sure you understand how traffic flows within the cluster, how service discovery works, and how DNS is configured.</li><li>Only move to a full-blown mesh or advanced CNI features when you actually need them, complex networking adds overhead.</li></ul><p>: I tried Istio on a small internal app once, then spent more time debugging Istio itself than the actual app. Eventually, I stepped back, removed Istio, and everything worked fine.</p><h2>7. Going too light on security and RBAC</h2><p>: Deploying workloads with insecure configurations, such as running containers as the root user, using the  image tag, disabling security contexts, or assigning overly broad RBAC roles like . These practices persist because Kubernetes does not enforce strict security defaults out of the box, and the platform is designed to be flexible rather than opinionated. Without explicit security policies in place, clusters can remain exposed to risks like container escape, unauthorized privilege escalation, or accidental production changes due to unpinned images.</p><ul><li>Use <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/rbac/\">RBAC</a> to define roles and permissions within Kubernetes. While RBAC is the default and most widely supported authorization mechanism, Kubernetes also allows the use of alternative authorizers. For more advanced or external policy needs, consider solutions like <a href=\"https://open-policy-agent.github.io/gatekeeper/\">OPA Gatekeeper</a> (based on Rego), <a href=\"https://kyverno.io/\">Kyverno</a>, or custom webhooks using policy languages such as CEL or <a href=\"https://cedarpolicy.com/\">Cedar</a>.</li><li>Pin images to specific versions (no more !). This helps you know what’s actually deployed.</li><li>Look into <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/\">Pod Security Admission</a> (or other solutions like Kyverno) to enforce non-root containers, read-only filesystems, etc.</li></ul><p>: I never had a huge security breach, but I’ve heard plenty of cautionary tales. If you don’t tighten things up, it’s only a matter of time before something goes wrong.</p><p>Kubernetes is amazing, but it’s not psychic, it won’t magically do the right thing if you don’t tell it what you need. By keeping these pitfalls in mind, you’ll avoid a lot of headaches and wasted time. Mistakes happen (trust me, I’ve made my share), but each one is a chance to learn more about how Kubernetes truly works under the hood.\nIf you’re curious to dive deeper, the <a href=\"https://kubernetes.io/docs/home/\">official docs</a> and the <a href=\"http://slack.kubernetes.io/\">community Slack</a> are excellent next steps. And of course, feel free to share your own horror stories or success tips, because at the end of the day, we’re all in this cloud native adventure together.</p>",
      "contentLength": 10623,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Spotlight on Policy Working Group",
      "url": "https://kubernetes.io/blog/2025/10/18/wg-policy-spotlight-2025/",
      "date": 1760745600,
      "author": "",
      "guid": 27039,
      "unread": true,
      "content": "<p><em>(Note: The Policy Working Group has completed its mission and is no longer active. This article reflects its work, accomplishments, and insights into how a working group operates.)</em></p><p>In the complex world of Kubernetes, policies play a crucial role in managing and securing clusters. But have you ever wondered how these policies are developed, implemented, and standardized across the Kubernetes ecosystem? To answer that, let's take a look back at the work of the Policy Working Group.</p><p>The Policy Working Group was dedicated to a critical mission: providing an overall architecture that encompasses both current policy-related implementations and future policy proposals in Kubernetes. Their goal was both ambitious and essential: to develop a universal policy architecture that benefits developers and end-users alike.</p><p>Through collaborative methods, this working group strove to bring clarity and consistency to the often complex world of Kubernetes policies. By focusing on both existing implementations and future proposals, they ensured that the policy landscape in Kubernetes remains coherent and accessible as the technology evolves.</p><p>This blog post dives deeper into the work of the Policy Working Group, guided by insights from its former co-chairs:</p><p>These co-chairs explained what the Policy Working Group was all about.</p><p><strong>Hello, thank you for the time! Let’s start with some introductions, could you tell us a bit about yourself, your role, and how you got involved in Kubernetes?</strong></p><p>: My name is Jim Bugwadia, and I am a co-founder and the CEO at Nirmata which provides solutions that automate security and compliance for cloud-native workloads. At Nirmata, we have been working with Kubernetes since it started in 2014. We initially built a Kubernetes policy engine in our commercial platform and later donated it to CNCF as the Kyverno project. I joined the CNCF Kubernetes Policy Working Group to help build and standardize various aspects of policy management for Kubernetes and later became a co-chair.</p><p>: My name is Andy Suderman and I am the CTO of Fairwinds, a managed Kubernetes-as-a-Service provider. I began working with Kubernetes in 2016 building a web conferencing platform. I am an author and/or maintainer of several Kubernetes-related open-source projects such as Goldilocks, Pluto, and Polaris. Polaris is a JSON-schema-based policy engine, which started Fairwinds' journey into the policy space and my involvement in the Policy Working Group.</p><p>: My name is Poonam Lamba, and I currently work as a Product Manager for Google Kubernetes Engine (GKE) at Google. My journey with Kubernetes began back in 2017 when I was building an SRE platform for a large enterprise, using a private cloud built on Kubernetes. Intrigued by its potential to revolutionize the way we deployed and managed applications at the time, I dove headfirst into learning everything I could about it. Since then, I've had the opportunity to build the policy and compliance products for GKE. I lead and contribute to GKE CIS benchmarks. I am involved with the Gatekeeper project as well as I have contributed to Policy-WG for over 2 years and served as a co-chair for the group.</p><p><em>Responses to the following questions represent an amalgamation of insights from the former co-chairs.</em></p><p><strong>One thing even I am not aware of is the difference between a working group and a SIG. Can you help us understand what a working group is and how it is different from a SIG?</strong></p><p>Unlike SIGs, working groups are temporary and focused on tackling specific, cross-cutting issues or projects that may involve multiple SIGs. Their lifespan is defined, and they disband once they've achieved their objective. Generally, working groups don't own code or have long-term responsibility for managing a particular area of the Kubernetes project.</p><p><strong>You mentioned that Working Groups involve multiple SIGS. What SIGS was the Policy WG closely involved with, and how did you coordinate with them?</strong></p><p>The group collaborated closely with Kubernetes SIG Auth throughout our existence, and more recently, the group also worked with SIG Security since its formation. Our collaboration occurred in a few ways. We provided periodic updates during the SIG meetings to keep them informed of our progress and activities. Additionally, we utilize other community forums to maintain open lines of communication and ensured our work aligned with the broader Kubernetes ecosystem. This collaborative approach helped the group stay coordinated with related efforts across the Kubernetes community.</p><p><strong>Why was the Policy Working Group created?</strong></p><p>To enable a broad set of use cases, we recognize that Kubernetes is powered by a highly declarative, fine-grained, and extensible configuration management system. We've observed that a Kubernetes configuration manifest may have different portions that are important to various stakeholders. For example, some parts may be crucial for developers, while others might be of particular interest to security teams or address operational concerns. Given this complexity, we believe that policies governing the usage of these intricate configurations are essential for success with Kubernetes.</p><p>Our Policy Working Group was created specifically to research the standardization of policy definitions and related artifacts. We saw a need to bring consistency and clarity to how policies are defined and implemented across the Kubernetes ecosystem, given the diverse requirements and stakeholders involved in Kubernetes deployments.</p><p><strong>Can you give me an idea of the work you did in the group?</strong></p><p>We worked on several Kubernetes policy-related projects. Our initiatives included:</p><ul><li>We worked on a Kubernetes Enhancement Proposal (KEP) for the Kubernetes Policy Reports API. This aims to standardize how policy reports are generated and consumed within the Kubernetes ecosystem.</li><li>We conducted a CNCF survey to better understand policy usage in the Kubernetes space. This helped gauge the practices and needs across the community at the time.</li><li>We wrote a paper that will guide users in achieving PCI-DSS compliance for containers. This is intended to help organizations meet important security standards in their Kubernetes environments.</li><li>We also worked on a paper highlighting how shifting security down can benefit organizations. This focuses on the advantages of implementing security measures earlier in the development and deployment process.</li></ul><p><strong>Can you tell us what were the main objectives of the Policy Working Group and some of your key accomplishments?</strong></p><p>The charter of the Policy WG was to help standardize policy management for Kubernetes and educate the community on best practices.</p><p>To accomplish this we updated the Kubernetes documentation (<a href=\"https://kubernetes.io/docs/concepts/policy\">Policies | Kubernetes</a>), produced several whitepapers (<a href=\"https://github.com/kubernetes/sig-security/blob/main/sig-security-docs/papers/policy/CNCF_Kubernetes_Policy_Management_WhitePaper_v1.pdf\">Kubernetes Policy Management</a>, <a href=\"https://github.com/kubernetes/sig-security/blob/main/sig-security-docs/papers/policy_grc/Kubernetes_Policy_WG_Paper_v1_101123.pdf\">Kubernetes GRC</a>), and created the Policy Reports API (<a href=\"https://github.com/kubernetes-retired/wg-policy-prototypes/blob/master/policy-report/docs/api-docs.md\">API reference</a>) which standardizes reporting across various tools. Several popular tools such as Falco, Trivy, Kyverno, kube-bench, and others support the Policy Report API. A major milestone for the Policy WG was promoting the Policy Reports API to a SIG-level API or finding it a stable home.</p><p>Beyond that, as <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/\">ValidatingAdmissionPolicy</a> and <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/mutating-admission-policy/\">MutatingAdmissionPolicy</a> approached GA in Kubernetes, a key goal of the WG was to guide and educate the community on the tradeoffs and appropriate usage patterns for these built-in API objects and other CNCF policy management solutions like OPA/Gatekeeper and Kyverno.</p><p><strong>What were some of the major challenges that the Policy Working Group worked on?</strong></p><p>During our work in the Policy Working Group, we encountered several challenges:</p><ul><li><p>One of the main issues we faced was finding time to consistently contribute. Given that many of us have other professional commitments, it can be difficult to dedicate regular time to the working group's initiatives.</p></li><li><p>Another challenge we experienced was related to our consensus-driven model. While this approach ensures that all voices are heard, it can sometimes lead to slower decision-making processes. We valued thorough discussion and agreement, but this can occasionally delay progress on our projects.</p></li><li><p>We've also encountered occasional differences of opinion among group members. These situations require careful navigation to ensure that we maintain a collaborative and productive environment while addressing diverse viewpoints.</p></li><li><p>Lastly, we've noticed that newcomers to the group may find it difficult to contribute effectively without consistent attendance at our meetings. The complex nature of our work often requires ongoing context, which can be challenging for those who aren't able to participate regularly.</p></li></ul><p><strong>Can you tell me more about those challenges? How did you discover each one? What has the impact been? What were some strategies you used to address them?</strong></p><p>There are no easy answers, but having more contributors and maintainers greatly helps! Overall the CNCF community is great to work with and is very welcoming to beginners. So, if folks out there are hesitating to get involved, I highly encourage them to attend a WG or SIG meeting and just listen in.</p><p>It often takes a few meetings to fully understand the discussions, so don't feel discouraged if you don't grasp everything right away. We made a point to emphasize this and encouraged new members to review documentation as a starting point for getting involved.</p><p>Additionally, differences of opinion were valued and encouraged within the Policy-WG. We adhered to the CNCF core values and resolve disagreements by maintaining respect for one another. We also strove to timebox our decisions and assign clear responsibilities to keep things moving forward.</p><p>This is where our discussion about the Policy Working Group ends. The working group, and especially the people who took part in this article, hope this gave you some insights into the group's aims and workings. You can get more info about Working Groups <a href=\"https://github.com/kubernetes/community/blob/master/committee-steering/governance/wg-governance.md\">here</a>.</p>",
      "contentLength": 9857,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "docs.rs: changed default targets",
      "url": "https://blog.rust-lang.org/2025/10/16/docsrs-changed-default-targets/",
      "date": 1760572800,
      "author": "Denis Cornehl",
      "guid": 26654,
      "unread": true,
      "content": "<p>This post announces two changes to the list of default targets used to build\ndocumentation on docs.rs.</p><p>Crate authors can specify a custom list of targets using\n<a href=\"https://docs.rs/about/metadata\">docs.rs metadata in </a>. If this\nmetadata is not provided, docs.rs falls back to a default list. We are updating\nthis list to better reflect the current state of the Rust ecosystem.</p><h2><a href=\"https://blog.rust-lang.org/2025/10/16/docsrs-changed-default-targets/#apple-silicon-arm64-replaces-x86-64\" aria-hidden=\"true\"></a>\nApple silicon (ARM64) replaces x86_64</h2><p>Reflecting Apple's transition from x86_64 to its own ARM64 silicon, the Rust\nproject has updated its platform support tiers. The \ntarget is now Tier 1, while  has moved to Tier 2. You can\nread more about this in <a href=\"https://github.com/rust-lang/rfcs/pull/3671\">RFC 3671</a>\nand <a href=\"https://github.com/rust-lang/rfcs/pull/3841\">RFC 3841</a>.</p><p>To align with this, docs.rs will now use  as the default\ntarget for Apple platforms instead of .</p><h2><a href=\"https://blog.rust-lang.org/2025/10/16/docsrs-changed-default-targets/#linux-arm64-replaces-32-bit-x86\" aria-hidden=\"true\"></a>\nLinux ARM64 replaces 32-bit x86</h2><p>Support for 32-bit  architectures is declining, and major Linux\ndistributions have begun to phase it out.</p><p>Consequently, we are replacing the  target with\n<code>aarch64-unknown-linux-gnu</code> in our default set.</p><p>The updated list of default targets is:</p><ul><li> (replaces )</li><li><code>aarch64-unknown-linux-gnu</code> (replaces )</li></ul><p>If your crate requires the previous default target list, you can explicitly\ndefine it in your :</p><pre data-lang=\"toml\"><code data-lang=\"toml\"></code></pre><p>Note that docs.rs continues to support any target available in the Rust\ntoolchain; only the  list has changed.</p>",
      "contentLength": 1228,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Python 3.15.0 alpha 1",
      "url": "https://pythoninsider.blogspot.com/2025/10/python-3150-alpha-1.html",
      "date": 1760508780,
      "author": "Hugo",
      "guid": 26238,
      "unread": true,
      "content": "<p><strong>This is an early developer preview of Python\n3.15</strong></p><p>https://www.python.org/downloads/release/python-3150a1/</p><p>Python 3.15 is still in development. This release, 3.15.0a1, is the\nfirst of seven planned alpha releases.</p><p>Alpha releases are intended to make it easier to test the current\nstate of new features and bug fixes and to test the release process.</p><p>During the alpha phase, features may be added up until the start of\nthe beta phase (2026-05-05) and, if necessary, may be modified or\ndeleted up until the release candidate phase (2026-07-28). Please keep\nin mind that this is a preview release and its use is\n recommended for production environments.</p><p>Many new features for Python 3.15 are still being planned and\nwritten. Among the new major new features and changes so far:</p><ul><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-sampling-profiler\">PEP\n799</a>: A dedicated profiling package for Python profiling tools</li><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-utf8-default\">PEP\n686</a>: Python now uses UTF-8 as the default encoding</li><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-pep782\">PEP\n782</a>: A new  C API to create a Python bytes\nobject</li><li><small>(Hey,  if a feature\nyou find important is missing from this list, let Hugo\nknow.)</small></li></ul><p>The next pre-release of Python 3.15 will be 3.15.0a2, currently\nscheduled for 2025-11-18.</p><blockquote><p>And hence not only at substantiated times, upon well known separate\nfeeding-grounds, could Ahab hope to encounter his prey; but in crossing\nthe widest expanses of water between those grounds he could, by his art,\nso place and time himself on his way, as even then not to be wholly\nwithout prospect of a meeting.</p></blockquote><p>Thanks to all of the many volunteers who help make Python Development\nand these releases possible! Please consider supporting our efforts by\nvolunteering yourself or through organisation contributions to the <a href=\"https://www.python.org/psf-landing/\">Python Software\nFoundation</a>.</p><p>Regards from Helsinki before the first PyCon Finland in 9 years,</p><p>Your release team,\n  Hugo van Kemenade\n  Steve Dower\n  </p>",
      "contentLength": 1774,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Announcing the New Rust Project Directors",
      "url": "https://blog.rust-lang.org/2025/10/15/announcing-the-new-rust-project-directors-2025/",
      "date": 1760486400,
      "author": "Leadership Council",
      "guid": 26319,
      "unread": true,
      "content": "<p>We are happy to announce that we have completed the annual process to elect new Project Directors.</p><p>The new Project Directors are:</p><p>They will join <a href=\"https://github.com/rylev\">Ryan Levick</a> and <a href=\"https://github.com/carols10cents\">Carol Nichols</a> to make up the five members of the Rust Foundation Board of Directors who represent the Rust Project.</p><p>We would also like to thank the outgoing going Project Directors for contributions and service:</p><p>The board is made up of Project Directors, who come from and represent the Rust Project, and Member Directors, who represent the corporate members of the Rust Foundation. Both of these director groups have equal voting power.</p><p>We look forward to working with and being represented by this new group of project directors.</p><p>We were fortunate to have a number of excellent candidates and this was a difficult decision. We wish to express our gratitude to all of the candidates who were considered for this role! We also extend our thanks to the project as a whole who participated by nominating candidates and providing additional feedback once the nominees were published.</p><p>Finally, we want to share our appreciation for <a href=\"https://github.com/tomassedovic\">Tomas Sedovic</a> for facilitating the election process. An overview of the election process can be found in a previous blog post <a href=\"https://blog.rust-lang.org/2023/08/30/electing-new-project-directors/\">here</a>.</p>",
      "contentLength": 1213,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Python 3.13.9 is now available!",
      "url": "https://pythoninsider.blogspot.com/2025/10/python-3139-is-now-available.html",
      "date": 1760468820,
      "author": "Thomas Wouters",
      "guid": 26129,
      "unread": true,
      "content": "<p>Thanks to all of the many volunteers who help make Python Development\n and this release possible! Please consider supporting our efforts by \nvolunteering yourself or through organisation contributions to the <a href=\"https://www.python.org/psf-landing/\">Python Software Foundation</a>.</p><p>Your expedited release team,\nYour release team,\nNed Deily \nŁukasz Langa </p>",
      "contentLength": 308,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    }
  ],
  "tags": [
    "dev"
  ]
}
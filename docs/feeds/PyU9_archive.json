{
  "id": "PyU9",
  "title": "Dev",
  "displayTitle": "Dev",
  "url": "",
  "feedLink": "",
  "isQuery": true,
  "isEmpty": false,
  "isHidden": false,
  "itemCount": 14,
  "items": [
    {
      "title": "Kubernetes v1.35 Sneak Peek",
      "url": "https://kubernetes.io/blog/2025/11/26/kubernetes-v1-35-sneak-peek/",
      "date": 1764115200,
      "author": "",
      "guid": 35806,
      "unread": true,
      "content": "<p>As the release of Kubernetes v1.35 approaches, the Kubernetes project continues to evolve. Features may be deprecated, removed, or replaced to improve the project's overall health. This blog post outlines planned changes for the v1.35 release that the release team believes you should be aware of to ensure the continued smooth operation of your Kubernetes cluster(s), and to keep you up to date with the latest developments. The information below is based on the current status of the v1.35 release and is subject to change before the final release date.</p><h2>Deprecations and removals for Kubernetes v1.35</h2><p>On Linux nodes, container runtimes typically rely on cgroups (short for \"control groups\").\nSupport for using cgroup v2 has been stable in Kubernetes since v1.25, providing an alternative to the original v1 cgroup support. While cgroup v1 provided the initial resource control mechanism, it suffered from well-known\ninconsistencies and limitations. Adding support for cgroup v2 allowed use of a unified control group hierarchy, improved resource isolation, and served as the foundation for modern features, making legacy cgroup v1 support ready for removal.\nThe removal of cgroup v1 support will only impact cluster administrators running nodes on older Linux distributions that do not support cgroup v2; on those nodes, the  will fail to start. Administrators must migrate their nodes to systems with cgroup v2 enabled. More details on compatibility requirements will be available in a blog post soon after the v1.35 release.</p><h3>Deprecation of ipvs mode in kube-proxy</h3><p>Many releases ago, the Kubernetes project implemented an <a href=\"https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-ipvs\">ipvs</a> mode in . It was adopted as a way to provide high-performance service load balancing, with better performance than the existing  mode. However, maintaining feature parity between ipvs and other kube-proxy modes became difficult, due to technical complexity and diverging requirements. This created significant technical debt and made the ipvs backend impractical to support alongside newer networking capabilities.</p><p>The Kubernetes project intends to deprecate kube-proxy  mode in the v1.35 release, to streamline the  codebase. For Linux nodes, the recommended  mode is already <a href=\"https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-nftables\">nftables</a>.</p><h3>Kubernetes is deprecating containerd v1.y support</h3><p>While Kubernetes v1.35 still supports containerd 1.7 and other LTS releases of containerd, as a consequence of automated cgroup driver detection, the Kubernetes SIG Node community has formally agreed upon a final support timeline for containerd v1.X. Kubernetes v1.35 is the last release to offer this support (aligned with containerd 1.7 EOL).</p><p>This is a final warning that if you are using containerd 1.X, you must switch to 2.0 or later before upgrading Kubernetes to the next version. You are able to monitor the <code>kubelet_cri_losing_support</code> metric to determine if any nodes in your cluster are using a containerd version that will soon be unsupported.</p><h2>Featured enhancements of Kubernetes v1.35</h2><p>The following enhancements are some of those likely to be included in the v1.35 release. This is not a commitment, and the release content is subject to change.</p><p>When scheduling Pods, Kubernetes uses node labels, taints, and tolerations to match workload requirements with node capabilities. However, managing feature compatibility becomes challenging during cluster upgrades due to version skew between the control plane and nodes. This can lead to Pods being scheduled on nodes that lack required features, resulting in runtime failures.</p><p>The  framework will introduce a standard mechanism for nodes to declare their supported Kubernetes features. With the new alpha feature enabled, a Node reports the features it can support, publishing this information to the control plane through a new  field. Then, the , admission controllers and third-party components can use these declarations. For example, you can enforce scheduling and API validation constraints, ensuring that Pods run only on compatible nodes.</p><p>This approach reduces manual node labeling, improves scheduling accuracy, and prevents incompatible pod placements proactively. It also integrates with the Cluster Autoscaler for informed scale-up decisions. Feature declarations are temporary and tied to Kubernetes feature gates, enabling safe rollout and cleanup.</p><p>Targeting alpha in v1.35,  aims to solve version skew scheduling issues by making node capabilities explicit, enhancing reliability and cluster stability in heterogeneous version environments.</p><p>To learn more about this before the official documentation is published, you can read <a href=\"https://kep.k8s.io/5328\">KEP-5328</a>.</p><h3>In-place update of Pod resources</h3><p>Kubernetes is graduating in-place updates for Pod resources to General Availability (GA). This feature allows users to adjust  and  resources without restarting Pods or Containers. Previously, such modifications required recreating Pods, which could disrupt workloads, particularly for stateful or batch applications.\nPrevious Kubernetes releases already allowed you to change infrastructure resources settings (requests and limits) for existing Pods. This allows for smoother <a href=\"https://kubernetes.io/docs/concepts/workloads/autoscaling/vertical-pod-autoscale/\">vertical scaling</a>, improves efficiency, and can also simplify solution development.</p><p>The Container Runtime Interface (CRI) has also been improved, extending the  API for Windows and future runtimes while allowing  to report real-time resource configurations. Together, these changes make scaling in Kubernetes faster, more flexible, and disruption-free.\nThe feature was introduced as alpha in v1.27, graduated to beta in v1.33, and is targeting graduation to stable in v1.35.</p><p>When running microservices, Pods often require a strong cryptographic identity to authenticate with each other using mutual TLS (mTLS). While Kubernetes provides Service Account tokens, these are designed for authenticating to the API server, not for general-purpose workload identity.</p><p>Before this enhancement, operators had to rely on complex, external projects like SPIFFE/SPIRE or cert-manager to provision and rotate certificates for their workloads. But what if you could issue a unique, short-lived certificate to your Pods natively and automatically? KEP-4317 is designed to enable such native workload identity. It opens up various possibilities for securing pod-to-pod communication by allowing the  to request and mount certificates for a Pod via a projected volume.</p><p>This provides a built-in mechanism for workload identity, complete with automated certificate rotation, significantly simplifying the setup of service meshes and other zero-trust network policies. This feature was introduced as alpha in v1.34 and is targeting beta in v1.35.</p><h3>Numeric values for taints</h3><p>Kubernetes is enhancing <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\">taints and tolerations</a> by adding numeric comparison operators, such as  (Greater Than) and  (Less Than).</p><p>Previously, tolerations supported only exact () or existence () matches, which were not suitable for numeric properties such as reliability SLAs.</p><p>With this change, a Pod can use a toleration to \"opt-in\" to nodes that meet a specific numeric threshold. For example, a Pod can require a Node with an SLA taint value greater than 950 (, ).</p><p>This approach is more powerful than Node Affinity because it supports the NoExecute effect, allowing Pods to be automatically evicted if a node's numeric value drops below the tolerated threshold.</p><p>When running Pods, you can use  to drop privileges, but containers inside the pod often still run as root (UID 0). This simplicity poses a significant challenge, as that container UID 0 maps directly to the host's root user.</p><p>Before this enhancement, a container breakout vulnerability could grant an attacker full root access to the node. But what if you could dynamically remap the container's root user to a safe, unprivileged user on the host? KEP-127 specifically allows such native support for Linux User Namespaces. It opens up various possibilities for pod security by isolating container and host user/group IDs. This allows a process to have root privileges (UID 0) within its namespace, while running as a non-privileged, high-numbered UID on the host.</p><p>Released as alpha in v1.25 and beta in v1.30, this feature continues to progress through beta maturity, paving the way for truly \"rootless\" containers that drastically reduce the attack surface for a whole class of security vulnerabilities.</p><h3>Support for mounting OCI images as volumes</h3><p>When provisioning a Pod, you often need to bundle data, binaries, or configuration files for your containers.\nBefore this enhancement, people often included that kind of data directly into the main container image, or required a custom init container to download and unpack files into an . You can still take either of those approaches, of course.</p><p>But what if you could populate a volume directly from a data-only artifact in an OCI registry, just like pulling a container image? Kubernetes v1.31 added support for the  volume type, allowing Pods to pull and unpack OCI container image artifacts into a volume declaratively.</p><p>This allows for seamless distribution of data, binaries, or ML models using standard registry tooling, completely decoupling data from the container image and eliminating the need for complex init containers or startup scripts.\nThis volume type has been in beta since v1.33 and will likely be enabled by default in v1.35.</p><p>New features and deprecations are also announced in the Kubernetes release notes. We will formally announce what's new in <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.35.md\">Kubernetes v1.35</a> as part of the CHANGELOG for that release.</p><p>The Kubernetes v1.35 release is planned for . Stay tuned for updates!</p><p>You can also see the announcements of changes in the release notes for:</p><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\">Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=\"https://github.com/kubernetes/community/tree/master/communication\">community meeting</a>, and through the channels below. Thank you for your continued feedback and support.</p>",
      "contentLength": 9927,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Interview with Jan David Nose",
      "url": "https://blog.rust-lang.org/2025/11/25/interview-with-jan-david-nose/",
      "date": 1764028800,
      "author": "Pete LeVasseur",
      "guid": 35567,
      "unread": true,
      "content": "<p>On the <a href=\"https://rust-lang.org/governance/teams/launching-pad/#team-content\">Content Team</a>, we had our first whirlwind outing at RustConf 2025 in Seattle, Washington, USA. There we had a chance to speak with folks about interesting things happening in the Project and the wider community.</p><p>In this interview, <a href=\"https://github.com/MerrimanInd\">Xander Cesari</a> sits down with <a href=\"https://github.com/jdno\">Jan David Nose</a>, then one of the full-time engineers on the <a href=\"https://rust-lang.org/governance/teams/infra/\">Infrastructure Team</a>, which maintains and develops the infrastructure upon which Rust is developed and deployed -- including CI/CD tooling and crates.io.</p><p>We released this video on an accelerated timeline, some weeks ago, in light of the recent software supply chain attacks, but the interview was conducted prior to the news of compromised packages in other languages and ecosystems.</p><p>: Hey, this is Xander Cesari with the Rust Project\nContent Team, recording on the last hour of the last day of RustConf\n2025 here in Seattle. So it's been a long and amazing two days. And I'm\nsitting down here with a team member from the Rust Project Infra Team,\nthe unsung heroes of the Rust language. Want to introduce yourself and\nkind of how you got involved?</p><p>: Yeah, sure. I'm JD. Jan David is the full name, but\nespecially in international contexts, I just go with JD. I've been\nworking for the Rust Foundation for the past three years as a full-time\nemployee and I essentially hit the jackpot to work full-time on open\nsource and I've been in the Infra Team of the Rust Project for the\nwhole time. For the past two years I've led the team together with\nJake. So the Infra Team is kind of a thing that lets Rust happen and\nthere's a lot of different pieces.</p><p>: Could you give me an overview of the responsibility\nof the Infra Team?</p><p>: Sure. I think on a high level, we think about this\nin terms of, we serve two different groups of people. On one side, we\nhave users of the language, and on the other side, we really try to\nprovide good tooling for the maintainers of the language.</p><p>: Starting with the maintainer side, this is really\neverything about how Rust is built. From the moment someone makes a\ncontribution or opens a PR, we maintain the continuous integration that\nmakes sure that the PR actually works. There's a lot of bots and\ntooling helping out behind the scenes to kind of maintain a good status\nquo, a sane state. Lots of small things like triage tools on GitHub to\nset labels and ping people and these kinds of things. And that's kind\nof managed by the Infra Team at large.</p><p>: And then on the user side, we have a lot of, or the\ntwo most important things are making sure users can actually download\nRust. We don't develop crates.io, but we support the infrastructure to\nactually ship crates to users. All the downloads go through content\ndelivery networks that we provide. The same for Rust releases. So if I\ndon't do my job well, which has happened, there might be a global\noutage of crates.io and no one can download stuff. But those are kind\nof the two different buckets of services that we run and operate.</p><p>: Gotcha. So on the maintainer side, the Rust\norganization on GitHub is a large organization with a lot of activity,\na lot of code. There's obviously a lot of large code bases being\ndeveloped on GitHub, but there are not that many languages the size of\nRust being developed on GitHub. Are there unique challenges to\ndeveloping a language and the tooling that's required versus developing\nother software projects?</p><p>: I can think of a few things that have less to do\nwith the language specifically, but with some of the architecture\ndecisions that were made very early on in the life cycle of Rust. So\none of the things that actually caused a lot of headache for mostly\nGitHub, and then when they complained to us, for us as well, is that\nfor a long, long time, the index for crates.io was a Git repo on\nGitHub. As Rust started to grow, the activity on the repo became so big\nthat it actually caused some issues, I would say, in a friendly way on\nGitHub, just in terms of how much resources that single repository was\nconsuming. That then kind of started this work on a web-based,\nHTTP-based index to shift that away. That's certainly one area where\nwe've seen how Rust has struggled a little bit with the platform, but\nalso the platform provider struggled with us.</p><p>: I think for Rust itself, especially when we look at\nCI, we really want to make sure that Rust works well on all of the\ntargets and all the platforms we support. That means we have an\nextremely wide CI pipeline where, for every Tier 1 target, we want to\nrun all the tests, we want to build the release artifacts, we want to\nupload all of that to S3. We want to do as much as we reasonably can\nfor Tier 2 targets and, to a lesser extent, maybe even test some stuff\non Tier 3. That has turned into a gigantic build pipeline. Marco gave a\ntalk today on what we've done with CI over the last year. One of the\nnumbers that came out of doing the research for this talk is that we\naccumulate over three million build minutes per month, which is about\nsix years of CPU time every month.</p><p>: Especially when it comes to open source projects, I\nthink we're one of the biggest consumers of GitHub Actions in that\nsense. Not the biggest in total; there are definitely bigger commercial\nprojects. But that's a unique challenge for us to manage because we\nwant to provide as good a service as we can to the community and make\nsure that what we ship is high quality. That comes at a huge cost in\nterms of scaling. As Rust gets more popular and we want to target more\nand more platforms, this is like a problem that just continues to\ngrow.</p><p>: We'll probably never remove a lot of targets, so\nthere's an interesting challenge to think about. If it's already big\nnow, how does this look in 5 years, 10 years, 15 years, and how can we\nmake sure we can maintain the level of quality we want to ship? When\nyou build and run for a target in the CI pipeline, some of those Tier 1\ntargets you can just ask a cloud service provider to give you a VM\nrunning on that piece of hardware, but some of them are probably not\nthings that you can just run in the cloud.</p><p>: Is there some HIL (Hardware-In-the-Loop) lab\nsomewhere?</p><p>: So you're touching on a conversation that's\nhappening pretty much as we speak. So far, as part of our target tier\npolicy, there is a clause that says it needs to be able to run in CI.\nThat has meant being very selective about only promoting things to Tier\n1 that we can actually run and test. For all of this, we had a\nprerequisite that it runs on GitHub Actions. So far we've used very\nlittle hardware that is not natively supported or provided by GitHub.</p><p>: But this is exactly the point with Rust increasing\nin popularity. We just got requests to support IBM platforms and\nRISC-V, and those are not natively supported on GitHub. That has kicked\noff an internal conversation about how we even support this. How can we\nas a project enable companies that can provide us hardware to test on?\nWhat are the implications of that?</p><p>: On one side, there are interesting constraints and\nconsiderations. For example, you don't want your PRs to randomly fail\nbecause someone else's hardware is not available. We're already so\nresource-constrained on how many PRs we can merge each day that adding\nnoise to that process would really slow down contributions to Rust. On\nthe other side, there are security implications. Especially if we talk\nabout promoting something to Tier 1 and we want to build release\nartifacts on that hardware, we need to make sure that those are\nactually secure and no one sneaks a back door into the Rust compiler\ntarget for RISC-V.</p><p>: So there are interesting challenges for us,\nespecially in the world we live in where supply chain security is a\nmassive concern. We need to figure out how we can both support the\ngrowth of Rust and the growth of the language, the community, and the\necosystem at large while also making sure that the things we ship are\nreliable, secure, and performant. That is becoming an increasingly\nrelevant and interesting piece to work on. So far we've gotten away\nwith the platforms that GitHub supports, but it's really cool to see\nthat this is starting to change and people approach us and are willing\nto provide hardware, provide sponsorship, and help us test on their\nplatforms. But essentially we don't have a good answer for this yet.\nWe're still trying to figure out what this means, what we need to take\ninto consideration, and what our requirements are to use external\nhardware.</p><p>: Yeah, everyone is so excited about Rust will run\neverywhere, but there's a maintenance cost there that is almost\nexponential in scope.</p><p>: It's really interesting as well because there's a\ntension there. I think with IBM, for example, approaching us, it's an\ninteresting example. Who has IBM platforms at home? The number of users\nfor that platform is really small globally, but IBM also invests\nheavily in Rust, tries to make this happen, and is willing to provide\nthe hardware.</p><p>: For us, that leads to a set of questions. Is there\na line? Is there a certain requirement? Is there a certain amount of\nusage that a platform would need for us to promote it? Or do we say we\nwant to promote as much as we can to Tier 1? This is a conversation we\nhaven't really had to have yet. It's only now starting to creep in as\nRust is adopted more widely and companies pour serious money and\nresources into it. That's exciting to see.</p><p>: In this specific case, companies approach the Infra\nTeam to figure out how we can add their platforms to CI as a first step\ntowards Tier 1 support. But it's also a broader discussion we need to\nhave with larger parts of the Rust Project. For Tier 1 promotions, for\nexample, the Compiler Team needs to sign off, Infra needs to sign off.\nMany more people need to be involved in this discussion of how we can\nsupport the growing needs of the ecosystem at large.</p><p>: I get the feeling that's going to be a theme\nthroughout this interview.</p><p>: So one other tool that's part of this pipeline that\nI totally didn't know about for a long time, and I think a talk at a\ndifferent conference clued me into it, is Crater. It's a tool that\nattempts to run all of the Rust code it can find on the internet. Can\nyou talk about what that tool does and how it integrates into the\nrelease process?</p><p>: Whenever someone creates a pull request on GitHub\nto add a new feature or bug fix to the Rust compiler, they can start\nwhat's called a Crater run, or an experiment. Crater is effectively a\nlarge fleet of machines that tries to pull in as many crates as it can.\nIdeally, we would love to test all crates, but for a variety of reasons\nthat's not possible. Some crates simply don't build reliably, so we\nmaintain lists to exclude those. From the top of my head, I think we\ncurrently test against roughly 60% of crates.</p><p>: The experiment takes the code from your pull\nrequest, builds the Rust compiler with it, and then uses that compiler\nto build all of these crates. It reports back whether there are any\nregressions related to the change you proposed. That is a very\nimportant tool for us to maintain backwards compatibility with new\nversions and new features in Rust. It lets us ask: does the ecosystem\nstill compile if we add this feature to the compiler, and where do we\nrun into issues? Then, and this is more on the Compiler Team side,\nthere's a decision about how to proceed. Is the breakage acceptable? Do\nwe need to adjust the feature? Having Crater is what makes that\nconversation possible because it gives us real data on the impact on\nthe wider ecosystem.</p><p>: I think that's so interesting because as more and\nmore companies adopt Rust, they're asking whether the language is going\nto be stable and backward compatible. You hear about other programming\nlanguages that had a big version change that caused a lot of drama and\ncode changes. The fact that if you have code on crates.io, the Compiler\nTeam is probably already testing against it for backwards compatibility\nis pretty reassuring.</p><p>: Yeah, the chances are high, I would say. Especially\nlooking at the whole Python 2 to Python 3 migration, I think as an\nindustry we've learned a lot from those big version jumps. I can't\nreally speak for the Compiler Team because I'm not a member and I\nwasn't involved in the decision-making, but I feel this is one of the\nreasons why backwards compatibility is such a big deal in Rust's\ndesign. We want to make it as painless as possible to stay current,\nstay up to date, and make sure we don't accidentally break the language\nor create painful migration points where the entire ecosystem has to\nmove at once.</p><p>: Do you know if there are other organizations pulling\nin something like Crater and running it on their own internal crate\nrepositories, maybe some of the big tech companies or other compiler\ndevelopers or even other languages? Or is this really bespoke for the\nRust compiler team?</p><p>: I don't know of anyone who runs Crater itself as a\ntool. Crater is built on a sandboxing framework that we also use in\nother places. For example, docs.rs uses some of the same underlying\ninfrastructure to build all of the documentation. We try to share as\nmuch as we can of the functionality that exists in Crater, but I'm not\naware of anyone using Crater in the same way we do.</p><p>: Gotcha. The other big part of your job is that the\nInfra Team works on supporting maintainers, but it also supports users\nand consumers of Rust who are pulling from crates.io. It sounds like\ncrates.io is not directly within your team, but you support a lot of\nthe backend there.</p><p>: Yeah, exactly. crates.io has its own team, and that\nteam maintains the web application and the APIs. The crates themselves,\nall the individual files that people download, are hosted within our\ninfrastructure. The Infra Team maintains the content delivery network\nthat sits in front of that. Every download of a crate goes through\ninfrastructure that we maintain. We collaborate very closely with the\ncrates.io team on this shared interface. They own the app and the API,\nand we make sure that the files get delivered to the end user.</p><p>: So it sounds like there's a lot of verification of\nthe files that get uploaded and checks every time someone pushes a new\nversion to crates.io. That part all happens within crates.io as an\napplication.</p><p>: Cargo uses the crates.io API to upload the crate\nfile. crates.io has a lot of internal logic to verify that it is valid\nand that everything looks correct. For us, as the Infra Team, we treat\nthat as a black box. crates.io does its work, and if it is happy with\nthe upload, it stores the file in S3. From that point onward,\ninfrastructure makes sure that the file is accessible and can be\ndownloaded so people can start using your crate.</p><p>: In this theme of Rust being a bit of a victim of its\nown success, I assume all of the traffic graphs and download graphs are\nvery much up and to the right.</p><p>: On the Foundation side, one of our colleagues likes\nto check how long it takes for one billion downloads to happen on\ncrates.io, and that number has been falling quickly. I don't remember\nwhat it was three years ago, but it has come down by orders of\nmagnitude. In our download traffic we definitely see exponential\ngrowth. Our traffic tends to double year over year, and that trend has\nbeen pretty stable. It really seems like Rust is getting a lot of\nadoption in the ecosystem and people are using it for more and more\nthings.</p><p>: How has the Infra Team scaled with that? Are you\nstaying ahead of it, or are there a lot of late nights?</p><p>: There have definitely been late nights. In the\nthree years I've been working in the Infra Team, every year has had a\ndifferent theme that was essentially a fire to put out.</p><p>: It changes because we fix one thing and then the\nnext thing breaks. So far, luckily, those fires have been mostly\nsequential, not parallel. When I joined, bandwidth was the big topic.\nOver the last year, it has been more about CI. About three years ago,\nwe hit this inflection point where traffic was doubling and the\nsponsorship capacity we had at the time was reaching its limits.</p><p>: Two or three years ago, Fastly welcomed us into\ntheir Fast Forward program and has been sponsoring all of our bandwidth\nsince then. That has mostly helped me sleep at night. It has been a\nvery good relationship. They have been an amazing partner and have\nhelped us at every step to remove the fear that we might hit limits.\nThey are very active in the open source community at large; most\nfamously they also sponsor PyPI and the Python ecosystem, compared to\nwhich we're a tiny fish in a very big pond. That gives us a lot of\nconfidence that we can sustain this growth and keep providing crates\nand releases at the level of quality people expect.</p><p>: In some ways, Rust did such a good job of making all\nof that infrastructure feel invisible. You just type Cargo commands\ninto your terminal and it feels magical.</p><p>: I'm really happy about that. It's an interesting\naspect of running an infrastructure team in open source. If you look at\nthe ten-year history since the first stable release, or even the\nfifteen years since Rust really started, infrastructure was\nvolunteer-run for most of that time. I've been here for three years,\nand I was the first full-time infrastructure engineer. So for ten to\ntwelve years, volunteers ran the infrastructure.</p><p>: For them, it was crucial that things just worked,\nbecause you can't page volunteers in the middle of the night because a\nserver caught fire or downloads stopped working. From the beginning,\nour infrastructure has been designed to be as simple and as reliable as\npossible. The same is true for our CDNs. I always feel a bit bad\nbecause Fastly is an amazing sponsor. Every time we meet them at\nconferences or they announce new features, they ask whether we want to\nuse them or talk about how we use Fastly in production. And every time\nI have to say: we have the simplest configuration possible. We set some\nHTTP headers. That's pretty much it.</p><p>: It's a very cool platform, but we use the smallest\nset of features because we need to maintain all of this with a\nvery small team that is mostly volunteer-based. Our priority has always\nbeen to keep things simple and reliable and not chase every fancy new\ntechnology, so that the project stays sustainable.</p><p>: Volunteer-based organizations seem to have to care\nabout work-life balance, which is probably terrific, and there are\nlessons to be learned there.</p><p>: Yeah, it's definitely a very interesting\nenvironment to work in. It has different rules than corporations or\ncommercial teams. We have to think about how much work we can do in a\ngiven timeframe in a very different way, because it's unpredictable\nwhen volunteers have time, when they're around, and what is happening\nin their lives.</p><p>: Over the last few years, we've tried to reduce the\nnumber of fires that can break out. And when they do happen, we try to\nshield volunteers from them and take that work on as full-time\nemployees. That started with me three years ago. Last year Marco\njoined, which increased the capacity we have, because there is so much\nto do on the Infra side that even with me working full-time, we simply\ndid not have enough people.</p><p>: So you're two full-time and everything else is\nvolunteer.</p><p>: Exactly. The team is around eight people. Marco and\nI work full-time and are paid by the Rust Foundation to focus\nexclusively on infrastructure. Then we have a handful of volunteers who\nwork on different things.</p><p>: Because our field of responsibility is so wide, the\nInfra Team works more in silos than other teams might. We have people\nwho care deeply about very specific parts of the infrastructure.\nOtherwise there is simply too much to know for any one person. It has\nbeen a really nice mix, and it's amazing to work with the people on the\nteam.</p><p>: As someone who is privileged enough to work\nfull-time on this and has the time and resources, we try to bear the\nbigger burden and create a space that is fun for volunteers to join. We\nwant them to work on exciting things where there is less risk of\nsomething catching fire, where it's easier to come in, do a piece of\nwork, and then step away. If your personal life takes over for two\nweeks, that's okay, because someone is there to make sure the servers\nand the lights stay on.</p><p>: A lot of that work lives more on the maintainer\nside: the GitHub apps, the bots that help with triage. It's less risky\nif something goes wrong there. On the user side, if you push the wrong\nDNS setting, as someone might have done, you can end up in a situation\nwhere for 30 minutes no one can download crates. And in this case,\n\"no one\" literally means no user worldwide. That's not\nan experience I want volunteers to have. It's extremely stressful and\nwas ultimately one of the reasons I joined in the first place—there was\na real feeling of burnout from carrying that responsibility.</p><p>: It's easier to carry that as a full-timer. We have\nmore time and more ways to manage the stress. I'm honestly extremely\namazed by what the Infra Team was able to do as volunteers. It's\nunbelievable what they built and how far they pushed Rust to get to\nwhere we are now.</p><p>: I think anyone who's managing web traffic in 2025 is\ntalking about traffic skyrocketing due to bots and scrapers for AI or\nother purposes. Has that hit the Rust network as well?</p><p>: Yeah, we've definitely seen that. It's handled by a\nslightly different team, but on the docs.rs side in particular we've\nseen crawlers hit us hard from time to time, and that has caused\nnoticeable service degradation. We're painfully aware of the increase\nin traffic that comes in short but very intense bursts when crawlers go\nwild.</p><p>: That introduces a new challenge for our\ninfrastructure. We need to figure out how to react to that traffic and\nprotect our services from becoming unavailable to real users who want\nto use docs.rs to look up something for their work. On the CDN side,\nour providers can usually handle the traffic. It is more often the\napplication side where things hurt.</p><p>: On the CDN side we also see people crawling\ncrates.io, presumably to vacuum up the entire crates ecosystem into an\nLLM. Fortunately, over the last two years we've done a lot of work to\nmake sure crates.io as an application is less affected by these traffic\nspikes. Downloads now bypass crates.io entirely and go straight to the\nCDN, so the API is not hit by these bursts. In the past, this would\nhave looked like a DDoS attack, with so many requests from so many\nsources that we couldn't handle it.</p><p>: We've done a lot of backend work to keep our stack\nreliable, but it's definitely something that has changed the game over\nthe last year. We can clearly see that crawlers are much more active\nthan before.</p><p>: That makes sense. I'm sure Fastly is working on this\nas well. Their business has to adapt to be robust to this new internet.</p><p>: Exactly. For example, one of the conversations\nwe're having right now is about docs.rs. It's still hosted on AWS\nbehind CloudFront, but we're talking about putting it behind Fastly\nbecause through Fastly we get features like bot protection that can\nhelp keep crawlers out.</p><p>: This is a good example of how our conversations\nhave changed in the last six months. At the start of the year I did not\nthink this would be a topic we would be discussing. We were focused on\nother things. For docs.rs we have long-term plans to rebuild the\ninfrastructure that powers it, and I expected us to spend our energy\nthere. But with the changes in the industry and everyone trying to\naccumulate as much data as possible, our priorities have shifted. The\nproblems we face and the order in which we tackle them have changed.</p><p>: And I assume as one of the few paid members of a\nmostly volunteer team, you often end up working on the fires, not the\ninteresting next feature that might be more fun.</p><p>: That is true, although it sounds a bit negative to\nsay I only get to work on fires. Sometimes it feels like that because,\nas with any technology stack, there is a lot of maintenance overhead.\nWe definitely pay that price on the infrastructure side.</p><p>: Marco, for example, spent time this year going\nthrough all the servers we run, cataloging them, and making sure\nthey're patched and on the latest operating system version. We updated\nour Ubuntu machines to the latest LTS. It feels a bit like busy\nwork—you just have to do it because it's important and necessary, but\nit's not the most exciting project.</p><p>: On the other hand, when it comes to things like CDN\nconfiguration and figuring out how bot protection features work and\nwhether they are relevant to us, that is also genuinely interesting\nwork. It lets us play with new tools vendors provide, and we're working\non challenges that the wider industry is facing. How do you deal with\nthis new kind of traffic? What are the implications of banning bots?\nHow high is the risk of blocking real users? Sometimes someone just\nmisconfigures a curl script, and from the outside it looks like they're\ncrawling our site.</p><p>: So it's an interesting field to work in, figuring\nout how we can use new features and address new challenges. That keeps\nit exciting even for us full-timers who do more of the\n\"boring\" work. We get to adapt alongside how the world\naround us is changing. If there's one constant, it's change.</p><p>: Another ripped-from-the-headlines change around this\ntopic is software supply chain security, and specifically xz-utils and\nthe conversation around open source security. How much has that changed\nthe landscape you work in?</p><p>: The xz-utils compromise was scary. I don't want to\ncall it a wake-up call, because we've been aware that supply chain\nsecurity is a big issue and this was not the first compromise. But the\nway it happened felt very unsettling. You saw an actor spend a year and\na half building social trust in an open source project and then using\nthat to introduce a backdoor.</p><p>: Thinking about that in the context of Rust: every\nteam in the project talks about how we need more maintainers, how\nthere's too much workload on the people who are currently contributing,\nand how Rust's growth puts strain on the organization as a whole. We\nwant to be an open and welcoming project, and right now we also need to\nbring new people in. If someone shows up and says, \"I'm\nwilling to help, please onboard me,\" and they stick around for\na year and then do something malicious, we would be susceptible to\nthat. I don't think this is unique to Rust. This is an inherent problem\nin open source.</p><p>: Yeah, it's antithetical to the culture.</p><p>: Exactly. So we're trying to think through how we,\nas a project and as an ecosystem, deal with persistent threat actors\nwho have the time and resources to play a long game. Paying someone to\nwork full-time on open source for a year is a very different threat\nmodel than what we used to worry about.</p><p>: I used to joke that the biggest threat to crates.io\nwas me accidentally pulling the plug on a CDN. I think that has\nchanged. Today the bigger threat is someone managing to insert\nmalicious code into our releases, our supply chain, or crates.io\nitself. They could find ways to interfere with our systems in ways\nwe're simply not prepared for, where, as a largely volunteer\norganization, we might be too slow to react to a new kind of attack.</p><p>: Looking back over the last three years, this shift\nbecame very noticeable, especially after the first year. Traffic was\ndoubling, Rust usage was going up a lot, and there were news stories\nabout Rust being used in the Windows kernel, in Android, and in parts\nof iOS. Suddenly Rust is everywhere. If you want to attack\n\"everywhere,\" going after Rust becomes attractive.\nThat definitely puts a target on our back and has changed the game.</p><p>: I'm very glad the Rust Foundation has a dedicated\nsecurity engineer who has done a lot of threat modeling and worked with\nus on infrastructure security. There's also a lot of work happening\nspecifically around the crates ecosystem and preventing supply chain\nattacks through crates. Luckily, it's not something the Infra side has\nto solve alone. But it is getting a lot more attention, and I think it\nwill be one of the big challenges for the future: how a mostly\nvolunteer-run project keeps up with this looming threat.</p><p>: And it is the industry at large. This is not a\nunique problem to the Rust package manager. All package registries,\nfrom Python to JavaScript to Nix, deal with this. Is there an\nindustry-wide conversation about how to help each other out and share\nlearnings?</p><p>: Yeah, there's definitely a lot happening. I have to\nsmile a bit because, with a lot of empathy but also a bit of relief, we\nsometimes share news when another package ecosystem gets compromised.\nIt is a reminder that it's not just us, sometimes it's npm this time.</p><p>: We really try to stay aware of what's happening in\nthe industry and in other ecosystems: what new threats or attack\nvectors are emerging, what others are struggling with. Sometimes that\nis security; sometimes it's usability. A year and a half ago, for\nexample, npm had the \"everything\" package where\nsomeone declared every package on npm as a dependency, which blew up\nthe index. We look at incidents like that and ask whether crates.io\nwould struggle with something similar and whether we need to make\nchanges.</p><p>: On the security side we also follow closely what\nothers are doing. In the packaging community, the different package\nmanagers are starting to come together more often to figure out which\nproblems everyone shares. There is a bit of a joke that we're all just\nshipping files over the internet. Whether it's an npm package or a\ncrate, ultimately it's a bunch of text files in a zip. So from an\ninfrastructure perspective the problems are very similar.</p><p>: These communities are now talking more about what\nproblems PyPI has, what problems crates.io has, what is happening in\nthe npm space. One thing every ecosystem has seen—even the very\nestablished ones—is a big increase in bandwidth needs, largely\nconnected to the emergence of AI. PyPI, for example, publishes download\ncharts, and it's striking. Python had steady growth—slightly\nexponential, but manageable—for many years. Then a year or two ago you\nsee a massive hockey stick. People discovered that PyPI was a great\ndistribution system for their models. There were no file size limits at\nthe time, so you could publish precompiled GPU models there.</p><p>: That pattern shows up everywhere. It has kicked off\na new era for packaging ecosystems to come together and ask: in a time\nwhere open source is underfunded and traffic needs keep growing, how\ncan we act together to find solutions to these shared problems?\ncrates.io is part of those conversations. It's interesting to see how\nwe, as an industry, share very similar problems across\necosystems—Python, npm, Rust, and others.</p><p>: With a smaller, more hobbyist-focused community, you\ncan have relaxed rules about what goes into your package manager.\nEveryone knows the spirit of what you're trying to do and you can get\naway without a lot of hard rules and consequences. Is the Rust world\ngoing to have to think about much harder rules around package sizes,\nallowed files, and how you're allowed to distribute things?</p><p>: Funnily enough, we're coming at this from the\nopposite direction. Compared to other ecosystems, we've always had\nfairly strict limits. A crate can be at most around ten megabytes in\nsize. There are limits on what kinds of files you can put in there.\nIronically, those limits have helped us keep traffic manageable in this\nperiod.</p><p>: At the same time, there is a valid argument that\nthese limits may not serve all Rust use cases. There are situations\nwhere you might want to include something precompiled in your crate\nbecause it is hard to compile locally, takes a very long time, or\ndepends on obscure headers no one has. I don't think we've reached the\nfinal state of what the crates.io package format should look like.</p><p>: That has interesting security implications. When we\ntalk about precompiled binaries or payloads, we all have that little\nvoice in our head every time we see a curl | sh command: can I trust\nthis? The same is true if you download a crate that contains a\nprecompiled blob you cannot easily inspect.</p><p>: The Rust Foundation is doing a lot of work and\nresearch here. My colleague Adam, who works on the crates.io team, is\nworking behind the scenes to answer some of these questions. For\nexample: what kind of security testing can we do before we publish\ncrates to make sure they are secure and don't contain malicious\npayloads? How do we surface this information? How do we tell a\npublisher that they included files that are not allowed? And from the\nuser's perspective, when you visit crates.io, how can you judge how\nwell maintained and how secure a crate is?</p><p>: Those conversations are happening quite broadly in\nthe ecosystem. On the Infra side we're far down the chain. Ultimately\nwe integrate with whatever security scanning infrastructure crates.io\nbuilds. We don't have to do the security research ourselves, but we do\nhave to support it.</p><p>: There's still a lot that needs to happen. As\nawesome as Rust already is, and as much as I love using it, it's\nimportant to remember that we're still a very young ecosystem. Python\nis now very mature and stable, but it's more than 25 years old. Rust is\nabout ten years old as a stable language. We still have a lot to learn\nand figure out.</p><p>: Is the Rust ecosystem running into problems earlier\nthan other languages because we're succeeding at being foundational\nsoftware and Rust is used in places that are even more\nsecurity-critical than other languages, so you have to hit these hard\nproblems earlier than the Python world did?</p><p>: I think that's true. Other ecosystems probably had\nmore time to mature and answer these questions. We're operating on a\nmore condensed timeline. There is also simply more happening now. Open\nsource has been very successful; it's everywhere. That means there are\nmore places where security is critical.</p><p>: So this comes with the success of open source, with\nwhat is happening in the ecosystem at large, and with the industry\nwe're in. It does mean we have less time to figure some things out. On\nthe flip side, we also have less baggage. We have less technical debt\nand fifteen fewer years of accumulated history. That lets us be on the\nforefront in some areas, like how a package ecosystem can stay secure\nand what infrastructure a 21st century open source project needs.</p><p>: Here I really want to call out the Rust Foundation.\nThey actively support this work: hiring people like Marco and me to\nwork full-time on infrastructure, having Walter and Adam focus heavily\non security, and as an organization taking supply chain considerations\nvery seriously. The Foundation also works with other ecosystems so we\ncan learn and grow together and build a better industry.</p><p>: Behind the scenes, colleagues constantly work to\nopen doors for us as a relatively young language, so we can be part of\nthose conversations and sit at the table with other ecosystems. That\nlets us learn from what others have already gone through and also help\nshape where things are going. Sustainability is a big part of that: how\ndo we fund the project long term? How do we make sure we have the human\nresources and financial resources to run the infrastructure and support\nmaintainers? I definitely underestimated how much of my job would be\nrelationship management and budget planning, making sure credits last\nuntil new ones arrive.</p><p>: Most open core business models give away the thing\nthat doesn't cost much—the software—and charge for the thing that\nscales with use—the service. In Rust's case, it's all free, which is\nexcellent for adoption, but it must require a very creative perspective\non the business side.</p><p>: Yeah, and that's where different forces pull in\nopposite directions. As an open source project, we want everyone to be\nable to use Rust for free. We want great user experience. When we talk\nabout downloads, there are ways for us to make them much cheaper, but\nthat might mean hosting everything in a single geographic location.\nThen everyone, including people in Australia, would have to download\nfrom, say, Europe, and their experience would get much worse.</p><p>: Instead, we want to use services that are more\nexpensive but provide a better experience for Rust users. There's a\nreal tension there. On one side we want to do the best we can; on the\nother side we need to be realistic that this costs money.</p><p>: I had been thinking of infrastructure as a binary:\nit either works or it doesn't. But you're right, it's a slider. You can\npick how much money you want to spend and what quality of service you\nget. Are there new technologies coming, either for the Rust Infra Team\nor the packaging world in general, to help with these security\nproblems? New sandboxing technologies or higher-level support?</p><p>: A lot of people are working on this problem from\ndifferent angles. Internally we've talked a lot about it, especially in\nthe context of Crater. Crater pulls in all of those crates to build\nthem and get feedback from the Rust compiler. That means if someone\npublishes malicious code, we will download it and build it.</p><p>: In Rust this is a particular challenge because\nbuild scripts can essentially do anything on your machine. For us that\nmeans we need strong sandboxing. We've built our own sandboxing\nframework so every crate build runs in an isolated container, which\nprevents malicious code from escaping and messing with the host systems.</p><p>: We feel that pain in Crater, but if we can solve it\nin a way that isn't exclusive to Crater—if it also protects user\nmachines from the same vulnerabilities—that would be ideal. People like\nWalter on the Foundation side are actively working on that. I'm sure\nthere are conversations in the Cargo and crates teams as well, because\nevery team that deals with packages sees a different angle of the\nproblem. We all have to come together to solve it, and there is a lot\nof interesting work happening in that area.</p><p>: I hope help is coming.</p><p>: I'm optimistic.</p><p>: We have this exponential curve with traffic and\neverything else. It seems like at some point it has to taper off.</p><p>: We'll see. Rust is a young language. I don't know\nwhen that growth will slow down. I think there's a good argument that\nit will continue for quite a while as adoption grows.</p><p>: Being at a conference like RustConf, it's exciting\nto see how the mix of companies has changed over time. We had a talk\nfrom Rivian on how they use Rust in their cars. We've heard from other\ncar manufacturers exploring it. Rust is getting into more and more\napplications that a few years ago would have been hard to imagine or\nwhere the language simply wasn't mature enough yet.</p><p>: As that continues, I think we'll see new waves of\ngrowth that sustain the exponential curve we currently have, because\nwe're moving into domains that are new for us. It's amazing to see who\nis talking about Rust and how they're using it, sometimes in areas like\nspace that you wouldn't expect.</p><p>: I'm very optimistic about Rust's future. With this\nincrease in adoption, we'll see a lot of interesting lessons about how\nto use Rust and a lot of creative ideas from people building with it.\nWith more corporate adoption, I also expect a new wave of investment\ninto the ecosystem: companies paying people to work full-time on\ndifferent parts of Rust, both in the ecosystem and in the core project.\nI'm very curious what the next ten years will look like, because I\ngenuinely don't know.</p><p>: The state of Rust right now does feel a bit like the\ndog that caught the car and now doesn't know what to do with it.</p><p>: Yeah, I think that's a good analogy. Suddenly we're\nin a situation where we realize we haven't fully thought through every\nconsequence of success. It's fascinating to see how the challenges\nchange every year. We keep running into new growing pains where\nsomething that wasn't an issue a year ago suddenly becomes one because\ngrowth keeps going up.</p><p>: We're constantly rebuilding parts of our\ninfrastructure to keep up with that growth, and I don't see that\nstopping soon. As a user, that makes me very excited. With the language\nand the ecosystem growing at this pace, there are going to be very\ninteresting things coming that I can't predict today.</p><p>: For the project, it also means there are real\nchallenges: financing the infrastructure we need, finding maintainers\nand contributors, and creating a healthy environment where people can\nwork without burning out. There is a lot of work to be done, but it's\nan exciting place to be.</p><p>: Well, thank you for all your work keeping those\nmagic Cargo commands I can type into my terminal just working in the\nbackground. If there's any call to action from this interview, it's\nthat if you're a company using Rust, maybe think about donating to keep\nthe Infra Team working.</p><p>: We always love new Rust Foundation members.\nEspecially if you're a company, that's one of the best ways to support\nthe work we do. Membership gives us a budget we can use either to fund\npeople who work full-time on the project or to fill gaps in our\ninfrastructure sponsorship where we don't get services for free and\nhave to pay real money.</p><p>: And if you're not a company, we're always looking\nfor people to help out. The Infra Team has a lot of Rust-based bots and\nother areas where people can contribute relatively easily.</p><p>: Small scoped bots that you can wrap your head around\nand help out with.</p><p>: Exactly. It is a bit harder on the Infra side\nbecause we can't give people access to our cloud infrastructure. There\nare areas where it's simply not possible to contribute as a volunteer\nbecause you can't have access to the production systems. But there is\nstill plenty of other work that can be done.</p><p>: Like every other team in the project, we're a bit\nshort-staffed. So when you're at conferences, come talk to me or Marco.\nWe have work to do.</p><p>: Well, thank you for doing the work that keeps Rust\nrunning.</p><p>: I'm happy to.</p><p>: Awesome. Thank you so much.</p>",
      "contentLength": 41849,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Kubernetes Configuration Good Practices",
      "url": "https://kubernetes.io/blog/2025/11/25/configuration-good-practices/",
      "date": 1764028800,
      "author": "",
      "guid": 35566,
      "unread": true,
      "content": "<p>Configuration is one of those things in Kubernetes that seems small until it's not. Configuration is at the heart of every Kubernetes workload.\nA missing quote, a wrong API version or a misplaced YAML indent can ruin your entire deploy.</p><p>This blog brings together tried-and-tested configuration best practices. The small habits that make your Kubernetes setup clean, consistent and easier to manage.\nWhether you are just starting out or already deploying apps daily, these are the little things that keep your cluster stable and your future self sane.</p><p><em>This blog is inspired by the original <em>Configuration Best Practices</em> page, which has evolved through contributions from many members of the Kubernetes community.</em></p><h2>General configuration practices</h2><h3>Use the latest stable API version</h3><p>Kubernetes evolves fast. Older APIs eventually get deprecated and stop working. So, whenever you are defining resources, make sure you are using the latest stable API version.\nYou can always check with</p><p>This simple step saves you from future compatibility issues.</p><h3>Store configuration in version control</h3><p>Never apply manifest files directly from your desktop. Always keep them in a version control system like Git, it's your safety net.\nIf something breaks, you can instantly roll back to a previous commit, compare changes or recreate your cluster setup without panic.</p><h3>Write configs in YAML not JSON</h3><p>Write your configuration files using YAML rather than JSON. Both work technically, but YAML is just easier for humans. It's cleaner to read and less noisy and widely used in the community.</p><p>YAML has some sneaky gotchas with boolean values:\nUse only  or .\nDon't write , ,  or .\nThey might work in one version of YAML but break in another. To be safe, quote anything that looks like a Boolean (for example ).</p><h3>Keep configuration simple and minimal</h3><p>Avoid setting default values that are already handled by Kubernetes. Minimal manifests are easier to debug, cleaner to review and less likely to break things later.</p><p>If your Deployment, Service and ConfigMap all belong to one app, put them in a single manifest file.\nIt's easier to track changes and apply them as a unit.\nSee the <a href=\"https://github.com/kubernetes/examples/blob/master/web/guestbook/all-in-one/guestbook-all-in-one.yaml\">Guestbook all-in-one.yaml</a> file for an example of this syntax.</p><p>You can even apply entire directories with:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>One command and boom everything in that folder gets deployed.</p><p>Manifest files are not just for machines, they are for humans too. Use annotations to describe why something exists or what it does. A quick one-liner can save hours when debugging later and also allows better collaboration.</p><p>The most helpful annotation to set is <code>kubernetes.io/description</code>. It's like using comment, except that it gets copied into the API so that everyone else can see it even after you deploy.</p><h2>Managing Workloads: Pods, Deployments, and Jobs</h2><p>A common early mistake in Kubernetes is creating Pods directly. Pods work, but they don't reschedule themselves if something goes wrong.</p><p> (Pods not managed by a controller, such as <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\">Deployment</a> or a <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSet</a>) are fine for testing, but in real setups, they are risky.</p><p>Why?\nBecause if the node hosting that Pod dies, the Pod dies with it and Kubernetes won't bring it back automatically.</p><h3>Use Deployments for apps that should always be running</h3><p>A Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is always available, and specifies a strategy to replace Pods (such as <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment\">RollingUpdate</a>), is almost always preferable to creating Pods directly.\nYou can roll out a new version, and if something breaks, roll back instantly.</p><h3>Use Jobs for tasks that should finish</h3><p>A <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/\">Job</a> is perfect when you need something to run once and then stop like database migration or batch processing task.\nIt will retry if the pods fails and report success when it's done.</p><h2>Service Configuration and Networking</h2><p>Services are how your workloads talk to each other inside (and sometimes outside) your cluster. Without them, your pods exist but can't reach anyone. Let's make sure that doesn't happen.</p><h3>Create Services before workloads that use them</h3><p>When Kubernetes starts a Pod, it automatically injects environment variables for existing Services.\nSo, if a Pod depends on a Service, create a <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">Service</a> its corresponding backend workloads (Deployments or StatefulSets), and before any workloads that need to access it.</p><p>For example, if a Service named foo exists, all containers will get the following variables in their initial environment:</p><pre tabindex=\"0\"><code>FOO_SERVICE_HOST=&lt;the host the Service runs on&gt;\nFOO_SERVICE_PORT=&lt;the port the Service runs on&gt;\n</code></pre><p>DNS based discovery doesn't have this problem, but it's a good habit to follow anyway.</p><h3>Use DNS for Service discovery</h3><p>If your cluster has the DNS <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/addons/\">add-on</a> (most do), every Service automatically gets a DNS entry. That means you can access it by name instead of IP:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>It's one of those features that makes Kubernetes networking feel magical.</p><h3>Avoid  and  unless absolutely necessary</h3><p>You'll sometimes see these options in manifests:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>But here's the thing:\nThey tie your Pods to specific nodes, making them harder to schedule and scale. Because each &lt;, , &gt; combination must be unique. If you don't specify the  and  explicitly, Kubernetes will use  as the default  and  as the default .\nUnless you're debugging or building something like a network plugin, avoid them.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><h3>Use headless Services for internal discovery</h3><p>Sometimes, you don't want Kubernetes to load balance traffic. You want to talk directly to each Pod. That's where <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#headless-services\">headless Services</a> come in.</p><p>You create one by setting .\nInstead of a single IP, DNS gives you a list of all Pods IPs, perfect for apps that manage connections themselves.</p><h2>Working with labels effectively</h2><p><a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\">Labels</a> are key/value pairs that are attached to objects such as Pods.\nLabels help you organize, query and group your resources.\nThey don't do anything by themselves, but they make everything else from Services to Deployments work together smoothly.</p><p>Good labels help you understand what's what, even after months later.\nDefine and use <a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\">labels</a> that identify semantic attributes of your application or Deployment.\nFor example;</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><ul><li> : what the app is</li><li> : which layer it belongs to (frontend/backend)</li><li> : which stage it's in (test/prod)</li></ul><p>You can then use these labels to make powerful selectors.\nFor example:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>This will list all frontend Pods across your cluster, no matter which Deployment they came from.\nBasically you are not manually listing Pod names; you are just describing what you want.\nSee the <a href=\"https://github.com/kubernetes/examples/tree/master/web/guestbook/\">guestbook</a> app for examples of this approach.</p><p>Kubernetes actually recommends a set of <a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/\">common labels</a>. It's a standardized way to name things across your different workloads or projects.\nFollowing this convention makes your manifests cleaner, and it means that tools such as <a href=\"https://headlamp.dev/\">Headlamp</a>, <a href=\"https://github.com/kubernetes/dashboard#introduction\">dashboard</a>, or third-party monitoring systems can all\nautomatically understand what's running.</p><h3>Manipulate labels for debugging</h3><p>Since controllers (like ReplicaSets or Deployments) use labels to manage Pods, you can remove a label to “detach” a Pod temporarily.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>The  part removes the label key .\nOnce that happens, the controller won’t manage that Pod anymore.\nIt’s like isolating it for inspection, a “quarantine mode” for debugging. To interactively remove or add labels, use <a href=\"https://kubernetes.io/docs/reference/kubectl/generated/kubectl_label/\"></a>.</p><p>You can then check logs, exec into it and once done, delete it manually.\nThat’s a super underrated trick every Kubernetes engineer should know.</p><p>These small tips make life much easier when you are working with multiple manifest files or clusters.</p><p>Instead of applying one file at a time, apply the whole folder:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>This command looks for ,  and  files in that folder and applies them all together.\nIt's faster, cleaner and helps keep things grouped by app.</p><h3>Use label selectors to get or delete resources</h3><p>You don't always need to type out resource names one by one.\nInstead, use <a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors\">selectors</a> to act on entire groups at once:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>It's especially useful in CI/CD pipelines, where you want to clean up test resources dynamically.</p><h3>Quickly create Deployments and Services</h3><p>For quick experiments, you don't always need to write a manifest. You can spin up a Deployment right from the CLI:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>Then expose it as a Service:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>Cleaner configuration leads to calmer cluster administrators.\nIf you stick to a few simple habits: keep configuration simple and minimal, version-control everything,\nuse consistent labels, and avoid relying on naked Pods, you'll save yourself hours of debugging down the road.</p><p>The best part?\nClean configurations stay readable. Even after months, you or anyone on your team can glance at them and know exactly what’s happening.</p>",
      "contentLength": 8447,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Switching to Rust's own mangling scheme on nightly",
      "url": "https://blog.rust-lang.org/2025/11/20/switching-to-v0-mangling-on-nightly/",
      "date": 1763596800,
      "author": "David Wood",
      "guid": 34592,
      "unread": true,
      "content": "<p> rustc will use its own \"v0\" mangling scheme by default on nightly\nversions instead of the previous default, which re-used C++'s mangling\nscheme, starting in </p><p>When Rust is compiled into object files and binaries, each item (functions,\nstatics, etc) must have a globally unique  \"symbol\" identifying it.</p><p>In C, the symbol name of a function is just the name that the function was\ndefined with, such as . This is straightforward and easy to\nunderstand, but requires that each item have a globally unique name\nthat doesn't overlap with any symbols from libraries that it is linked\nagainst. If two items had the same symbol then when the linker tried to resolve\na symbol to an address in memory (of a function, say), then it wouldn't know\nwhich symbol is the correct one.</p><p>Languages like Rust and C++ define \"symbol mangling schemes\", leveraging information\nfrom the type system to give each item a unique symbol name. Without this, it would be\npossible to produce clashing symbols in a variety of ways - for example, every\ninstantiation of a generic or templated function (or an overload in C++), which all\nhave the same name in the surface language would end up with clashing symbols; or\nthe same name in different modules, such as  and  would have clashing\nsymbols.</p><p>Rust originally used a symbol mangling scheme based on the\n<a href=\"https://refspecs.linuxbase.org/cxxabi-1.86.html#mangling\">Itanium ABI's name mangling scheme</a> used by C++ (sometimes). Over\nthe years, it was extended in an inconsistent and ad-hoc way to support Rust\nfeatures that the mangling scheme wasn't originally designed for. Rust's current legacy\nmangling scheme has a number of drawbacks:</p><ul><li>Information about generic parameter instantiations is lost during mangling</li><li>It is internally inconsistent - some paths use an Itanium ABI-style encoding\nbut some don't</li><li>Symbol names can contain  characters which aren't supported on all platforms</li><li>Symbol names include an opaque hash which depends on compiler internals and\ncan't be easily replicated by other compilers or tools</li><li>There is no straightforward way to differentiate between Rust and C++ symbols</li></ul><p>If you've ever tried to use Rust with a debugger or a profiler and found it hard\nto work with because you couldn't work out which functions were which, it's probably\nbecause information was being lost in the mangling scheme.</p><p>Rust's compiler team started working on our own mangling scheme back in 2018\nwith <a href=\"https://rust-lang.github.io/rfcs/2603-rust-symbol-name-mangling-v0.html\">RFC 2603</a> (see the <a href=\"https://doc.rust-lang.org/nightly/rustc/symbol-mangling/v0.html\">\"v0 Symbol Format\"</a> chapter in\nrustc book for our current documentation on the format). Our \"v0\" mangling scheme has\nmultiple advantageous properties:</p><ul><li>An unambiguous encoding for everything that can end up in a binary's symbol table</li><li>Information about generic parameters are encoded in a reversible way</li><li>Mangled symbols are decodable such that it should be possible to identify concrete\ninstances of generic functions</li><li>It doesn't rely on compiler internals</li><li>Symbols are restricted to only , ,  and , helping ensure\ncompatibility with tools on varied platforms</li><li>It tries to stay efficient and avoid unnecessarily long names and\ncomputationally-expensive decoding</li></ul><p>However, rustc is not the only tool that interacts with Rust symbol names: the\naforementioned debuggers, profilers and other tools all need to be updated to\nunderstand Rust's v0 symbol mangling scheme so that Rust's users can continue\nto work with Rust binaries using all the tools they're used to without having\nto look at mangled symbols. Furthermore, all of those tools need to have new\nreleases cut and then those releases need to be picked up by distros. This takes\ntime!</p><p>Fortunately, the compiler team now believe that support for our v0 mangling\nscheme is now sufficiently widespread that it can start to be used by default by\nrustc.</p><p>Reading Rust backtraces, or using Rust with debuggers, profilers and other\ntools that operate on compiled Rust code, will be able to output much more\nuseful and readable names. This will especially help with async code,\nclosures and generic functions.</p><p>It's easy to see the new mangling scheme in action, consider the following\nexample:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>With the legacy mangling scheme, all of the useful information about the generic\ninstantiation of  is lost in the symbol ..</p><pre><code></code></pre><p>..but with the v0 mangling scheme, the useful details of the generic instantiation\nare preserved with  <code>f::foo::&lt;alloc::vec::Vec&lt;(alloc::string::String, &amp;[u8; 123])&gt;&gt;</code>:</p><pre><code></code></pre><p>Symbols using the v0 mangling scheme can be larger than symbols with the\nlegacy mangling scheme, which can result in a slight increase in linking\ntimes and binary sizes if symbols aren't stripped (which they aren't by default).\nFortunately this impact should be minor, especially with modern linkers like\nlld, which Rust <a href=\"https://blog.rust-lang.org/2025/09/01/rust-lld-on-1.90.0-stable/\">will now default to on some targets</a>.</p><p>Some old versions of tools/distros or niche tools that the compiler team are\nunaware of may not have had support for the v0 mangling scheme added. When\nusing these tools, the only consequence is that users may encounter mangled\nsymbols. <a href=\"https://github.com/luser/rustfilt\">rustfilt</a> can be used to demangle Rust symbols if a tool does not.</p><p>In any case, using the new mangling scheme can be disabled if any problem\noccurs: use the <code>-Csymbol-mangling-version=legacy -Zunstable-options</code> flag\nto revert to using the legacy mangling scheme.</p><p>Explicitly enabling the legacy mangling scheme requires nightly, it is not\nintended to be stabilised so that support can eventually be removed.</p><p>If you maintain a tool that interacts with Rust symbols and does not\nsupport the v0 mangling scheme, there are Rust and C implementations\nof a v0 symbol demangler available in the <a href=\"https://github.com/rust-lang/rustc-demangle\">rust-lang/rustc-demangle</a>\nrepository that can be integrated into your project.</p><p>rustc will use our \"v0\" mangling scheme on nightly for all targets\nstarting in tomorrow's rustup nightly ().</p><p>If that happens, you can use the legacy mangling scheme with\nthe <code>-Csymbol-mangling-version=legacy -Zunstable-options</code> flag.\nEither by adding it to the usual  environment variable, or to a\nproject's <a href=\"https://blog.rust-lang.org/2025/11/20/switching-to-v0-mangling-on-nightly/(https://doc.rust-lang.org/cargo/reference/config.html)\"></a> configuration file, like so:</p><pre data-lang=\"toml\"><code data-lang=\"toml\"></code></pre><p>If you like the sound of the new symbol mangling version and would\nlike to start using it on stable or beta channels of Rust, then you can\nsimilarly use the <code>-Csymbol-mangling-version=v0</code> flag today via\n or <a href=\"https://blog.rust-lang.org/2025/11/20/switching-to-v0-mangling-on-nightly/(https://doc.rust-lang.org/cargo/reference/config.html)\"></a>:</p><pre data-lang=\"toml\"><code data-lang=\"toml\"></code></pre>",
      "contentLength": 6065,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Python 3.15.0 alpha 2",
      "url": "https://pythoninsider.blogspot.com/2025/11/python-3150a2.html",
      "date": 1763546160,
      "author": "Hugo",
      "guid": 34224,
      "unread": true,
      "content": "<p><strong>This is an early developer preview of Python\n3.15</strong></p><p>https://www.python.org/downloads/release/python-3150a2/</p><p>Python 3.15 is still in development. This release, 3.15.0a2, is the\nsecond of seven planned alpha releases.</p><p>Alpha releases are intended to make it easier to test the current\nstate of new features and bug fixes and to test the release process.</p><p>During the alpha phase, features may be added up until the start of\nthe beta phase (2026-05-05) and, if necessary, may be modified or\ndeleted up until the release candidate phase (2026-07-28). Please keep\nin mind that this is a preview release and its use is\n recommended for production environments.</p><p>Many new features for Python 3.15 are still being planned and\nwritten. Among the new major new features and changes so far:</p><ul><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-sampling-profiler\">PEP\n799</a>: A new high-frequency, low-overhead, statistical sampling\nprofiler and dedicated profiling package</li><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-utf8-default\">PEP\n686</a>: Python now uses UTF-8 as the default encoding</li><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-pep782\">PEP\n782</a>: A new  C API to create a Python bytes\nobject</li><li><small>(Hey,  if a feature\nyou find important is missing from this list, let Hugo\nknow.)</small></li></ul><p>The next pre-release of Python 3.15 will be 3.15.0a3, currently\nscheduled for 2025-12-16.</p><blockquote><p>“An hour,” said Ahab, standing rooted in his boat’s stern; and he\ngazed beyond the whale’s place, towards the dim blue spaces and wide\nwooing vacancies to leeward. It was only an instant; for again his eyes\nseemed whirling round in his head as he swept the watery circle. The\nbreeze now freshened; the sea began to swell.</p><p>“The birds!—the birds!” cried Tashtego.</p></blockquote><p>Thanks to all of the many volunteers who help make Python Development\nand these releases possible! Please consider supporting our efforts by\nvolunteering yourself or through organisation contributions to the <a href=\"https://donate.python.org/\">Python Software Foundation</a>.</p><p>Regards from a crisp and sunny subzero Helsinki,</p><p>Your release team,\n  Hugo van Kemenade\n  Steve Dower\n  </p>",
      "contentLength": 1863,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Project goals update — September 2025",
      "url": "https://blog.rust-lang.org/2025/11/19/Project-Goals-2025-September-Update/",
      "date": 1763510400,
      "author": "Tomas Sedovic",
      "guid": 34275,
      "unread": true,
      "content": "<ul><li>coordinating with <code>#![feature(pin_ergonomics)]</code> (https://github.com/rust-lang/rust/issues/130494) to ensure compatibility between the two features (allow custom pin projections to be the same as the ones for )</li><li>identified connection to auto reborrowing\n<ul><li>https://github.com/rust-lang/rust-project-goals/issues/399</li><li>https://github.com/rust-lang/rust/issues/145612</li></ul></li><li>held a <a href=\"https://hackmd.io/PgVxFwBDQlGXPGTQrI0i3A\">design meeting</a><ul><li>very positive feedback from the language team</li><li>got a vibe check on design axioms</li></ul></li><li>created a new Zulip channel <a href=\"https://rust-lang.zulipchat.com/#narrow/channel/522311-t-lang.2Fcustom-refs\">#t-lang/custom-refs</a> for all new features needed to make custom references more similar to / such as field projections, auto reborrowing and more</li><li>opened https://github.com/rust-lang/rust/pull/146307 to implement field representing types (FRTs) in the compiler</li></ul><ul><li>Get https://github.com/rust-lang/rust/pull/146307 reviewed &amp; merged</li></ul><ul><li>When the PR for FRTs lands, try out the feature &amp; provide feedback on FRTs</li></ul><h3>Shared &amp; Exclusive Projections</h3><p>We want users to be able to have two different types of projections analogous to  and . Each field can be projected independently and a single field can only be projected multiple times in a shared way. The current design uses two different traits to model this. The two traits are almost identical, except for their safety documentation.</p><p>We were thinking if it is possible to unify them into a single trait and have coercions similar to autoreborrowing that would allow the borrow checker to change the behavior depending on which type is projected.</p><p>There are lots of different possibilities for which syntax we can choose, here are a couple options: /, /, /<code>x.mut[Fatih Kadir Akın][]</code>, <code>x.ref.[Fatih Kadir Akın][]</code>/. Also many alternatives for the sigils used: , , .</p><p>We have yet to decide on a direction we want to go in. If we are able to merge the two project traits, we can also settle on a single syntax which would be great.</p><h3>Splitting Projections into Containers &amp; Pointers</h3><p>There are two categories of projections: Containers and Pointers:</p><ul><li> are types like , , , . They are  and apply themselves to each field, so  has a field of type  (if  has a field of type ).</li><li> are types like , , , /, . They support projecting  to .</li></ul><p>In the current design, these two classes of projections are unified by just implementing <code>Pointer&lt;'_, Container&lt;Struct&gt;&gt; -&gt; Pointer&lt;'_, Container&lt;Field&gt;&gt;</code> manually for the common use-cases (for example <code>&amp;mut MaybeUninit&lt;Struct&gt; -&gt; &amp;mut MaybeUninit&lt;Field&gt;</code>). However this means that things like <code>&amp;Cell&lt;MaybeUninit&lt;Struct&gt;&gt;</code> doesn't have native projections unless we explicitly implement them.</p><p>We could try to go for a design that has two different ways to implement projections -- one for containers and one for pointers. But this has the following issues:</p><ul><li>there are two ways to implement projections, which means that some people will get confused which one they should use.</li><li>making projections through multiple container types work out of the box is great, however this means that when defining a new container type and making it available for projections, one needs to consider all other container types and swear coherence with them. If we instead have an explicit way to opt in to projections through multiple container types, the implementer of that trait only has to reason about the types involved in that operation.\n<ul><li>so to rephrase, the current design allows more container types that users actually use to be projected whereas the split design allows arbitrary nestings of container types to be projected while disallowing certain types to be considered container types.</li></ul></li><li>The same problem exists for allowing all container types to be projected by pointer types, if I define a new pointer type I again need to reason about all container types and if it's sound to project them.</li></ul><p>We might be able to come up with a sensible definition of \"container type\" which then resolves these issues, but further investigation is required.</p><h3>Projections for </h3><p>We want to be able to have both a blanket <code>impl&lt;T, F: Field&lt;Base = T&gt;&gt; Project&lt;F&gt; for &amp;T</code> as well as allow people to have custom projections on . The motivating example for custom projections is the Rust-for-Linux  that wants these projections for <a href=\"https://hackmd.io/PgVxFwBDQlGXPGTQrI0i3A#RCU-Read-Copy-Update\">safe RCU abstractions</a>.</p><p>During the design meeting, it was suggested we could add a generic to  that only the compiler is allowed to insert, this would allow disambiguation between the two impls. We have now found an alternative approach that requires less specific compiler magic:</p><ul><li>Add a new marker trait  that's implemented for all types by default.</li><li>People can opt out of implementing it by writing <code>impl !ProjectableBase for MyStruct;</code> (needs negative impls for ).</li><li>We add  to the .</li><li>The compiler needs to consider the negative impls in the overlap check for users to be able to write their own <code>impl&lt;U, F&gt; Project&lt;F&gt; for &amp;Custom&lt;U&gt; where ...</code> (needs negative impl overlap reasoning)</li></ul><p>We probably want negative impls for marker traits as well as improved overlap reasoning for different reasons too, so it is probably fine to depend on them here.</p><p> and  shouldn't be available for projections by default, take for example , if we project to a variant, someone else could overwrite the value with a different variant, invalidating our . This also needs a new trait, probably  (needs more name bikeshedding, but too early for that) that marks fields in structs and tuples.</p><p>To properly project an , we need:</p><ul><li>a new  (TBB) trait that provides a way to read the discriminant that's currently inhabiting the value.\n<ul><li>it also needs to guarantee that the discriminant doesn't change while fields are being projected (this rules out implementing it for )</li></ul></li><li>a new  operator that will project all mentioned fields (for  this already is the behavior for )</li></ul><h3>Field Representing Types (FRTs)</h3><p>While implementing https://github.com/rust-lang/rust/pull/146307 we identified the following problems/design decisions:</p><ul><li>a FRT is considered local to the orphan check when each container base type involved in the field path is local or a tuple (see the top comment on the PR for more infos)</li><li>FRTs cannot implement </li><li>the  trait is not user-implementable</li><li>types with fields that are dynamically sized don't have a statically known offset, which complicates the  trait,</li></ul><p>I decided to simplify the first implementation of FRTs and restrict them to sized structs and tuples. It also doesn't support packed structs. Future PRs will add support for enums, unions and packed structs as well as dynamically sized types.</p>",
      "contentLength": 6359,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Project goals update — October 2025",
      "url": "https://blog.rust-lang.org/2025/11/19/project-goals-update-october-2025/",
      "date": 1763510400,
      "author": "Tomas Sedovic",
      "guid": 34386,
      "unread": true,
      "content": "<p>The focus right now is on the \"non-const\" parts of the proposal, as the \"const\" parts are blocked on the new trait solver (https://github.com/rust-lang/rust-project-goals/issues/113). Now that the types team FCP https://github.com/rust-lang/rust/pull/144064 has completed, work can proceed to land the implementation PRs. <a href=\"https://github.com/davidtwco\">David Wood</a> plans to split the RFC to separate out the \"non-const\" parts of the proposal so it can move independently, which will enable extern types.</p><p>To that end, there are three interesting T-lang design questions to be considered.</p><p>The RFC currently proposes the following names</p><ul></ul><p>However, these names do not follow the \"best practice\" of naming the trait after the capability that it provides. As champion Niko is recommending we shift to the following names:</p><ul><li> -- should righly be called , but oh well, not worth changing.</li><li> -- named after the method  that you get access to.</li><li> -- the only thing you can do is point at it.</li></ul><p>The last trait name is already used by the (unstable) <a href=\"https://doc.rust-lang.org/std/ptr/trait.Pointee.html\"></a> trait. We do not want to have these literally be the same trait because that trait adds a  associated type which would be backwards incompatible; if existing code uses  to mean <code>&lt;T as SomeOtherTrait&gt;::Metadata</code>, it could introduce ambiguity if now  due to defaults. My proposal is to rename  to <code>std::ptr::PointeeMetadata</code> for now, since that trait is unstable and the design remains under some discussion. The two traits could either be merged eventually or remain separate.</p><p>Note that  be implemented automatically by the compiler for anything that implements .</p><p>The RFC proposes that an explicit bound like  disabled the default  bound. However, this gives no signal that this trait bound is \"special\" or different than any other trait bound. Naming conventions can help here, signalling to users that these are special traits, but that leads to constraints on naming and may not scale as we consider using this mechanism to relax other defaults as proposed in <a href=\"https://smallcultfollowing.com/babysteps/blog/2025/10/21/move-destruct-leak/\">my recent blog post</a>. One idea is to use some form of syntax, so that  is just a regular bound, but (for example)  indicates that this bound \"disables\" the default  bound. This gives users some signal that something special is going on. This  syntax is borrowing from semver constraints, although it's not a precise match (it does not mean that  doesn't hold, after all). Other proposals would be some other sigil (, but it means \"opt out from the traits above you\"; , ...) or a keyword (no idea).</p><p>To help us get a feel for it, I'll use  throughout this post.</p><h2>Implicit trait supertrait bounds, edition interaction</h2><p>In Rust 2024, a trait is implicitly  which gets mapped to :</p><pre><code>trait Marker {} // cannot be implemented by extern types\n</code></pre><p>This is not desirable but changing it would be backwards incompatible if traits have default methods that take advantage of this bound:</p><pre><code>trait NotQuiteMarker {\n    fn dummy(&amp;self) {\n        let s = size_of_val(self);\n    }\n}\n</code></pre><p>We need to decide how to handle this. Options are</p><ul><li>Just change it, breakage will be small (have to test that).</li><li>Default to  but let users explicitly write  if they want that. Bad because all traits will be incompatible with extern types.</li><li>Default to  only if defaulted methods are present. Bad because it's a backwards incompatible change to add a defaulted method now.</li><li>Default to  but add  implicitly to defaulted methods. Now it's not backwards incompatible to add a new defaulted method, but it is backwards incompatible to change an existing method to have a default.</li></ul><p>If we go with one of the latter options, Niko proposes that we should relax this in the next Edition (Rust 2026?) so that the default becomes  (or maybe not even that, if we can).</p><h2>Relaxing associated type bounds</h2><p>Under the RFC, existing  bounds would be equivalent to . This is mostly fine but will cause problems in (at least) two specific cases: closure bounds and the  trait. For closures, we can adjust the bound since the associated type is unstable and due to the peculiarities of our  syntax. Failure to adjust the Deref bound in particular would prohibit the use of  where  is an extern type, etc.</p><p>For deref bounds, <a href=\"https://github.com/davidtwco\">David Wood</a> is preparing a PR that simply changes the bound in a backwards incompatible way to assess breakage on crater. There is some chance the breakage will be small.</p><p>If the breakage proves problematic, or if we find other traits that need to be relaxed in a similar fashion, we do have the option of:</p><ul><li>In Rust 2024,  becomes equivalent to <code>T: Deref&lt;Target: SizeOfVal&gt;</code> unless written like <code>T: Deref&lt;Target: =Pointee&gt;</code>. We add that annotation throughout stdlib.</li><li>In Rust 202X, we change the default, so that  does not add any special bounds, and existing Rust 2024  is rewritten to <code>T: Deref&lt;Target: SizeOfVal&gt;</code> as needed.</li></ul><p>One topic that came up in discussion is that we may eventually wish to add a level \"below\" , perhaps , that signifies webassembly external values which cannot be pointed at. That is not currently under consideration but should be backwards compatible.</p>",
      "contentLength": 4945,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Google Summer of Code 2025 results",
      "url": "https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/",
      "date": 1763424000,
      "author": "Jakub Beránek, Jack Huey",
      "guid": 33976,
      "unread": true,
      "content": "<p>As we have <a href=\"https://blog.rust-lang.org/2025/05/08/gsoc-2025-selected-projects/\">announced</a> previously this year, the Rust Project participated\nin <a href=\"https://summerofcode.withgoogle.com\">Google Summer of Code (GSoC)</a> for the second time. Almost twenty contributors have been working very hard on their projects for several months. Same as last year, the projects had various durations, so some of them have ended in September, while the last ones have been concluded in the middle of November. Now that the final reports of all projects have been submitted, we are happy to announce that 18 out of 19 projects have been successful! We had a very large number of projects this year, so we consider this number of successfully finished projects to be a great result.</p><p>We had awesome interactions with our GSoC contributors over the summer, and through a video call, we also had a chance to see each other and discuss the accepted GSoC projects. Our contributors have learned a lot of new things and collaborated with us on making Rust better for everyone, and we are very grateful for all their contributions! Some of them have even continued contributing after their project has ended, and we hope to keep working with them in the future, to further improve open-source Rust software. <strong>We would like to thank all our Rust GSoC 2025 contributors. You did a great job!</strong></p><p>Same as last year, Google Summer of Code 2025 was overall a success for the Rust Project, this time with more than double the number of projects. We think that GSoC is a great way of introducing new contributors to our community, and we are looking forward to participating in GSoC (or similar programs) again in the near future. If you are interested in becoming a (GSoC) contributor, check out our <a href=\"https://github.com/rust-lang/google-summer-of-code\">GSoC project idea list</a> and our <a href=\"https://forge.rust-lang.org/how-to-start-contributing.html\">guide for new contributors</a>.</p><p>Below you can find a brief summary of our GSoC 2025 projects. You can find more information about the original goals of the projects <a href=\"https://github.com/rust-lang/google-summer-of-code/blob/main/gsoc/runs/2025.md\">here</a>. For easier navigation, here is an index of the project descriptions in alphabetical order:</p><p>And now strap in, as there is a ton of great content to read about here!</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#abi-layout-handling-for-the-automatic-differentiation-feature\" aria-hidden=\"true\"></a>\nABI/Layout handling for the automatic differentiation feature</h3><p>The  module allows computing gradients and derivatives in the calculus sense. It provides two autodiff macros, which can be applied to user-written functions and automatically generate modified versions of those functions, which also compute the requested gradients and derivatives. This functionality is very useful especially in the context of scientific computing and implementation of machine-learning models.</p><p>Our autodiff frontend was facing two challenges.</p><ul><li>First, we would generate a new function through our macro expansion, however, we would not have a suitable function body for it yet. Our autodiff implementation relies on an LLVM plugin to generate the function body. However, this plugin only gets called towards the end of the compilation pipeline. Earlier optimization passes, either on the LLVM or the Rust side, could look at the placeholder body and either \"optimize\" or even delete the function since it has no clear purpose yet.</li><li>Second, the flexibility of our macros was causing issues, since it allows requesting derivative computations on a per-argument basis. However, when we start to lower Rust arguments to our compiler backends like LLVM, we do not always have a 1:1 match of Rust arguments to LLVM arguments. As a simple example, an array with two double values might be passed as two individual double values on LLVM level, whereas an array with three doubles might be passed via a pointer.</li></ul><p>Marcelo helped rewrite our  macros to not generate hacky placeholder function bodies, but instead introduced a proper  intrinsic. This is the proper way for us to declare that an implementation of this function is not available yet and will be provided later in the compilation pipeline. As a consequence, our generated functions were not deleted or incorrectly optimized anymore. The intrinsic PR also allowed removing some previous hacks and therefore reduced the total lines of code in the Rust compiler by over 500! You can find more details in <a href=\"https://github.com/rust-lang/rust/pull/142640\">this PR</a>.</p><p>Beyond autodiff work, Marcelo also initiated work on GPU offloading intrinsics, and helped with multiple bugs in our argument handling. We would like to thank Marcelo for all his great work!</p><p>The Rust Project has an ambitious goal to <a href=\"https://rust-lang.github.io/rust-project-goals/2025h1/std-contracts.html\">instrument the Rust standard library with safety contracts</a>, moving from informal comments that specify safety requirements of  functions to executable Rust code. This transformation represents a significant step toward making Rust's safety guarantees more explicit and verifiable. To prioritize which functions should receive contracts first, there is a <a href=\"https://github.com/model-checking/verify-rust-std\">verification contest</a> ongoing.</p><p>Given that Rust contracts are still in their <a href=\"https://github.com/rust-lang/rust/issues/128044\">early stages</a>, Dawid's project was intentionally open-ended in scope and direction. This flexibility allowed Dawid to identify and tackle several key areas that would add substantial value to the contracts ecosystem. His contributions were in the following three main areas:</p><ul><li><p><strong>Pragmatic Contracts Integration</strong>: Refactoring <a href=\"https://github.com/rust-lang/rust/pull/144438\">contract HIR lowering</a> to ensure no contract code is executed when contract-checks are disabled. This has major impact as it ensures that contracts do not have runtime cost when contract checks are disabled.</p></li><li><p><strong>Variable Reference Capability</strong>: Adding the ability to <a href=\"https://github.com/rust-lang/rust/pull/144444\">refer</a> to variables from preconditions within postconditions. This fundamental enhancement to the contracts system has been fully implemented and merged into the compiler. This feature provides developers with much more expressive power when writing contracts, allowing them to establish relationships between input and output states.</p></li><li><p><strong>Separation Logic Integration</strong>: The bulk of Dawid's project involved identifying, understanding, and planning the introduction of owned and block ownership predicates for separation-logic style reasoning in contracts for unsafe Rust code. This work required extensive research and collaboration with experts in the field. Dawid engaged in multiple discussions with authors of Rust validation tools and Miri developers, both in person and through Zulip discussion threads. The culmination of this research is captured in a comprehensive MCP (Major Change Proposal) that Dawid <a href=\"https://github.com/rust-lang/compiler-team/issues/942\">created</a>.</p></li></ul><p>Dawid's work represents crucial foundational progress for Rust's safety contracts initiative. By successfully implementing variable reference capabilities and laying the groundwork for separation logic integration, he has positioned the contracts feature for significant future development. His research and design work will undoubtedly influence the direction of this important safety feature as it continues to mature. Thank you very much!</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#bootstrap-of-rustc-with-rustc-codegen-gcc\" aria-hidden=\"true\"></a>\nBootstrap of rustc with rustc_codegen_gcc</h3><p>The goal of this project was to improve the Rust GCC codegen backend (<a href=\"https://github.com/rust-lang/rustc_codegen_gcc\"></a>), so that it would be able to compile the \"stage 2\" Rust compiler () itself <a href=\"https://blog.antoyo.xyz/rustc_codegen_gcc-progress-report-10\">again</a>.</p><p>You might remember that Michał already participated in GSoC <a href=\"https://blog.rust-lang.org/2024/11/07/gsoc-2024-results/#rust-to-net-compiler-add-support-for-compiling-running-cargo-tests\">last year</a>, where he was working on his own .NET Rust codegen backend, and he did an incredible amount of work. This year, his progress was somehow even faster. Even before the official GSoC implementation period started (!), he essentially completed his original project goal and managed to build  with GCC. This was no small feat, as he had to investigate and fix several miscompilations that occurred when functions marked with  were called recursively or when the compiled program was trying to work with 128-bit integers. You can read more about this initial work at his <a href=\"https://fractalfir.github.io/generated_html/cg_gcc_bootstrap.html\">blog</a>.</p><p>After that, he immediately started working on stretch goals of his project. The first one was to get a \"stage-3\"  build working, for which he had to vastly <a href=\"https://github.com/rust-lang/rustc_codegen_gcc/pull/680\">improve</a> the memory consumption of the codegen backend.</p><p>Once that was done, he moved on to yet another goal, which was to build  for a platform not supported by LLVM. He made progress on this for <a href=\"https://github.com/rust-lang/rustc_codegen_gcc/issues/742\">Dec Alpha</a> and <a href=\"https://github.com/rust-lang/rustc_codegen_gcc/issues/744\">m68k</a>. He also attempted to compile  on Aarch64, which led to him finding an ABI bug. Ultimately, he managed to build a  for m68k (with a few workarounds that we will need to fix in the future). That is a very nice first step to porting Rust to new platforms unsupported by LLVM, and is important for initiatives such as <a href=\"https://rust-for-linux.com/\">Rust for Linux</a>.</p><p>Michał had to spend a lot of time starting into assembly code and investigating arcane ABI problems. In order to make this easier for everyone, he implemented support for <a href=\"https://github.com/rust-lang/rustc_codegen_gcc/pull/688\">fuzzing</a> and automatically checking <a href=\"https://github.com/rust-lang/rustc_codegen_gcc/pull/710\">ABI mismatches</a> in the GCC codegen backend. You can read more about his testing and fuzzing efforts <a href=\"https://fractalfir.github.io/generated_html/cg_gcc_bootstrap_2.html\">here</a>.</p><p>We were really impressed with what Michał was able to achieve, and we really appreciated working with him this summer. Thank you for all your work, Michał!</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#cargo-build-script-delegation\" aria-hidden=\"true\"></a>\nCargo: Build script delegation</h3><p>Cargo build scripts come at a compile-time cost, because even to run , they must be built as if you ran , so that they can be executed during compilation. Even though we try to identify ways to reduce the <a href=\"https://github.com/rust-lang/cargo/issues/14948\">need</a> to write build scripts in the first place, that may not always be doable. However, if we could shift build scripts from being defined in every package that needs them, into a few core build script packages, we could both reduce the compile-time overhead, and also improve their auditability and transparency. You can find more information about this idea <a href=\"https://doc.rust-lang.org/nightly/cargo/reference/unstable.html#metabuild\">here</a>.</p><p>The first step required to delegate build scripts to packages is to be able to run multiple build scripts per crate, so that is what Naman was primarily working on. He introduced a new unstable <a href=\"https://github.com/rust-lang/cargo/pull/15704\"></a> feature to Cargo, implemented support for parsing an array of build scripts in , and extended Cargo so that it can now execute multiple build scripts while building a single crate. He also added a set of tests to ensure that this feature will work as we expect it to.</p><p>Then he worked on ensuring that the execution of builds scripts is performed in a deterministic order, and that crates can access the output of each build script separately. For example, if you have the following configuration:</p><pre data-lang=\"toml\"><code data-lang=\"toml\"></code></pre><p>then the corresponding crate is able to access the s of both build scripts using <code>env!(\"windows-manifest_OUT_DIR\")</code> and <code>env!(\"release-info_OUTDIR\")</code>.</p><p>As future work, we would like to implement the ability to pass parameters to build scripts through metadata specified in  and then implement the actual build script delegation to external build scripts using <a href=\"https://doc.rust-lang.org/nightly/cargo/reference/unstable.html#artifact-dependencies\">artifact-dependencies</a>.</p><p>We would like to thank Naman for helping improving Cargo and laying the groundwork for a feature that could have compile-time benefits across the Rust ecosystem!</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#distributed-and-resource-efficient-verification\" aria-hidden=\"true\"></a>\nDistributed and resource-efficient verification</h3><p>The goal of this project was to address critical scalability challenges of formally verifying Rust's standard library by developing a distributed verification system that intelligently manages computational resources and minimizes redundant work. The <a href=\"https://github.com/model-checking/verify-rust-std\">Rust standard library verification project</a> faces significant computational overhead when verifying large codebases, as traditional approaches re-verify unchanged code components. With Rust's standard library containing thousands of functions and continuous development cycles, this inefficiency becomes a major bottleneck for practical formal verification adoption.</p><p>Jiping implemented a distributed verification system with several key innovations:</p><ul><li><strong>Intelligent Change Detection</strong>: The system uses hash-based analysis to identify which parts of the codebase have actually changed, allowing verification to focus only on modified components and their dependencies.</li><li>: The project coordinates multiple verification backends including Kani model checker, with careful version pinning and compatibility management.</li><li>: The verification workload is distributed across multiple compute nodes, with intelligent scheduling that considers both computational requirements and dependency graphs.</li><li>: Jiping built a comprehensive web interface that provides live verification status, interactive charts, and detailed proof results. You can check it out <a href=\"https://os-checker.github.io/distributed-verification/chart\">here</a>!</li></ul><p>You can find the created distributed verification tool in <a href=\"https://github.com/os-checker/distributed-verification\">this</a> repository. Jiping's work established a foundation for scalable formal verification that can adapt to the growing complexity of Rust's ecosystem, while maintaining verification quality and completeness, which will go a long way towards ensuring that Rust's standard library remains safe and sound. Thank you for your great work!</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#enable-witness-generation-in-cargo-semver-checks\" aria-hidden=\"true\"></a>\nEnable Witness Generation in cargo-semver-checks</h3><p><a href=\"https://github.com/obi1kenobi/cargo-semver-checks\"></a> is a Cargo subcommand for finding SemVer API breakages in Rust crates. Talyn's project aimed to lay the groundwork for it to tackle our most vexing limitation: the inability to catch SemVer breakage due to type changes.</p><p>Imagine a crate makes the following change to its public API:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>This is  a major breaking change, right? And yet  with its hundreds of lints is  unable to flag this. While this case seems trivial, it's just the tip of an enormous iceberg. Instead of changing  to , what if the change was from  to , or worse, into some monstrosity like:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Figuring out whether this change is breaking requires checking whether the original  parameter type can \"fit\" into that monstrosity of an  type. But reimplementing a Rust type checker and trait solver inside  is out of the question! Instead, we turn to a technique created for <a href=\"https://predr.ag/blog/semver-violations-are-common-better-tooling-is-the-answer/\">a previous study of SemVer breakage on crates.io</a>—we generate a \"witness\" program that will fail to compile if, and only if, there's a breaking change between the two versions.</p><p>The witness program is a separate crate that can be made to depend on either the old or the new version of the crate being scanned. If our  function comes from a crate called , its witness program would look something like:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><blockquote><p>This example is cherry-picked to be easy to understand. Witness programs are rarely this straightforward!</p></blockquote><p>Attempting to  the witness while plugging in the new version of  forces  to decide whether  matches the new  parameter. If  passes without errors, there's no breaking change here. But if there's a compilation error, then this is concrete, incontrovertible evidence of breakage!</p><p>Over the past 22+ weeks, Talyn worked tirelessly to move this from an idea to a working proof of concept. For every problem we foresaw needing to solve, ten more emerged along the way. Talyn did a lot of design work to figure out an approach that would be able to deal with crates coming from various sources (crates.io, a path on disk, a git revision), would support multiple rustdoc JSON formats for all the hundreds of existing lints, and do so in a fashion that doesn't get in the way of adding hundreds more lints in the future.</p><p>Even the above list of daunting challenges fails to do justice to the complexity of this project. Talyn created a witness generation prototype that lays the groundwork for robust checking of type-related SemVer breakages in the future. The success of this work is key to the  roadmap for 2026 and beyond. We would like to thank Talyn for their work, and we hope to continue working with them on improving witness generation in the future.</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#extend-behavioural-testing-of-std-arch-intrinsics\" aria-hidden=\"true\"></a>\nExtend behavioural testing of std::arch intrinsics</h3><p>The  module contains target-specific intrinsics (low-level functions that typically correspond to single machine instructions) which are intended to be used by other libraries. These are intended to match the equivalent intrinsics available as vendor-specific extensions in C.</p><p>The intrinsics are tested with three approaches. We test that:</p><ul><li>The signatures of the intrinsics match the one specified by the architecture.</li><li>The intrinsics generate the correct instruction.</li><li>The intrinsics have the correct runtime behavior.</li></ul><p>These behavior tests are implemented in the <a href=\"https://github.com/rust-lang/stdarch/tree/master/crates/intrinsic-test\">intrinsics-test</a> crate. Initially, this test framework only covered the AArch64 and AArch32 targets, where it was very useful in finding bugs in the implementation of the intrinsics. Madhav's project was about refactoring and improving this framework to make it easier (or really, possible) to extend it to other CPU architectures.</p><p>First, Madhav <a href=\"https://github.com/rust-lang/stdarch/pull/1758\">split</a> the codebase into a module with shared (architecturally independent) code and a module with ARM-specific logic. Then he implemented support for testing intrinsics for the x86 architecture, which is Rust's most widely used target. In doing so, he allowed us to discover real bugs in the implementation of some intrinsics, which is a great result! Madhav also did a lot of work in optimizing how the test suite is compiled and executed, to reduce CI time needed to run tests, and he laid the groundwork for supporting even more architectures, specifically LoongArch and WebAssembly.</p><p>We would like to thank Madhav for all his work on helping us make sure that Rust intrinsics are safe and correct!</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#implement-merge-functionality-in-bors\" aria-hidden=\"true\"></a>\nImplement merge functionality in bors</h3><p>The main <a href=\"https://github.com/rust-lang/rust\">Rust repository</a> uses a pull request merge queue bot that we call . Its current <a href=\"https://github.com/rust-lang/homu\">Python implementation</a> has a lot of issues and was difficult to maintain. The goal of this GSoC project was thus to implement the primary merge queue functionality in our <a href=\"https://github.com/rust-lang/bors\">Rust rewrite</a> of this bot.</p><p>Sakibul first examined the original Python codebase to figure out what it was doing, and then he implemented several bot commands that allow contributors to approve PRs, set their priority, delegate approval rights, temporarily close the merge tree, and many others. He also implemented an asynchronous background process that checks whether a given pull request is mergeable or not (this process is relatively involved, due to how GitHub works), which required implementing a specialized synchronized queue for deduplicating mergeability check requests to avoid overloading the GitHub API. Furthermore, Sakibul also reimplemented (a nicer version of) the merge queue status <a href=\"https://bors-prod.rust-lang.net/queue/rust\">webpage</a> that can be used to track which pull requests are currently being tested on CI, which ones are approved, etc.</p><p>After the groundwork was prepared, Sakibul could work on the merge queue itself, which required him to think about many tricky race conditions and edge cases to ensure that bors doesn't e.g. merge the wrong PR into the default branch or merge a PR multiple times. He covered these edge cases with many integration tests, to give us more confidence that the merge queue will work as we expect it to, and also prepared a script for creating simulated PRs on a test GitHub repository so that we can test bors \"in the wild\". And so far, it seems to be working very well!</p><p>After we finish the final piece of the merge logic (creating so-called <a href=\"https://forge.rust-lang.org/release/rollups.html\">\"rollups\"</a>) together with Sakibul, we will start using bors fully in the main Rust repository. Sakibul's work will thus be used to merge all  pull requests. Exciting!</p><p>Apart from working on the merge queue, Sakibul made many other awesome contributions to the codebase, like refactoring the test suite or analyzing performance of SQL queries. In total, Sakibul sent around <a href=\"https://github.com/rust-lang/bors/pulls?q=is%3Apr+author%3Asakib25800+is%3Amerged+\">fifty pull requests</a> that were already merged into bors! What can we say, other than: Awesome work Sakibul, thank you!</p><p><a href=\"https://rustc-dev-guide.rust-lang.org/building/bootstrapping/what-bootstrapping-does.html\">bootstrap</a> is the build system of Rust itself, which is responsible for building the compiler, standard library, and pretty much everything else that you can download through . This project's goal was very open-ended: \"improve bootstrap\".</p><p>And Shourya did just that! He made meaningful contributions to several parts of bootstrap. First, he added much-needed documentation to several core bootstrap data structures and modules, which were quite opaque and hard to understand without any docs. Then he moved to improving command execution, as each bootstrap invocation invokes hundreds of external binaries, and it was difficult to track them. Shourya finished a long-standing refactoring that routes almost all executed commands through a single place. This allowed him to also implement command caching and also command profiling, which shows us which commands are the slowest.</p><p>After that, Shourya moved on to refactoring config parsing. This was no easy task, because bootstrap has A LOT of config options; the single  that parses them had over a thousand lines of code (!). A set of complicated config precedence rules was frequently causing bugs when we had to modify that function. It took him several weeks to untangle this mess, but the result is worth it. The <a href=\"https://github.com/rust-lang/rust/blob/master/src/bootstrap/src/core/config/config.rs#L356\">refactored function</a> is much less brittle and easier to understand and modify, which is great for future maintenance.</p><p>The final area that Shourya improved were bootstrap tests. He made it possible to run them using bare , which enables debugging them e.g. in an IDE, which is very useful, and mainly he found a way to run the tests in parallel, which makes contributing to bootstrap itself much more pleasant, as it reduced the time to execute the tests from a minute to under ten seconds. These changes required refactoring many bootstrap tests that were using global state, which was not compatible with parallel execution.</p><p>Overall, Shourya made more than <a href=\"https://github.com/rust-lang/rust/pulls?q=is%3Apr+author%3Ashourya742+is%3Amerged+\">30 PRs</a> to bootstrap since April! We are very thankful for all his contributions, as they made bootstrap much easier to maintain. Thank you!</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#improve-wild-linker-test-suites\" aria-hidden=\"true\"></a>\nImprove Wild linker test suites</h3><p><a href=\"https://github.com/davidlattimore/wild\">Wild</a> is a very fast linker for Linux that’s written in Rust. It can be used to build executables and shared objects.</p><p>Kei’s project was to leverage the test suite of one of the other Linux linkers to help test the Wild linker. This goal was accomplished. Thanks to Kei’s efforts, we now run the Mold test suite against Wild in our CI. This has helped to prevent regressions on at least a couple of occasions and has also helped to show places where Wild has room for improvement.</p><p>In addition to this core work, Kei also undertook numerous other changes to Wild during GSoC. Of particular note was the reworking of argument parsing to support , which we had wanted for some time. Kei also fixed a number of bugs and implemented various previously missing features. This work has helped to expand the range of projects that can use Wild to build executables.</p><p>Kei has continued to contribute to Wild even after the GSoC project finished and has now contributed over <a href=\"https://github.com/davidlattimore/wild/pulls?q=is%3Apr+author%3Alapla-cogito+is%3Amerged+\">seventy PRs</a>. We thank Kei for all the hard work and look forward to continued collaboration in the future!</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#improving-the-rustc-parallel-frontend-parallel-macro-expansion\" aria-hidden=\"true\"></a>\nImproving the Rustc Parallel Frontend: Parallel Macro Expansion</h3><p>The Rust compiler has a (currently unstable) parallel compilation mode in which some compiler passes run in parallel.\nOne major part of the compiler that is not yet affected by parallelization is name resolution.\nIt has several components, but those selected for this GSoC project were import resolution and macro expansion (which are in fact intermingled into a single fixed-point algorithm).\nBesides the parallelization itself, another important point of the work was improving the correctness of import resolution by eliminating accidental order dependencies in it, as those also prevent parallelization.</p><p>We should note that this was a  ambitious project, and we knew from the beginning that it would likely be quite challenging to reach the end goal within the span of just a few months. And indeed, Lorrens did in fact run into several unexpected issues that showed us that the complexity of this work is well beyond a single GSoC project, so he didn't actually get to parallelizing the macro expansion algorithm. Nevertheless, he did a lot of important work to improve the name resolver and prepare it for being parallelized.</p><p>The first thing that Lorrens had to do was actually understand how Rust name resolution works and how it is implemented in the compiler. That is, to put it mildly, a  piece of logic, and is affected by legacy burden in the form of backward compatibility lints, outdated naming conventions, and other technical debt. Even this learned knowledge itself is incredibly useful, as the set of people that understand Rust's name resolution today is very low, so it is important to grow it.</p><p>Using this knowledge, he made a lot of refactorings to separate  mutability in name resolver data structures from \"cache-like\" mutability used for things like lazily loading otherwise immutable data from extern crates, which was needed to unblock parallelization work. He split <a href=\"https://github.com/rust-lang/rust/pull/143657\">various</a><a href=\"https://github.com/rust-lang/rust/pull/143884\">parts</a> of the name resolver, got rid of <a href=\"https://github.com/rust-lang/rust/pull/144059\">unnecessary</a><a href=\"https://github.com/rust-lang/rust/pull/144605\">mutability</a> and performed a bunch of <a href=\"https://github.com/rust-lang/rust/pull/145322\">other</a><a href=\"https://github.com/rust-lang/rust/pull/147805\">refactorings</a>. He also had to come up with a very tricky <a href=\"https://github.com/rust-lang/rust/pull/144912\">data structure</a> that allows providing conditional mutable access to some data.</p><p>These refactorings allowed him to implement something called <a href=\"https://github.com/rust-lang/rust/pull/145108\">\"batched import resolution\"</a>, which splits unresolved imports in the crate into \"batches\", where all imports in a single batch can be resolved independently and potentially in parallel, which is crucial for parallelizing name resolution. We have to resolve a few remaining language <a href=\"https://github.com/rust-lang/rust/pull/147984\">compatibility</a><a href=\"https://github.com/rust-lang/rust/pull/147995\">issues</a>, after which the batched import resolution work will hopefully be merged.</p><p>Lorrens laid an important groundwork for fixing potential correctness issues around name resolution and macro expansion, which unblocks further work on parallelizing these compiler passes, which is exciting. His work also helped unblock some <a href=\"https://github.com/rust-lang/rust/pull/137487\">library</a><a href=\"https://github.com/rust-lang/rust/pull/139493\">improvements</a> that were stuck for a long time. We are grateful for your hard work on improving tricky parts of Rust and its compiler, Lorrens. Thank you!</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#make-cargo-semver-checks-faster\" aria-hidden=\"true\"></a>\nMake cargo-semver-checks faster</h3><p><a href=\"https://github.com/obi1kenobi/cargo-semver-checks\"></a> is a Cargo subcommand for finding SemVer API breakages in Rust crates. It is adding SemVer lints at an  pace: the number of lints has been doubling every year, and currently stands at . More lints mean more work for  to do, as well as more work for its test suite which runs over 250000 lint checks!</p><p>Joseph's contributions took three forms:</p><ul><li>Improving  runtime performance—on large crates, our query runtime went from ~8s to ~2s, a 4x improvement!</li><li>Improving the test suite's performance, enabling us to iterate faster. Our test suite used to take ~7min and now finishes in ~1min, a 7x improvement!</li><li>Improving our ability to profile query performance and inspect performance anomalies, both of which were proving a bottleneck for our ability to ship further improvements.</li></ul><p>Joseph described all the clever optimization tricks leading to these results in his <a href=\"https://clidragon.github.io/blog/gsoc-2025\">final report</a>. To encourage you to check out the post, we'll highlight a particularly elegant optimization described there.</p><p> relies on rustdoc JSON, an unstable component of Rust whose output format often has breaking changes. Since each release of  supports a range of Rust versions, it must also support a range of rustdoc JSON formats. Fortunately, each file carries a version number that tells us which version's  types to use to deserialize the data.</p><p>Previously, we used to deserialize the JSON file twice: once with a  type that only loaded the  field, and a second time with the appropriate  type that matches the format. This works fine, but many large crates generate rustdoc JSON files that are 500 MiB+ in size, requiring us to walk all that data twice. While  is quite fast, there's nothing as fast as  doing the work twice in the first place!</p><p>So we used a trick: <a href=\"https://github.com/obi1kenobi/trustfall-rustdoc/pull/98/files#diff-b1a35a68f14e696205874893c07fd24fdb88882b47c23cc0e0c80a30c7d53759R46-R82\"> check</a> if the  field is the last field in the JSON file, which happens to be the case every time (even though it is not guaranteed). Rather than parsing JSON, we merely look for a  character in the last few dozen bytes, then look for  after the  character, and for  between them. If this is successful, we've discovered the version number while avoiding going through hundreds of MB of data! If we failed for any reason, we just fall back to the original approach having only wasted the effort of looking at 20ish extra bytes.</p><p>Joseph did a lot of profiling and performance optimizations to make  faster for everyone, with awesome results. Thank you very much for your work!</p><p>As a very important part of the Rustup team's vision of migrating the <a href=\"https://github.com/rust-lang/rustup\">rustup</a> codebase to using async IO since the introduction of the global  runtime in <a href=\"https://github.com/rust-lang/rustup/issues/3367\">#3367</a>, this project's goal was to introduce proper concurrency to rustup. Francisco did that by attacking two aspects of the codebase at once:</p><ol><li>He created a new set of user interfaces for displaying concurrent progress.</li><li>He implemented a new toolchain update checking &amp; installation flow that is idiomatically concurrent.</li></ol><p>As a warmup, Francisco made  concurrent, resulting in a rather easy <a href=\"https://web.tecnico.ulisboa.pt/francisco.t.gouveia/posts/02-rustup-concurrent-checks\">3x performance boost</a> in certain cases. Along the way, he also introduced a new <a href=\"https://crates.io/crates/indicatif\">indicatif</a>-based progress bar for reporting progress of concurrent operations, which replaced the original hand-rolled solution.</p><p>After that, the focus of the project has moved on to the toolchain installation flow used in commands like  and . In this part, Francisco developed two main improvements:</p><ol><li>The possibility of downloading multiple components at once when setting up a toolchain, controlled by the <code>RUSTUP_CONCURRENT_DOWNLOADS</code> environment variable. Setting this variable to a value greater than 1 is particularly useful in certain internet environments where the speed of a single download connection could be restricted by QoS (Quality of Service) limits.</li><li>The ability to interleave component network downloads and disk unpacking. For the moment, unpacking will still happen sequentially, but disk and net I/O can finally be overlapped! This introduces a net gain in toolchain installation time, as only the last component being downloaded will have noticeable unpacking delays. In our tests, this typically results in a <a href=\"https://web.tecnico.ulisboa.pt/francisco.t.gouveia/posts/03-gsoc-final-report/#my-internet-is-fast-can-i-also-have-faster-installations\">reduction of 4-6 seconds</a> (on fast connections, that's ~33% faster!) when setting up a toolchain with the  profile.</li></ol><p>We have to say that these results are very impressive! While a few seconds shorter toolchain installation might not look so important at a first glance, rustup is ubiquitously used to install Rust toolchains on CI of tens of thousands of Rust projects, so this improvement (and also further improvements that it unlocks) will have an enormous effect across the Rust ecosystem. Many thanks to Francisco Gouveia's enthusiasm and active participation, without which this wouldn't have worked out!</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#mapping-the-maze-of-rust-s-ui-test-suite-with-established-continuous-integration-practices\" aria-hidden=\"true\"></a>\nMapping the Maze of Rust's UI Test Suite with Established Continuous Integration Practices</h3><p>The snapshot-based <a href=\"https://github.com/rust-lang/rust/tree/master/tests/ui\">UI test suite</a> is a crucial part of the Rust compiler's test suite. It contains  of tests: over 19000 at the time of writing. The organization of this test suite is thus\nvery important, for at least two reasons:</p><ol><li>We want to be able to find specific tests, identify related tests, and have some sort of logical grouping of related tests.</li><li>We have to ensure that no directory contains so many entries such that GitHub gives up rendering the directory.</li></ol><p>Furthermore, having informative test names and having some context for each test is particularly important, as otherwise contributors would have to reverse-engineer test intent from  and friends.</p><p>Over the years, we have accumulated a lot of unorganized stray test files in the\ntop level  directory, and have a lot of generically named \ntests in the  directory. The former makes it annoying to find\nmore meaningful subdirectories, while the latter makes it completely non-obvious\nwhat each test is about.</p><p>Julien's project was about introducing some order into the chaos. And that was indeed achieved!\nThrough Julien's efforts (in conjunction with efforts from other contributors), we now have:</p><ul><li>No more stray tests under the immediate  top-level directory, and are organized into more meaningful subdirectories. We were able to then introduce a style check to prevent new stray tests from being added.</li><li>A top-level document contains TL;DRs for each of the immediate subdirectories.</li><li>Substantially fewer generically-named under .</li></ul><p>Test organization (and more generally, test suite ergonomics) is an often under-\nappreciated aspect of maintaining complex codebases. Julien spent a lot of effort\nimproving test ergonomics of the Rust compiler, both in last year's GSoC (where he vastly\nimproved our <a href=\"https://blog.rust-lang.org/2024/11/07/gsoc-2024-results/#rewriting-esoteric-error-prone-makefile-tests-using-robust-rust-features\">\"run-make\"</a> test suite), and then again this year, where he made our UI test suite more ergonomic.\nWe would like to appreciate your meticulous work, Julien! Thank you very much.</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#modernising-the-libc-crate\" aria-hidden=\"true\"></a>\nModernising the libc Crate</h3><p><a href=\"https://github.com/rust-lang/libc\"></a> is a crucial crate in the Rust ecosystem (on average, it has ~1.5 million  downloads), providing bindings to system C API. This GSoC project had two goals: improve testing for what we currently have, and make progress toward a stable 1.0 release of .</p><p>Test generation is handled by the <a href=\"https://github.com/rust-lang/libc/tree/main/ctest\"></a> crate, which creates unit tests that compare properties of Rust API to properties of the C interfaces it binds. Prior to the project,  used an obsolete Rust parser that had stopped receiving major updates about eight years ago, meaning  could not easily use any syntax newer than that. Abdul completely rewrote  to use  as its parser and make it much easier to add new tests, then went through and switched everything over to the more modern . After this change, we were able to remove a number of hacks that had been needed to work with the old parser.</p><p>The other part of the project was to make progress toward the 1.0 release of . Abdul helped with this by going through and addressing a number of issues that need to be resolved before the release, many of which were made possible with all the  changes.</p><p>While there is still a lot of work left to do before  can reach 1.0, Abdul's improvements will go a long way towards making that work easier, as they give us more confidence in the test suite, which is now much easier to modify and extend. Thank you very much for all your work!</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#prepare-stable-mir-crate-for-publishing\" aria-hidden=\"true\"></a>\nPrepare stable_mir crate for publishing</h3><p>This project's goal was to prepare the Rust compiler's \ncrate (eventually renamed to <a href=\"https://doc.rust-lang.org/nightly/nightly-rustc/rustc_public/index.html\"></a>), which provides a way to interface\nwith the Rust compiler for analyzing Rust code, for publication on crates.io. While the\nexisting crate provided easier APIs for tool developers, it lacked proper\nversioning and was tightly coupled with compiler versions. The goal was to\nenable independent publication with semantic versioning.</p><p>The main technical work involved restructuring  and \n(previously named ) by inverting their dependency relationship.\nMakai resolved circular dependencies by temporarily merging the crates and\ngradually separating them with the new architecture. They also split the existing\ncompiler interface to separate public APIs from internal compiler details.</p><p>Furthermore, Makai established infrastructure for dual maintenance: keeping an internal\nversion in the Rust repository to track compiler changes while developing\nthe publishable version in a dedicated repository. Makai automated a\nsystem to coordinate between versions, and developed custom tooling to validate\ncompiler version compatibility and to run tests.</p><p>Makai successfully completed the core refactoring and infrastructure\nsetup, making it possible to publish  independently with proper\nversioning support for the Rust tooling ecosystem! As a bonus, Makai contributed\nseveral bug fixes and implemented new APIs that had been requested by the\ncommunity. Great job Makai!</p><h3><a href=\"https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/#prototype-an-alternative-architecture-for-cargo-fix-using-cargo-check\" aria-hidden=\"true\"></a>\nPrototype an alternative architecture for cargo fix using cargo check</h3><p>The  command applies fixes suggested by lints, which makes it useful for cleaning up sloppy code,\nreducing the annoyance of toolchain upgrades when lints change and helping with edition migrations and new lint adoption. However, it has a number of issues. It can be <a href=\"https://github.com/rust-lang/cargo/issues/13214\">slow</a>, it only applies a subset of possible lints, and doesn't provide an easy way to select which lints to fix.</p><p>These problems are caused by its current architecture; it is implemented as a variant of  that replaces  with  being run in a special mode that will call  in a loop, applying fixes until there are none. While this special -proxy mode is running,\na cross-process lock is held to force only one build target to be fixed at a time to avoid race conditions.\nThis ensures correctness at the cost of performance and difficulty in making the -proxy interactive.</p><p>Glen implemented a proof of concept of an alternative design called <a href=\"https://github.com/crate-ci/cargo-fixit\">cargo-fixit</a>.  spawns  in a loop, determining which build targets are safe to fix in a given pass, and then applying the suggestions. This puts the top-level program in charge of what fixes get applied, making it easier to coordinate. It also allows the locking to be removed and opens the door to an interactive mode.</p><p>Glen performed various <a href=\"https://github.com/crate-ci/cargo-fixit/blob/main/benchsuite/runs/2025-07-31-af6627c.md\">benchmarks</a> to test how the new approach performs.  And in some benchmarks,  was able to finish within a few hundred milliseconds, where before the same task took  almost a minute! As always, there are trade-offs; the new approach comes at the cost that fixes in packages lower in the dependency tree can cause later packages to be rebuilt multiple times, slowing things down, so there were also benchmarks where the old design was a bit faster. The initial results are still very promising and impressive!</p><p>Further work remains to be done on  to investigate how it could be optimized better and how should its interface look like before being stabilized. We thank Glen for all the hard work on this project, and we hope that one day the new design will become used by default in Cargo, to bring faster and more flexible fixing of lint suggestions to everyone!</p><p>The goal of this project was to move forward our <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/cargo-plumbing.html\">Project Goal</a> for creating low-level (\"plumbing\") Cargo subcommands to make it easier to reuse parts of Cargo by other tools.</p><p>Vito created a prototype of several plumbing commands in the <a href=\"https://github.com/crate-ci/cargo-plumbing\">cargo-plumbing</a> crate. The idea was to better understand how the plumbing commands should look like, and what is needed from Cargo to implement them. Vito had to make compromises in some of these commands to not be blocked on making changes to the current Cargo Rust APIs, and he helpfully documented those <a href=\"https://github.com/crate-ci/cargo-plumbing/issues/82\">blockers</a>. For example, instead of solely relying on the manifests that the user passed in, the plumbing commands will re-read the manifests within each command, preventing callers from being able to edit them to get specific behavior out of Cargo, e.g. dropping all workspace members to allow resolving dependencies on a per-package basis.</p><p>Vito did a lot of work, as he implemented seven different plumbing subcommands:</p><ul></ul><p>As future work, we would like to deal with some unresolved questions around how to integrate these plumbing commands within Cargo itself, and extend the set of plumbing commands.</p><p>We thank Vito for all his work on improving the flexibility of Cargo.</p><p>We would like to thank all contributors that have participated in Google Summer of Code 2025 with us! It was a blast, and we cannot wait to see which projects GSoC contributors will come up with in the next year. We would also like to thank Google for organizing the Google Summer of Code program and for allowing us to have so many projects this year. And last, but not least, we would like to thank all the Rust mentors who were tirelessly helping our contributors to complete their projects. Without you, Rust GSoC would not be possible.</p>",
      "contentLength": 38465,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Launching the 2025 State of Rust Survey",
      "url": "https://blog.rust-lang.org/2025/11/17/launching-the-2025-state-of-rust-survey/",
      "date": 1763337600,
      "author": "apiraino, Jakub Beránek",
      "guid": 33726,
      "unread": true,
      "content": "<p>The Rust Project has been collecting valuable information about the Rust programming language community through our annual <a href=\"https://www.surveyhero.com/c/state-of-rust-2025\">State of Rust Survey</a> since 2016. Which means that this year marks the tenth edition of this survey!</p><p>We invite you to take this year’s survey whether you have just begun using Rust, you consider yourself an intermediate to advanced user, or you have not yet used Rust but intend to one day. The results will allow us to more deeply understand the global Rust community and how it evolves over time.</p><p>Like last year, the <a href=\"https://www.surveyhero.com/c/state-of-rust-2025\">2025 State of Rust Survey</a> will likely take you between 10 and 25 minutes, and responses are anonymous. We will accept submissions until December 17. Trends and key insights will be shared on <a href=\"https://blog.rust-lang.org\">blog.rust-lang.org</a> as soon as possible.</p><p><strong>We are offering the State of Rust Survey in the following languages (if you speak multiple languages, please pick one). Language options are available on the <a href=\"https://www.surveyhero.com/c/state-of-rust-2025\">main survey page</a>:</strong></p><ul></ul><blockquote><p>Note: the non-English translations of the survey are provided in a best-effort manner. If you find any issues with the\ntranslations, we would be glad if you could send us a <a href=\"https://github.com/rust-lang/surveys/tree/main/surveys/2025/annual-survey/translations\">pull request</a> to improve the quality of the translations!</p></blockquote><p>Please help us spread the word by sharing the <a href=\"https://www.surveyhero.com/c/state-of-rust-2025\">survey link</a> via your social media networks, at meetups, with colleagues, and in any other community that makes sense to you.</p><p>This survey would not be possible without the time, resources, and attention of the Rust Survey Team, the Rust Foundation, and other collaborators. We would also like to thank the following contributors who helped with translating the survey (in no particular order):</p><p>We appreciate your participation!</p><p><em>Click <a href=\"https://blog.rust-lang.org/2025/02/13/2024-State-Of-Rust-Survey-results\">here</a> to read a summary of last year's survey findings.</em></p><blockquote><p>By the way, the Rust Survey team is looking for new members. If you like working with data and coordinating people, and would like to help us out with managing various Rust surveys, please drop by our <a href=\"https://rust-lang.zulipchat.com/#narrow/channel/402479-t-community.2Frust-survey\">Zulip channel</a> and say hi.</p></blockquote>",
      "contentLength": 1931,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Go’s Sweet 16",
      "url": "https://go.dev/blog/16years",
      "date": 1763078400,
      "author": "Austin Clements, for the Go team",
      "guid": 33354,
      "unread": true,
      "content": "<p>This past Monday, November 10th, we celebrated the 16th anniversary of Go’s\n<a href=\"https://opensource.googleblog.com/2009/11/hey-ho-lets-go.html\" rel=\"noreferrer\" target=\"_blank\">open source\nrelease</a>!</p><p>We released <a href=\"https://go.dev/blog/go1.24\">Go 1.24 in February</a> and <a href=\"https://go.dev/blog/go1.25\">Go 1.25 in\nAugust</a>, following our now well-established and dependable release\ncadence. Continuing our mission to build the most productive language platform\nfor building production systems, these releases included new APIs for building\nrobust and reliable software, significant advances in Go’s track record for\nbuilding secure software, and some serious under-the-hood improvements.\nMeanwhile, no one can ignore the seismic shifts in our industry brought by\ngenerative AI. The Go team is applying its thoughtful and uncompromising mindset\nto the problems and opportunities of this dynamic space, working to bring Go’s\nproduction-ready approach to building robust AI integrations, products, agents,\nand infrastructure.</p><p>First released in Go 1.24 as an experiment and then graduated in Go 1.25, the\nnew <a href=\"https://pkg.go.dev/testing/synctest\" rel=\"noreferrer\" target=\"_blank\"></a> package\nsignificantly simplifies writing tests for <a href=\"https://go.dev/blog/testing-time\">concurrent, asynchronous\ncode</a>. Such code is particularly common in network services,\nand is traditionally very hard to test well. The  package works by\nvirtualizing time itself. It takes tests that used to be slow, flaky, or both,\nand makes them easy to rewrite into reliable and nearly instantaneous tests,\noften with just a couple extra lines of code. It’s also a great example of Go’s\nintegrated approach to software development: behind an almost trivial API, the\n package hides a deep integration with the Go runtime and other parts\nof the standard library.</p><p>This isn’t the only boost the  package got over the past year. The new\n<a href=\"https://pkg.go.dev/testing#B.Loop\" rel=\"noreferrer\" target=\"_blank\"></a> API is both easier to use\nthan the original  API and addresses many of the traditional—and\noften invisible!—<a href=\"https://go.dev/blog/testing-b-loop\">pitfalls</a> of writing Go benchmarks. The\n package also has new APIs that <a href=\"https://pkg.go.dev/testing#T.Context\" rel=\"noreferrer\" target=\"_blank\">make it easy to\ncleanup</a> in tests that use\n<a href=\"https://pkg.go.dev/context#Context\" rel=\"noreferrer\" target=\"_blank\"></a>, and that <a href=\"https://pkg.go.dev/testing#T.Output\" rel=\"noreferrer\" target=\"_blank\">make it\neasy</a> to write to the test’s log.</p><p>Go and containerization grew up together and work great with each other. Go 1.25\nlaunched <a href=\"https://go.dev/blog/container-aware-gomaxprocs\">container-aware scheduling</a>, making\nthis pairing even stronger. Without developers having to lift a finger, this\ntransparently adjusts the parallelism of Go workloads running in containers,\npreventing CPU throttling that can impact tail latency and improving Go’s\nout-of-the-box production-readiness.</p><p>Go 1.25’s new <a href=\"https://go.dev/blog/flight-recorder\">flight recorder</a> builds on our already\npowerful execution tracer, enabling deep insights into the dynamic behavior of\nproduction systems. While the execution tracer generally collected \ninformation to be practical in long-running production services, the flight\nrecorder is like a little time machine, allowing a service to snapshot recent\nevents in great detail  something has gone wrong.</p><h2>Secure software development</h2><p>Go continues to strengthen its commitment to secure software development, making\nsignificant strides in its native cryptography packages and evolving its\nstandard library for enhanced safety.</p><p>Go ships with a full suite of native cryptography packages in the standard\nlibrary, which reached two major milestones over the past year. A security\naudit conducted by independent security firm <a href=\"https://www.trailofbits.com/\" rel=\"noreferrer\" target=\"_blank\">Trail of\nBits</a> yielded <a href=\"https://go.dev/blog/tob-crypto-audit\">excellent\nresults</a>, with only a single low-severity finding.\nFurthermore, through a collaborative effort between the Go Security Team and\n<a href=\"https://geomys.org/\" rel=\"noreferrer\" target=\"_blank\">Geomys</a>, these packages achieved CAVP certification,\npaving the way for <a href=\"https://go.dev/blog/fips140\">full FIPS 140-3 certification</a>. This is a\nvital development for Go users in certain regulated environments. FIPS 140\ncompliance, previously a source of friction due to the need for unsupported\nsolutions, will now be seamlessly integrated, addressing concerns related to\nsafety, developer experience, functionality, release velocity, and compliance.</p><p>The Go standard library has continued to evolve to be  and\n. For example, the <a href=\"https://pkg.go.dev/os#Root\" rel=\"noreferrer\" target=\"_blank\"></a>\nAPI—added in Go 1.24—enables <a href=\"https://go.dev/blog/osroot\">traversal-resistant file system\naccess</a>, effectively combating a class of vulnerabilities where an\nattacker could manipulate programs into accessing files intended to be\ninaccessible. Such vulnerabilities are notoriously challenging to address\nwithout underlying platform and operating system support, and the new\n<a href=\"https://pkg.go.dev/os#Root\" rel=\"noreferrer\" target=\"_blank\"></a> API offers a straightforward,\nconsistent, and portable solution.</p><h2>Under-the-hood improvements</h2><p>In addition to user-visible changes, Go has made significant improvements under\nthe hood over the past year.</p><p>For Go 1.24, we completely <a href=\"https://go.dev/blog/swisstable\">redesigned the \nimplementation</a>, building on the latest and greatest ideas in\nhash table design. This change is completely transparent, and brings significant\nimprovements to  performance, lower tail latency of  operations, and\nin some cases even significant memory wins.</p><p>Go 1.25 includes an experimental and significant advancement in Go’s garbage\ncollector called <a href=\"https://go.dev/blog/greenteagc\">Green Tea</a>. Green Tea reduces garbage\ncollection overhead in many applications by at least 10% and sometimes as much\nas 40%. It uses a novel algorithm designed for the capabilities and constraints\nof today’s hardware and opens up a new design space that we’re eagerly\nexploring. For example, in the forthcoming Go 1.26 release, Green Tea will\nachieve an additional 10% reduction in garbage collector overhead on hardware\nthat supports AVX-512 vector instructions—something that would have been nigh\nimpossible to take advantage of in the old algorithm. Green Tea will be enabled\nby default in Go 1.26; users need only upgrade their Go version to benefit.</p><p>Go is about far more than the language and standard library. It’s a software\ndevelopment platform, and over the past year, we’ve also made four regular\nreleases of the <a href=\"https://go.dev/gopls\">gopls language server</a>, and have formed partnerships to\nsupport emerging new frameworks for agentic applications.</p><p>Gopls provides Go support to VS Code and other LSP-powered editors and IDEs.\nEvery release sees a litany of features and improvements to the experience of\nreading and writing Go code (see the <a href=\"https://go.dev/gopls/release/v0.17.0\">v0.17.0</a>,\n<a href=\"https://go.dev/gopls/release/v0.18.0\">v0.18.0</a>, <a href=\"https://go.dev/gopls/release/v0.19.0\">v0.19.0</a>, and\n<a href=\"https://go.dev/gopls/release/v0.20.0\">v0.20.0</a> release notes for full details, or our new\n<a href=\"https://go.dev/gopls/features\">gopls feature documentation</a>!). Some highlights include many\nnew and enhanced analyzers to help developers write more idiomatic and robust Go\ncode; refactoring support for variable extraction, variable inlining, and JSON\nstruct tags; and an <a href=\"https://go.dev/gopls/features/mcp\">experimental built-in server</a> for the\nModel Context Protocol (MCP) that exposes a subset of gopls’ functionality to AI\nassistants in the form of MCP tools.</p><p>With gopls v0.18.0, we began exploring <em>automatic code modernizers</em>. As Go\nevolves, every release brings new capabilities and new idioms; new and better\nways to do things that Go programmers have been finding other ways to do. Go\nstands by its <a href=\"https://go.dev/doc/go1compat\">compatibility promise</a>—the old way will continue\nto work in perpetuity—but nevertheless this creates a bifurcation between old\nidioms and new idioms. Modernizers are static analysis tools that recognize old\nidioms and suggest faster, more readable, more secure, more \nreplacements, and do so with push-button reliability. What  did for\n<a href=\"https://go.dev/blog/gofmt\">stylistic consistency</a>, we hope modernizers can do for idiomatic\nconsistency. We’ve integrated modernizers as IDE suggestions, where they can\nhelp developers not only maintain more consistent coding standards, but where we\nbelieve they will help developers discover new features and keep up with the\nstate of the art. We believe modernizers can also help AI coding assistants keep\nup with the state of the art and combat their proclivity to reinforce outdated\nknowledge of the Go language, APIs, and idioms. The upcoming Go 1.26 release\nwill include a total overhaul of the long-dormant  command to make it\napply the full suite of modernizers in bulk, a return to its <a href=\"https://go.dev/blog/introducing-gofix\">pre-Go 1.0\nroots</a>.</p><p>At the end of September, in collaboration with\n<a href=\"https://www.anthropic.com/\" rel=\"noreferrer\" target=\"_blank\">Anthropic</a> and the Go community, we released\n<a href=\"https://github.com/modelcontextprotocol/go-sdk/releases/tag/v1.0.0\" rel=\"noreferrer\" target=\"_blank\">v1.0.0</a> of\nthe <a href=\"https://github.com/modelcontextprotocol/go-sdk\" rel=\"noreferrer\" target=\"_blank\">official Go SDK</a> for the\n<a href=\"https://modelcontextprotocol.io/\" rel=\"noreferrer\" target=\"_blank\">Model Context Protocol (MCP)</a>. This SDK\nsupports both MCP clients and MCP servers, and underpins the new MCP\nfunctionality in gopls. Contributing this work in open source helps empower\nother areas of the growing open source agentic ecosystem built around Go, such\nas the recently released <a href=\"https://github.com/google/adk-go\" rel=\"noreferrer\" target=\"_blank\">Agent Development Kit (ADK) for\nGo</a> from <a href=\"https://www.google.com/\" rel=\"noreferrer\" target=\"_blank\">Google</a>.\nADK Go builds on the Go MCP SDK to provide an idiomatic framework for building\nmodular multi-agent applications and systems. The Go MCP SDK and ADK Go\ndemonstrate how Go’s unique strengths in concurrency, performance, and\nreliability differentiate Go for production AI development and we are expecting\nmore AI workloads to be written in Go in the coming years.</p><p>Go has an exciting year ahead of it.</p><p>We’re working on advancing developer productivity through the brand new \ncommand, deeper support for AI coding assistants, and ongoing improvements to\ngopls and VS Code Go. General availability of the Green Tea garbage collector,\nnative support for Single Instruction Multiple Data (SIMD) hardware features,\nand runtime and standard library support for writing code that scales even\nbetter to massive multicore hardware will continue to align Go with modern\nhardware and improve production efficiency. We’re focusing on Go’s “production\nstack” libraries and diagnostics, including a massive (and long in the making)\n<a href=\"https://go.dev/issue/71497\">upgrade to </a>, driven by Joe Tsai and people across\nthe Go community; <a href=\"https://go.dev/design/74609-goroutine-leak-detection-gc\">leaked goroutine\nprofiling</a>, contributed by\n<a href=\"https://www.uber.com/us/en/about/\" rel=\"noreferrer\" target=\"_blank\">Uber’s</a> Programming Systems team; and many\nother improvements to , , and other foundational packages.\nWe’re working to provide well-lit paths for building with Go and AI, evolving\nthe language platform with care for the evolving needs of today’s developers,\nand building tools and capabilities that help both human developers and AI\nassistants and systems alike.</p><p>On this 16th anniversary of Go’s open source release, we’re also looking to the\nfuture of the Go open source project itself. From its <a href=\"https://www.youtube.com/watch?v=wwoWei-GAPo\" rel=\"noreferrer\" target=\"_blank\">humble\nbeginnings</a>, Go has formed a\nthriving contributor community. To continue to best meet the needs of our\never-expanding user base, especially in a time of upheaval in the software\nindustry, we’re working on ways to better scale Go’s development\nprocesses—without losing sight of Go’s fundamental principles—and more deeply\ninvolve our wonderful contributor community.</p><p>Go would not be where it is today without our incredible user and contributor\ncommunities. We wish you all the best in the coming year!</p>",
      "contentLength": 10210,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Ingress NGINX Retirement: What You Need to Know",
      "url": "https://kubernetes.io/blog/2025/11/11/ingress-nginx-retirement/",
      "date": 1762885800,
      "author": "",
      "guid": 32734,
      "unread": true,
      "content": "<p>To prioritize the safety and security of the ecosystem, Kubernetes SIG Network and the Security Response Committee are announcing the upcoming retirement of <a href=\"https://github.com/kubernetes/ingress-nginx/\">Ingress NGINX</a>. Best-effort maintenance will continue until March 2026. Afterward, there will be no further releases, no bugfixes, and no updates to resolve any security vulnerabilities that may be discovered. <strong>Existing deployments of Ingress NGINX will continue to function and installation artifacts will remain available.</strong></p><p>We recommend migrating to one of the many alternatives. Consider <a href=\"https://gateway-api.sigs.k8s.io/guides/\">migrating to Gateway API</a>, the modern replacement for Ingress. If you must continue using Ingress, many alternative Ingress controllers are <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/\">listed in the Kubernetes documentation</a>. Continue reading for further information about the history and current state of Ingress NGINX, as well as next steps.</p><p><a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress/\">Ingress</a> is the original user-friendly way to direct network traffic to workloads running on Kubernetes. (<a href=\"https://kubernetes.io/docs/concepts/services-networking/gateway/\">Gateway API</a> is a newer way to achieve many of the same goals.) In order for an Ingress to work in your cluster, there must be an <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/\">Ingress controller</a> running. There are many Ingress controller choices available, which serve the needs of different users and use cases. Some are cloud-provider specific, while others have more general applicability.</p><p><a href=\"https://www.github.com/kubernetes/ingress-nginx\">Ingress NGINX</a> was an Ingress controller, developed early in the history of the Kubernetes project as an example implementation of the API. It became very popular due to its tremendous flexibility, breadth of features, and independence from any particular cloud or infrastructure provider. Since those days, many other Ingress controllers have been created within the Kubernetes project by community groups, and by cloud native vendors. Ingress NGINX has continued to be one of the most popular, deployed as part of many hosted Kubernetes platforms and within innumerable independent users’ clusters.</p><p>The breadth and flexibility of Ingress NGINX has caused maintenance challenges. Changing expectations about cloud native software have also added complications. What were once considered helpful options have sometimes come to be considered serious security flaws, such as the ability to add arbitrary NGINX configuration directives via the \"snippets\" annotations. Yesterday’s flexibility has become today’s insurmountable technical debt.</p><p>Despite the project’s popularity among users, Ingress NGINX has always struggled with insufficient or barely-sufficient maintainership. For years, the project has had only one or two people doing development work, on their own time, after work hours and on weekends. Last year, the Ingress NGINX maintainers <a href=\"https://kccncna2024.sched.com/event/1hoxW/securing-the-future-of-ingress-nginx-james-strong-isovalent-marco-ebert-giant-swarm\">announced</a> their plans to wind down Ingress NGINX and develop a replacement controller together with the Gateway API community. Unfortunately, even that announcement failed to generate additional interest in helping maintain Ingress NGINX or develop InGate to replace it. (InGate development never progressed far enough to create a mature replacement; it will also be retired.)</p><h2>Current State and Next Steps</h2><p>Currently, Ingress NGINX is receiving best-effort maintenance. SIG Network and the Security Response Committee have exhausted our efforts to find additional support to make Ingress NGINX sustainable. To prioritize user safety, we must retire the project.</p><p>In March 2026, Ingress NGINX maintenance will be halted, and the project will be <a href=\"https://github.com/kubernetes-retired/\">retired</a>. After that time, there will be no further releases, no bugfixes, and no updates to resolve any security vulnerabilities that may be discovered. The GitHub repositories will be made read-only and left available for reference.</p><p>Existing deployments of Ingress NGINX will not be broken. Existing project artifacts such as Helm charts and container images will remain available.</p><p>In most cases, you can check whether you use Ingress NGINX by running <code>kubectl get pods \\--all-namespaces \\--selector app.kubernetes.io/name=ingress-nginx</code> with cluster administrator permissions.</p><p>We would like to thank the Ingress NGINX maintainers for their work in creating and maintaining this project–their dedication remains impressive. This Ingress controller has powered billions of requests in datacenters and homelabs all around the world. In a lot of ways, Kubernetes wouldn’t be where it is without Ingress NGINX, and we are grateful for so many years of incredible effort.</p><p><strong>SIG Network and the Security Response Committee recommend that all Ingress NGINX users begin migration to Gateway API or another Ingress controller immediately.</strong> Many options are listed in the Kubernetes documentation: <a href=\"https://gateway-api.sigs.k8s.io/guides/\">Gateway API</a>, <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/\">Ingress</a>. Additional options may be available from vendors you work with.</p>",
      "contentLength": 4650,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Announcing Rust 1.91.1",
      "url": "https://blog.rust-lang.org/2025/11/10/Rust-1.91.1/",
      "date": 1762732800,
      "author": "The Rust Release Team",
      "guid": 32223,
      "unread": true,
      "content": "<p>The Rust team has published a new point release of Rust, 1.91.1. Rust is a\nprogramming language that is empowering everyone to build reliable and\nefficient software.</p><p>If you have a previous version of Rust installed via rustup, getting Rust\n1.91.1 is as easy as:</p><p>If you don't have it already, you can <a href=\"https://www.rust-lang.org/install.html\">get </a> from the\nappropriate page on our website.</p><p>Rust 1.91.1 includes fixes for two regressions introduced in the 1.91.0 release.</p><h3><a href=\"https://blog.rust-lang.org/2025/11/10/Rust-1.91.1/#linker-and-runtime-errors-on-wasm\" aria-hidden=\"true\"></a>\nLinker and runtime errors on Wasm</h3><p>Most targets supported by Rust identify symbols by their name, but Wasm\nidentifies them with a symbol name  a Wasm module name. The\n<a href=\"https://doc.rust-lang.org/reference/items/external-blocks.html#r-items.extern.attributes.link.wasm_import_module\"><code>#[link(wasm_import_module)]</code></a> attribute allows to\ncustomize the Wasm module name an  block refers to:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Rust 1.91.0 introduced a regression in the attribute, which could cause linker\nfailures during compilation ( errors) or the wrong\nfunction being used at runtime (leading to undefined behavior, including crashes\nand silent data corruption). This happened when the same symbol name was\nimported from two different Wasm modules across multiple Rust crates.</p><p>Rust 1.91.1 fixes the regression. More details are available in <a href=\"https://github.com/rust-lang/rust/issues/148347\">issue #148347</a>.</p><h3><a href=\"https://blog.rust-lang.org/2025/11/10/Rust-1.91.1/#cargo-target-directory-locking-broken-on-illumos\" aria-hidden=\"true\"></a>\nCargo target directory locking broken on illumos</h3><p>Cargo relies on locking the  directory during a build to prevent\nconcurrent invocations of Cargo from interfering with each other. Not all\nfilesystems support locking (most notably some networked ones): if the OS\nreturns the  error when attempting to lock, Cargo assumes locking\nis not supported and proceeds without it.</p><p>Cargo 1.91.0 switched from custom code interacting with the OS APIs to the\n<a href=\"https://doc.rust-lang.org/stable/std/fs/struct.File.html#method.lock\"></a> standard library method (recently stabilized in Rust 1.89.0). Due\nto an oversight, that method always returned  on the illumos\ntarget, causing Cargo to never lock the build directory on illumos regardless of\nwhether the filesystem supported it.</p><p>Rust 1.91.1 fixes the oversight in the standard library by enabling the\n<a href=\"https://doc.rust-lang.org/stable/std/fs/struct.File.html#method.lock\"></a> family of functions on illumos, indirectly fixing the Cargo\nregression.</p><p>Many people came together to create Rust 1.91.1. We couldn't have done it\nwithout all of you. <a href=\"https://thanks.rust-lang.org/rust/1.91.1/\">Thanks!</a></p>",
      "contentLength": 2054,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Announcing the 2025 Steering Committee Election Results",
      "url": "https://kubernetes.io/blog/2025/11/09/steering-committee-results-2025/",
      "date": 1762719000,
      "author": "",
      "guid": 32143,
      "unread": true,
      "content": "<p>The <a href=\"https://github.com/kubernetes/community/tree/master/elections/steering/2025\">2025 Steering Committee Election</a> is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2025. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.</p><p>The Steering Committee oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their <a href=\"https://github.com/kubernetes/steering/blob/master/charter.md\">charter</a>.</p><p>Thank you to everyone who voted in the election; your participation helps support the community’s continued health and success.</p><p>Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):</p><p>They join continuing members:</p><p>Maciej Szulik and Paco Xu are returning Steering Committee Members.</p><p>Thank you and congratulations on a successful election to this round’s election officers:</p><p>Thanks to the Emeritus Steering Committee Members. Your service is appreciated by the community:</p><p>And thank you to all the candidates who came forward to run for election.</p><p>You can see what the Steering Committee meetings are all about by watching past meetings on the <a href=\"https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM\">YouTube Playlist</a>.</p><p><em>This post was adapted from one written by the <a href=\"https://github.com/kubernetes/community/tree/master/communication/contributor-comms\">Contributor Comms Subproject</a>. If you want to write stories about the Kubernetes community, learn more about us.</em></p><p><em>This article was revised in November 2025 to update the information about when the steering committee meets.</em></p>",
      "contentLength": 1462,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Gateway API 1.4: New Features",
      "url": "https://kubernetes.io/blog/2025/11/06/gateway-api-v1-4/",
      "date": 1762448400,
      "author": "",
      "guid": 31561,
      "unread": true,
      "content": "<p>Ready to rock your Kubernetes networking? The Kubernetes SIG Network community presented the General Availability (GA) release of Gateway API (v1.4.0)! Released on October 6, 2025, version 1.4.0 reinforces the path for modern, expressive, and extensible service networking in Kubernetes.</p><p>Gateway API v1.4.0 brings three new features to the \n(Gateway API's GA release channel):</p><ul><li><strong>BackendTLSPolicy for TLS between gateways and backends</strong></li><li><strong> in GatewayClass status</strong></li></ul><p>and introduces three new experimental features:</p><ul><li><strong>Mesh resource for service mesh configuration</strong></li><li> to ease configuration burden**</li><li><strong> filter for HTTPRoute</strong></li></ul><h2>Graduations to Standard Channel</h2><p><a href=\"https://gateway-api.sigs.k8s.io/api-types/backendtlspolicy\">BackendTLSPolicy</a> is a new Gateway API type for specifying the TLS configuration\nof the connection from the Gateway to backend pod(s).\n. Prior to the introduction of BackendTLSPolicy, there was no API specification\nthat allowed encrypted traffic on the hop from Gateway to backend.</p><p>The  configuration requires a hostname. This \nserves two purposes. It is used as the SNI header when connecting to the backend and\nfor authentication, the certificate presented by the backend must match this hostname,\n is explicitly specified.</p><p>If  (SANs) are specified, the  is only used for SNI, and authentication is performed against the SANs instead. If you still need to authenticate against the hostname value in this case, you MUST add it to the  list.</p><p>BackendTLSPolicy  configuration also requires either  or .\n refer to one or more (up to 8) PEM-encoded TLS certificate bundles. If there are no specific certificates to use,\nthen depending on your implementation, you may use ,\nset to \"System\" to tell the Gateway to use an implementation-specific set of trusted CA Certificates.</p><p>In this example, the BackendTLSPolicy is configured to use certificates defined in the auth-cert ConfigMap\nto connect with a TLS-encrypted upstream connection where pods backing the auth service are expected to serve a\nvalid certificate for . It uses  with a Hostname type, but you may also use a URI type.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>In this example, the BackendTLSPolicy is configured to use system certificates to connect with a TLS-encrypted backend connection where Pods backing the dev Service are expected to serve a valid certificate for .</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>GatewayClass status has a new field, .\nThis addition allows implementations to declare the set of features they support. This provides a clear way for users and tools to understand the capabilities of a given GatewayClass.</p><p>This feature's name for conformance tests (and GatewayClass status reporting) is .\nImplementations must populate the  field in the  of the GatewayClass  the GatewayClass\nis accepted, or in the same operation.</p><p>Here’s an example of a  published under GatewayClass' :</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Graduation of SupportedFeatures to Standard, helped improve the conformance testing process for Gateway API.\nThe conformance test suite will now automatically run tests based on the features populated in the GatewayClass' status.\nThis creates a strong, verifiable link between an implementation's declared capabilities and the test results,\nmaking it easier for implementers to run the correct conformance tests and for users to trust the conformance reports.</p><p>This means when the SupportedFeatures field is populated in the GatewayClass status there will be no need for additional\nconformance tests flags like , or  or .\nIt's important to note that Mesh features are an exception to this and can be tested for conformance by using\n, or by manually providing any combination of features related flags until the dedicated resource\ngraduates from the experimental channel.</p><p>This enhancement enables route rules to be explicitly identified and referenced across the Gateway API ecosystem.\nSome of the key use cases include:</p><ul><li> Allowing status conditions to reference specific rules directly by name.</li><li> Making it easier to identify individual rules in logs, traces, and metrics.</li><li> Enabling policies (<a href=\"https://gateway-api.sigs.k8s.io/geps/gep-773\">GEP-713</a>) to target specific route rules via the  field in their .</li><li> Simplifying filtering and referencing of route rules in tools such as , , and general-purpose utilities like  and .</li><li><strong>Internal configuration mapping:</strong> Facilitating the generation of internal configurations that reference route rules by name within gateway and mesh implementations.</li></ul><p>This follows the same well-established pattern already adopted for Gateway listeners, Service ports, Pods (and containers),\nand many other Kubernetes resources.</p><p>While the new name field is  (so existing resources remain valid), its use is .\nImplementations are not expected to assign a default value, but they may enforce constraints such as immutability.</p><p>Finally, keep in mind that the <a href=\"https://gateway-api.sigs.k8s.io/geps/gep-995/?h=995#format\">name format</a> is validated,\nand other fields (such as <a href=\"https://gateway-api.sigs.k8s.io/reference/spec/?h=sectionname#sectionname\"></a>)\nmay impose additional, indirect constraints.</p><h2>Experimental channel changes</h2><h3>Enabling external Auth for HTTPRoute</h3><p>Giving Gateway API the ability to enforce authentication and maybe authorization as well at the Gateway or HTTPRoute level has been a highly requested feature for a long time. (See the <a href=\"https://github.com/kubernetes-sigs/gateway-api/issues/1494\">GEP-1494 issue</a> for some background.)</p><p>This Gateway API release adds an Experimental filter in HTTPRoute that tells the Gateway API implementation to call out to an external service to authenticate (and, optionally, authorize) requests.</p><p>This filter is based on the <a href=\"https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/ext_authz_filter#config-http-filters-ext-authz\">Envoy ext_authz API</a>, and allows talking to an Auth service that uses either gRPC or HTTP for its protocol.</p><p>Both methods allow the configuration of what headers to forward to the Auth service, with the HTTP protocol allowing some extra information like a prefix path.</p><p>A HTTP example might look like this (noting that this example requires the Experimental channel to be installed and an implementation that supports External Auth to actually understand the config):</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>This allows the backend Auth service to use the supplied headers to make a determination about the authentication for the request.</p><p>When a request is allowed, the external Auth service will respond with a 200 HTTP response code, and optionally extra headers to be included in the request that is forwarded to the backend. When the request is denied, the Auth service will respond with a 403 HTTP response.</p><p>Since the Authorization header is used in many authentication methods, this method can be used to do Basic, Oauth, JWT, and other common authentication and authorization methods.</p><p>Gateway API v1.4.0 introduces a new experimental Mesh resource, which provides a way to configure mesh-wide settings and discover the features supported by a given mesh implementation. This resource is analogous to the Gateway resource and will initially be mainly used for conformance testing, with plans to extend its use to off-cluster Gateways in the future.</p><p>The Mesh resource is cluster-scoped and, as an experimental feature, is named  and resides in the <code>gateway.networking.x-k8s.io</code> API group. A key field is controllerName, which specifies the mesh implementation responsible for the resource. The resource's  stanza indicates whether the mesh implementation has accepted it and lists the features the mesh supports.</p><p>One of the goals of this GEP is to avoid making it more difficult for users to adopt a mesh. To simplify adoption, mesh implementations are expected to create a default Mesh resource upon startup if one with a matching  doesn't already exist. This avoids the need for manual creation of the resource to begin using a mesh.</p><p>The new XMesh API kind, within the gateway.networking.x-k8s.io/v1alpha1 API group,\nprovides a central point for mesh configuration and feature discovery (source).</p><p>A minimal XMesh object specifies the :</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>The mesh implementation populates the status field to confirm it has accepted the resource and to list its supported features ( source):</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h3>Introducing default Gateways</h3><p>For application developers, one common piece of feedback has been the need to explicitly name a parent Gateway for every single north-south Route. While this explicitness prevents ambiguity, it adds friction, especially for developers who just want to expose their application to the outside world without worrying about the underlying infrastructure's naming scheme. To address this, we have introduce the concept of .</p><h4>For application developers: Just \"use the default\"</h4><p>As an application developer, you often don't care about the specific Gateway your traffic flows through, you just want it to work. With this enhancement, you can now create a Route and simply ask it to use a default Gateway.</p><p>This is done by setting the new  field in your Route's .</p><p>Here’s a simple  that uses a default Gateway:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>That's it! No more need to hunt down the correct Gateway name for your environment. Your Route is now a \"defaulted Route.\"</p><h4>For cluster operators: You're still in control</h4><p>This feature doesn't take control away from cluster operators (\"Chihiro\").\nIn fact, they have explicit control over which Gateways can act as a default. A Gateway will only accept these  if it is configured to do so.</p><p>You can also use a ValidatingAdmissionPolicy to either require or even forbid for Routes to rely on a default Gateway.</p><p>As a cluster operator, you can designate a Gateway as a default\nby setting the (new)  field:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Operators can choose to have no default Gateways, or even multiple.</p><h4>How it works and key details</h4><ul><li><p>To maintain a clean, GitOps-friendly workflow, a default Gateway does  modify the  of your Route. Instead, the binding is reflected in the Route's  field. You can always inspect the  stanza of your Route to see exactly which Gateway or Gateways have accepted it. This preserves your original intent and avoids conflicts with CD tools.</p></li><li><p>The design explicitly supports having multiple Gateways designated as defaults within a cluster. When this happens, a defaulted Route will bind to  of them. This enables cluster operators to perform zero-downtime migrations and testing of new default Gateways.</p></li><li><p>You can create a single Route that handles both north-south traffic (traffic entering or leaving the cluster, via a default Gateway) and east-west/mesh traffic (traffic between services within the cluster), by explicitly referencing a Service in .</p></li></ul><p>Default Gateways represent a significant step forward in making the Gateway API simpler and more intuitive for everyday use cases, bridging the gap between the flexibility needed by operators and the simplicity desired by developers.</p><h3>Configuring client certificate validation</h3><p>This release brings updates for configuring client certificate validation, addressing a critical security vulnerability related to connection reuse.\nHTTP connection coalescing is a web performance optimization that allows a client to reuse an existing TLS connection\nfor requests to different domains. While this reduces the overhead of establishing new connections, it introduces a security risk\nin the context of API gateways.\nThe ability to reuse a single TLS connection across multiple Listeners brings the need to introduce shared client certificate\nconfiguration in order to avoid unauthorized access.</p><h4>Why SNI-based mTLS is not the answer</h4><p>One might think that using Server Name Indication (SNI) to differentiate between Listeners would solve this problem.\nHowever, TLS SNI is not a reliable mechanism for enforcing security policies in a connection coalescing scenario.\nA client could use a single TLS connection for multiple peer connections, as long as they are all covered by the same certificate.\nThis means that a client could establish a connection by indicating one peer identity (using SNI), and then reuse that connection\nto access a different virtual host that is listening on the same IP address and port. That reuse, which is controlled by client side\nheuristics, could bypass mutual TLS policies that were specific to the second listener configuration.</p><p>Here's an example to help explain it:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>I have configured a Gateway with two listeners, both having overlapping hostnames.\nMy intention is for the  listener to be accessible only by clients presenting the  certificate.\nIn contrast, the  listener should allow access to a broader audience using any certificate valid for the  domain.</p><p>Consider a scenario where a client initially connects to . The server requests and successfully validates the\n certificate, establishing the connection. Subsequently, the same client wishes to access other sites within this domain,\nsuch as , which is handled by the  listener. Due to connection reuse,\nclients can access  backends without an additional TLS handshake on the existing connection.\nThis process functions as expected.</p><p>However, a critical security vulnerability arises when the order of access is reversed.\nIf a client first connects to  and presents a valid  certificate, the connection is successfully established.\nIf this client then attempts to access , the existing connection's client certificate will not be re-validated.\nThis allows the client to bypass the specific certificate requirement for the  backend, leading to a serious security breach.</p><h4>The solution: per-port TLS configuration</h4><p>The updated Gateway API gains a  field in the  of a Gateway, that allows you to define a default client certificate\nvalidation configuration for all Listeners, and then if needed override it on a per-port basis. This provides a flexible and\npowerful way to manage your TLS policies.</p><p>Here’s a look at the updated API definitions (shown as Go source code):</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><h3>Standard GRPCRoute -  field required (technicality)</h3><p>The promotion of GRPCRoute to Standard introduces a minor but technically breaking change regarding the presence of the top-level  field.\nAs part of achieving Standard status, the Gateway API has tightened the OpenAPI schema validation within the GRPCRoute\nCustomResourceDefinition (CRD)\nto explicitly ensure the spec field is required for all GRPCRoute resources.\nThis change enforces stricter conformance to Kubernetes object standards and enhances the resource's stability and predictability.\nWhile it is highly unlikely that users were attempting to define a GRPCRoute without any specification, any existing automation\nor manifests that might have relied on a relaxed interpretation allowing a completely absent  field will now fail validation\nand  be updated to include the  field, even if empty.</p><h3>Experimental CORS support in HTTPRoute - breaking change for  field</h3><p>The Gateway API subproject has introduced a breaking change to the Experimental CORS support in HTTPRoute, concerning the  field\nwithin the CORS policy.\nThis field's definition has been strictly aligned with the upstream CORS specification, which dictates that the corresponding\n<code>Access-Control-Allow-Credentials</code> header must represent a Boolean value.\nPreviously, the implementation might have been overly permissive, potentially accepting non-standard or string representations such as\n due to relaxed schema validation.\nUsers who were configuring CORS rules must now review their manifests and ensure the value for \nstrictly conforms to the new, more restrictive schema.\nAny existing HTTPRoute definitions that do not adhere to this stricter validation will now be rejected by the API server,\nrequiring a configuration update to maintain functionality.</p><h2>Improving the development and usage experience</h2><p>As part of this release, we have improved some of the developer experience workflow:</p><ul><li>Added <a href=\"https://github.com/kubernetes-sigs/kube-api-linter\">Kube API Linter</a> to the CI/CD pipelines, reducing the burden of API reviewers and also reducing the amount of common mistakes.</li><li>Improving the execution time of CRD tests with the usage of <a href=\"https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/envtest\"></a>.</li></ul><p>Additionally, as part of the effort to improve Gateway API usage experience, some efforts were made to remove some ambiguities and some old tech-debts from our documentation website:</p><ul><li>The API reference is now explicit when a field is .</li><li>The GEP (GatewayAPI Enhancement Proposal) navigation bar is automatically generated, reflecting the real status of the enhancements.</li></ul><p>Unlike other Kubernetes APIs, you don't need to upgrade to the latest version of\nKubernetes to get the latest version of Gateway API. As long as you're running\nKubernetes 1.26 or later, you'll be able to get up and running with this version\nof Gateway API.</p><p>As of this writing, seven implementations are already conformant with Gateway API v1.4.0. In alphabetical order:</p><p>Wondering when a feature will be added? There are lots of opportunities to get\ninvolved and help define the future of Kubernetes routing APIs for both ingress\nand service mesh.</p><p>The maintainers would like to thank  who's contributed to Gateway\nAPI, whether in the form of commits to the repo, discussion, ideas, or general\nsupport. We could never have made this kind of progress without the support of\nthis dedicated and active community.</p><h2>Related Kubernetes blog articles</h2>",
      "contentLength": 16523,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    }
  ],
  "tags": [
    "dev"
  ]
}
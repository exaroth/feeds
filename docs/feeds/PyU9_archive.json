{
  "id": "PyU9",
  "title": "Dev",
  "displayTitle": "Dev",
  "url": "",
  "feedLink": "",
  "isQuery": true,
  "isEmpty": false,
  "isHidden": false,
  "itemCount": 12,
  "items": [
    {
      "title": "Python 3.15.0 alpha 5 (yes, another alpha!)",
      "url": "https://pythoninsider.blogspot.com/2026/01/python-3150-alpha-5-yes-another-alpha.html",
      "date": 1768412280,
      "author": "Hugo",
      "guid": 35961,
      "unread": true,
      "content": "<p><i>Note: 3.15.0a4 was accidentally built against  from\n2025-12-23 instead of 2026-01-13, so this 3.15.0a5 is an extra release\ncorrectly built against 2026-01-14.</i></p><p><strong>This is an early developer preview of Python\n3.15</strong></p><p>Python 3.15 is still in development. This release, 3.15.0a5, is the\nfifth of  eight planned alpha releases.</p><p>Alpha releases are intended to make it easier to test the current\nstate of new features and bug fixes and to test the release process.</p><p>During the alpha phase, features may be added up until the start of\nthe beta phase (2026-05-05) and, if necessary, may be modified or\ndeleted up until the release candidate phase (2026-07-28). Please keep\nin mind that this is a preview release and its use is\n recommended for production environments.</p><p>Many new features for Python 3.15 are still being planned and\nwritten. Among the new major new features and changes so far:</p><ul><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-sampling-profiler\">PEP\n799</a>: A new high-frequency, low-overhead, statistical sampling\nprofiler and dedicated profiling package</li><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-utf8-default\">PEP\n686</a>: Python now uses UTF-8 as the default encoding</li><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-pep782\">PEP\n782</a>: A new  C API to create a Python bytes\nobject</li><li>The <a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-jit\">JIT\ncompiler</a> has been significantly upgraded, with 4-5% geometric mean\nperformance improvement on x86-64 Linux over the standard interpreter,\nand 7-8% speedup on AArch64 macOS over the tail-calling interpreter</li><li><small>(Hey,  if a feature\nyou find important is missing from this list, let Hugo\nknow.)</small></li></ul><p>The next pre-release of Python 3.15 will be 3.15.0a6, currently\nscheduled for 2026-02-10.</p><blockquote><p>At last it was given out that some time next day the ship would\ncertainly sail. So next morning, Queequeg and I took a very early\nstart.</p></blockquote><p>Thanks to all of the many volunteers who help make Python Development\nand these releases possible! Please consider supporting our efforts by\nvolunteering yourself or through organisation contributions to the <a href=\"https://www.python.org/psf/donations/\">Python Software\nFoundation</a>.</p><p>Regards from a still snowfully subzero Helsinki,</p><p>Your release team,\n  Hugo van Kemenade\n  Steve Dower\n  </p>",
      "contentLength": 1941,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "What does it take to ship Rust in safety-critical?",
      "url": "https://blog.rust-lang.org/2026/01/14/what-does-it-take-to-ship-rust-in-safety-critical/",
      "date": 1768348800,
      "author": "Pete LeVasseur",
      "guid": 35952,
      "unread": true,
      "content": "<p><em>This is another post in our series covering what we learned through the Vision Doc process. In <a href=\"https://blog.rust-lang.org/2025/12/03/lessons-learned-from-the-rust-vision-doc-process/\">our first post</a>, we described the overall approach and what we learned about doing user research. In <a href=\"https://blog.rust-lang.org/2025/12/19/what-do-people-love-about-rust/\">our second post</a>, we explored what people love about Rust. This post goes deep on one domain: safety-critical software.</em></p><p>When we set out on the Vision Doc work, one area we wanted to explore in depth was safety-critical systems: software where malfunction can result in injury, loss of life, or environmental harm. Think vehicles, airplanes, medical devices, industrial automation. We spoke with engineers at OEMs, integrators, and suppliers across automotive (mostly), industrial, aerospace, and medical contexts.</p><p>What we found surprised us a bit. The conversations kept circling back to a single tension: Rust's compiler-enforced guarantees support much of what Functional Safety Engineers and Software Engineers in these spaces spend their time preventing, but once you move beyond prototyping into the higher-criticality parts of a system, the ecosystem support thins out fast. There is no MATLAB/Simulink Rust code generation. There is no OSEK or AUTOSAR Classic-compatible RTOS written in Rust or with first-class Rust support. The tooling for qualification and certification is still maturing.</p><h2><a href=\"https://blog.rust-lang.org/2026/01/14/what-does-it-take-to-ship-rust-in-safety-critical/#quick-context-what-makes-software-safety-critical\" aria-hidden=\"true\"></a>\nQuick context: what makes software \"safety-critical\"</h2><p>If you've never worked in these spaces, here's the short version. Each safety-critical domain has standards that define a ladder of integrity levels: ISO 26262 in automotive, IEC 61508 in industrial, IEC 62304 in medical devices, DO-178C in aerospace. The details differ, but the shape is similar: as you climb the ladder toward higher criticality, the demands on your development process, verification, and evidence all increase, and so do the costs.</p><p>This creates a strong incentive for : isolate the highest-criticality logic into the smallest surface area you can, and keep everything else at lower levels where costs are more manageable and you can move faster.</p><p>We'll use automotive terminology in this post (QM through ASIL D) since that's where most of our interviews came from, but the patterns generalize. These terms represent increasing levels of safety-criticality, with QM being the lowest and ASIL D being the highest. The story at low criticality looks very different from the story at high criticality, regardless of domain.</p><h2><a href=\"https://blog.rust-lang.org/2026/01/14/what-does-it-take-to-ship-rust-in-safety-critical/#rust-is-already-in-production-for-safety-critical-systems\" aria-hidden=\"true\"></a>\nRust is already in production for safety-critical systems</h2><p>Before diving into the challenges, it is worth noting that Rust is not just being evaluated in these domains. It is deployed and running in production.</p><p>We spoke with a principal firmware engineer working on mobile robotics systems certified to IEC 61508 SIL 2:</p><blockquote><p>\"We had a new project coming up that involved a safety system. And in the past, we'd always done these projects in C using third party stack analysis and unit testing tools that were just generally never very good, but you had to do them as part of the safety rating standards. Rust presented an opportunity where 90% of what the stack analysis stuff had to check for is just done by the compiler. That combined with the fact that now we had a safety qualified compiler to point to was kind of a breakthrough.\" -- Principal Firmware Engineer (mobile robotics)</p></blockquote><p>We also spoke with an engineer at a medical device company deploying IEC 62304 Class B software to intensive care units:</p><blockquote><p>\"All of the product code that we deploy to end users and customers is currently in Rust. We do EEG analysis with our software and that's being deployed to ICUs, intensive care units, and patient monitors.\" -- Rust developer at a medical device company</p></blockquote><blockquote><p>\"We changed from this Python component to a Rust component and I think that gave us a 100-fold speed increase.\" -- Rust developer at a medical device company</p></blockquote><p>These are not proofs of concept. They are shipping systems in regulated environments, going through audits and certification processes. The path is there. The question is how to make it easier for the next teams coming through.</p><h2><a href=\"https://blog.rust-lang.org/2026/01/14/what-does-it-take-to-ship-rust-in-safety-critical/#rust-adoption-is-easiest-at-qm-and-the-constraints-sharpen-fast\" aria-hidden=\"true\"></a>\nRust adoption is easiest at QM, and the constraints sharpen fast</h2><p>At low criticality, teams described a pragmatic approach: use Rust and the crates ecosystem to move quickly, then harden what you ship. One architect at an automotive OEM told us:</p><blockquote><p>\"We can use any crate [from crates.io] [..] we have to take care to prepare the software components for production usage.\" -- Architect at Automotive OEM</p></blockquote><p>But at higher levels, third-party dependencies become difficult to justify. Teams either rewrite, internalize, or strictly constrain what they use. An embedded systems engineer put it bluntly:</p><blockquote><p>\"We tend not to use 3rd party dependencies or nursery crates [..] solutions become kludgier as you get lower in the stack.\" -- Firmware Engineer</p></blockquote><p>Some teams described building escape hatches, abstraction layers designed for future replacement:</p><blockquote><p>\"We create an interface that we'd eventually like to have to simplify replacement later on [..] sometimes rewrite, but even if re-using an existing crate we often change APIs, write more tests.\" -- Team Lead at Automotive Supplier (ASIL D target)</p></blockquote><p>Even teams that do use crates from crates.io described treating that as a temporary accelerator, something to track carefully and remove from critical paths before shipping:</p><blockquote><p>\"We use crates mainly for things in the beginning where we need to set up things fast, proof of concept, but we try to track those dependencies very explicitly and for the critical parts of the software try to get rid of them in the long run.\" -- Team lead at an automotive software company developing middleware in Rust</p></blockquote><p>In aerospace, the \"control the whole stack\" instinct is even stronger:</p><blockquote><p>\"In aerospace there's a notion of we must own all the code ourselves. We must have control of every single line of code.\" -- Engineering lead in aerospace</p></blockquote><p>This is the first big takeaway: <strong>a lot of \"Rust in safety-critical\" is not just about whether Rust compiles for a target. It is about whether teams can assemble an evidence-friendly software stack and keep it stable over long product lifetimes.</strong></p><p>Many interviewees framed Rust's value in terms of work shifted earlier and made more repeatable by the compiler. This is not just \"nice,\" it changes how much manual review you can realistically afford. Much of what was historically process-based enforcement through coding standards like MISRA C and CERT C becomes a language-level concern in Rust, checked by the compiler rather than external static analysis or manual review.</p><blockquote><p>\"Roughly 90% of what we used to check with external tools is built into Rust's compiler.\" -- Principal Firmware Engineer (mobile robotics)</p></blockquote><p>We heard variations of this from teams dealing with large codebases and varied skill levels:</p><blockquote><p>\"We cannot control the skill of developers from end to end. We have to check the code quality. Rust by checking at compile time, or Clippy tools, is very useful for our domain.\" -- Engineer at a major automaker</p></blockquote><p>Even on smaller teams, the review load matters:</p><blockquote><p>\"I usually tend to work on teams between five and eight. Even so, it's too much code. I feel confident moving faster, a certain class of flaws that you aren't worrying about.\" -- Embedded systems engineer (mobile robotics)</p></blockquote><p>Closely related: people repeatedly highlighted Rust's consistency around error handling:</p><blockquote><p>\"Having a single accepted way of handling errors used throughout the ecosystem is something that Rust did completely right.\" -- Automotive Technical Lead</p></blockquote><p>For teams building products with 15-to-20-year lifetimes and \"teams of teams,\" compiler-enforced invariants scale better than \"we will just review harder.\"</p><p>A common pattern in safety-critical environments is conservative toolchain selection. But engineers pointed out a tension: older toolchains carry their own defect history.</p><blockquote><p>\"[..] traditional wisdom is that after something's been around and gone through motions / testing then considered more stable and safer [..] older compilers used tend to have more bugs [and they become] hard to justify\" -- Software Engineer at an Automotive supplier</p></blockquote><p>Rust's edition system was described as a real advantage here, especially for incremental migration strategies that are common in automotive programs:</p><blockquote><p>\"[The edition system is] golden for automotive, where incremental migration is essential.\" -- Software Engineer at major Automaker</p></blockquote><p>In practice, \"stability\" is also about managing the mismatch between what the platform supports and what the ecosystem expects. Teams described pinning Rust versions, then fighting dependency drift:</p><blockquote><p>\"We can pin the Rust toolchain, but because almost all crates are implemented for the latest versions, we have to downgrade. It's very time-consuming.\" -- Engineer at a major automaker</p></blockquote><p>For safety-critical adoption, \"stability\" is operational. Teams need to answer questions like: What does a Rust upgrade change, and what does it not change? What are the bounds on migration work? How do we demonstrate we have managed upgrade risk?</p><h2><a href=\"https://blog.rust-lang.org/2026/01/14/what-does-it-take-to-ship-rust-in-safety-critical/#target-support-matters-in-practical-ways\" aria-hidden=\"true\"></a>\nTarget support matters in practical ways</h2><p>Safety-critical software often runs on long-lived platforms and RTOSs. Even when \"support exists,\" there can be caveats. Teams described friction around targets like QNX, where upstream Rust support exists but with limitations (for example, QNX 8.0 support is currently  only).</p><p>This connects to Rust's target tier policy: the policy itself is clear, but regulated teams still need to map \"tier\" to \"what can I responsibly bet on for this platform and this product lifetime.\"</p><blockquote><p>\"I had experiences where all of a sudden I was upgrading the compiler and my toolchain and dependencies didn't work anymore for the Tier 3 target we're using. That's simply not acceptable. If you want to invest in some technology, you want to have a certain reliability.\" -- Senior software engineer at a major automaker</p></blockquote><h2><a href=\"https://blog.rust-lang.org/2026/01/14/what-does-it-take-to-ship-rust-in-safety-critical/#core-is-the-spine-and-it-sets-expectations\" aria-hidden=\"true\"></a> is the spine, and it sets expectations</h2><p>In  environments,  becomes the spine of Rust. Teams described it as both rich enough to build real products and small enough to audit.</p><p>A lot of Rust's safety leverage lives there:  and , slices, iterators,  and , atomics, , . But we also heard a consistent shape of gaps: many embedded and safety-critical projects want -friendly building blocks (fixed-size collections, queues) and predictable math primitives, but do not want to rely on \"just any\" third-party crate at higher integrity levels.</p><blockquote><p>\"Most of the math library stuff is not in core, it's in std. Sin, cosine... the workaround for now has been the libm crate. It'd be nice if it was in core.\" -- Principal Firmware Engineer (mobile robotics)</p></blockquote><h2><a href=\"https://blog.rust-lang.org/2026/01/14/what-does-it-take-to-ship-rust-in-safety-critical/#async-is-appealing-but-the-long-run-story-is-not-settled\" aria-hidden=\"true\"></a>\nAsync is appealing, but the long-run story is not settled</h2><p>Some safety-critical-adjacent systems are already heavily asynchronous: daemons, middleware frameworks, event-driven architectures. That makes Rust's async story interesting.</p><p>But people also expressed uncertainty about ecosystem lock-in and what it would take to use async in higher-criticality components. One team lead developing middleware told us:</p><blockquote><p>\"We're not sure how async will work out in the long-run [in Rust for safety-critical]. [..] A lot of our software is highly asynchronous and a lot of our daemons in the AUTOSAR Adaptive Platform world are basically following a reactor pattern. [..] [C++14] doesn't really support these concepts, so some of this is lack of familiarity.\" -- Team lead at an automotive software company developing middleware in Rust</p></blockquote><p>And when teams look at async through an ISO 26262 lens, the runtime question shows up immediately:</p><blockquote><p>\"If we want to make use of async Rust, of course you need some runtime which is providing this with all the quality artifacts and process artifacts for ISO 26262.\" -- Team lead at an automotive software company developing middleware in Rust</p></blockquote><p>Async is not \"just a language feature\" in safety-critical contexts. It pulls in runtime choices, scheduling assumptions, and, at higher integrity levels, the question of what it would mean to certify or qualify the relevant parts of the stack.</p><p><strong>Find ways to help the safety-critical community support their own needs.</strong> Open source helps those who help themselves. The Ferrocene Language Specification (FLS) shows this working well: it started as an industry effort to create a specification suitable for safety-qualification of the Rust compiler, companies invested in the work, and it now has a sustainable home under the Rust Project with a team actively maintaining it.</p><p>Contrast this with MC/DC coverage support in rustc. Earlier efforts stalled due to lack of sustained engagement from safety-critical companies. The technical work was there, but without industry involvement to help define requirements, validate the implementation, and commit to maintaining it, the effort lost momentum. A major concern was that the MC/DC code added maintenance burden to the rest of the coverage infrastructure without a clear owner. Now in 2026, there is renewed interest in doing this the right way: companies are working through the Safety-Critical Rust Consortium to create a Rust Project Goal in 2026 to collaborate with the Rust Project on MC/DC support. The model is shared ownership of requirements, with primary implementation and maintenance done by companies with a vested interest in safety-critical, done in a way that does not impede maintenance of the rest of the coverage code.</p><p>The remaining recommendations follow this pattern: the Safety-Critical Rust Consortium can help the community organize requirements and drive work, with the Rust Project providing the deep technical knowledge of Rust Project artifacts needed for successful collaboration. The path works when both sides show up.</p><p><strong>Establish ecosystem-wide MSRV conventions.</strong> The dependency drift problem is real: teams pin their Rust toolchain for stability, but crates targeting the latest compiler make this difficult to sustain. An LTS release scheme, combined with encouraging libraries to maintain MSRV compatibility with LTS releases, could reduce this friction. This would require coordination between the Rust Project (potentially the release team) and the broader ecosystem, with the Safety-Critical Rust Consortium helping to articulate requirements and adoption patterns.</p><p><strong>Turn \"target tier policy\" into a safety-critical onramp.</strong> The friction we heard is not about the policy being unclear, it is about translating \"tier\" into practical decisions. A short, target-focused readiness checklist would help: Which targets exist? Which ones are  only? What is the last known tested OS version? What are the top blockers? The raw ingredients exist in rustc docs, release notes, and issue trackers, but pulling them together in one place would lower the barrier. Clearer, consolidated information also makes it easier for teams who depend on specific targets to contribute to maintaining them. The Safety-Critical Rust Consortium could lead this effort, working with compiler team members and platform maintainers to keep the information accurate.</p><p><strong>Document \"dependency lifecycle\" patterns teams are already using.</strong> The QM story is often: use crates early, track carefully, shrink dependencies for higher-criticality parts. The ASIL B+ story is often: avoid third-party crates entirely, or use abstraction layers and plan to replace later. Turning those patterns into a reusable playbook would help new teams make the same moves with less trial and error. This seems like a natural fit for the Safety-Critical Rust Consortium's liaison work.</p><p><strong>Define requirements for a safety-case friendly async runtime.</strong> Teams adopting async in safety-critical contexts need runtimes with appropriate quality and process artifacts for standards like ISO 26262. Work is already happening in this space. The Safety-Critical Rust Consortium could lead the effort to define what \"safety-case friendly\" means in concrete terms, working with the async working group and libs team on technical feasibility and design.</p><p><strong>Treat interop as part of the safety story.</strong> Many teams are not going to rewrite their world in Rust. They are going to integrate Rust into existing C and C++ systems and carry that boundary for years. Guidance and tooling to keep interfaces correct, auditable, and in sync would help. The compiler team and lang team could consider how FFI boundaries are surfaced and checked, informed by requirements gathered through the Safety-Critical Rust Consortium.</p><blockquote><p>\"We rely very heavily on FFI compatibility between C, C++, and Rust. In a safety-critical space, that's where the difficulty ends up being, generating bindings, finding out what the problem was.\" -- Embedded systems engineer (mobile robotics)</p></blockquote><p>To sum up the main points in this post:</p><ul><li>Rust is already deployed in production for safety-critical systems, including mobile robotics (IEC 61508 SIL 2) and medical devices (IEC 62304 Class B). The path exists.</li><li>Rust's defaults (memory safety, thread safety, strong typing) map directly to much of what Functional Safety Engineers spend their time preventing. But ecosystem support thins out as you move toward higher-criticality software.</li><li>At low criticality (QM), teams use crates freely and harden later. At higher levels (ASIL B+), third-party dependencies become difficult to justify, and teams rewrite, internalize, or build abstraction layers for future replacement.</li><li>The compiler is doing work that used to require external tools and manual review. Much of what was historically process-based enforcement through standards like MISRA C and CERT C becomes a language-level concern, checked by the compiler. That can scale better than \"review harder\" for long-lived products with large teams and supports engineers in these domains feeling more secure in the systems they ship.</li><li>Stability is operational: teams need to explain what upgrades change, manage dependency drift, and map target tier policies to their platform reality.</li><li>Async is appealing for middleware and event-driven systems, but the runtime and qualification story is not settled for higher-criticality use.</li></ul><p>We make six recommendations: find ways to help the safety-critical community support their own needs, establish ecosystem-wide MSRV conventions, create target-focused readiness checklists, document dependency lifecycle patterns, define requirements for safety-case friendly async runtimes, and treat C/C++ interop as part of the safety story.</p><p>Hearing concrete constraints, examples of assessor feedback, and what \"evidence\" actually looks like in practice is incredibly helpful. The goal is to make Rust's strengths more accessible in environments where correctness and safety are not optional.</p>",
      "contentLength": 18462,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Python 3.15.0 alpha 4",
      "url": "https://pythoninsider.blogspot.com/2026/01/python-3150-alpha-4.html",
      "date": 1768335840,
      "author": "Hugo",
      "guid": 35960,
      "unread": true,
      "content": "<i>Edit: This 3.15.0a4 was accidentally built against `main` from 2025-12-23 instead of 2026-01-13, so <a href=\"https://discuss.python.org/t/python-3-15-0-alpha-5-yes-another-alpha/105721/1\">3.15.0a5</a> is an extra release correctly built against 2026-01-14.</i><p><strong>This is an early developer preview of Python\n3.15</strong></p><p>Python 3.15 is still in development. This release, 3.15.0a4, is the\nfourth of seven planned alpha releases.</p><p>Alpha releases are intended to make it easier to test the current\nstate of new features and bug fixes and to test the release process.</p><p>During the alpha phase, features may be added up until the start of\nthe beta phase (2026-05-05) and, if necessary, may be modified or\ndeleted up until the release candidate phase (2026-07-28). Please keep\nin mind that this is a preview release and its use is\n recommended for production environments.</p><p>Many new features for Python 3.15 are still being planned and\nwritten. Among the new major new features and changes so far:</p><ul><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-sampling-profiler\">PEP\n799</a>: A new high-frequency, low-overhead, statistical sampling\nprofiler and dedicated profiling package</li><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-utf8-default\">PEP\n686</a>: Python now uses UTF-8 as the default encoding</li><li><a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-pep782\">PEP\n782</a>: A new  C API to create a Python bytes\nobject</li><li>The <a href=\"https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-jit\">JIT\ncompiler</a> has been significantly upgraded, with 3-4% geometric mean\nperformance improvement on x86-64 Linux over the standard interpreter,\nand 7-8% speedup on AArch64 macOS over the tail-calling interpreter</li><li><small>(Hey,  if a feature\nyou find important is missing from this list, let Hugo\nknow.)</small></li></ul><p>The next pre-release of Python 3.15 will be 3.15.0a5, currently\nscheduled for 2026-02-10.</p><blockquote><p>Upon this every soul was confounded; for the phenomenon just then\nobserved by Ahab had unaccountably escaped every one else; but its very\nblinding palpableness must have been the cause.</p><p>Thrusting his head half way into the binnacle, Ahab caught one\nglimpse of the compasses; his uplifted arm slowly fell; for a moment he\nalmost seemed to stagger. Standing behind him Starbuck looked, and lo!\nthe two compasses pointed East, and the Pequod was as infallibly going\nWest.</p><p>But ere the first wild alarm could get out abroad among the crew, the\nold man with a rigid laugh exclaimed, “I have it! It has happened\nbefore. Mr.&nbsp;Starbuck, last night’s thunder turned our compasses—that’s\nall. Thou hast before now heard of such a thing, I take it.”</p><p>“Aye; but never before has it happened to me, sir,” said the pale\nmate, gloomily.</p></blockquote><p>Thanks to all of the many volunteers who help make Python Development\nand these releases possible! Please consider supporting our efforts by\nvolunteering yourself or through organisation contributions to the <a href=\"https://www.python.org/psf/donations/\">Python Software\nFoundation</a>.</p><p>Regards from a snowfully subzero Helsinki,</p><p>Your release team,\n  Hugo van Kemenade\n  Steve Dower\n  </p>",
      "contentLength": 2635,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Kubernetes v1.35: Restricting executables invoked by kubeconfigs via exec plugin allowList added to kuberc",
      "url": "https://kubernetes.io/blog/2026/01/09/kubernetes-v1-35-kuberc-credential-plugin-allowlist/",
      "date": 1767983400,
      "author": "",
      "guid": 36409,
      "unread": true,
      "content": "<p>Did you know that  can run arbitrary executables, including shell\nscripts, with the full privileges of the invoking user, and without your\nknowledge? Whenever you download or auto-generate a , the\n field can specify an executable to fetch credentials on\nyour behalf. Don't get me wrong, this is an incredible feature that allows you\nto authenticate to the cluster with external identity providers. Nevertheless,\nyou probably see the problem: Do you know exactly what executables your \nis running on your system? Do you trust the pipeline that generated your ?\nIf there has been a supply-chain attack on the code that generates the kubeconfig,\nor if the generating pipeline has been compromised, an attacker might well be\ndoing unsavory things to your machine by tricking your  into running\narbitrary code.</p><p>To give the user more control over what gets run on their system, <a href=\"https://git.k8s.io/community/sig-auth\">SIG-Auth</a> and <a href=\"https://git.k8s.io/community/sig-cli\">SIG-CLI</a> added the credential plugin policy and allowlist as a beta feature to\nKubernetes 1.35. This is available to all clients using the  library,\nby filling out the <a href=\"https://github.com/kubernetes/client-go/blob/master/tools/clientcmd/api/types.go#L290\">ExecProvider.PluginPolicy</a> struct on a REST config. To\nbroaden the impact of this change, Kubernetes v1.35 also lets you manage this without\nwriting a line of application code. You can configure  to enforce\nthe policy and allowlist by adding two fields to the  configuration\nfile:  and <code>credentialPluginAllowlist</code>. Adding one or\nboth of these fields restricts which credential plugins  is allowed to execute.</p><p>A full description of this functionality is available in our <a href=\"https://kubernetes.io/docs/reference/kubectl/kuberc/\">official documentation</a> for kuberc,\nbut this blog post will give a brief overview of the new security knobs. The new\nfeatures are in beta and available without using any feature gates.</p><p>The following example is the simplest one: simply don't specify the new fields.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>This will keep  acting as it always has, and all plugins will be\nallowed.</p><p>The next example is functionally identical, but it is more explicit and\ntherefore preferred if it's actually what you want:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>If you  whether or not you're using exec credential plugins, try\nsetting your policy to :</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>If you  using credential plugins, you'll quickly find out what  is\ntrying to execute. You'll get an error like the following.</p><blockquote><p>Unable to connect to the server: getting credentials: plugin \"cloudco-login\" not allowed: policy set to \"DenyAll\"</p></blockquote><p>If there is insufficient information for you to debug the issue, increase the\nlogging verbosity when you run your next command. For example:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><h3>Selectively allowing plugins</h3><p>What if you need the  plugin to do your daily work? That is why\nthere's a third option for your policy, . To allow a specific plugin,\nset the policy and add the <code>credentialPluginAllowlist</code>:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>You'll notice that there are two entries in the allowlist. One of them is\nspecified by full path, and the other,  is just a basename. When\nyou specify just the basename, the full path will be looked up using\n, which does not expand globbing or handle wildcards.\nGlobbing is not supported at this time. Both forms\n(basename and full path) are acceptable, but the full path is preferable because\nit narrows the scope of allowed binaries even further.</p><p>Currently, an allowlist entry has only one field, . In the future, we\n(Kubernetes SIG CLI) want to see other requirements added. One idea that seems\nuseful is checksum verification whereby, for example, a binary would only be allowed\nto run if it has the sha256 sum\n<code>b9a3fad00d848ff31960c44ebb5f8b92032dc085020f857c98e32a5d5900ff9c</code>\nexists at the path .</p><p>Another possibility is only allowing binaries that have been signed by one of a\nset of a trusted signing keys.</p><p>The credential plugin policy is still under development and we are very interested\nin your feedback. We'd love to hear what you like about it and what problems\nyou'd like to see it solve. Or, if you have the cycles to contribute one of the\nabove enhancements, they'd be a great way to get started contributing to\nKubernetes. Feel free to join in the discussion on slack:</p>",
      "contentLength": 3936,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Kubernetes v1.35: Mutable PersistentVolume Node Affinity (alpha)",
      "url": "https://kubernetes.io/blog/2026/01/08/kubernetes-v1-35-mutable-pv-nodeaffinity/",
      "date": 1767897000,
      "author": "",
      "guid": 36408,
      "unread": true,
      "content": "<p>The PersistentVolume <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#node-affinity\">node affinity</a> API\ndates back to Kubernetes v1.10.\nIt is widely used to express that volumes may not be equally accessible by all nodes in the cluster.\nThis field was previously immutable,\nand it is now mutable in Kubernetes v1.35 (alpha). This change opens a door to more flexible online volume management.</p><h2>Why make node affinity mutable?</h2><p>This raises an obvious question: why make node affinity mutable now?\nWhile stateless workloads like Deployments can be changed freely\nand the changes will be rolled out automatically by re-creating every Pod,\nPersistentVolumes (PVs) are stateful and cannot be re-created easily without losing data.</p><p>However, Storage providers evolve and storage requirements change.\nMost notably, multiple providers are offering regional disks now.\nSome of them even support live migration from zonal to regional disks, without disrupting the workloads.\nThis change can be expressed through the\n<a href=\"https://kubernetes.io/docs/concepts/storage/volume-attributes-classes/\">VolumeAttributesClass</a> API,\nwhich recently graduated to GA in 1.34.\nHowever, even if the volume is migrated to regional storage,\nKubernetes still prevents scheduling Pods to other zones because of the node affinity recorded in the PV object.\nIn this case, you may want to change the PV node affinity from:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>As another example, providers sometimes offer new generations of disks.\nNew disks cannot always be attached to older nodes in the cluster.\nThis accessibility can also be expressed through PV node affinity and ensures the Pods can be scheduled to the right nodes.\nBut when the disk is upgraded, new Pods using this disk can still be scheduled to older nodes.\nTo prevent this, you may want to change the PV node affinity from:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>So, it is mutable now, a first step towards a more flexible online volume management.\nWhile it is a simple change that removes one validation from the API server,\nwe still have a long way to go to integrate well with the Kubernetes ecosystem.</p><p>This feature is for you if you are a Kubernetes cluster administrator,\nand your storage provider allows online update that you want to utilize,\nbut those updates can affect the accessibility of the volume.</p><p>Note that changing PV node affinity alone will not actually change the accessibility of the underlying volume.\nBefore using this feature,\nyou must first update the underlying volume in the storage provider,\nand understand which nodes can access the volume after the update.\nYou can then enable this feature and keep the PV node affinity in sync.</p><p>Currently, this feature is in alpha state.\nIt is disabled by default, and may subject to change.\nTo try it out, enable the  feature gate on APIServer, then you can edit the PV  field.\nTypically only administrators can edit PVs, please make sure you have the right RBAC permissions.</p><h3>Race condition between updating and scheduling</h3><p>There are only a few factors outside of a Pod that can affect the scheduling decision, and PV node affinity is one of them.\nIt is fine to allow more nodes to access the volume by relaxing node affinity,\nbut there is a race condition when you try to tighten node affinity:\nit is unclear how the Scheduler will see the modified PV in its cache,\nso there is a small window where the scheduler may place a Pod on an old node that can no longer access the volume.\nIn this case, the Pod will stuck at  state.</p><p>One mitigation currently under discussion is for the kubelet to fail Pod startup if the PersistentVolume’s node affinity is violated.\nThis has not landed yet.\nSo if you are trying this out now, please watch subsequent Pods that use the updated PV,\nand make sure they are scheduled onto nodes that can access the volume.\nIf you update PV and immediately start new Pods in a script, it may not work as intended.</p><h2>Future integration with CSI (Container Storage Interface)</h2><p>Currently, it is up to the cluster administrator to modify both PV's node affinity and the underlying volume in the storage provider.\nBut manual operations are error-prone and time-consuming.\nIt is preferred to eventually integrate this with VolumeAttributesClass,\nso that an unprivileged user can modify their PersistentVolumeClaim (PVC) to trigger storage-side updates,\nand PV node affinity is updated automatically when appropriate, without the need for cluster admin's intervention.</p><p>As noted earlier, this is only a first step.</p><p>If you are a Kubernetes user,\nwe would like to learn how you use (or will use) PV node affinity.\nIs it beneficial to update it online in your case?</p><p>If you are a CSI driver developer,\nwould you be willing to implement this feature? How would you like the API to look?</p><p>Please provide your feedback via:</p><p>For any inquiries or specific questions related to this feature, please reach out to the <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">SIG Storage community</a>.</p>",
      "contentLength": 4691,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Kubernetes v1.35: A Better Way to Pass Service Account Tokens to CSI Drivers",
      "url": "https://kubernetes.io/blog/2026/01/07/kubernetes-v1-35-csi-sa-tokens-secrets-field-beta/",
      "date": 1767810600,
      "author": "",
      "guid": 36407,
      "unread": true,
      "content": "<p>If you maintain a CSI driver that uses service account tokens,\nKubernetes v1.35 brings a refinement you'll want to know about.\nSince the introduction of the <a href=\"https://kubernetes-csi.github.io/docs/token-requests.html\">TokenRequests feature</a>,\nservice account tokens requested by CSI drivers have been passed to them through the  field.\nWhile this has worked, it's not the ideal place for sensitive information,\nand we've seen instances where tokens were accidentally logged in CSI drivers.</p><p>Kubernetes v1.35 introduces a beta solution to address this:\n<em>CSI Driver Opt-in for Service Account Tokens via Secrets Field</em>.\nThis allows CSI drivers to receive service account tokens\nthrough the  field in ,\nwhich is the appropriate place for sensitive data in the CSI specification.</p><h2>Understanding the existing approach</h2><p>When CSI drivers use the <a href=\"https://kubernetes-csi.github.io/docs/token-requests.html\">TokenRequests feature</a>,\nthey can request service account tokens for workload identity\nby configuring the  field in the CSIDriver spec.\nThese tokens are passed to drivers as part of the volume attributes map,\nusing the key <code>csi.storage.k8s.io/serviceAccount.tokens</code>.</p><p>The  field works, but it's not designed for sensitive data.\nBecause of this, there are a few challenges:</p><p>First, the <a href=\"https://github.com/kubernetes-csi/csi-lib-utils/tree/master/protosanitizer\"></a> tool that CSI drivers use doesn't treat volume context as sensitive,\nso service account tokens can end up in logs when gRPC requests are logged.\nThis happened with <a href=\"https://github.com/kubernetes-sigs/secrets-store-csi-driver/security/advisories/GHSA-g82w-58jf-gcxx\">CVE-2023-2878</a> in the Secrets Store CSI Driver\nand <a href=\"https://github.com/kubernetes/kubernetes/issues/124759\">CVE-2024-3744</a> in the Azure File CSI Driver.</p><p>Second, each CSI driver that wants to avoid this issue needs to implement its own sanitization logic,\nwhich leads to inconsistency across drivers.</p><p>The CSI specification already has a  field in \nthat's designed exactly for this kind of sensitive information.\nThe challenge is that we can't just change where we put the tokens\nwithout breaking existing CSI drivers that expect them in volume context.</p><h2>How the opt-in mechanism works</h2><p>Kubernetes v1.35 introduces an opt-in mechanism that lets CSI drivers choose\nhow they receive service account tokens.\nThis way, existing drivers continue working as they do today,\nand drivers can move to the more appropriate secrets field when they're ready.</p><p>CSI drivers can set a new field in their CSIDriver spec:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>The behavior depends on the <code>serviceAccountTokenInSecrets</code> field:</p><p>When set to  (the default), tokens are placed in  with the key <code>csi.storage.k8s.io/serviceAccount.tokens</code>, just like today.\nWhen set to , tokens are placed only in the  field with the same key.</p><p>The <code>CSIServiceAccountTokenSecrets</code> feature gate is enabled by default\non both kubelet and kube-apiserver.\nSince the <code>serviceAccountTokenInSecrets</code> field defaults to ,\nenabling the feature gate doesn't change any existing behavior.\nAll drivers continue receiving tokens via volume context unless they explicitly opt in.\nThis is why we felt comfortable starting at beta rather than alpha.</p><h2>Guide for CSI driver authors</h2><p>If you maintain a CSI driver that uses service account tokens, here's how to adopt this feature.</p><p>First, update your driver code to check both locations for tokens.\nThis makes your driver compatible with both the old and new approaches:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>This fallback logic is backward compatible and safe to ship in any driver version,\neven before clusters upgrade to v1.35.</p><p>CSI driver authors need to follow a specific sequence when adopting this feature to avoid breaking existing volumes.</p><p> (can happen anytime)</p><p>You can start preparing your driver right away by adding fallback logic that checks both the secrets field and volume context for tokens.\nThis code change is backward compatible and safe to ship in any driver version, even before clusters upgrade to v1.35.\nWe encourage you to add this fallback logic early, cut releases, and even backport to maintenance branches where feasible.</p><p><strong>Cluster upgrade and feature enablement</strong></p><p>Once your driver has the fallback logic deployed, here's the safe rollout order for enabling the feature in a cluster:</p><ol><li>Complete the kube-apiserver upgrade to 1.35 or later</li><li>Complete kubelet upgrade to 1.35 or later on all nodes</li><li>Ensure CSI driver version with fallback logic is deployed (if not already done in preparation phase)</li><li>Fully complete CSI driver DaemonSet rollout across all nodes</li><li>Update your CSIDriver manifest to set <code>serviceAccountTokenInSecrets: true</code></li></ol><p>The most important thing to remember is timing.\nIf your CSI driver DaemonSet and CSIDriver object are in the same manifest or Helm chart,\nyou need two separate updates.\nDeploy the new driver version with fallback logic first,\nwait for the DaemonSet rollout to complete,\nthen update the CSIDriver spec to set <code>serviceAccountTokenInSecrets: true</code>.</p><p>Also, don't update the CSIDriver before all driver pods have rolled out.\nIf you do, volume mounts will fail on nodes still running the old driver version,\nsince those pods only check volume context.</p><p>Adopting this feature helps in a few ways:</p><ul><li>It eliminates the risk of accidentally logging service account tokens as part of volume context in gRPC requests</li><li>It uses the CSI specification's designated field for sensitive data, which feels right</li><li>The  tool automatically handles the secrets field correctly, so you don't need driver-specific workarounds</li><li>It's opt-in, so you can migrate at your own pace without breaking existing deployments</li></ul><p>We (Kubernetes SIG Storage) encourage CSI driver authors to adopt this feature and provide feedback\non the migration experience.\nIf you have thoughts on the API design or run into any issues during adoption,\nplease reach out to us on the\n<a href=\"https://kubernetes.slack.com/archives/C8EJ01Z46\">#csi</a> channel on Kubernetes Slack\n(for an invitation, visit <a href=\"https://slack.k8s.io/\">https://slack.k8s.io/</a>).</p><p>You can follow along on\n<a href=\"https://kep.k8s.io/5538\">KEP-5538</a>\nto track progress across the coming Kubernetes releases.</p>",
      "contentLength": 5556,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Kubernetes v1.35: Extended Toleration Operators to Support Numeric Comparisons (Alpha)",
      "url": "https://kubernetes.io/blog/2026/01/05/kubernetes-v1-35-numeric-toleration-operators/",
      "date": 1767637800,
      "author": "",
      "guid": 36406,
      "unread": true,
      "content": "<p>Many production Kubernetes clusters blend on-demand (higher-SLA) and spot/preemptible (lower-SLA) nodes to optimize costs while maintaining reliability for critical workloads. Platform teams need a safe default that keeps most workloads away from risky capacity, while allowing specific workloads to opt-in with explicit thresholds like \"I can tolerate nodes with failure probability up to 5%\".</p><p>Today, Kubernetes taints and tolerations can match exact values or check for existence, but they can't compare numeric thresholds. You'd need to create discrete taint categories, use external admission controllers, or accept less-than-optimal placement decisions.</p><p>In Kubernetes v1.35, we're introducing <strong>Extended Toleration Operators</strong> as an alpha feature. This enhancement adds  (Greater Than) and  (Less Than) operators to , enabling threshold-based scheduling decisions that unlock new possibilities for SLA-based placement, cost optimization, and performance-aware workload distribution.</p><h2>The evolution of tolerations</h2><p>Historically, Kubernetes supported two primary toleration operators:</p><ul><li>: The toleration matches a taint if the key and value are exactly equal</li><li>: The toleration matches a taint if the key exists, regardless of value</li></ul><p>While these worked well for categorical scenarios, they fell short for numeric comparisons. Starting with v1.35, we are closing this gap.</p><p>Consider these real-world scenarios:</p><ul><li>: Schedule high-availability workloads only on nodes with failure probability below a certain threshold</li><li>: Allow cost-sensitive batch jobs to run on cheaper nodes that exceed a specific cost-per-hour value</li><li>: Ensure latency-sensitive applications run only on nodes with disk IOPS or network bandwidth above minimum thresholds</li></ul><p>Without numeric comparison operators, cluster operators have had to resort to workarounds like creating multiple discrete taint values or using external admission controllers, neither of which scale well or provide the flexibility needed for dynamic threshold-based scheduling.</p><h2>Why extend tolerations instead of using NodeAffinity?</h2><p>You might wonder: NodeAffinity already supports numeric comparison operators, so why extend tolerations? While NodeAffinity is powerful for expressing pod preferences, taints and tolerations provide critical operational benefits:</p><ul><li>: NodeAffinity is per-pod, requiring every workload to explicitly opt-out of risky nodes. Taints invert control—nodes declare their risk level, and only pods with matching tolerations may land there. This provides a safer default; most pods stay away from spot/preemptible nodes unless they explicitly opt-in.</li><li>: NodeAffinity has no eviction capability. Taints support the  effect with , enabling operators to drain and evict pods when a node's SLA degrades or spot instances receive termination notices.</li><li>: Centralized, node-side policy is consistent with other safety taints like disk-pressure and memory-pressure, making cluster management more intuitive.</li></ul><p>This enhancement preserves the well-understood safety model of taints and tolerations while enabling threshold-based placement for SLA-aware scheduling.</p><h2>Introducing Gt and Lt operators</h2><p>Kubernetes v1.35 introduces two new operators for tolerations:</p><ul><li>: The toleration matches if the taint's numeric value is less than the toleration's value</li><li>: The toleration matches if the taint's numeric value is greater than the toleration's value</li></ul><p>When a pod tolerates a taint with , it's saying \"I can tolerate nodes where this metric is  my threshold\". Since tolerations allow scheduling, the pod can run on nodes where the taint value is greater than the toleration value. Think of it as: \"I tolerate nodes that are above my minimum requirements\".</p><p>These operators work with numeric taint values and enable the scheduler to make sophisticated placement decisions based on continuous metrics rather than discrete categories.</p><div role=\"alert\"><p>Numeric values for  and  operators must be positive 64-bit integers without leading zeros. For example,  is valid, but  (with leading zero) and  (zero value) are not permitted.</p><p>The  and  operators work with all taint effects: , , and .</p></div><p>Let's explore how Extended Toleration Operators solve real-world scheduling challenges.</p><h3>Example 1: Spot instance protection with SLA thresholds</h3><p>Many clusters mix on-demand and spot/preemptible nodes to optimize costs. Spot nodes offer significant savings but have higher failure rates. You want most workloads to avoid spot nodes by default, while allowing specific workloads to opt-in with clear SLA boundaries.</p><p>First, taint spot nodes with their failure probability (for example, 15% annual failure rate):</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>On-demand nodes have much lower failure rates:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Critical workloads can specify strict SLA requirements:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>This pod will  schedule on nodes with  less than 5 (meaning  with 2% but not  with 15%). The  effect with  means if a node's SLA degrades (for example, cloud provider changes the taint value), the pod gets 30 seconds to gracefully terminate before forced eviction.</p><p>Meanwhile, a fault-tolerant batch job can explicitly opt-in to spot instances:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>This batch job tolerates nodes with failure probability up to 20%, so it can run on both on-demand and spot nodes, maximizing cost savings while accepting higher risk.</p><h3>Example 2: AI workload placement with GPU tiers</h3><p>AI and machine learning workloads often have specific hardware requirements. With Extended Toleration Operators, you can create GPU node tiers and ensure workloads land on appropriately powered hardware.</p><p>Taint GPU nodes with their compute capability score:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>A heavy training workload can require high-performance GPUs:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>This ensures the training pod only schedules on nodes with compute scores greater than 800 (like the A100 node), preventing placement on lower-tier GPUs that would slow down training.</p><p>Meanwhile, inference workloads with less demanding requirements can use any available GPU:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h3>Example 3: Cost-optimized workload placement</h3><p>For batch processing or non-critical workloads, you might want to minimize costs by running on cheaper nodes, even if they have lower performance characteristics.</p><p>Nodes can be tainted with their cost rating:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>A cost-sensitive batch job can express its tolerance for expensive nodes:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>This batch job will schedule on nodes costing less than $100/hour but avoid more expensive nodes. Combined with Kubernetes scheduling priorities, this enables sophisticated cost-tiering strategies where critical workloads get premium nodes while batch workloads efficiently use budget-friendly resources.</p><p>Storage-intensive applications often require minimum disk performance guarantees. With Extended Toleration Operators, you can enforce these requirements at the scheduling level.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>This toleration ensures the pod only schedules on nodes where  exceeds 3000. The  operator means \"I need nodes that are greater than this minimum\".</p><p>Extended Toleration Operators is an  in Kubernetes v1.35. To try it out:</p><ol><li><p> on both your API server and scheduler:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div></li><li><p> with numeric values representing the metrics relevant to your scheduling needs:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div></li><li><p> in your pod specifications:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div></li></ol><div role=\"alert\">As an alpha feature, Extended Toleration Operators may change in future releases and should be used with caution in production environments. Always test thoroughly in non-production clusters first.</div><p>This alpha release is just the beginning. As we gather feedback from the community, we plan to:</p><ul><li>Improve integration with cluster autoscaling for threshold-aware capacity planning</li><li>Graduate the feature to beta and eventually GA with production-ready stability</li></ul><p>We're particularly interested in hearing about your use cases! Do you have scenarios where threshold-based scheduling would solve problems? Are there additional operators or capabilities you'd like to see?</p><p>This feature is driven by the <a href=\"https://github.com/kubernetes/community/tree/master/sig-scheduling\">SIG Scheduling</a> community. Please join us to connect with the community and share your ideas and feedback around this feature and beyond.</p><p>You can reach the maintainers of this feature at:</p><p>For questions or specific inquiries related to Extended Toleration Operators, please reach out to the SIG Scheduling community. We look forward to hearing from you!</p>",
      "contentLength": 8019,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Project goals update — December 2025",
      "url": "https://blog.rust-lang.org/2026/01/05/project-goals-2025-december-update/",
      "date": 1767571200,
      "author": "Tomas Sedovic",
      "guid": 35951,
      "unread": true,
      "content": "<p>David has made \"progress on the non-Sized Hierarchy part of the goal, the infrastructure for defining scalable vector types has been merged (with them being Sized in the interim) and that'll make it easier to iterate on those and find issues that need solving\".</p><p>On the Sized hierarchy part of the goal, no progress. We discussed options for migrating. There seem to be three big options:</p><p>(A) The <strong>conservative-but-obvious route</strong> where the in the old edition is expanded to <code>T: Deref&lt;Target: SizeOfVal&gt;</code> (but in the new edition it means <code>T: Deref&lt;Target: Pointee&gt;</code>, i.e., no additional bounds). The main  is that new Edition code using  can't call old Edition code using  as the old edition code has stronger bounds. Therefore new edition code must either use stronger bounds than it needs  wait until that old edition code has been updated.</p><p>(B) You do something smart with Edition.Old code where you figure out if the bound can be loose or strict by bottom-up computation. So  in the old could mean either <code>T: Deref&lt;Target: Pointee&gt;</code> or <code>T: Deref&lt;Target: SizeOfVal&gt;</code>, depending on what the function actually does.</p><p>(C) You make Edition.Old code always mean <code>T: Deref&lt;Target: Pointee&gt;</code> and you still allow calls to  but have them cause post-monomorphization errors if used inappropriately. In Edition.New you use stricter checking.</p><p>Options (B) and (C) have the downside that changes to the function body (adding a call to , specifically) in the old edition can stop callers from compiling. In the case of Option (B), that breakage is at type-check time, because it can change the where-clauses. In Option (C), the breakage is post-monomorphization.</p><p>Option (A) has the disadvantage that it takes longer for the new bounds to roll out.</p><p>Given this, (A) seems the preferred path. We discussed options for how to encourage that roll-out. We discussed the idea of a lint that would warn Edition.Old code that its bounds are stronger than needed and suggest rewriting to <code>T: Deref&lt;Target: Pointee&gt;</code> to explicitly disable the stronger Edition.Old default. This lint could be implemented in one of two ways</p><ul><li>at type-check time, by tracking what parts of the environment are used by the trait solver. This may be feasible in the new trait solver, someone from @rust-lang/types would have to say.</li><li>at post-mono time, by tracking which functions  and propagating that information back to callers. You could then compare against the generic bounds declared on the caller.</li></ul><p>The former is more useful (knowing what parts of the environment are necessary could be useful for more things, e.g., better caching); the latter may be easier or more precise.</p>",
      "contentLength": 2607,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Kubernetes v1.35: New level of efficiency with in-place Pod restart",
      "url": "https://kubernetes.io/blog/2026/01/02/kubernetes-v1-35-restart-all-containers/",
      "date": 1767378600,
      "author": "",
      "guid": 36405,
      "unread": true,
      "content": "<p>The release of Kubernetes 1.35 introduces a powerful new feature that provides a much-requested capability: the ability to trigger a full, in-place restart of the Pod. This feature,  (alpha in 1.35), allows for an efficient way to reset a Pod's state compared to resource-intensive approach of deleting and recreating the entire Pod. This feature is especially useful for AI/ML workloads allowing application developers to concentrate on their core training logic while offloading complex failure-handling and recovery mechanisms to sidecars and declarative Kubernetes configuration. With  and other planned enhancements, Kubernetes continues to add building blocks for creating the most flexible, robust, and efficient platforms for AI/ML workloads.</p><p>This new functionality is available by enabling the <code>RestartAllContainersOnContainerExits</code> feature gate. This alpha feature extends the <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-restart-rules\"> feature</a>, which graduated to beta in Kubernetes 1.35.</p><h2>The problem: when a single container restart isn't enough and recreating pods is too costly</h2><p>Kubernetes has long supported restart policies at the Pod level () and, more recently, at the <a href=\"https://kubernetes.io/blog/2025/08/29/kubernetes-v1-34-per-container-restart-policy/\">individual container level</a>. These policies are great for handling crashes in a single, isolated process. However, many modern applications have more complex inter-container dependencies. For instance:</p><ul><li>An  prepares the environment by mounting a volume or generating a configuration file. If the main application container corrupts this environment, simply restarting that one container is not enough. The entire initialization process needs to run again.</li><li>A  monitors system health. If it detects an unrecoverable but retriable error state, it must trigger a restart of the main application container from a clean slate.</li><li>A  that manages a remote resource fails. Even if the sidecar restarts on its own, the main container may be stuck trying to access an outdated or broken connection.</li></ul><p>In all these cases, the desired action is not to restart a single container, but all of them. Previously, the only way to achieve this was to delete the Pod and have a controller (like a Job or ReplicaSet) create a new one. This process is slow and expensive, involving the scheduler, node resource allocation and re-initialization of networking and storage.</p><p>This inefficiency becomes even worse when handling large-scale AI/ML workloads (&gt;= 1,000 Nodes with one Pod per Node). A common requirement for these synchronous workloads is that when a failure occurs (such as a Node crash), all Pods in the fleet must be recreated to reset the state before training can resume, even if all the other Pods were not directly affected by the failure. Deleting, creating and scheduling thousands of Pods simultaneously creates a massive bottleneck. The estimated overhead of this failure could cost <a href=\"https://docs.google.com/document/d/16zexVooHKPc80F4dVtUjDYK9DOpkVPRNfSv0zRtfFpk/edit?tab=t.0#bookmark=id.qwqcnzf96avw\">$100,000 per month in wasted resources</a>.</p><p>Handling these failures for AI/ML training jobs requires a complex integration touching both the training framework and Kubernetes, which are often fragile and toilsome. This feature introduces a Kubernetes-native solution, improving system robustness and allowing application developers to concentrate on their core training logic.</p><p>Another major benefit of restarting Pods in place is that keeping Pods on their assigned Nodes allows for further optimizations. For example, one can implement node-level caching tied to a specific Pod identity, something that is impossible when Pods are unnecessarily being recreated on different Nodes.</p><h2>Introducing the  action</h2><p>To address this, Kubernetes v1.35 adds a new action to the container restart rules: . When a container exits in a way that matches a rule with this action, the kubelet initiates a fast,  restart of the Pod.</p><p>This in-place restart is highly efficient because it preserves the Pod's most important resources:</p><ul><li>The Pod's UID, IP address and network namespace.</li><li>The Pod's sandbox and any attached devices.</li><li>All volumes, including  and mounted volumes from PVCs.</li></ul><p>After terminating all running containers, the Pod's startup sequence is re-executed from the very beginning. This means all  are run again in order, followed by the sidecar and regular containers, ensuring a completely fresh start in a known-good environment. With the exception of ephemeral containers (which are terminated), all other containers—including those that previously succeeded or failed—will be restarted, regardless of their individual restart policies.</p><h3>1. Efficient restarts for ML/Batch jobs</h3><p>With  actions you can address this by enabling a much faster, hybrid recovery strategy: recreate only the \"bad\" Pods (e.g., those on unhealthy Nodes) while triggering  for the remaining healthy Pods. Benchmarks show this reduces the recovery overhead <a href=\"https://docs.google.com/document/d/16zexVooHKPc80F4dVtUjDYK9DOpkVPRNfSv0zRtfFpk/edit?tab=t.0#bookmark=id.cwkee8kar0i5\">from minutes to a few seconds</a>.</p><p>With in-place restarts, a watcher sidecar can monitor the main training process. If it encounters a specific, retriable error, the watcher can exit with a designated code to trigger a fast reset of the worker Pod, allowing it to restart from the last checkpoint without involving the Job controller. This capability is now natively supported by Kubernetes.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h3>2. Re-running init containers for a clean state</h3><p>Imagine a scenario where an init container is responsible for fetching credentials or setting up a shared volume. If the main application fails in a way that corrupts this shared state, you need the <a href=\"https://github.com/kubernetes/enhancements/issues/3676\">init container to rerun</a>.</p><p>By configuring the main application to exit with a specific code upon detecting such a corruption, you can trigger the  action, guaranteeing that the init container provides a clean setup before the application restarts.</p><h3>3. Handling high rate of similar tasks execution</h3><p>There are cases when tasks are best represented as a Pod execution. And each task requires a clean execution. The task may be a game session backend or some queue item processing. If the rate of tasks is high, running the whole cycle of Pod creation, scheduling and initialization is simply too expensive, especially when tasks can be short. The ability to restart all containers from scratch enables a Kubernetes-native way to handle this scenario without custom solutions or frameworks.</p><p>To try this feature, you must enable the <code>RestartAllContainersOnContainerExits</code> feature gate on your Kubernetes cluster components (API server and kubelet) running Kubernetes v1.35+. This alpha feature extends the  feature, which graduated to beta in v1.35 and is enabled by default.</p><p>Once enabled, you can add  to any container (init, sidecar, or regular) and use the  action.</p><p>The feature is designed to be easily usable on existing apps. However, if an application does not follow some best practices, it may cause issues for the application or for observability tooling. When enabling the feature, make sure that all containers are reentrant and that external tooling is prepared for init containers to re-run. Also, when restarting all containers, the kubelet does not run  hooks. This means containers must be designed to handle abrupt termination without relying on  hooks for graceful shutdown.</p><p>To make this process observable, a new Pod condition, , is added to the Pod's status. When a restart is triggered, this condition becomes  and it reverts to  once all containers have terminated and the Pod is ready to start its lifecycle anew. This provides a clear signal to users and other cluster components about the Pod's state.</p><p>All containers restarted by this action will have their restart count incremented in the container status.</p><p>As an alpha feature,  is ready for you to experiment with and any use cases and feedback are welcome. This feature is driven by the <a href=\"https://github.com/kubernetes/community/blob/master/sig-node/README.md\">SIG Node</a> community. If you are interested in getting involved, sharing your thoughts, or contributing, please join us!</p><p>You can reach SIG Node through:</p>",
      "contentLength": 7726,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Kubernetes 1.35: Enhanced Debugging with Versioned z-pages APIs",
      "url": "https://kubernetes.io/blog/2025/12/31/kubernetes-v1-35-structured-zpages/",
      "date": 1767205800,
      "author": "",
      "guid": 36404,
      "unread": true,
      "content": "<p>Debugging Kubernetes control plane components can be challenging, especially when you need to quickly understand the runtime state of a component or verify its configuration. With Kubernetes 1.35, we're enhancing the z-pages debugging endpoints with structured, machine-parseable responses that make it easier to build tooling and automate troubleshooting workflows.</p><p>z-pages are special debugging endpoints exposed by Kubernetes control plane components. Introduced as an alpha feature in Kubernetes 1.32, these endpoints provide runtime diagnostics for components like , , ,  and . The name \"z-pages\" comes from the convention of using  paths for debugging endpoints.</p><p>Currently, Kubernetes supports two primary z-page endpoints:</p><dl><dd>Displays high-level component information including version information, start time, uptime, and available debug paths</dd><dd>Shows all command-line arguments and their values used to start the component (with confidential values redacted for security)</dd></dl><p>These endpoints are valuable for human operators who need to quickly inspect component state, but until now, they only returned plain text output that was difficult to parse programmatically.</p><h2>What's new in Kubernetes 1.35?</h2><p>Kubernetes 1.35 introduces structured, versioned responses for both  and  endpoints. This enhancement maintains backward compatibility with the existing plain text format while adding support for machine-readable JSON responses.</p><p>The new structured responses are opt-in. Without specifying an  header, the endpoints continue to return the familiar plain text format:</p><pre tabindex=\"0\"><code>$ curl --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\\n--key /etc/kubernetes/pki/apiserver-kubelet-client.key \\\n--cacert /etc/kubernetes/pki/ca.crt \\\nhttps://localhost:6443/statusz\nkube-apiserver statusz\nWarning: This endpoint is not meant to be machine parseable, has no formatting compatibility guarantees and is for debugging purposes only.\nStarted: Wed Oct 16 21:03:43 UTC 2024\nUp: 0 hr 00 min 16 sec\nGo version: go1.23.2\nBinary version: 1.35.0-alpha.0.1595\nEmulation version: 1.35\nPaths: /healthz /livez /metrics /readyz /statusz /version\n</code></pre><h3>Structured JSON responses</h3><p>To receive a structured response, include the appropriate  header:</p><pre tabindex=\"0\"><code>Accept: application/json;v=v1alpha1;g=config.k8s.io;as=Statusz\n</code></pre><p>This returns a versioned JSON response:</p><div><pre tabindex=\"0\"><code data-lang=\"json\"></code></pre></div><p>Similarly,  supports structured responses with the header:</p><pre tabindex=\"0\"><code>Accept: application/json;v=v1alpha1;g=config.k8s.io;as=Flagz\n</code></pre><div><pre tabindex=\"0\"><code data-lang=\"json\"></code></pre></div><h2>Why structured responses matter</h2><p>The addition of structured responses opens up several new possibilities:</p><h3>1. <strong>Automated health checks and monitoring</strong></h3><p>Instead of parsing plain text, monitoring tools can now easily extract specific fields. For example, you can programmatically check if a component has been running with an unexpected emulated version or verify that critical flags are set correctly.</p><p>Developers can build sophisticated debugging tools that compare configurations across multiple components or track configuration drift over time. The structured format makes it trivial to  configurations or validate that components are running with expected settings.</p><h3>3. <strong>API versioning and stability</strong></h3><p>By introducing versioned APIs (starting with ), we provide a clear path to stability. As the feature matures, we'll introduce  and eventually , giving you confidence that your tooling won't break with future Kubernetes releases.</p><h2>How to use structured z-pages</h2><p>Both endpoints require feature gates to be enabled:</p><ul><li>: Enable the  feature gate</li><li>: Enable the  feature gate</li></ul><h3>Example: Getting structured responses</h3><p>Here's an example using  to retrieve structured JSON responses from the kube-apiserver:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><div role=\"alert\">The examples above use client certificate authentication and verify the server's certificate using .\nIf you need to bypass certificate verification in a test environment, you can use  (or ),\nbut this should never be done in production as it makes you vulnerable to man-in-the-middle attacks.</div><p>The structured z-page responses are an  feature in Kubernetes 1.35. This means:</p><ul><li>The API format may change in future releases</li><li>These endpoints are intended for debugging, not production automation</li><li>You should avoid relying on them for critical monitoring workflows until they reach beta or stable status</li></ul><h3>Security and access control</h3><p>z-pages expose internal component information and require proper access controls. Here are the key security considerations:</p><p>: Access to z-page endpoints is restricted to members of the  group, which follows the same authorization model as other debugging endpoints like , , and . This ensures that only authorized users and service accounts can access debugging information. If your cluster uses RBAC, you can manage access by granting appropriate permissions to this group.</p><p>: The authentication requirements for these endpoints depend on your cluster's configuration. Unless anonymous authentication is enabled for your cluster, you typically need to use authentication mechanisms (such as client certificates) to access these endpoints.</p><p>: These endpoints reveal configuration details about your cluster components, including:</p><ul><li>Component versions and build information</li><li>All command-line arguments and their values (with confidential values redacted)</li><li>Available debug endpoints</li></ul><p>Only grant access to trusted operators and debugging tools. Avoid exposing these endpoints to unauthorized users or automated systems that don't require this level of access.</p><p>As the feature matures, we (Kubernetes SIG Instrumentation) expect to:</p><ul><li>Introduce  and eventually  versions of the API</li><li>Gather community feedback on the response schema</li><li>Potentially add additional z-page endpoints based on user needs</li></ul><p>We encourage you to experiment with structured z-pages in a test environment:</p><ol><li>Enable the  and  feature gates on your control plane components</li><li>Try querying the endpoints with both plain text and structured formats</li><li>Build a simple tool or script that uses the structured data</li><li>Share your feedback with the community</li></ol><p>We'd love to hear your feedback! The structured z-pages feature is designed to make Kubernetes easier to debug and monitor. Whether you're building internal tooling, contributing to open source projects, or just exploring the feature, your input helps shape the future of Kubernetes observability.</p><p>If you have questions, suggestions, or run into issues, please reach out to SIG Instrumentation. You can find us on Slack or at our regular <a href=\"https://github.com/kubernetes/community/tree/master/sig-instrumentation\">community meetings</a>.</p>",
      "contentLength": 6343,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Kubernetes v1.35: Watch Based Route Reconciliation in the Cloud Controller Manager",
      "url": "https://kubernetes.io/blog/2025/12/30/kubernetes-v1-35-watch-based-route-reconciliation-in-ccm/",
      "date": 1767119400,
      "author": "",
      "guid": 36403,
      "unread": true,
      "content": "<p>Up to and including Kubernetes v1.34, the route controller in Cloud Controller Manager (CCM)\nimplementations built using the <a href=\"https://github.com/kubernetes/cloud-provider\">k8s.io/cloud-provider</a> library reconciles\nroutes at a fixed interval. This causes unnecessary API requests to the cloud provider when\nthere are no changes to routes. Other controllers implemented through the same library already\nuse watch-based mechanisms, leveraging informers to avoid unnecessary API calls. A new feature gate\nis being introduced in v1.35 to allow changing the behavior of the route controller to use watch-based informers.</p><p>The feature gate <code>CloudControllerManagerWatchBasedRoutesReconciliation</code> has been\nintroduced to <a href=\"https://github.com/kubernetes/cloud-provider\">k8s.io/cloud-provider</a> in alpha stage by <a href=\"https://github.com/kubernetes/community/blob/master/sig-cloud-provider/README.md\">SIG Cloud Provider</a>.\nTo enable this feature you can use <code>--feature-gate=CloudControllerManagerWatchBasedRoutesReconciliation=true</code>\nin the CCM implementation you are using.</p><p>This feature gate will trigger the route reconciliation loop whenever a node is\nadded, deleted, or the fields  or  are updated.</p><p>An additional reconcile is performed in a random interval between 12h and 24h,\nwhich is chosen at the controller's start time.</p><p>This feature gate does not modify the logic within the reconciliation loop.\nTherefore, users of a CCM implementation should not experience significant\nchanges to their existing route configurations.</p>",
      "contentLength": 1318,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Kubernetes v1.35: Introducing Workload Aware Scheduling",
      "url": "https://kubernetes.io/blog/2025/12/29/kubernetes-v1-35-introducing-workload-aware-scheduling/",
      "date": 1767033000,
      "author": "",
      "guid": 36402,
      "unread": true,
      "content": "<p>Scheduling large workloads is a much more complex and fragile operation than scheduling a single Pod,\nas it often requires considering all Pods together instead of scheduling each one independently.\nFor example, when scheduling a machine learning batch job, you often need to place each worker strategically,\nsuch as on the same rack, to make the entire process as efficient as possible.\nAt the same time, the Pods that are part of such a workload are very often identical\nfrom the scheduling perspective, which fundamentally changes how this process should look.</p><p>There are many custom schedulers adapted to perform workload scheduling efficiently,\nbut considering how common and important workload scheduling is to Kubernetes users,\nespecially in the AI era with the growing number of use cases,\nit is high time to make workloads a first-class citizen for  and support them natively.</p><h2>Workload aware scheduling</h2><p>The recent 1.35 release of Kubernetes delivered the first tranche of <em>workload aware scheduling</em> improvements.\nThese are part of a wider effort that is aiming to improve scheduling and management of workloads.\nThe effort will span over many SIGs and releases, and is supposed to gradually expand\ncapabilities of the system toward reaching the north star goal,\nwhich is seamless workload scheduling and management in Kubernetes including,\nbut not limited to, preemption and autoscaling.</p><p>Kubernetes v1.35 introduces the Workload API that you can use to describe the desired shape\nas well as scheduling-oriented requirements of the workload. It comes with an initial implementation\nof  that instructs the  to schedule gang Pods in the  fashion.\nFinally, we improved scheduling of identical Pods (that typically make a gang) to speed up the process\nthanks to the  feature.</p><p>The new Workload API resource is part of the <code>scheduling.k8s.io/v1alpha1</code><a title=\"A set of related paths in the Kubernetes API.\" data-toggle=\"tooltip\" data-placement=\"top\" href=\"https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning\" target=\"_blank\" aria-label=\"API group\">API group</a>.\nThis resource acts as a structured, machine-readable definition of the scheduling requirements\nof a multi-Pod application. While user-facing workloads like Jobs define what to run, the Workload resource\ndetermines how a group of Pods should be scheduled and how its placement should be managed\nthroughout its lifecycle.</p><p>A Workload allows you to define a group of Pods and apply a scheduling policy to them.\nHere is what a gang scheduling configuration looks like. You can define a  named \nand apply the  policy with a  of 4.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>When you create your Pods, you link them to this Workload using the new  field:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h2>How gang scheduling works</h2><p>The  policy enforces  placement. Without gang scheduling,\na Job might be partially scheduled, consuming resources without being able to run,\nleading to resource wastage and potential deadlocks.</p><p>When you create Pods that are part of a gang-scheduled pod group, the scheduler's \nplugin manages the lifecycle independently for each pod group (or replica key):</p><ol><li><p>When you create your Pods (or a controller makes them for you),\nthe scheduler blocks them from scheduling, until:</p><ul><li>The referenced Workload object is created.</li><li>The referenced pod group exists in a Workload.</li><li>The number of pending Pods in that group meets your .</li></ul></li><li><p>Once enough Pods arrive, the scheduler tries to place them. However,\ninstead of binding them to nodes immediately, the Pods wait at a  gate.</p></li><li><p>The scheduler checks if it has found valid assignments for the entire group (at least the ).</p><ul><li>If there is room for the group, the gate opens, and all Pods are bound to nodes.</li><li>If only a subset of the group pods was successfully scheduled within a timeout (set to 5 minutes),\nthe scheduler rejects  of the Pods in the group.\nThey go back to the queue, freeing up the reserved resources for other workloads.</li></ul></li></ol><p>We'd like to point out that that while this is a first implementation, the Kubernetes project firmly\nintends to improve and expand the gang scheduling algorithm in future releases.\nBenefits we hope to deliver include a single-cycle scheduling phase for a whole gang,\nworkload-level preemption, and more, moving towards the north star goal.</p><p>In addition to explicit gang scheduling, v1.35 introduces .\nThis is a Beta feature that improves scheduling latency for identical Pods.</p><p>Unlike gang scheduling, this feature does not require the Workload API\nor any explicit opt-in on the user's part. It works opportunistically within the scheduler\nby identifying Pods that have identical scheduling requirements (container images, resource requests,\naffinities, etc.). When the scheduler processes a Pod, it can reuse the feasibility calculations\nfor subsequent identical Pods in the queue, significantly speeding up the process.</p><p>Most users will benefit from this optimization automatically, without taking any special steps,\nprovided their Pods meet the following criteria.</p><p>Opportunistic batching works under specific conditions. All fields used by the \nto find a placement must be identical between Pods. Additionally, using some features\ndisables the batching mechanism for those Pods to ensure correctness.</p><p>Note that you may need to review your  configuration\nto ensure it is not implicitly disabling batching for your workloads.</p><p>See the <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/scheduler-perf-tuning/#enabling-opportunistic-batching\">docs</a> for more details about restrictions.</p><p>The project has a broad ambition to deliver workload aware scheduling.\nThese new APIs and scheduling enhancements are just the first steps.\nIn the near future, the effort aims to tackle:</p><ul><li>Introducing a workload scheduling phase</li><li>Improved support for multi-node <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/\">DRA</a>\nand topology aware scheduling</li><li>Workload-level preemption</li><li>Improved integration between scheduling and autoscaling</li><li>Improved interaction with external workload schedulers</li><li>Managing placement of workloads throughout their entire lifecycle</li><li>Multi-workload scheduling simulations</li></ul><p>And more. The priority and implementation order of these focus areas\nare subject to change. Stay tuned for further updates.</p><p>To try the workload aware scheduling improvements:</p><ul><li>Workload API: Enable the\n<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#GenericWorkload\"></a>\nfeature gate on both  and , and ensure the <code>scheduling.k8s.io/v1alpha1</code><a title=\"A set of related paths in the Kubernetes API.\" data-toggle=\"tooltip\" data-placement=\"top\" href=\"https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning\" target=\"_blank\" aria-label=\"API group\">API group</a> is enabled.</li><li>Gang scheduling: Enable the\n<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#GangScheduling\"></a>\nfeature gate on  (requires the Workload API to be enabled).</li><li>Opportunistic batching: As a Beta feature, it is enabled by default in v1.35.\nYou can disable it using the\n<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#OpportunisticBatching\"></a>\nfeature gate on  if needed.</li></ul><p>We encourage you to try out workload aware scheduling in your test clusters\nand share your experiences to help shape the future of Kubernetes scheduling.\nYou can send your feedback by:</p>",
      "contentLength": 6321,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    }
  ],
  "tags": [
    "dev"
  ]
}
{
  "id": "i2nisf4o",
  "title": "Reddit",
  "displayTitle": "Reddit",
  "url": "",
  "feedLink": "",
  "isQuery": true,
  "isEmpty": false,
  "isHidden": false,
  "itemCount": 173,
  "items": [
    {
      "title": "First WIP release for DX12 perf testing is out!",
      "url": "https://www.reddit.com/r/linux/comments/1qndq5s/first_wip_release_for_dx12_perf_testing_is_out/",
      "date": 1769427513,
      "author": "/u/gilvbp",
      "guid": 38977,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "How to start",
      "url": "https://www.reddit.com/r/golang/comments/1qndfa4/how_to_start/",
      "date": 1769426526,
      "author": "/u/octebrenok",
      "guid": 38978,
      "unread": true,
      "content": "<p>I am AQA Engineer in js/ta scope. I got few tasks really to ho lang recently and seems like fall in love. Despite this lang is counterintuitive to me i like it. I want to switch from qa to dev team. </p><p>When I learned java (about 10 years ago) I learned on JavaRush, unfortunately they don’t have go courses right now. When I learned js i read the “You don’t know JS” book. Is there any the same level of resources you can advice me. </p>",
      "contentLength": 438,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "How minimal is “minimal enough” for production containers?",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qnd10w/how_minimal_is_minimal_enough_for_production/",
      "date": 1769425218,
      "author": "/u/Heavy_Banana_1360",
      "guid": 38967,
      "unread": true,
      "content": "<div><p>we have tried stripping base images but developers complain certain utilities are missing breaking CI/CD scripts. every dependency we remove seems to cause a subtle runtime bug somewhere.</p><p>how do you decide what is essential vs optional when creating minimal images for production?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Heavy_Banana_1360\"> /u/Heavy_Banana_1360 </a>",
      "contentLength": 319,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "I deleted production at my job today and nobody knows it was me",
      "url": "https://www.reddit.com/r/linux/comments/1qncjyd/i_deleted_production_at_my_job_today_and_nobody/",
      "date": 1769423557,
      "author": "/u/Fit-Original1314",
      "guid": 38965,
      "unread": true,
      "content": "<p>Throwaway for obvious reasons.</p><p>So I’m a junior dev at a small company and was SSH’d into what I thought was the dev server, ran a cleanup script, and it was not the dev server.</p><p>I spent the next four hours in a cold sweat pretending to help investigate what happened, while secretly restoring from backups and covering my tracks.</p><p>Everything is fixed now, but my soul has left my body. My hands are still shaking typing this lol.</p><p>Please tell me I’m not the only one who’s done something like this. I need to feel less alone right now.</p>",
      "contentLength": 536,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Holmes-go: a visual diff checker",
      "url": "https://github.com/jroden2/holmes-go",
      "date": 1769423433,
      "author": "/u/JackJack_IOT",
      "guid": 38979,
      "unread": true,
      "content": "<p>I work for a software consultancy and regularly require the ability to compare code, sensitive client/customer data, maybe .env files etc - and I'm always weary of using tools like diffchecker, the cmd-line tools suck especially on larger files. So I decided to build a tool using Gin-Gonic, Zerolog, HTML/Template, JS and Bootstrap5</p><p>I wanted to build something that was local-only, has no 3rd party integrations so could be completely air-gapped for projects which require security consciousness.</p><ul><li>It supports content-aware type switching (primitive, based on Text field A input using JS)</li><li>It supports pretty-printing of XML and JSON input (on POST)</li><li>it has a SHA256 comparison</li><li>whitespace and case ignore functions</li><li>Line-by-line and character highlighting.</li><li>Its built using goreleaser so has an executable for windows, mac and linux</li><li>Its dockerised so you can run it locally or on a web-service if you wanted</li></ul><p>Edit: I forgot to add, If you have feedback drop it here and I'll take a look at improvements. I've got 2 things to currently look at:</p><ul><li>node sorting for json/xml input to ensure all inputs are the same - I've had cases where github will say something changed when its just the order has shuffled</li><li>fix blank lines on xml comparisons, I've had this in the past with Java (POI/Jsoup etc) and I had to just filter blank lines out</li></ul><p>Edit edit: I've attached 3 screenshots to the repo, I'll add them to the readme too</p>",
      "contentLength": 1399,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qncioe/holmesgo_a_visual_diff_checker/"
    },
    {
      "title": "Study finds many software developers feel ethical pressure to ship products that may conflict with democratic values",
      "url": "https://www.tandfonline.com/doi/full/10.1080/1369118X.2025.2566814",
      "date": 1769416012,
      "author": "/u/SentFromHeav3n",
      "guid": 38954,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qnaguj/study_finds_many_software_developers_feel_ethical/"
    },
    {
      "title": "Go lang course",
      "url": "https://www.reddit.com/r/golang/comments/1qna80v/go_lang_course/",
      "date": 1769415117,
      "author": "/u/Durga_81",
      "guid": 38966,
      "unread": true,
      "content": "<div><p>Can someone please suggest a best go pang course in udemy I am a beginner </p><p>I want to build my career as Go lang developer </p></div>   submitted by   <a href=\"https://www.reddit.com/user/Durga_81\"> /u/Durga_81 </a>",
      "contentLength": 152,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Researchers warn of a “slop economy” where AI-generated content may undermine democratic discourse",
      "url": "https://www.tandfonline.com/doi/full/10.1080/1369118X.2025.2566814",
      "date": 1769413726,
      "author": "/u/Longjumping-Aide3157",
      "guid": 38956,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qn9tns/researchers_warn_of_a_slop_economy_where/"
    },
    {
      "title": "FF gave my uptime a !. What's your longest uptime?",
      "url": "https://www.reddit.com/r/linux/comments/1qn906i/ff_gave_my_uptime_a_whats_your_longest_uptime/",
      "date": 1769410881,
      "author": "/u/LauraLaughter",
      "guid": 38942,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Declarative GitOps CD for Kubernetes [ArgoCD, CloudNative PG, Kustomize, K8s Gateway, Istio Ambient, Grafana, Kiali & Go 1.26 based production ready API]",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qn7yjl/declarative_gitops_cd_for_kubernetes_argocd/",
      "date": 1769407456,
      "author": "/u/dumindunuwan",
      "guid": 38935,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "One-Minute Daily AI News 1/25/2026",
      "url": "https://www.reddit.com/r/artificial/comments/1qn7sy2/oneminute_daily_ai_news_1252026/",
      "date": 1769406975,
      "author": "/u/Excellent-Target-847",
      "guid": 38980,
      "unread": true,
      "content": "   submitted by   <a href=\"https://www.reddit.com/user/Excellent-Target-847\"> /u/Excellent-Target-847 </a>",
      "contentLength": 43,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "AST‑Powered Codebase Intelligence: Meet Drift, the Context Engine Behind Truly Useful AI Agents.",
      "url": "https://www.reddit.com/r/golang/comments/1qn7ksm/astpowered_codebase_intelligence_meet_drift_the/",
      "date": 1769406259,
      "author": "/u/Fluffy_Citron3547",
      "guid": 38927,
      "unread": true,
      "content": "<p>So I've been working on this thing called Drift and just wanted to share since Go support is now fully baked in.</p><p>Basically the problem is AI writes Go that compiles but its not YOUR Go. Like it doesnt know you always do if err != nil a certain way or how you structure your Gin handlers or whatever patterns youve established in your codebase.</p><p>What this does is scan your code with tree sitter, figure out your patterns, then expose all that to Claude/Cursor/Copilot through MCP. So when you ask it to write something it actually knows how YOU do things.</p><p>Works with Gin, Echo, Fiber, Chi, standard net/http. For data stuff it picks up GORM, sqlx, database/sql, ent.</p><p>The Go specific stuff it tracks:</p><p>error handling patterns (your if err != nil style, how you wrap errors, custom error types) interface implementations goroutine usage defer patterns struct and method organization</p><p>When you ask \"add a new endpoint\" the AI calls drift_context and gets back your actual route patterns, your middleware setup, your error handling, real examples from your code. Then generates something that fits.</p><p>Got a CLI too if you want to poke around yourself</p><p>All the docs are on the wiki if you want to try it:</p><p>8 languages total now (TS, Python, Java, C#, PHP, Go, Rust, C++), 45+ MCP tools, full call graph stuff.</p><p>Anyway happy to answer questions about how it works. Tree sitter parsing with regex fallback for edge cases, the whole thing runs local so your code stays on your machine.</p>",
      "contentLength": 1459,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Where do you deploy your Go backend?, that too if u wanna scale it in future n still be affordable and best performance.",
      "url": "https://www.reddit.com/r/golang/comments/1qn6tpr/where_do_you_deploy_your_go_backend_that_too_if_u/",
      "date": 1769403950,
      "author": "/u/MarsupialAntique1054",
      "guid": 38926,
      "unread": true,
      "content": "<p>I already built the backend fully working, but i wanna deploy it but testing it and containerizing it live , render gives a good and easy set up but I wanna explore other hosting providers. </p>",
      "contentLength": 190,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Developers are building programming languages in 24 hours with AI",
      "url": "https://medium.com/@jpcaparas/developers-are-building-programming-languages-in-24-hours-with-ai-153effe39177?sk=6e49dea9f56ed20d5bb010398b4e7a18",
      "date": 1769402057,
      "author": "/u/jpcaparas",
      "guid": 38931,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qn66k3/developers_are_building_programming_languages_in/"
    },
    {
      "title": "Long branches in compilers, assemblers, and linkers",
      "url": "https://maskray.me/blog/2026-01-25-long-branches-in-compilers-assemblers-and-linkers",
      "date": 1769401044,
      "author": "/u/MaskRay",
      "guid": 38928,
      "unread": true,
      "content": "<p>Branch instructions on most architectures use PC-relative addressing\nwith a limited range. When the target is too far away, the branch\nbecomes \"out of range\" and requires special handling.</p><p>Consider a large binary where  at address 0x10000\ncalls  at address 0x8010000-over 128MiB away. On\nAArch64, the  instruction can only reach ±128MiB, so this\ncall cannot be encoded directly. Without proper handling, the linker\nwould fail with an error like \"relocation out of range.\" The toolchain\nmust handle this transparently to produce correct executables.</p><p>This article explores how compilers, assemblers, and linkers work\ntogether to solve the long branch problem.</p><ul><li>Compiler (IR to assembly): Handles branches within a function that\nexceed the range of conditional branch instructions</li><li>Assembler (assembly to relocatable file): Handles branches within a\nsection where the distance is known at assembly time</li><li>Linker: Handles cross-section and cross-object branches discovered\nduring final layout</li></ul><p>Different architectures have different branch range limitations.\nHere's a quick comparison of unconditional / conditional branch\nranges:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>In  code, pseudo-absolute\n/ can be used for a 256MiB region.</td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>Use register-indirect if needed</td></tr><tr><td>Large code model changes call sequence</td></tr><tr></tr><tr></tr></tbody></table><p>The following subsections provide detailed per-architecture\ninformation, including relocation types relevant for linker\nimplementation.</p><ul><li>Branch (/), conditional\nbranch and link ()\n(): ±32MiB</li><li>Unconditional branch and link (/,\n): ±32MiB</li></ul><p>Note:  is for unconditional\n/ which can be relaxed to BLX inline;\n is for branches which require a veneer for\ninterworking.</p><p>In T32 state (Thumb state pre-ARMv8):</p><ul><li>Conditional branch (,\n): ±256 bytes</li><li>Short unconditional branch (,\n): ±2KiB</li><li>ARMv5T branch and link (/,\n): ±4MiB</li><li>ARMv6T2 wide conditional branch (,\n): ±1MiB</li><li>ARMv6T2 wide branch (,\n): ±16MiB</li><li>ARMv6T2 wide branch and link (/,\n): ±16MiB.  can be\nrelaxed to BLX.</li></ul><ul><li>Test bit and branch (/,\n): ±32KiB</li><li>Compare and branch (/,\n): ±1MiB</li><li>Conditional branches (,\n): ±1MiB</li><li>Unconditional branches (/,\n/):\n±128MiB</li></ul><p>The compiler's  pass handles\nout-of-range conditional branches by inverting the condition and\ninserting an unconditional branch. The AArch64 assembler does not\nperform branch relaxation; out-of-range branches produce linker errors\nif not handled by the compiler.</p><ul><li>Conditional branches\n(/////,\n): ±128KiB (18-bit signed)</li><li>Compare-to-zero branches (/,\n): ±4MiB (23-bit signed)</li><li>Unconditional branch/call (/,\n): ±128MiB (28-bit signed)</li><li>Medium range call (+,\n): ±2GiB</li><li>Long range call (+,\n): ±128GiB</li></ul><ul><li>Short branch\n(//): ±128 bytes\n(8-bit displacement)</li><li>Word branch\n(//): ±32KiB\n(16-bit displacement)</li><li>Long branch\n(//, 68020+):\n±2GiB (32-bit displacement)</li></ul><p>GNU Assembler provides <a target=\"_blank\" rel=\"noopener\" href=\"https://sourceware.org/binutils/docs/as/M68K_002dBranch.html\">pseudo\nopcodes</a> (, , ) that\n\"automatically expand to the shortest instruction capable of reaching\nthe target\". For example,  emits one of\n, , and  depending\non the displacement.</p><p>With the long forms available on 68020 and later, M68k doesn't need\nlinker range extension thunks.</p><ul><li>Conditional branches\n(////etc,\n): ±128KiB</li><li>PC-relative jump (\n()): ±128KiB</li><li>PC-relative call (\n()): ±128KiB</li><li>Pseudo-absolute jump/call (/,\n): branch within the current 256MiB region, only\nsuitable for  code. Deprecated in R6 in favor of\n/</li></ul><p>16-bit instructions removed in Release 6:</p><ul><li>Conditional branch (,\n): ±128 bytes</li><li>Unconditional branch (,\n): ±1KiB</li></ul><ul><li>Unconditional branch, compact (, unclear toolchain\nimplementation): ±1KiB</li><li>Compare and branch, compact\n(////etc,\n): ±128KiB</li><li>Compare register to zero and branch, compact\n(//etc,\n): ±4MiB</li><li>Branch (and link), compact (/,\n): ±128MiB</li></ul><p>LLVM's  pass handles out-of-range\nbranches.</p><p>lld implements LA25 thunks for MIPS PIC/non-PIC interoperability, but\nnot range extension thunks.</p><ul><li>Conditional branch (/,\n): ±32KiB</li><li>Unconditional branch (/,\n/):\n±32MiB</li></ul><p>GCC-generated code relies on linker thunks. However, the legacy\n can be used to generate long code sequences.</p><ul><li>Compressed : ±256 bytes</li><li> (I-type immediate): ±2KiB</li><li>Conditional branches\n(/////,\nB-type immediate): ±4KiB</li><li> (J-type immediate, ): ±1MiB\n(notably smaller than other RISC architectures: AArch64 ±128MiB,\nPowerPC64 ±32MiB, LoongArch ±128MiB)</li><li> (using  +\n): ±2GiB</li><li>/ (Zibi extension, 5-bit compare\nimmediate (1 to 31 and -1)): ±4KiB</li></ul><p>Qualcomm uC Branch Immediate extension (Xqcibi):</p><ul><li>/////\n(32-bit, 5-bit compare immediate): ±4KiB</li><li>/////\n(48-bit, 16-bit compare immediate): ±4KiB</li></ul><p>Qualcomm uC Long Branch extension (Xqcilb):</p><ul><li>/ (48-bit,\n<code>R_RISCV_VENDOR(QUALCOMM)+R_RISCV_QC_E_CALL_PLT</code>): ±2GiB</li></ul><ul><li>The <a target=\"_blank\" rel=\"noopener\" href=\"https://go-review.googlesource.com/c/go/+/345051\">Go\ncompiler</a> emits a single  for calls and relies on its\nlinker to generate trampolines when the target is out of range.</li><li>In contrast, GCC and Clang emit +\nand rely on linker relaxation to shrink the sequence when possible.</li></ul><p>The  range (±1MiB) is notably smaller than other RISC\narchitectures (AArch64 ±128MiB, PowerPC64 ±32MiB, LoongArch ±128MiB).\nThis limits the effectiveness of linker relaxation (\"start large and\nshrink\"), and leads to frequent trampolines when the compiler\noptimistically emits  (\"start small and grow\").</p><ul><li>Compare and branch (, ): ±64\nbytes</li><li>Conditional branch (, ):\n±1MiB</li><li>Unconditional branch (, ):\n±8MiB</li><li>\n(/): ±2GiB</li></ul><p>With ±2GiB range for , SPARC doesn't need range\nextension thunks in practice.</p><p>SuperH uses fixed-width 16-bit instructions, which limits branch\nranges.</p><ul><li>Conditional branch (/): ±256 bytes\n(8-bit displacement)</li><li>Unconditional branch (): ±4KiB (12-bit\ndisplacement)</li><li>Branch to subroutine (): ±4KiB (12-bit\ndisplacement)</li></ul><p>For longer distances, register-indirect branches\n(/) are used. The compiler inverts\nconditions and emits these when targets exceed the short ranges.</p><p>SuperH is supported by GCC and binutils, but not by LLVM.</p><p>Xtensa uses variable-length instructions: 16-bit (narrow,\n suffix) and 24-bit (standard).</p><ul><li>Narrow conditional branch (/,\n16-bit): -28 to +35 bytes (6-bit signed + 4)</li><li>Conditional branch (compare two registers)\n(////etc,\n24-bit): ±256 bytes</li><li>Conditional branch (compare with zero)\n(///,\n24-bit): ±2KiB</li><li>Unconditional jump (, 24-bit): ±128KiB</li><li>Call\n(///,\n24-bit): ±512KiB</li></ul><p>The assembler performs branch relaxation: when a conditional branch\ntarget is too far, it inverts the condition and inserts a \ninstruction.</p><ul><li>Short conditional jump (): -128 to +127\nbytes</li><li>Short unconditional jump (): -128 to +127\nbytes</li><li>Near conditional jump (): ±2GiB</li><li>Near unconditional jump (): ±2GiB</li></ul><p>With a ±2GiB range for near jumps, x86-64 rarely encounters\nout-of-range branches in practice. That said, Google and Meta Platforms\ndeploy mostly statically linked executables on x86-64 production servers\nand have run into the huge executable problem for certain\nconfigurations.</p><ul><li>Short conditional branch (,\n): ±64KiB (16-bit halfword displacement)</li><li>Long conditional branch (,\n): ±4GiB (32-bit halfword displacement)</li><li>Short call (, ):\n±64KiB</li><li>Long call (, ):\n±4GiB</li></ul><p>With ±4GiB range for long forms, z/Architecture doesn't need linker\nrange extension thunks. LLVM's  pass\nrelaxes short branches (/) to long\nforms (/) when targets are out of\nrange.</p><p>Conditional branch instructions usually have shorter ranges than\nunconditional ones, making them less suitable for linker thunks (as we\nwill explore later). Compilers typically keep conditional branch targets\nwithin the same section, allowing the compiler to handle out-of-range\ncases via branch relaxation.</p><p>Within a function, conditional branches may still go out of range.\nThe compiler measures branch distances and relaxes out-of-range branches\nby inverting the condition and inserting an unconditional branch:</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>Some architectures have conditional branch instructions that compare\nwith an immediate, with even shorter ranges due to encoding additional\nimmediates. For example, AArch64's /\n(compare and branch if zero/non-zero) and\n/ (test bit and branch) have only\n±32KiB range. RISC-V Zibi / have ±4KiB\nrange. The compiler handles these in a similar way:</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>An Intel employee contributed <a target=\"_blank\" rel=\"noopener\" href=\"https://reviews.llvm.org/D41634\">https://reviews.llvm.org/D41634</a> (in 2017) when inversion\nof a branch condintion is impossible. This is for an out-of-tree\nbackend. As of Jan 2026 there is no in-tree test for this code path.</p><p>In LLVM, this is handled by the  pass,\nwhich runs just before . Different backends have\ntheir own implementations:</p><ul><li>: AArch64, AMDGPU, AVR, RISC-V</li><li>: Hexagon</li><li>: PowerPC</li><li>: SystemZ</li><li>: MIPS</li></ul><p>The generic  pass computes block sizes\nand offsets, then iterates until all branches are in range. For\nconditional branches, it tries to invert the condition and insert an\nunconditional branch. For unconditional branches that are still out of\nrange, it calls <code>TargetInstrInfo::insertIndirectBranch</code> to\nemit an indirect jump sequence (e.g.,\n++ on AArch64) or a long\njump sequence (e.g., pseudo  on RISC-V).</p><p>Note: The size estimates may be inaccurate due to inline assembly.\nLLVM uses heuristics to estimate inline assembly sizes, but for certain\nassembly constructs the size is not precisely known at compile time.</p><p>Unconditional branches and calls can target different sections since\nthey have larger ranges. If the target is out of reach, the linker can\ninsert thunks to extend the range.</p><p>For x86-64, the large code model uses multiple instructions for calls\nand jumps to support text sections larger than 2GiB (see <a href=\"https://maskray.me/blog/2023-05-14-relocation-overflow-and-code-models#x86-64-large-code-model\">Relocation\noverflow and code models: x86-64 large code model</a>). This is a\npessimization if the callee ends up being within reach. Google and Meta\nPlatforms have interest in allowing range extension thunks as a\nreplacement for the multiple instructions.</p><h2>Assembler: instruction\nrelaxation</h2><p>The assembler converts assembly to machine code. When the target of a\nbranch is within the same section and the distance is known at assembly\ntime, the assembler can select the appropriate encoding. This is\ndistinct from linker thunks, which handle cross-section or cross-object\nreferences where distances aren't known until link time.</p><ul><li><strong>Span-dependent instructions</strong>: Select an appropriate\nencoding based on displacement.\n<ul><li>On x86, a short jump () can be relaxed to a\nnear jump () when the target is far.</li><li>On RISC-V,  may be assembled to the 2-byte\n when the displacement fits within ±256 bytes.</li></ul></li><li><strong>Conditional branch transform</strong>: Invert the condition\nand insert an unconditional branch. On RISC-V, a  might\nbe relaxed to  plus an unconditional branch.</li></ul><p>The assembler uses an iterative layout algorithm that alternates\nbetween fragment offset assignment and relaxation until all fragments\nbecome legalized. See <a href=\"https://maskray.me/blog/2024-06-30-integrated-assembler-improvements-in-llvm-19\">Integrated\nassembler improvements in LLVM 19</a> for implementation details.</p><h2>Linker: range extension\nthunks</h2><p>When the linker resolves relocations, it may discover that a branch\ntarget is out of range. At this point, the instruction encoding is\nfixed, so the linker cannot simply change the instruction. Instead, it\ngenerates  (also called veneers,\nbranch stubs, or trampolines).</p><p>A thunk is a small piece of linker-generated code that can reach the\nactual target using a longer sequence of instructions. The original\nbranch is redirected to the thunk, which then jumps to the real\ndestination.</p><p>Range extension thunks are one type of linker-generated thunk. Other\ntypes include:</p><h3>Short range vs long range\nthunks</h3><p>A  (see <a target=\"_blank\" rel=\"noopener\" href=\"https://reviews.llvm.org/D148701\">lld/ELF's AArch64\nimplementation</a>) contains just a single branch instruction. Since it\nuses a branch, its reach is also limited by the branch range—it can only\nextend coverage by one branch distance. For targets further away,\nmultiple short range thunks can be chained, or a long range thunk with\naddress computation must be used.</p><p>Long range thunks use indirection and can jump to (practically)\narbitrary locations.</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><h3>Thunk impact on\ndebugging and profiling</h3><p>Thunks are transparent at the source level but visible in low-level\ntools:</p><ul><li>: May show thunk symbols (e.g.,\n) between caller and callee</li><li>: Samples may attribute time to thunk\ncode; some profilers aggregate thunk time with the target function</li><li>:  or\n will show thunk sections interspersed with\nregular code</li><li>: Each thunk adds bytes; large binaries\nmay have thousands of thunks</li></ul><h3>lld/ELF's thunk creation\nalgorithm</h3><p>lld/ELF uses a multi-pass algorithm in\n<code>finalizeAddressDependentContent</code>:</p><figure><table><tbody><tr><td><pre></pre></td><td><pre></pre></td></tr></tbody></table></figure><ul><li>: Iterates until convergence (max 30\npasses). Adding thunks changes addresses, potentially putting\npreviously-in-range calls out of range.</li><li><strong>Pre-allocated ThunkSections</strong>: On pass 0,\n<code>createInitialThunkSections</code> places empty\ns at regular intervals\n(). For AArch64: 128 MiB - 0x30000 ≈\n127.8 MiB.</li><li>:  returns existing\nthunk if one exists for the same target;\n checks if a previously-created thunk\nis still in range.</li><li>: \nfinds a ThunkSection within branch range of the call site, or creates\none adjacent to the calling InputSection.</li></ul><h3>lld/MachO's thunk creation\nalgorithm</h3><p>lld/MachO uses a single-pass algorithm in\n<code>TextOutputSection::finalize</code>:</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>Key differences from lld/ELF:</p><ul><li>: Addresses are assigned monotonically\nand never revisited</li><li>: Reserves\n bytes (default: 256 × 12 = 3072 bytes\non ARM64) to leave room for future thunks</li><li>:\n<code>&lt;function&gt;.thunk.&lt;sequence&gt;</code> where sequence\nincrements per target</li></ul><p><a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/llvm/llvm-project/issues/50920\">Thunk\nstarvation problem</a>: If many consecutive branches need thunks, each\nthunk (12 bytes) consumes slop faster than call sites (4 bytes apart)\nadvance. The test <code>lld/test/MachO/arm64-thunk-starvation.s</code>\ndemonstrates this edge case. Mitigation is increasing\n, but pathological cases with hundreds of\nconsecutive out-of-range callees can still fail.</p><h3>mold's thunk creation\nalgorithm</h3><p>mold uses a two-pass approach:</p><ul><li>Pessimistically over-allocate thunks. Out-of-section relocations and\nrelocations referencing to a section not assigned address yet\npessimistically need thunks.\n(<code>requires_thunk(ctx, isec, rel, first_pass)</code> when\n)</li><li>Then remove unnecessary ones.</li></ul><ul><li> calls\n<code>create_range_extension_thunks()</code> — final section addresses\nare NOT yet known</li><li> assigns section addresses</li><li><code>remove_redundant_thunks()</code> is called AFTER addresses are\nknown — check unneeded thunks due to out-of-section relocations</li></ul><p> (<code>create_range_extension_thunks</code>):\nProcess sections in batches using a sliding window. The window tracks\nfour positions:</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><ul><li> = current batch of sections to process (size\n≤ branch_distance/5)</li><li> = earliest section still reachable from C (for\nthunk expiration)</li><li> = where to place the thunk (furthest point\nreachable from B)</li></ul><figure><table><tbody><tr><td><pre></pre></td><td><pre></pre></td></tr></tbody></table></figure><p> (): After\nfinal addresses are known, remove thunk entries for symbols actually in\nrange.</p><ul><li><strong>Pessimistic over-allocation</strong>: Assumes all\nout-of-section calls need thunks; safe to shrink later</li><li>: branch_distance/5 (25.6 MiB for\nAArch64, 3.2 MiB for AArch32)</li><li>: Uses TBB for parallel relocation\nscanning within each batch</li><li>: Uses one conservative\n per architecture. For AArch32, uses ±16 MiB\n(Thumb limit) for all branches, whereas lld/ELF uses ±32 MiB for A32\nbranches.</li><li><strong>Thunk size not accounted in D-advancement</strong>: The\nactual thunk group size is unknown when advancing D, so the end of a\nlarge thunk group may be unreachable from the beginning of the\nbatch.</li><li>: Single forward pass for\naddress assignment, no risk of non-convergence</li></ul><h3>GNU ld's thunk creation\nalgorithm</h3><p>Each port implements the algorithm on their own. There is no code\nsharing.</p><p>GNU ld's AArch64 port () uses an\niterative algorithm but with a single stub type and no lookup table.</p><p>\n(<code>elfNN_aarch64_size_stubs()</code>):</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>GNU ld's ppc64 port () uses an iterative\nmulti-pass algorithm with a branch lookup table\n() for long-range stubs.</p><p>: Sections are grouped by\n (~28-30 MiB default); each group gets one\nstub section. For 14-bit conditional branches\n(, ±32KiB range), group size is reduced by\n1024x.</p><p>\n():</p><figure><table><tbody><tr><td><pre></pre></td><td><pre></pre></td></tr></tbody></table></figure><ul><li> (<a target=\"_blank\" rel=\"noopener\" href=\"https://sourceware.org/PR28827\">PR28827</a>): After 20 iterations,\nstub sections only grow (prevents oscillation)</li><li>Convergence when:\n<code>!stub_changed &amp;&amp; all section sizes stable</code></li></ul><p>: \ninitially returns  for out-of-range\nbranches. Later,  checks if the stub's\nbranch can reach; if not, it upgrades to\n and allocates an 8-byte entry in\n.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>Some architectures take a different approach: instead of only\nexpanding branches, the linker can also \ninstruction sequences when the target is close enough. RISC-V and\nLoongArch both use this technique. See <a href=\"https://maskray.me/blog/2021-03-14-the-dark-side-of-riscv-linker-relaxation\">The\ndark side of RISC-V linker relaxation</a> for a deeper dive into the\ncomplexities and tradeoffs.</p><p>Consider a function call using the \npseudo-instruction, which expands to  +\n: </p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>If  is within ±1MiB, the linker can relax this to:\n</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>This is enabled by  relocations that\naccompany  relocations. The\n relocation signals to the linker that this\ninstruction sequence is a candidate for shrinking.</p><p>Example object code before linking: </p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>After linking with relaxation enabled, the 8-byte\n+ pairs become 4-byte\n instructions: </p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>When the linker deletes instructions, it must also adjust:</p><ul><li>Subsequent instruction offsets within the section</li><li>Other relocations that reference affected locations</li><li>Alignment directives ()</li></ul><p>This makes RISC-V linker relaxation more complex than thunk\ninsertion, but it provides code size benefits that other architectures\ncannot achieve at link time.</p><p>LoongArch uses a similar approach. A\n+ sequence\n(, ±128GiB range) can be relaxed to a single\n instruction (, ±128MiB range)\nwhen the target is close enough.</p><h2>Diagnosing out-of-range\nerrors</h2><p>When you encounter a \"relocation out of range\" error, check the\nlinker diagnostic and locate the relocatable file and function.\nDetermine how the function call is lowered in assembly.</p><p>Handling long branches requires coordination across the\ntoolchain:</p><table><tbody><tr><td>Invert condition + add unconditional jump</td></tr><tr><td>Invert condition + add unconditional jump</td></tr><tr></tr><tr><td>Shrink + to \n(RISC-V)</td></tr></tbody></table><p>The linker's thunk generation is particularly important for large\nprograms where function calls may exceed branch ranges. Different\nlinkers use different algorithms with various tradeoffs between\ncomplexity, optimality, and robustness.</p><p>Linker relaxation approaches adopted by RISC-V and LoongArch is an\nalternative that avoids range extension thunks but introduces other\ncomplexities.</p>",
      "contentLength": 17445,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qn5twv/long_branches_in_compilers_assemblers_and_linkers/"
    },
    {
      "title": "\"Make Adobe Creative Cloud run on Linux\", I thought. \"It'll be a fun project/puzzle\", I thought.",
      "url": "https://www.reddit.com/r/linux/comments/1qn54wn/make_adobe_creative_cloud_run_on_linux_i_thought/",
      "date": 1769399101,
      "author": "/u/QwertyChouskie",
      "guid": 38911,
      "unread": true,
      "content": "<p>I'm slowly losing what marbles of mine remain...</p>",
      "contentLength": 48,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Studying ESP32 firmware, feels like Go isn’t really used in production",
      "url": "https://www.reddit.com/r/golang/comments/1qn4ve6/studying_esp32_firmware_feels_like_go_isnt_really/",
      "date": 1769398395,
      "author": "/u/ConsiderationMean593",
      "guid": 38912,
      "unread": true,
      "content": "<p>I’ve been studying ESP32 firmware lately.</p><p>I like Go a lot and naturally looked into TinyGo and other Go-based approaches.</p><p>They’re interesting and fun to experiment with.</p><p>But when I look at how firmware is actually written in production,</p><p>almost everything still seems to be C or C++.</p><p>It doesn’t feel like people “hate” Go,</p><p>more like the ecosystem, tooling, and long-term trust just aren’t there yet for firmware.</p><p>Curious if anyone here has seen Go used seriously in ESP32 or embedded projects,</p><p>or if it’s still mostly a hobby / experiment space.</p>",
      "contentLength": 550,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ChatGPT Is Using Elon Musk’s Grokipedia as a Source",
      "url": "https://techputs.com/chatgpt-pulling-answers-from-grokipedia/",
      "date": 1769395376,
      "author": "/u/i-drake",
      "guid": 38901,
      "unread": true,
      "content": "<p>ChatGPT’s latest model, , has been spotted pulling answers from , an AI-generated online encyclopedia created by xAI. The discovery has surprised many in the tech community, especially given the ongoing rivalry between OpenAI and Musk’s AI ventures.</p><p>The development has triggered discussions around AI transparency, source reliability, and how large language models decide which information to trust.</p><h2>Grokipedia Enters ChatGPT’s Source Pool</h2><p>Grokipedia launched in  as an AI-driven alternative to Wikipedia. According to Elon Musk, the project was built to counter what he believes are ideological biases in traditional encyclopedias. Unlike Wikipedia, Grokipedia is fully generated by artificial intelligence and does not allow human editing.</p><p>Recent testing by multiple <a href=\"https://www.theguardian.com/technology/2026/jan/24/latest-chatgpt-model-uses-elon-musks-grokipedia-as-source-tests-reveal\" target=\"_blank\" rel=\"noopener\">publications</a> found that ChatGPT cited Grokipedia several times while answering user queries. These citations appeared across topics such as political structures, historical figures, and academic subjects.</p><p>Notably, ChatGPT did not rely on Grokipedia for highly sensitive or widely documented topics. Instead, it appeared more frequently in responses related to niche or lesser-known subjects, where fewer authoritative sources are available online.</p><p>OpenAI states that ChatGPT draws from a wide range of publicly available and licensed data. However, Grokipedia’s AI-only editorial process has raised concerns among researchers and fact-checkers. Without human oversight, errors or biased interpretations may be harder to identify and correct.</p><p>Several studies have questioned Grokipedia’s sourcing standards, noting instances of weak citations and inconsistent references. When AI models reuse content from AI-generated platforms, there is a risk of reinforcing inaccuracies, especially when those sources gain visibility through search indexing.</p><h3><strong>Blurring Lines Between Rival AI Platforms</strong></h3><p>Despite public tensions between OpenAI and xAI, this situation highlights how interconnected the AI ecosystem has become. Algorithms do not account for corporate competition. They prioritize relevance, availability, and indexing signals, regardless of who created the content.</p><p>AI researchers point out that modern language models often surface content that is highly indexed or frequently referenced online. If Grokipedia continues to grow in visibility, it may increasingly appear in AI-generated responses.</p><p>Some academics warn that AI-generated encyclopedias could redefine how authority is established online, shifting trust from human editorial systems to algorithmic outputs that lack traditional accountability.</p><p>For users, the takeaway is simple. AI-generated answers should always be verified. Cross-checking important information with trusted sources remains essential, especially when unfamiliar citations appear in responses.</p><p>As AI systems continue integrating more live web data, questions around source quality, filtering, and accountability will become increasingly important. The emergence of Grokipedia as a referenced source highlights the growing challenge of balancing open information access with accuracy and trust.</p><ul></ul>",
      "contentLength": 3089,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qn3qk2/chatgpt_is_using_elon_musks_grokipedia_as_a_source/"
    },
    {
      "title": "Megathread Submission Cosmic Al: GitHub Repo Scanner to Quantify Tech Debt in Dollars - Early Feedback?",
      "url": "http://cosmic-ai.pages.dev/",
      "date": 1769393900,
      "author": "/u/Tech_News_Blog",
      "guid": 38900,
      "unread": true,
      "content": "<h2>\nTech Debt Is  Your Team\n</h2><p>\nDevelopers spend  dealing with technical debt. \n        The worst part? Management doesn't see it.\n</p><p> Stripe Developer Coefficient Report 2023 | Data from Stack Overflow Developer Survey\n</p>",
      "contentLength": 211,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qn36hr/megathread_submission_cosmic_al_github_repo/"
    },
    {
      "title": "[D] How did Microsoft's Tay work?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qn34ea/d_how_did_microsofts_tay_work/",
      "date": 1769393748,
      "author": "/u/RhubarbSimilar1683",
      "guid": 38941,
      "unread": true,
      "content": "<p>How did AI like Microsoft's Tay work? This was 2016, before LLMs. No powerful GPUs with HBM and Google's first TPU is cutting edge. Transformers didn't exist. It seems much better than other contemporary chatbots like SimSimi. It adapts to user engagement and user generated text very quickly, adjusting the text it generates which is grammatically coherent and apparently context appropriate and contains information unlike SimSimi. There is zero information on its inner workings. Could it just have been RL on an RNN trained on text and answer pairs? Maybe Markov chains too? How can an AI model like this learn continuously? Could it have used Long short-term memory? I am guessing it used word2vec to capture \"meaning\"</p>",
      "contentLength": 723,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "zerobrew is a Rust-based, 5-20x faster drop-in Homebrew alternative",
      "url": "https://github.com/lucasgelfond/zerobrew",
      "date": 1769391532,
      "author": "/u/lucasgelfond",
      "guid": 38908,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/rust/comments/1qn2aev/zerobrew_is_a_rustbased_520x_faster_dropin/"
    },
    {
      "title": "Who else is excited to see more alternatives than just iOS and Android phones",
      "url": "https://www.reddit.com/r/linux/comments/1qn0qg6/who_else_is_excited_to_see_more_alternatives_than/",
      "date": 1769387527,
      "author": "/u/DavidNorena",
      "guid": 38895,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "In humble defense of the .zip TLD",
      "url": "https://luke.zip/posts/zip-defense/",
      "date": 1769385858,
      "author": "/u/yathern",
      "guid": 38910,
      "unread": true,
      "content": "<p>I recently released a small (infinite) <a href=\"https://words.zip/\" rel=\"noreferrer noopener\" target=\"_blank\">word game</a> which had the pleasure of getting some good exposure from Gizmodo in <a href=\"https://gizmodo.com/give-the-internet-an-infinite-word-search-and-the-internet-will-draw-a-dick-on-it-2000709697\" rel=\"noreferrer noopener\" target=\"_blank\">this article</a>:</p><img src=\"https://luke.zip/gizmodo-article.png\" alt=\"Article Screenshot\"><p>The headline suggests my wholesome game is a wasteland of phallic imagery… but any publicity is good publicity! I was very happy with the (indecent) exposure, and got a decent bump in traffic as a result.</p><p><strong>There was, however, a paragraph which I took personal affront to</strong>:</p><blockquote><p>It’s also notable for using the .zip domain, which—despite what one might assume—is not exclusively for phishing attacks based around fooling people who believe they’re downloading a .zip file (see: when Google opened registrations for the domain in 2023, multiple security researchers and companies condemned the idea, warning that people generally associate “.zip” with a file type, not a top level domain).</p></blockquote><p>Now I  not be the most unbiased person to voice my opinion here, I happen to have a small horde of .zip domains; <a href=\"https://monkeys.zip\" rel=\"noreferrer noopener\" target=\"_blank\">monkeys.zip</a>, <a href=\"https://words.zip\" rel=\"noreferrer noopener\" target=\"_blank\">words.zip</a>, <a href=\"https://hn.zip\" rel=\"noreferrer noopener\" target=\"_blank\">hn.zip</a> and my <a href=\"https://luke.zip\" rel=\"noreferrer noopener\" target=\"_blank\">personal site</a> that you’re currently on… but I would like to get a chance to defend my choice.</p><p>When the .zip TLD was opened back in 2023, it was indeed met with <a href=\"https://redcanary.com/blog/threat-detection/google-zip-domains/\" rel=\"noreferrer noopener\" target=\"_blank\">widespread disdain</a> from <a href=\"https://www.gendigital.com/blog/insights/research/unpacking-the-threats-within-the-hidden-dangers-of-zip-domains\" rel=\"noreferrer noopener\" target=\"_blank\">a number</a> of Web Security researchers &amp; bloggers. It quickly became common wisdom that zip websites were dangerous - and that in no time, <a href=\"https://www.reddit.com/r/cybersecurity/comments/13i2h6v/are_new_domains_zip_and_mov_a_possible_security/\" rel=\"noreferrer noopener\" target=\"_blank\">a flood</a> of new phishing scams will spread chaos throughout the internet.</p><p>Well it’s 2026 now, so let’s talk about it. What’s the big deal with .zip domains?</p><p>The most frequently referenced source for why .zip is bad is <a href=\"https://medium.com/@bobbyrsec/the-dangers-of-googles-zip-tld-5e1e675e59a5\" rel=\"noreferrer noopener\" target=\"_blank\">this article</a> from Bobby Rauch. The author shows off a new attack where one can click a link expecting to arrive at one place, and instead arrive at another:</p><img src=\"https://luke.zip/zip-link.png\" alt=\".zip trick url\"><p>This is a very clever demo, using some esoteric URL knowledge that can deceive even technical users. The trick involves sneaking in an ’@’ symbol into the URL, and a handful of fake unicode forward slashes - so that even though the URL starts with ‘github.com’ - the link actually points to <a href=\"http://v1271.zip\" rel=\"noreferrer noopener\" target=\"_blank\">v1271.zip</a>. This site downloads a nasty virus and now you’re SCREWED!</p><p>But before we condemn .zip domains as a result of this creative demo, let’s think about how this attack would work in practice,  if the .zip TLD is responsible for it.</p><p>If a malicious person wants to make a deceptive link, there’s a much, much easier way than with unicode trickery. For example, just follow this link to Wikipedia <a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" rel=\"noreferrer noopener\" target=\"_blank\">https://wikipedia.org/wiki/Phishing</a> to see for yourself! On the web, we can make links say anything -  it’s been a feature of HTML for <a href=\"https://www.ietf.org/rfc/rfc1866.txt\" rel=\"noreferrer noopener\" target=\"_blank\">30+ years</a>, and is likewise available in email clients or anywhere markdown is supported. The author argues that especially in an email client, our clever villain may “<em>change the size of the @ operator to a size 1 font, [making it] visually non-existent for the user, but still present as part of the URL</em>”… but how about this little trick?</p><img src=\"https://luke.zip/evil-email.png\" alt=\"evil-email.png\"><p>The argument here is that domain names have their own world, file names have theirs, and never the twain shall meet - you can find an example of this thinking in <a href=\"https://www.threatdown.com/blog/zip-domains-a-bad-idea-nobody-asked-for/\" rel=\"noreferrer noopener\" target=\"_blank\">this article</a>:</p><blockquote><p>Domain names and filenames are not the same thing, not even close, but both of them play an important role in modern cyberattacks, and correctly identifying them has formed part of lots of basic security advice for a long, long time.</p></blockquote><p>First, I disagree that the distinction between domain names and filenames is inherently relevant to security. I also think the suggestion that “both of them play an important role in modern cyberattacks” is a nearly tautological statement - every aspect of computing is relevant to cyberattacks!</p><p>But more importantly, there has never been an explicit or implicit rule that these two worlds cannot overlap. In fact, you may be surprised to learn that our sacred ‘.com’ TLD was a widely used executable <a href=\"https://en.wikipedia.org/wiki/COM_file\" rel=\"noreferrer noopener\" target=\"_blank\">file extension</a> for decades, and some <a href=\"https://gaussian.com/gaussview6/\" rel=\"noreferrer noopener\" target=\"_blank\">modern software</a> uses it as well. There’s plenty of other examples as well - ai is used by Adobe Illustrator, .app is the extension of MacOS packages. Poland’s .pl is used for Perl scripts, and Saint Helena’s .sh is commonly used for shell scripts. Besides tradition, I don’t see any reason ‘.zip’ is too precious to preserve.</p><p>It seems to me that the cat is out of the bag on TLDs colliding with filename extensions, and has been for a while. The best security advice is to be suspicious of ALL links - not tempering our suspicion based on the TLD. Let’s also not instill fearmonger users into avoiding .zip URLs altogether, thereby driving them away from my <a href=\"https://monkeys.zip\" rel=\"noreferrer noopener\" target=\"_blank\">precious monkeys</a>.</p><p>This is  the most interesting one for sure - and funnily enough, it’s the one least referenced in popular articles and blog posts.</p><p>A lot of web apps and software will automatically ‘linkify’ any user submitted text that looks like it could be a link, so when you write “go to google.com” - the text editor swaps in “go to <a href=\"https://google.com\" rel=\"noreferrer noopener\" target=\"_blank\">google.com</a>”.</p><img src=\"https://luke.zip/linkification.jpg\" alt=\"linkification\"><p>So if a malicious actor gets a domain that sounds like a common zip file - this could open up an attack where you try to tell your friend “hey I sent <a href=\"http://weddingpictures.zip\" rel=\"noreferrer noopener\" target=\"_blank\">weddingpictures.zip</a> to your email” and your friend clicks the resulting link, thereby being redirected to a trick site that steals your SSN.</p><p>I actually think there’s merit to this idea, however I haven’t seen a single live example of this attack, nor do I suspect there has been. There’s been <a href=\"https://www.ghacks.net/2023/05/15/googles-zip-top-level-domain-is-already-used-in-phishing-attacks/\" rel=\"noreferrer noopener\" target=\"_blank\">a few</a> ‘<a href=\"https://www.fortinet.com/blog/industry-trends/threat-actors-add-zip-domains-to-phishing-arsenals\" rel=\"noreferrer noopener\" target=\"_blank\">I told you so</a>’ <a href=\"https://www.cyberdefensemagazine.com/new-phishing-attacks-use-zip-to-target-brands/\" rel=\"noreferrer noopener\" target=\"_blank\">articles</a> about .zip sites being used for phishing, but as far as I can tell, they’re just phishing sites that use .zip not for the association with the archive format, but because it was cheap to buy ‘microsoft.zip’. But this is a problem for any new TLD, as every large brand is compelled to have their domain in every TLD, otherwise someone winds up with <a href=\"http://facebook.sucks\" rel=\"noreferrer noopener\" target=\"_blank\">facebook.sucks</a> and makes a stink of things.</p><p>But, back to linkification - I actually think it’s a broader issue - and that the user should be in greater control on whether they want to link to something or not. I initially doubted this would be a problem, but check out this brief survey of platforms, and what happens when I input either “<a href=\"https://luke.zip\" rel=\"noreferrer noopener\" target=\"_blank\">https://luke.zip</a>” or “luke.zip” into them, and if I can edit/remove this link before submitting.</p><table><tbody></tbody></table><p>While most do not automatically linkify ‘luke.zip’ - Twitter and WhatsApp do - this isn’t so bad. What is worse is that on these platforms you CANNOT remove the link! There is no ability to say “I don’t want to add a link here, DON’T make this text a link please.</p><p>This is annoying, and I think it should be fixed… but I still don’t believe anyone is clicking zip files in tweets and getting phished. They’re certainly not clicking <a href=\"https://x.com/LukeSchaef/status/2015456095209197811\" rel=\"noreferrer noopener\" target=\"_blank\">any of mine</a> at least, but that’s probably because I have like 20 followers and hardly tweet anything.</p>",
      "contentLength": 6682,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qn02yc/in_humble_defense_of_the_zip_tld/"
    },
    {
      "title": "I built a 2x faster lexer, then discovered I/O was the real bottleneck",
      "url": "https://modulovalue.com/blog/syscall-overhead-tar-gz-io-performance/",
      "date": 1769384812,
      "author": "/u/modulovalue",
      "guid": 38891,
      "unread": true,
      "content": "<p>I built an ARM64 assembly lexer (well, I generated one from my own parser generator, but this post is not about that) that processes Dart code 2x faster than the official scanner, a result I achieved using <a href=\"https://modulovalue.com/blog/statistical-methods-for-reliable-benchmarks/\">statistical methods to reliably measure small performance differences</a>. Then I benchmarked it on 104,000 files and discovered my lexer was not the bottleneck. I/O was. This is the story of how I accidentally learned why <a href=\"https://pub.dev\">pub.dev</a> stores packages as tar.gz files.</p><p>I wanted to benchmark my lexer against the official Dart scanner. The pub cache on my machine had 104,000 Dart files totaling 1.13 GB, a perfect test corpus. I wrote a benchmark that:</p><ol><li>Reads each file from disk</li><li>Measures time separately for I/O and lexing</li></ol><h2>The first surprise: lexing is fast</h2><table><thead><tr></tr></thead><tbody><tr></tr></tbody></table><p>My lexer was 2.17x faster. Success! But wait:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table><p>The total speedup was only 1.22x. My 2.17x lexer improvement was being swallowed by I/O. Reading files took 5x longer than lexing them.</p><h2>The second surprise: the SSD is not the bottleneck</h2><p>My MacBook has an NVMe SSD that can read at 5-7 GB/s. I was getting 80 MB/s. That is 1.5% of the theoretical maximum.</p><p>The problem was not the disk. It was the syscalls.</p><p>For 104,000 files, the operating system had to execute:</p><ul></ul><p>That is over 300,000 syscalls. Each syscall involves:</p><ul><li>A context switch from user space to kernel space</li><li>Kernel bookkeeping and permission checks</li><li>A context switch back to user space</li></ul><p>Each syscall costs roughly 1-5 microseconds. Multiply that by 300,000 and you get 0.3-1.5 seconds of pure overhead, before any actual disk I/O happens. Add filesystem metadata lookups, directory traversal, and you understand where the time goes.</p><p>I tried a few things that did not help much. Memory-mapping the files made things worse due to the per-file mmap/munmap overhead. Replacing Dart's file reading with direct FFI syscalls (open/read/close) only gave a 5% improvement. The problem was not Dart's I/O layer, it was the sheer number of syscalls.</p><p>I have mirrored pub.dev several times in the past and noticed that all packages are stored as tar.gz archives. I never really understood why, but this problem reminded me of that fact. If syscalls are the problem, the solution is fewer syscalls. What if instead of 104,000 files, I had 1,351 files (one per package)?</p><p>I wrote a script to package each cached package into a tar.gz archive:</p><div><div><pre><code>104,000 individual files -&gt; 1,351 tar.gz archives\n1.13 GB uncompressed     -&gt; 169 MB compressed (6.66x ratio)\n</code></pre></div></div><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>The I/O speedup was . Reading 1,351 sequential files instead of 104,000 random files reduced I/O from 14.5 seconds to 339 milliseconds.</p><p>The total speedup was . Even with decompression overhead, the archive approach was more than twice as fast.</p><h2>Breaking down the numbers</h2><p>This is the syscall overhead in action. Going from 300,000+ syscalls to roughly 4,000 syscalls (open/read/close for 1,351 archives) eliminated most of the overhead.</p><p>Additionally, reading 1,351 files sequentially is far more cache-friendly than reading 104,000 files scattered across the filesystem. The OS can prefetch effectively, the SSD can batch operations, and the page cache stays warm.</p><p>gzip decompression ran at about 250 MB/s using the  package from pub.dev. This is now the new bottleneck. I did not put much effort into optimizing decompression, an FFI-based solution using native zlib could be significantly faster. Modern alternatives like lz4 or zstd might also help.</p><p>Source code compresses well. The 1.13 GB of Dart code compressed to 169 MB. This means less data to read from disk, which helps even on fast SSDs.</p><p>This experiment accidentally explains the pub.dev package format. When you run , you download tar.gz archives, not individual files. The reasons are now obvious:</p><ol><li> One request per package instead of hundreds.</li><li> 6-7x smaller downloads.</li><li> Sequential writes beat random writes.</li><li><strong>Reduced syscall overhead.</strong> Both on the server (fewer files to serve) and the client (fewer files to write).</li><li> A package is either fully downloaded or not. No partial states.</li></ol><p>The same principles apply to npm (tar.gz), Maven (JAR/ZIP), PyPI (wheel/tar.gz), and virtually every package manager.</p><p>Modern storage is fast. NVMe SSDs can sustain gigabytes per second. But that speed is only accessible for sequential access to large files. The moment you introduce thousands of small files, syscall overhead dominates.</p><ul><li> Compiling a project with 10,000 source files? The filesystem overhead might exceed the compilation time.</li><li> Millions of small log files? Concatenate them. Claude uses <a href=\"https://jsonlines.org/\">JSONL</a> for this reason.</li><li> This is why rsync and tar exist.</li></ul><h2>What I would do differently</h2><p>If I were optimizing this further:</p><ol><li><strong>Use zstd instead of gzip.</strong> 4-5x faster decompression with similar compression ratios.</li><li><strong>Use uncompressed tar for local caching.</strong> Skip decompression entirely, still get the syscall reduction.</li><li><strong>Parallelize with isolates.</strong> Multiple cores decompressing multiple archives simultaneously.</li></ol><p>I set out to benchmark a lexer and ended up learning about syscall overhead. The lexer was 2x faster. The I/O optimization was 43x faster.</p><h2>Addendum: Reader Suggestions</h2><h3>Linux-Specific Optimizations</h3><p><a href=\"https://www.reddit.com/r/ProgrammingLanguages/comments/1qbvvpn/comment/nzdms8e/\">servermeta_net pointed out</a> two Linux-specific approaches: disabling speculative execution mitigations (which could improve performance in syscall-heavy scenarios) and using io_uring for asynchronous I/O. I ran these benchmarks on macOS, which does not support io_uring, but these Linux capabilities are intriguing. A follow-up post exploring how I/O performance can be optimized on Linux may be in order.</p><p><a href=\"https://news.ycombinator.com/item?id=46755420\">king_geedorah elaborated</a> on how io_uring could help with this specific workload: open the directory file descriptor, extract all filenames via readdir, then submit all openat requests as submission queue entries (SQEs) at once. This batches what would otherwise be 104,000 sequential open() syscalls into a single submission, letting the kernel process them concurrently. The io_uring_prep_openat function prepares these batched open operations. This is closer to the \"load an entire directory into an array of file descriptors\" primitive that this workload really needs.</p><h3>macOS-Specific Optimizations</h3><p><a href=\"https://www.reddit.com/r/ProgrammingLanguages/comments/1qbvvpn/comment/nze8xm7/\">tsanderdev pointed out</a> that macOS's  could potentially improve performance for this workload. While  is not equivalent to Linux's  (it lacks the same syscall batching through a shared ring buffer), it may still offer some improvement over synchronous I/O. I have not benchmarked this yet.</p><p><a href=\"https://news.ycombinator.com/item?id=46753360\">arter45 noted</a> that macOS may be significantly slower than Linux for certain syscalls, linking to a <a href=\"https://stackoverflow.com/questions/67483292/why-is-the-c-function-open-4x-slower-on-macos-vs-an-ubuntu-vm\">Stack Overflow question</a> showing open() being 4x slower on macOS compared to an Ubuntu VM. <a href=\"https://news.ycombinator.com/item?id=46753665\">jchw explained</a> that Linux's VFS layer is aggressively optimized: it uses RCU (Read-Copy-Update) schemes liberally to make filesystem operations minimally contentious, and employs aggressive dentry caching. Linux also separates dentries and generic inodes, whereas BSD/UNIX systems consolidate these into vnode structures. This suggests my benchmark results on macOS may actually understate the syscall overhead problem on that platform relative to Linux, or alternatively, that Linux users might see smaller gains from the tar.gz approach since their baseline is already faster.</p><h3>Is it really the syscalls?</h3><p><a href=\"https://news.ycombinator.com/item?id=46757655\">ori_b pushed back</a> on the claim that syscall overhead is the bottleneck. On a Ryzen machine, entering and exiting the kernel takes about 150 cycles (~50ns). Even at 1 microsecond per mode switch, 300,000 syscalls would account for only 0.3 seconds of the 14.5-second I/O time. That is roughly 2%. The remaining time likely comes from filesystem metadata lookups, inode resolution, directory traversal, and random seek latency. Even NVMe SSDs have ~50-100 microseconds of latency per random read, and 300,000 random reads at that latency would account for most of the measured I/O time. So the framing might be more precisely stated as \"per-file overhead\" rather than \"syscall overhead\" since the expensive part is the work happening inside each syscall, not the context switch itself. It is also worth noting that ori_b's numbers are from a Linux Ryzen machine, where syscalls are faster than on macOS (as discussed above), adding another variable. I do not currently have tooling to break down where the 14.5 seconds actually goes, so this is something I want to investigate in the future.</p><h3>Avoiding lstat with getdents64</h3><p><a href=\"https://news.ycombinator.com/item?id=46755100\">stabbles pointed out</a> that when scanning directories, you can avoid separate lstat() calls by using the  field from . On most popular filesystems (ext4, XFS, Btrfs), the kernel populates this field with the file type directly, so you do not need an additional syscall to determine if an entry is a file or directory. The caveat: some filesystems return , in which case you still need to call lstat(). For my workload of scanning the pub cache, this could eliminate tens of thousands of stat syscalls during the directory traversal phase, before even getting to the file opens.</p><h3>Go Monorepo: 60x Speedup by Avoiding Disk I/O</h3><p><a href=\"https://news.ycombinator.com/item?id=46752237\">ghthor shared</a> a similar experience optimizing dependency graph analysis in a Go monorepo. Initial profiling pointed to GC pressure, but the real bottleneck was I/O from shelling out to , which performed stat calls and disk reads for every file. By replacing  with a custom import parser using Go's standard library and reading file contents from git blobs (using  instead of disk stat calls), they reduced analysis time from 30-45 seconds to 500 milliseconds. This is a 60-90x improvement from the same fundamental insight: avoid per-file syscalls when you can batch or bypass them entirely.</p><p><a href=\"https://news.ycombinator.com/item?id=46756528\">smallstepforman described</a> how <a href=\"https://www.haiku-os.org/\">Haiku OS</a> solves this problem at the operating system level. Haiku packages are single compressed files that are never extracted. Instead, the OS uses <a href=\"https://www.haiku-os.org/docs/develop/packages/Infrastructure.html\">packagefs</a>, a virtual filesystem that presents the contents of all activated packages as a unified directory tree. Applications see normal paths like , but the data is actually read from compressed package files in . Install and uninstall are instant since you are just adding or removing a single file, not extracting or deleting thousands. This eliminates the syscall overhead entirely at the OS level rather than working around it at the application level. Haiku is an open-source OS recreating BeOS, known for its responsiveness and clean design. While not mainstream, its package architecture demonstrates that the \"extract everything to disk\" model most package managers use is not the only option.</p><h3>SquashFS for Container Runtimes</h3><p><a href=\"https://news.ycombinator.com/item?id=46752085\">stabbles suggested</a> SquashFS with zstd compression as another alternative. It is used by various container runtimes and is popular in HPC environments where filesystems often have high latency. SquashFS can be mounted natively on Linux or via FUSE, letting you access files normally while the data stays compressed on disk. When <a href=\"https://news.ycombinator.com/item?id=46753468\">questioned about syscall overhead</a>, stabbles noted that even though syscall counts remain high, latency is reduced because the SquashFS file ensures files are stored close together, benefiting significantly from filesystem cache. This is a different tradeoff than tar.gz: you still pay per-file syscall costs, but you gain file locality and can use standard file APIs without explicit decompression. <a href=\"https://news.ycombinator.com/item?id=46757072\">One commenter warned</a> that when mounting a SquashFS image via a loop device, you should use  to avoid double caching (the compressed backing file and the decompressed contents both being cached), which can <a href=\"https://lwn.net/Articles/654701/\">reduce memory usage significantly</a>.</p><p><a href=\"https://www.reddit.com/r/ProgrammingLanguages/comments/1qbvvpn/comment/nze0jrf/\">tsanderdev mentioned</a> that this is also why SQLite can be much faster than a directory with lots of small files. I had completely forgotten about SQLite as an option. Storing file contents in a SQLite database would eliminate the syscall overhead while providing random access to individual files, something tar.gz does not offer.</p><p>This also explains something I have heard multiple times: Apple uses SQLite extensively for its applications, <a href=\"https://news.ycombinator.com/item?id=26218218\">essentially simulating a filesystem within SQLite databases</a>. If 100,000 files on a modern Mac with NVMe storage takes 14 seconds to read, imagine what it was like on older, slower machines. The syscall overhead would have been even more punishing. Using SQLite instead of the filesystem lets them avoid those syscalls entirely. This is worth exploring.</p><h3>Skip the Cleanup Syscalls</h3><p><a href=\"https://www.reddit.com/r/ProgrammingLanguages/comments/1qbvvpn/comment/nzeaegy/\">matthieum suggested</a> a common trick used by batch compilers: never call , , or , and instead let the OS reap all resources when the process ends. For a one-shot batch process like a compiler (or a lexer benchmark), there is no point in carefully releasing resources that the OS will reclaim anyway.</p><p><a href=\"https://www.reddit.com/r/ProgrammingLanguages/comments/1qbvvpn/comment/nzguiwt/\">GabrielDosReis added a caveat</a>: depending on the workload, you might actually need to call , or you could run out of file descriptors. On macOS, you can check your limits with:</p><div><div><pre><code>$ launchctl limit maxfiles\nmaxfiles    256            unlimited\n\n$ sysctl kern.maxfilesperproc\nkern.maxfilesperproc: 61440\n</code></pre></div></div><p>The first number (256) is the soft limit per process, the second is the hard limit.  shows the kernel's per-process maximum. With 104,000 files, skipping  calls would exhaust even the maximum limit. <a href=\"https://news.ycombinator.com/item?id=46757150\">dinosaurdynasty noted</a> that the low default soft limit is <a href=\"https://0pointer.net/blog/file-descriptor-limits.html\">a historical artifact of the select() syscall</a>, which can only handle file descriptors below 1024. Modern programs can simply raise their soft limit to the hard limit and not worry about it.</p><p>There is even a further optimization: use a wrapper process. The wrapper launches a worker process that does all the work. When the worker signals completion (via stdout or a pipe), the wrapper terminates immediately without waiting for its detached child. Any script waiting on the wrapper can now proceed, while the OS asynchronously reaps the worker's resources in the background. I had not considered this approach before, but it seems worth trying.</p><p><a href=\"https://news.ycombinator.com/item?id=46757879\">Dwedit noted</a> that on Windows, a similar optimization is to call  from a secondary thread, keeping the main thread unblocked while handles are being released.</p><h3>Linker Strategies for Fast Exits</h3><p><a href=\"https://www.reddit.com/r/ProgrammingLanguages/comments/1qbvvpn/comment/nzrrrnk/\">MaskRay added context</a> about how production linkers handle this exact problem. The <a href=\"https://github.com/rui314/mold\">mold linker</a> uses the wrapper process approach mentioned above, forking a child to do all the work while the parent exits immediately after the child signals completion. This lets build systems proceed without waiting for resource cleanup. The  flag disables this behavior for debugging. The <a href=\"https://github.com/nickelpacket/wild\">wild linker</a> follows the same pattern.</p><p><a href=\"https://github.com/llvm/llvm-project/tree/main/lld\">lld</a> takes a different approach with two targeted hacks: <a href=\"https://github.com/llvm/llvm-project/blob/a72958a95dcb7d815c01e20cc819532151d1856d/lld/Common/Filesystem.cpp#L44\">async unlink</a> to remove old output files in a background thread, and <a href=\"https://github.com/llvm/llvm-project/blob/a72958a95dcb7d815c01e20cc819532151d1856d/lld/Common/ErrorHandler.cpp#L108\">calling  instead of </a> to skip the C runtime's cleanup routines (unless  is set for testing).</p><p>MaskRay notes a tradeoff with the wrapper process approach: when the heavy work runs in a child process, the parent process of the linker (typically a build system) cannot accurately track resource usage of the actual work. This matters for build systems that monitor memory consumption or CPU time.</p><h3>Why pub.dev Actually Uses tar.gz</h3><p><a href=\"https://www.reddit.com/r/ProgrammingLanguages/comments/1qbvvpn/comment/nzggloc/\">Bob Nystrom from the Dart team clarified</a> that my speculation about pub.dev's format choice was partially wrong. Fewer HTTP requests and bandwidth savings definitely factored into the decision, as did reduced storage space on the server. Atomicity is important too, though archives do not fully solve the problem since downloads and extracts can still fail. However, it is unlikely that the I/O performance benefits (faster extraction, reduced syscall overhead) were considered: pub extracts archives immediately after download, the extraction benefit only occurs once during , that single extraction is a tiny fraction of a fairly expensive process, and pub never reads the files again except for the pubspec. The performance benefit I measured only applies when repeatedly reading from archives, which is not how pub works.</p><p>This raises an interesting question: what if pub did not extract archives at all? For a clean (non-incremental) compilation of a large project like the Dart Analyzer with hundreds of dependencies, the compiler needs to access thousands of files across many packages. If packages remained in an archive format with random access support (like ZIP), the syscall overhead from opening and closing all those files could potentially be reduced. Instead of thousands of open/read/close syscalls scattered across the filesystem, you would have one open call per package archive, then seeks within each archive. Whether the decompression overhead would outweigh the syscall savings is unclear, but it might be worth exploring for build systems where clean builds of large dependency trees are common.</p><h3>Use dart:io for gzip Instead of package:archive</h3><p><a href=\"https://www.reddit.com/r/ProgrammingLanguages/comments/1qbvvpn/comment/nzi3muf/\">Simon Binder pointed out</a> that dart:io already includes gzip support backed by zlib, so there is no need to use package:archive for decompression. Since dart:io does not support tar archives, I used package:archive for everything and did not think of mixing in dart:io's gzip support separately. Using dart:io's  for decompression while only relying on package:archive for tar extraction could yield better performance. I will try this approach when I attempt to lex a bigger corpus.</p><h3>TAR vs ZIP: Sequential vs Random Access</h3><p><a href=\"https://www.reddit.com/r/ProgrammingLanguages/comments/1qbvvpn/comment/nzih9eh/\">vanderZwan pointed out</a> that ZIP files could provide SQLite-like random access benefits. This highlights a fundamental architectural difference between TAR and ZIP:</p><p> was designed in 1979 for sequential tape drives. Each file's metadata is stored in a header immediately before its contents, with no central index. To find a specific file, you must read through the archive sequentially. When compressed as tar.gz, the entire stream is compressed together, so accessing any file requires decompressing everything before it. The format was standardized by POSIX (POSIX.1-1988 for ustar, POSIX.1-2001 for pax), is well-documented, and preserves Unix file attributes fully.</p><p> was designed in 1989 with a central directory stored at the end of the archive. This directory contains offsets to each file's location, enabling random access: read the central directory once, then seek directly to any file. Each file is compressed individually, so you can decompress just the file you need. This is why JAR files, OpenDocument files, and EPUB files all use the ZIP format internally.</p><table><tbody><tr></tr><tr><td>PKWARE-controlled specification</td></tr><tr></tr><tr><td>External (gzip, zstd, etc.)</td></tr></tbody></table><p>There seems to be no widely-adopted Unix-native format that combines random access with proper Unix metadata support. TAR handles sequential access with full Unix semantics. ZIP handles random access but originated from MS-DOS and has inconsistent Unix permission support. What we lack is something like \"ZIP for Unix\": random access with proper ownership, permissions, extended attributes, and ACLs.</p><p>The closest answer is <a href=\"http://dar.linux.free.fr/\">dar (Disk ARchive)</a>, designed explicitly as a tar replacement with modern features. It stores a catalogue index at the end of the archive for O(1) file extraction, preserves full Unix metadata including extended attributes and ACLs, supports per-file compression with choice of algorithm, and can isolate the catalogue separately for fast browsing without the full archive. However, dar has not achieved the ubiquity of tar or zip.</p><p>For my lexer benchmark, random access would not help since I process all files anyway. But for use cases requiring access to specific files within an archive, this architectural distinction matters.</p><p><a href=\"https://news.ycombinator.com/item?id=46756484\">cb321 pointed out</a> that there is a middle ground between uncompressed archives (random access but large) and fully compressed streams (small but sequential). Standard gzip compresses everything into a single block, so accessing any byte requires decompressing from the beginning. <a href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\">BGZF</a> (Blocked GNU Zip Format), developed by genomics researchers for tools like <a href=\"http://www.htslib.org/\">samtools</a>, compresses data in independent 64KB blocks. Each block is a valid gzip stream, so the file remains compatible with standard gunzip, but with an index you can seek directly to any block and decompress just that portion. This allows random access to multi-gigabyte genome files without decompressing terabytes of data. <a href=\"https://github.com/facebook/zstd/blob/dev/contrib/seekable_format/zstd_seekable_compression_format.md\">Zstd offers a similar seekable format</a> with better compression ratios and faster decompression. For tar archives, combining block-based compression with an external file offset index could provide random access to individual files while still benefiting from compression.</p><h3>RE2C: A Faster Approach to Lexer Generation</h3><p><a href=\"https://news.ycombinator.com/item?id=46754081\">rurban mentioned</a> that <a href=\"https://re2c.org/\">RE2C</a> generates lexers that are roughly 10x faster than flex. The key difference is architectural: while flex generates table-driven lexers that look up transitions in arrays at runtime, RE2C generates direct-coded lexers where the finite automaton is encoded directly as conditional jumps and comparisons. This eliminates table lookup overhead and produces code that is both faster and easier for CPU branch predictors to handle.</p><p>RE2C also supports <a href=\"https://re2c.org/manual/manual_c.html\">computed gotos</a> (via the  flag), a GCC/Clang extension that compiles switch statements into indirect jumps through a label address table. For lexers with many states, this can significantly reduce branch mispredictions. Other optimizations include DFA minimization and tunnel automaton construction.</p><p>My ARM64 assembly lexer currently uses a table-driven approach, so exploring direct-coded generation is an interesting avenue. Another option is profile-guided optimization: compiling the lexer to LLVM IR and using PGO to optimize hot paths based on real Dart code patterns, something I mentioned as a future direction in my <a href=\"https://modulovalue.com/blog/benchmarking-against-llvm-parser/\">LLVM parser benchmarking post</a>. Part of my lexer's speed advantage over the official Dart scanner likely comes from simplicity: my lexer is pure, maintaining only a stack for lexer states across multiple finite automata, while the Dart scanner must construct a linked list of tokens, handle error recovery, and manage additional bookkeeping. Isolating how much of the performance difference comes from architecture versus feature set is something I want to investigate further.</p><p><a href=\"https://www.reddit.com/r/programming/comments/1qmznm8/comment/o1q0jzb/\">fun__friday pointed out</a> that the main takeaway is to measure before you start optimizing something, referencing <a href=\"https://en.wikipedia.org/wiki/Amdahl%27s_law\">Amdahl's law</a>. This is a fair point, and this blog post is a textbook illustration of it: when lexing accounts for only ~17% of total execution time, even a 2x improvement in lexing yields only a 1.22x overall speedup. The theoretical maximum speedup from improving just the lexing component is bounded by the fraction of time spent on everything else. Measure first, optimize second.</p><p>That said, from a \"business\" standpoint it makes sense to focus on the largest bottlenecks (by following, e.g., the <a href=\"https://en.wikipedia.org/wiki/Critical_path_method\">Critical path method</a>) and those parts that take up the most time. However, software can be reused, and making a single component faster can have significant benefits to other consumers of that component. A faster lexer benefits not just this benchmark but every tool that uses it: formatters, linters, analyzers, compilers. I think our software community thrives in part because we don't strictly follow the common sense that business optimization dictates.</p>",
      "contentLength": 22783,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qmznm8/i_built_a_2x_faster_lexer_then_discovered_io_was/"
    },
    {
      "title": "Linux 6.19-rc7 Released With Kernel Continuity Plan, A Few Important Fixes",
      "url": "https://www.phoronix.com/news/Linux-6.19-rc7-Released",
      "date": 1769382075,
      "author": "/u/somerandomxander",
      "guid": 38893,
      "unread": true,
      "content": "\nThe <a href=\"https://www.phoronix.com/search/Linux+6.19\">Linux 6.19</a> kernel remains on track for its official release two weeks from today, with the extra RC being baked in due to the end of year holidays. Out today is Linux 6.19-rc7 with a few changes worth highlighting for the week.\n<p>With Linux 6.19-rc7 there is the newly-merged </p><a href=\"https://www.phoronix.com/news/Linux-Kernel-Continuity-Doc\">continuity planning for the Linux kernel development</a> should Linus Torvalds' official upstream Git kernel repository ever become inaccessible or other unforeseen circumstances arise. An important fix/revert for the week is <a href=\"https://www.phoronix.com/news/AMDGPU-Linux-6.19-Regressions\">an AMDGPU revert to address various issues that have been reported on Linux 6.19</a> going back to its merge window. \n<p>Another notable fix for the week is </p><a href=\"https://www.phoronix.com/news/Linux-6.19-Disabling-Next-Buddy\">disabling the NEXT_BUDDY scheduler functionality for Linux 6.19</a> as it  was found to cause some performance regressions. I'll have up some interesting NEXT_BUDDY comparison benchmarks on Linux 6.19 Git tomorrow that I have been working on over the weekend.\n<a href=\"https://www.phoronix.com/news/Linux-6.19-Page-Fault-Code-Fix\">fix for Linux's \"subtly wrong\" page fault handling code the pasr 5 years</a> as another prominent fix for the week.\n<p>Yet another fix worth calling out in Linux 6.19-rc7 is </p><a href=\"https://www.phoronix.com/news/Linux-6.19-ATA-Power-Management\">ATA fix for a power management regression the past year</a> of the Linux kernel for some ATAPI devices and in turn preventing the CPU from reaching low-power C-states.\n<p>Also of note for the week are </p><a href=\"https://www.phoronix.com/news/ASUS-Armoury-More-Hardware\">more ASUS laptops being supported by the ASUS Armoury driver</a> that was merged via the x86 platform driver subsystem at the start of the Linux 6.19 cycle.\nLinus Torvalds wrote in today's <a href=\"https://lore.kernel.org/lkml/CAHk-=wjNrcnHNgDehAZ_HLYh-N3PHkOS1NO=yye12xmAGFL+mg@mail.gmail.com/\">6.19-rc7 announcement</a>:\n<blockquote>\"So normally this would be the last rc of the release, but as I've mentioned every rc (because I really want people to be aware and be able to plan for things) this release we'll have an rc8 due to the holiday season.\n<p>And while some of the early rc's were smaller than usual and it didn't seem necessary, right now I'm quite happy I made that call. Not because there's anything particularly scary here - the release seems to be going fairly smoothly - but because this rc7 really is larger than things normally are and should be at this point.\n</p><p>Now, it's not *hugely* larger than normal, so it's not something that makes me worry, but it's just large enough that it makes me go \"good that we have an extra week\".\n</p><p>Anyway, it all looks otherwise very normal. A bit over half is drivers (networking and gpu being most of it as usual, but there's a bit of everything in there), and the rest is the usual random mix: tooling, architecture fixes, VM, networking, rust driver base fixes, documentation, some filesystem work...\n</p><p>So we have two more weeks to go, and apart from the different timing, nothing looks particularly odd or worrisome.\"</p></blockquote>With the extra RC, Linux 6.19 stable should be out in two weeks on 8 February. See the <a href=\"https://www.phoronix.com/review/linux-619-features-changes\">Linux 6.19 feature overview</a> for a look at all the interesting changes in this next Linux kernel version.",
      "contentLength": 2801,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qmyid1/linux_619rc7_released_with_kernel_continuity_plan/"
    },
    {
      "title": "Replaced By",
      "url": "https://www.reddit.com/r/artificial/comments/1qmxxen/replaced_by/",
      "date": 1769380729,
      "author": "/u/stefantigro",
      "guid": 38920,
      "unread": true,
      "content": "<p>I wanted to share a project I've been working on called <a href=\"https://replacedby.net\"></a>. It's a simple site with a straightforward goal: to track the stories of people who have been replaced by AI, automation, or robots. The idea isn't to hate on AI (I don't!), but to create a space to talk about the human side of this big technological shift.</p><p>If you've been impacted, please come share your story. I've kept things simple... There's no user authentication, just some basic rate limiting and cloudflare to prevent spam. All posts are manually approved to keep the content respectful and on-topic. After enough posts are submitted, you will be able to see a very simple post carousel (that will be expanded on in the future).</p><p>The entire project is open source. You can find the source code on <a href=\"https://github.com/Michaelpalacce/ReplacedBy\">GitHub</a>. I'm not a designer, so a lot of the UI is AI-assisted (I hooked up the components, made them reactive, then AI placed it nicely... even tho honestly it kept messing up, but whatver). You can also find the AI disclosure in the repo's README.</p><p>There is a bit of data pre-seeded, a sort of best-effort research on my end and based on articles that wre concrete in who and how was impacted. The list is by no means complete, so if you feel strongly about a mass layoff that happened, do open an issue and I will add it.</p><p>There's a <a href=\"https://github.com/Michaelpalacce/ReplacedBy?tab=readme-ov-file#Roadmap\">roadmap</a> in the repo if you're curious about what's next.</p><p>I plan to do monthly posts with how the site has grown and the data collected.</p><p>Let me know what you think!</p>",
      "contentLength": 1453,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Failing Fast: Why Quick Failures Beat Slow Deaths",
      "url": "https://lukasniessen.medium.com/failing-fast-why-quick-failures-beat-slow-deaths-ffaa491fa510",
      "date": 1769379849,
      "author": "/u/trolleid",
      "guid": 38883,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qmxjft/failing_fast_why_quick_failures_beat_slow_deaths/"
    },
    {
      "title": "Feeling of Go",
      "url": "https://www.reddit.com/r/golang/comments/1qmwna7/feeling_of_go/",
      "date": 1769377877,
      "author": "/u/Waste-Present-4670",
      "guid": 38862,
      "unread": true,
      "content": "<p>I am trying to write a TCP client-server program and this is my function to handle connection.</p><p>What I am trying to achieve? - Read and Write from and to connection async, if one fails both should exit</p><p>Why the POST? - I want to understand if this much of code is really required for such a simple task<p> - Need an opinion on how to identify it's less or more^</p></p><pre><code>func HandleConnection(ctx context.Context, conn net.Conn) error { defer conn.Close() childContext, cancel := context.WithCancel(ctx) defer cancel() wg := sync.WaitGroup{} errChan := make(chan error, 2) wg.Add(2) //reader go func() { defer wg.Done() if err := readBytes(childContext, conn); err != nil { cancel() errChan &lt;- err } }() //writer go func() { defer wg.Done() if err := writeBytes(childContext, conn); err != nil { cancel() errChan &lt;- err } }() wg.Wait() //catch error select { case err := &lt;-errChan: return err default: return nil } }` </code></pre>",
      "contentLength": 901,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "GO & Gitlab : Private Packages / Library",
      "url": "https://www.reddit.com/r/golang/comments/1qmvega/go_gitlab_private_packages_library/",
      "date": 1769375114,
      "author": "/u/optimus_prime955",
      "guid": 38861,
      "unread": true,
      "content": "<p>Hello Community, Im building , custom go package , that will help me write less code across my projects .we use Gitlab (For companys) and every action, require password , </p><p>So when i push Code in Gitlab , Tag it with Version , and when i want to import it from go project , i cant get the pkg, i got 403 </p>",
      "contentLength": 302,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Latest ChatGPT model uses Elon Musk’s Grokipedia as source, tests reveal",
      "url": "https://www.theguardian.com/technology/2026/jan/24/latest-chatgpt-model-uses-elon-musks-grokipedia-as-source-tests-reveal",
      "date": 1769374608,
      "author": "/u/Practical_Chef_7897",
      "guid": 38863,
      "unread": true,
      "content": "<p>The latest model of ChatGPT has begun to cite Elon Musk’s Grokipedia as a source on a wide range of queries, including on Iranian conglomerates and Holocaust deniers, raising concerns about misinformation on the platform.</p><p>In tests done by the Guardian, GPT-5.2 cited Grokipedia nine times in response to more than a dozen different questions. These included queries on political structures in Iran, such as salaries of the Basij paramilitary force and the ownership of the Mostazafan Foundation, and questions on the biography of Sir Richard Evans, a British historian and expert witness against Holocaust denier David Irving in his libel trial.</p><p>Grokipedia, launched in <a href=\"https://www.theguardian.com/technology/2025/oct/28/elon-musk-grokipedia\" data-link-name=\"in body link\">October</a>, is an AI-generated online encyclopedia that aims to compete with Wikipedia, and which has been criticised for propagating rightwing narratives on topics including <a href=\"https://archive.ph/ag2Sh\" data-link-name=\"in body link\">gay marriage</a> and the 6 January insurrection in the US. Unlike Wikipedia, it does not allow direct human editing, instead an AI model writes content and responds to requests for changes.</p><p>ChatGPT did not cite Grokipedia when prompted directly to repeat misinformation about the insurrection, about media bias against Donald Trump, or about the HIV/Aids epidemic – areas where Grokipedia has been widely reported to promote falsehoods. Instead, Grokipedia’s information filtered into the model’s responses when it was prompted about more obscure topics.</p><p>For instance, ChatGPT, citing Grokipedia, repeated stronger claims about the Iranian government’s links to MTN-Irancell than are found on Wikipedia – such as asserting that the company has links to the office of Iran’s supreme leader.</p><p>ChatGPT also cited Grokipedia when repeating information that the Guardian has debunked, namely details about Sir Richard Evans’ <a href=\"https://www.theguardian.com/technology/2025/nov/03/grokipedia-academics-assess-elon-musk-ai-powered-encyclopedia\" data-link-name=\"in body link\">work</a> as an expert witness in David Irving’s trial.</p><p>GPT-5.2 is not the only large language model (LLM) that appears to be citing Grokipedia; anecdotally, Anthropic’s Claude has also referenced Musk’s encyclopedia on topics from petroleum <a href=\"https://x.com/AshitaOrbis/status/1994132818646192199\" data-link-name=\"in body link\">production</a> to Scottish <a href=\"https://www.reddit.com/r/ClaudeAI/comments/1pbdelb/claude_uses_grokipedia/\" data-link-name=\"in body link\">ales</a>.</p><p>An <a href=\"https://www.theguardian.com/technology/openai\" data-link-name=\"in body link\" data-component=\"auto-linked-tag\">OpenAI</a> spokesperson said the model’s web search “aims to draw from a broad range of publicly available sources and viewpoints”.</p><p>“We apply safety filters to reduce the risk of surfacing links associated with high-severity harms, and ChatGPT clearly shows which sources informed a response through citations,” they said, adding that they had ongoing programs to filter out low-credibility information and influence campaigns.</p><p>Anthropic did not respond to a request for comment.</p><p>But the fact that Grokipedia’s information is filtering – at times very subtly – into LLM responses is a concern for disinformation researchers. Last spring, security experts <a href=\"https://archive.ph/kXp4i\" data-link-name=\"in body link\">raised</a> concerns that malign actors, including Russian propaganda networks, were churning out massive volumes of disinformation in an effort to seed AI models with lies, a <a href=\"https://www.theguardian.com/world/2025/nov/21/english-language-websites-link-pro-kremlin-russian-propaganda-pravda-network\" data-link-name=\"in body link\">process</a> called “LLM grooming”.</p><p>In June, concerns were raised in the US Congress that Google’s Gemini repeated the Chinese government’s position on human rights abuses in Xinjiang and China’s Covid-19 policies.</p><p>Nina Jankowicz, a disinformation researcher who has worked on LLM grooming, said ChatGPT’s citing Grokipedia raised similar concerns. While Musk may not have intended to influence LLMs, Grokipedia entries she and colleagues had reviewed were “relying on sources that are untrustworthy at best, poorly sourced and deliberate disinformation at worst”, she said.</p><p>And the fact that LLMs cite sources such as Grokipedia or the Pravda network may, in turn, improve these sources’ credibility in the eyes of readers. “They might say, ‘oh, ChatGPT is citing it, these models are citing it, it must be a decent source, surely they’ve vetted it’ – and they might go there and look for news about Ukraine,” said Jankowicz.</p><p>Bad information, once it has filtered into an AI chatbot, can be challenging to remove. Jankowicz recently found that a large news outlet had included a <a href=\"https://www.thewayfinder.net/p/i-got-trapped-in-the-ai-ouroboros\" data-link-name=\"in body link\">made-up quote</a> from her in a story about disinformation. She wrote to the news outlet asking for the quote to be removed, and <a href=\"https://bsky.app/profile/ninajankowicz.com/post/3lulq7ovmts25\" data-link-name=\"in body link\">posted</a> about the incident on social media.</p><p>The news outlet removed the quote. However, AI models for some time continued to cite it as hers. “Most people won’t do the work necessary to figure out where the truth actually lies,” she said.</p><p>When asked for comment, a spokesperson for xAI, the owner of Grokipedia, said: “Legacy media lies.”</p><figure data-spacefinder-role=\"inline\" data-spacefinder-type=\"model.dotcomrendering.pageElements.GuideAtomBlockElement\"></figure>",
      "contentLength": 4447,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qmv61y/latest_chatgpt_model_uses_elon_musks_grokipedia/"
    },
    {
      "title": "Things I miss in Rust",
      "url": "https://www.reddit.com/r/rust/comments/1qmu3m6/things_i_miss_in_rust/",
      "date": 1769372246,
      "author": "/u/OneWilling1",
      "guid": 38955,
      "unread": true,
      "content": "<p>Since most of my previous work was in C++ and C#, I sometimes catch myself missing certain OO features, especially:</p><ul><li>inheritance ( 😁)</li></ul><p>One thing that comes up a lot for me is constructors. I’d love to be able to define multiple  functions with different parameters, something like:</p><pre><code>pub fn new(...) pub fn new(..., extra_property: T) </code></pre><p>Right now this usually turns into patterns like  +  etc., which work but feel a bit more verbose.</p><p>Is there a fundamental reason why function overloading isn’t possible (or desirable) in Rust? Is it mostly a design philosophy or are there technical constraints? And is this something that’s ever been seriously considered for the language, or is it firmly off the table?</p><p>Curious to hear how others think about this, especially folks who came from C++/C# as well.</p><p>EDIT: Conclusion: Builders it is.<p> P.S. Thanks everyone for the insight!</p></p>",
      "contentLength": 867,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Wha is the best way to implement a readiness/liveness gate for a Kafka consumer application running in k8s?",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qmu2jb/wha_is_the_best_way_to_implement_a/",
      "date": 1769372180,
      "author": "/u/Impressive_Issue3791",
      "guid": 38852,
      "unread": true,
      "content": "<p>We have been using a rest api endpoint in our application as a Kafka consumer application. Recently i have some thought about this and realized it doesn’t make sense to measure the health of a message application using a rest API end point. </p><ol><li><p>Consumers starts processing messages before readiness gate pass</p></li><li><p>We had an incident application was reporting healthy but the consumer thread was blocked.</p></li></ol><p>What is the best way to handle this situation ? </p>",
      "contentLength": 443,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "PDF Made Easy: Gopdfsuit v4.0.0 is Here (Digital Signatures, PDF/A-4, and More!)",
      "url": "https://chinmay-sawant.github.io/gopdfsuit",
      "date": 1769371897,
      "author": "/u/chinmay06",
      "guid": 38851,
      "unread": true,
      "content": "<p>We just dropped , and it’s a massive leap forward. Here is the high-level breakdown of what’s new:</p><ul><li> Smaller PDF sizes by only including used characters.</li><li> Full PKCS#7 support for secure, professional documents.</li><li> Added Bookmarks, Outlines, and internal/external Hyperlinks.</li><li> Added PDF Splitting (with ZIP export) and enhanced text rendering.</li></ul><ul><li> A complete website revamp with a modular, faster React-based editor.</li><li> Drag-and-drop elements, blue-outline cell selection, and color presets.</li><li> Simplified Google Auth flow and \"Copied\" visual feedback.</li></ul><ul><li> Integrated XMP metadata and sRGB profiles for  compliance.</li><li> Implemented  support (Structure Trees) for screen readers.</li><li> Resolved color rendering discrepancies between Acrobat and Chrome.</li></ul><ul><li> Now running on App Engine for better performance and scaling.</li><li> Hardened GitHub Actions for seamless deployments.</li><li> New sample data (Legal, Financial, Books) and a  example.</li></ul><p><a href=\"https://github.com/chinmay-sawant/gopdfsuit\">Star the repo on GitHub</a> to help keep the momentum going! It really help us keep working on the project &lt;3</p>",
      "contentLength": 998,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qmtxt3/pdf_made_easy_gopdfsuit_v400_is_here_digital/"
    },
    {
      "title": "Someone created Got for Minecraft",
      "url": "https://youtu.be/ZdM-iNpv3nU?si=vc9BfDHU0MNE310y",
      "date": 1769370106,
      "author": "/u/Snowy32",
      "guid": 38849,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qmt3l1/someone_created_got_for_minecraft/"
    },
    {
      "title": "Sign and attest your manifests",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qmt1ui/sign_and_attest_your_manifests/",
      "date": 1769370003,
      "author": "/u/aliasxneo",
      "guid": 38844,
      "unread": true,
      "content": "<p>I recently developed <a href=\"https://github.com/meigma/blob\">Blob</a>, which allows you to push/pull arbitrary files to an OCI registry (including support for partial pulls). It's intended to be used with Sigstore signing and SLSA attestations out of the box (including support for validating policies before pulling files). </p><p>I wanted to experiment how this could be used to sign and attest k8s manifests the same way we do our images. So I created <a href=\"https://github.com/meigma/blob-argo-cmp\">blob-argo-cmp</a> which combines Blob with an Argo CD CMP to validate and pull manifests. Meaning, not only can you use something like Kyverno to enforce image signing/attestation, but you can also enforce the same policies against your manifests. </p><p>This is obviously experimental at this point, but you can see a <a href=\"https://github.com/meigma/blob-argo-cmp/blob/master/.github/workflows/integration.yml\">full example</a> that uses KinD and includes both positive/negative verifications. </p>",
      "contentLength": 791,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "For those using (or avoiding) Crossplane — what’s missing or overkill?",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qmspvc/for_those_using_or_avoiding_crossplane_whats/",
      "date": 1769369272,
      "author": "/u/PhilosopherHead1388",
      "guid": 38845,
      "unread": true,
      "content": "<div><p>I’ve built multiple control planes using  and Kubernetes-style reconcilers.</p><ul><li>Where does Crossplane shine for you?</li><li>Where does it feel too complex or not worth it?</li><li>What problems did you  a control plane for but didn’t build one?</li></ul><p>I’m exploring a startup idea and want to understand , not theoretical ones.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/PhilosopherHead1388\"> /u/PhilosopherHead1388 </a>",
      "contentLength": 345,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Go + HTMX + templ page not rendering (server runs but browser keeps loading)",
      "url": "https://www.reddit.com/r/golang/comments/1qmsnc3/go_htmx_templ_page_not_rendering_server_runs_but/",
      "date": 1769369118,
      "author": "/u/DifficultNews5991",
      "guid": 38843,
      "unread": true,
      "content": "<p>I’m facing a problem and it’s giving me a real headache.</p><p>I’m building a web app using Go, HTML, CSS, HTMX, and .templ.</p><p>I’m intentionally avoiding JavaScript.</p><p>The issue is: when I run my main.go, the server starts, but when I open the browser, the page never loads / nothing is rendered. It just keeps loading.</p><p>I’ve tried many things, but none of them worked. Has anyone faced this issue before?</p><p>Any idea what could be wrong or how to debug this properly?</p>",
      "contentLength": 459,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Concurrency in Go Book",
      "url": "https://www.reddit.com/r/golang/comments/1qmqteu/concurrency_in_go_book/",
      "date": 1769365218,
      "author": "/u/i-am-gopher",
      "guid": 38835,
      "unread": true,
      "content": "<p>Concurrency in Go: Tools and Techniques for Developers</p><p>Book by Katherine Cox-Buday</p><p>anyone who read it?? Worth reading? </p>",
      "contentLength": 117,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Stabilizing the `if let guard` feature",
      "url": "https://www.reddit.com/r/rust/comments/1qmqpbz/stabilizing_the_if_let_guard_feature/",
      "date": 1769364970,
      "author": "/u/Kivooeo1",
      "guid": 38842,
      "unread": true,
      "content": "<p>I've written a blog post about the  feature I've been working on stabilizing. It covers:</p><ul><li>What it is and why it's useful</li><li>Its history and interactions</li><li>The drop-order bugs we found</li></ul><p>(And for those who've been following my journey - there's a small note at the end about the next big step in my life </p><p>I also want to say a huge thank you here. Thank you for the support, and a special thanks to those who got genuinely interested, reached out, asked questions, and even started contributing themselves. Seeing that is the best part</p><p>Also, I want to check with you: would there be interest in a future, very detailed post about how to start contributing? I'm thinking of taking a random issue and walking through the entire process: how I think, where I get stuck, where I look for answers, and how I finally fix it — with all the messy details</p>",
      "contentLength": 832,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Wine-Staging 11.1 Adds Patches For Enabling Recent Adobe Photoshop Versions On Linux",
      "url": "https://www.phoronix.com/news/Wine-Staging-11.1",
      "date": 1769362940,
      "author": "/u/TheTwelveYearOld",
      "guid": 38815,
      "unread": true,
      "content": "<p>Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via <a href=\"https://twitter.com/MichaelLarabel\">Twitter</a>, <a href=\"https://www.linkedin.com/in/michaellarabel/\">LinkedIn</a>, or contacted via <a href=\"https://www.michaellarabel.com/\">MichaelLarabel.com</a>.</p>",
      "contentLength": 500,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qmpr5d/winestaging_111_adds_patches_for_enabling_recent/"
    },
    {
      "title": "From 10 Day Vacation Project to 100k Users: auto‑cpufreq v3 Story",
      "url": "https://foolcontrol.org/?p=5114",
      "date": 1769362301,
      "author": "/u/ahodzic",
      "guid": 38949,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qmpgmi/from_10_day_vacation_project_to_100k_users/"
    },
    {
      "title": "Any resources to learn about backend principles",
      "url": "https://www.reddit.com/r/golang/comments/1qmoz92/any_resources_to_learn_about_backend_principles/",
      "date": 1769361253,
      "author": "/u/PsychologicalYam7192",
      "guid": 38818,
      "unread": true,
      "content": "<p>I started learning go today for backend and i quickly realised that i need some backend knowledge to keep going like (routing middleware Auth) similar thing so suggest me some good resources to learn about these<p> i want to make some good projects and please suggest me some good beginner friendly projects </p></p>",
      "contentLength": 305,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Helm/Terraform users: What's your biggest frustration with configs and templating in K8s?",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qmoeiv/helmterraform_users_whats_your_biggest/",
      "date": 1769360010,
      "author": "/u/Kalin-Does-Code",
      "guid": 38820,
      "unread": true,
      "content": "<p>Im a Scala dev who primarily focuses on backend development, but begrudgingly gets dragged into that scary scary helmfile directory way more often than Id like... My company has a quite complex environment/subenvironment structure, and it makes managing configs a living nightmare. Thats before you even get to the complex domain specific helm chart that only the devops team truly understands, and stringly typed gotmpls that need to pipe nested configs through flat env vars. If I have to pipe a yaml into a gotmpl into an application.conf into my actual config class one more time, I might lose my mind, not to mention that literally every step of that process is untyped and can break without warning.</p><p>What are yalls biggest pain points in this area? Are all these pain points Im having a solved problem and my company just isnt using the right tools, or is there a real gap that we are all just putting up with because \"it works\"?</p><p>This whole thing has given me an idea for a solution that I think makes the whole process way easier, inverts control so the tool can do the core logic, and passes off to your programming language of choice so that your configs can be strongly typed. If it compiles, it runs. Ive got some initial POCs working, but wanted to get some feedback from the community on whether this is really an area that needs improvement, or if my company is just behind the times.</p>",
      "contentLength": 1396,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "C++ RAII guard to detect heap allocations in scopes",
      "url": "https://github.com/mkslge/noalloc-cpp",
      "date": 1769359379,
      "author": "/u/North_Chocolate7370",
      "guid": 38833,
      "unread": true,
      "content": "<p>Needed a lightweight way to catch heap allocations in cpp, couldn’t find anything simple, so I built this. Sharing in case it helps anyone</p>",
      "contentLength": 140,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qmo44v/c_raii_guard_to_detect_heap_allocations_in_scopes/"
    },
    {
      "title": "Looking for open source projects to contribute to",
      "url": "https://www.reddit.com/r/golang/comments/1qmnx3y/looking_for_open_source_projects_to_contribute_to/",
      "date": 1769358958,
      "author": "/u/Soft_Carpenter7444",
      "guid": 38819,
      "unread": true,
      "content": "<div><p>Starting my open source journey </p><p>Looking for first project that I can enhance </p></div>   submitted by   <a href=\"https://www.reddit.com/user/Soft_Carpenter7444\"> /u/Soft_Carpenter7444 </a>",
      "contentLength": 118,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "I built a small tool to help me keep up with cloud native releases without living in GitHub",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qmnreu/i_built_a_small_tool_to_help_me_keep_up_with/",
      "date": 1769358616,
      "author": "/u/pixelrobots",
      "guid": 38821,
      "unread": true,
      "content": "<p>For a long time I’ve had the same problem with cloud native tooling.</p><p>Between Kubernetes, CNCF projects, and everything built around them, keeping up with releases has been painful. I’ve spent years skimming GitHub releases and changelogs, and I still miss breaking changes or security fixes until they bite me during an upgrade.</p><p>A few weeks ago I finally decided to stop complaining about it and try to build a better workflow for myself.</p><p>I built a small tool that watches a set of CNCF and open source projects, pulls new releases, and uses AI to summarise long changelogs. It highlights breaking changes and flags CVEs when they appear in the notes. You can choose how you want updates delivered: per release, daily digest, or weekly summary, via email or webhooks.</p><p>As part of the project I also wrote my first MCP server so I could query release data from my own tools and from AI assistants. That part was mostly an excuse to learn how MCP works in practice.</p><p>It’s still early and very much something I built for my own use, but it’s already helped me plan upgrades more safely.</p><p>If anyone here deals with a lot of cluster and tooling upgrades, I’d genuinely appreciate feedback on:</p><p>How you currently track releases</p><p>What you find most painful about the process</p><p>What would actually make this kind of tool useful</p><p>Happy to take criticism, feature ideas, or “this is pointless because…” feedback.</p>",
      "contentLength": 1400,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "After my first months of being a Linux user: I love Linux... But I only have problems with KDE Plasma",
      "url": "https://www.reddit.com/r/linux/comments/1qmnqf1/after_my_first_months_of_being_a_linux_user_i/",
      "date": 1769358558,
      "author": "/u/TheGoodSatan666",
      "guid": 38805,
      "unread": true,
      "content": "<p>I know this will trigger some hardcore KDE Plasma users, so first I want to say that I don't hate KDE Plasma. I love how KDE Plasma works and I like using it. But that doesn't change the fact that I only really had issues with it. If You know fixes for the issues I named then let Me know</p><p>I've been using Linux (Fedora 43 to be exact) for some months now. I decided to use it with KDE Plasma as that was what People recommended when coming from Windows (And gnome just wasn't something for Me).</p><p>I really started to love Linux. I even removed my Windows install as I really don't have a need for it anymore and I exclusively do everything on Linux now. I also started ricing a bit, I first used Konsole but later switched to Kitty and ZSH, built my own config etc etc...</p><p>But... Every single bug and issue I encountered in this time was  an issue related to Kwin/Plasma.</p><p>Here is a list of all the bugs I encountered:</p><p>-Sleep does simply not work for Me. Once my main screen went into sleep it  manages to wake up again until I disconnect the power cable and plug it back in</p><p>-Everytime I want to move a focused window on my main screen to a second virtual desktop it'll move a completely unrelated window that isn't in focus on my second screen onto the second virtual desktop instead. And when I try to move a window from my second screen to my second virtual desktop it'll move one from my main screen instead.</p><p>-When I set the Window opening position to \"center\" or \"smart\" in KDE Plasma. It'll completely ignore those settings and always place them somewhere else. When I put it on \"center\" it'll place it on the bottom right of my screens, if I put it to \"smart\" it'll place it somewhere randomly.</p><p>-When I turn my second screen off, it'll crash the Plasma shell. If i turn my second screen back on, it'll crash Plasma shell as well.</p><p>-The widget configuration menu is completely bugged out, it's insanely laggy, often ignores input and honestly has so many issues that I can't even remember all of them</p><p>-When I place widgets on my desktop and reboot, they'll have a slightly different size than when I originally placed them</p><p>etc. These aren't even all the issues I had with it</p><p>And just to be clear. I use recent hardware, I don't use anything exotic and I do have all the drivers that I need. I also tested it with different screens and I rebuild many binaries to see if maybe something was currupted. BUT. Even after I reinstalled the entire system on seperate drives I had the exact same issues as before. My screens also don't use any specific drivers, firmware or software. So I can assure You that my screens aren't the problem here. My GPU is an MSI RTX 4070 Super which normally runs pretty great on Linux (Atleast for Nvidia cards)</p><p>I also want to say that every single other thing works absolutely perfect. My terminal has no issues, my programs run perfectly and generelly everything is perfect except for things related to Kwin/Plasma</p><p>Idk if it's because I'm running an Nvidia card (I am using the newest rpmfusion drivers), idk if it's some weird wayland issue. And idk if it's because my main screen is an ultrawide display. Whatever it is, it's driving Me insane.</p><p>So... I know I'll get tons of \"I never had these issues\" comments. And that doesn't surprise Me. Idk if there is just some weird demon that wants to ruin my KDE Plasma experience or something. But for now the only other option I see is to switch to something else</p><p>So. What would You recommend to Me? I need something customizable, beautiful, preferebly using Wayland that works great on Fedora 43 and with Nvidia cards. It should also not be GNOME please. It doesn't really need to be a traditional desktop environment.</p>",
      "contentLength": 3679,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "[Media] Pixel retro quiz website for refreshing key Rust concepts",
      "url": "https://www.reddit.com/r/rust/comments/1qmnawg/media_pixel_retro_quiz_website_for_refreshing_key/",
      "date": 1769357628,
      "author": "/u/capitanturkiye",
      "guid": 38860,
      "unread": true,
      "content": "<p>I built a small Rust quiz platform over the weekend to refresh my knowledge of core Rust concepts and turned it into a pixel retro website called Cratery. It is still early but the idea is a quest based quiz where you go through different realms focused on things like ownership lifetimes traits and concurrency, answer questions and track your progress. I'm pretty much inspired by classic pixel UIs. Right now it has questions from various topics and progress is saved locally. I mainly want feedback at this stage on question difficulty clarity and overall vibe since I plan to keep improving it over time.</p>",
      "contentLength": 609,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "BBC reports that Chinese open models continue to steadily muscle out closed offering from US companies",
      "url": "https://www.bbc.com/news/articles/c86v52gv726o",
      "date": 1769354824,
      "author": "/u/fattyfoods",
      "guid": 38789,
      "unread": true,
      "content": "<div data-component=\"text-block\"><p>Every month, hundreds of millions of users flock to Pinterest looking for the latest styles.</p><p>One page titled \"<a target=\"_blank\" href=\"https://business.pinterest.com/audience/\">the most ridiculous things</a>\" is filled with plenty of wacky ideas to inspire creatives. Crocs repurposed as flower pots. Cheeseburger-shaped eyeshadow. A gingerbread house made of vegetables.</p><p>But what would-be buyers may not know is the tech behind this isn't necessarily US-made. Pinterest is experimenting with Chinese AI models to hone its recommendation engine.</p><p>\"We've effectively made Pinterest an AI-powered shopping assistant,\" the firm's boss Bill Ready told me.</p><p>Of course, the San Francisco-based tastemaker could use any number of American AI labs to power things behind-the-scenes.</p><p>But since the launch of China's DeepSeek R-1 model in January 2025, Chinese AI tech has increasingly been a part of Pinterest.</p><p>Ready calls the so-called \"DeepSeek moment\" a breakthrough.</p><p>\"They chose to open source it, and that sparked a wave of open source models,\" he said.</p><p>Chinese competitors include Alibaba's Qwen and Moonshot's Kimi, while TikTok owner ByteDance is also working on similar technology.</p><p>Pinterest Chief Technology Officer Matt Madrigal said the strength of these models is that they can be freely downloaded and customised by companies like his - which is not the case with the majority of models offered by US rivals like OpenAI, which makes ChatGPT.</p><p>\"Open source techniques that we use to train our own in-house models are 30% more accurate than the leading off-the-shelf models,\" Madrigal said.</p><p>And those improved recommendations come at a much lower cost, he said, sometimes ninety percent less than using the proprietary models favoured by US AI developers.</p></div><div data-component=\"text-block\"><p>Pinterest is hardly the only US enterprise depending on AI tech from China.</p><p>These models are gaining traction across an array of Fortune 500 companies.</p><p>Airbnb boss Brian Chesky told Bloomberg in October his company relied \"a lot\" on Alibaba's Qwen to power its AI customer service agent.</p><p>He gave three simple reasons - it's \"very good\", \"fast\" and \"cheap\".</p><p>Further evidence can be found on Hugging Face, the place people go to download ready-made AI models - including from major developers Meta and Alibaba.</p><p>Jeff Boudier, who builds products at the platform, said it is the cost factor that leads young start-ups to look at Chinese models over their US counterparts.</p><p>\"If you look at the top trending models on Hugging Face - the ones that are most downloaded and liked by the community - typically, Chinese models from Chinese labs occupy many of the top 10 spots,\" he told me.</p><p>\"There are weeks where four out of five top training models on Hugging Face are from Chinese labs.\"</p><p>In September, Qwen topped Meta's Llama to become the most downloaded family of large language models on the Hugging Face platform.</p><p>Meta <a target=\"_blank\" href=\"https://www.bloomberg.com/news/articles/2025-12-10/inside-meta-s-pivot-from-open-source-to-money-making-ai-model\">released</a> its open-source Llama AI models in 2023. Up until the release of DeepSeek and Alibaba's models, they were considered the go-to choice for developers working on bespoke applications.</p><p>But the release of Llama 4 last year left developers underwhelmed, and Meta has reportedly been using open-source models with Alibaba, Google, and OpenAI to train a new model set for release this spring.</p><p>Airbnb also uses several models, including US-based ones, hosting them securely in the company’s own infrastructure. The data is never provided to the developers of the AI models they use, according to the company.</p></div><div data-component=\"text-block\"><p>Going into 2025, the consensus was despite billions of dollars being spent by US tech firms, Chinese companies were threatening to pull ahead.</p><p>\"That's not the story anymore,\" Boudier said. \"Now, the best model is an open-source model.\"</p><p>A report published last month by Stanford University found Chinese AI models \"seem to have caught up or even pulled ahead\" of their global counterparts - both in terms of what they're capable of, and how many people are using them.</p><p>In a recent interview with the BBC, former UK deputy prime minister Sir Nick Clegg said he felt US firms were overly focused on the pursuit of AI which may one day surpass human intelligence.</p><p>Last year, Sir Nick left his post as head of global affairs at Meta, the developer of Llama. Boss Mark Zuckerberg has committed billions of dollars to achieving what he calls \"superintelligence.\"</p><p>Some experts are now calling these ambitions vague and ill-defined – giving China an opening to dominate the open-source AI space.</p><p>\"Here's the irony,\" Sir Nick said.In the battle between \"the world's great autocracy\" and \"the world's greatest democracy\" - China and America - China is \"doing more to democratise the technology they're competing over\".</p><p>The Stanford report also suggested China's success in developing open-source models could be partly explained by government support.</p><p>On the other side of the world, US companies like OpenAI are under intense pressure to increase revenue and become profitable - <a target=\"_self\" href=\"https://www.bbc.co.uk/news/articles/cvgjn012k3do\">and is now turning to ads</a> to help get there.</p><p>The company released two open-source models last summer – its first in years. But it has poured most of its resources into proprietary models to help it make money.</p><p>OpenAI boss Sam Altman told me in October it has invested aggressively into securing ever more computing power and infrastructure deals with partners.</p><p>\"Revenue will grow super fast, but you should expect us to invest a ton in training, in the next model and the next and the next and the next,\" he said.</p></div>",
      "contentLength": 5363,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qmm0vd/bbc_reports_that_chinese_open_models_continue_to/"
    },
    {
      "title": "What do you do when you need to add a new pod/container to your infrastructure?",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qmlc98/what_do_you_do_when_you_need_to_add_a_new/",
      "date": 1769353265,
      "author": "/u/cursingpeople",
      "guid": 38790,
      "unread": true,
      "content": "<p>Do you create a pod and then make requests to that pod locally, and then use the config for the pod on the rest of your infra config by just connecting it to the gateway, and then do another test on the dev environment? What's the step-by-step process for doing this? There's a guy on my team who might leave and I might have to replace him.</p>",
      "contentLength": 341,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "PromptChart - generate charts with prompts",
      "url": "https://github.com/OvidijusParsiunas/PromptChart",
      "date": 1769350525,
      "author": "/u/ovi_nation",
      "guid": 38786,
      "unread": true,
      "content": "<p>I built an Open Source end to end system that uses GoLang for generating charts via llm prompts.A star is always appreciated!</p>",
      "contentLength": 125,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qmk74r/promptchart_generate_charts_with_prompts/"
    },
    {
      "title": "(AudioWave) a lightweight Winamp-style audio player",
      "url": "https://www.reddit.com/r/linux/comments/1qmjhrm/audiowave_a_lightweight_winampstyle_audio_player/",
      "date": 1769348721,
      "author": "/u/Kalen1987",
      "guid": 38834,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "[D] ICML new policy: reviewers will be reviewed by meta reviewer. Good policy?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qmi3oe/d_icml_new_policy_reviewers_will_be_reviewed_by/",
      "date": 1769344878,
      "author": "/u/Striking-Warning9533",
      "guid": 38813,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "[D] ICML 2026 - ICML desk-rejected my paper but kept me on as a reviewer. Wow?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qmhyin/d_icml_2026_icml_deskrejected_my_paper_but_kept/",
      "date": 1769344450,
      "author": "/u/ParticularWork8424",
      "guid": 38771,
      "unread": true,
      "content": "<p>As the title says, I admire the sheer audacity of the ICML committee. My paper gets desk-rejected, so technically I’m not part of the conference… and yet they’ve assigned me as a continued reviewer. Truly inspiring.</p><p>Rejected as an author, retained as unpaid labor. Academia really said: you don’t belong here, but your service does.</p><p>At this point, I assume my role is to review LLM-generated papers and reflect on my life choices.</p>",
      "contentLength": 436,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "I got tired of manual priority weights in proxies so I used a Reverse Radix Tree instead",
      "url": "https://getlode.app/blog/2026-01-25-stop-playing-priority-tetris",
      "date": 1769344256,
      "author": "/u/robbiedobbie",
      "guid": 38785,
      "unread": true,
      "content": "<p>Configuring a reverse proxy or a local development tool often feels like a chore. You start with one or two simple rules. Then you add a wildcard. Soon you have a list of fifty rules and a massive headache.</p><p>The main issue is ambiguity. If you have a rule for  and another for , which one should the system choose? Many routing implementations solve this with manual weights. You spend your afternoon assigning priority numbers like 10, 50, or 100. If you get the order wrong, your API traffic ends up in the wrong service.</p><p>I wanted a system where the most specific match is chosen automatically. By combining a specific data structure with a literal scoring algorithm, you can make domain routing feel intuitive.</p><h3>The Problem with Left-to-Right Thinking</h3><p>To solve domain routing, you first have to realize that domains are written backwards. We read them from left to right. However, the actual hierarchy of a domain works from right to left.</p><p>In the domain , the  part is the most general. The  part is the most specific tip of the leaf. If you try to process this as a standard string, you are fighting the natural shape of the data.</p><p>The solution is to split the domain by the dots and reverse the segments. Instead of a single string, you get a clear path: <code>[\"test\", \"myapp\", \"staging\", \"api\"]</code>.</p><p>When you store these in a , every domain ending in  shares the same root. As the resolver walks down the branches, it naturally narrows its search. It moves from the general Top-Level Domain (TLD) down to the specific subdomain. This structure is the foundation for everything else.</p><h3>Literal Density: The Scoring Secret</h3><p>How do we decide which pattern is “better” without asking the user for a priority number? We use a concept called .</p><p>Every segment in a domain can be a static string, a wildcard, or a pattern. We calculate a specificity score based on how many literal characters are in that segment. A literal character is any fixed part of the string that is not a wildcard or a parameter placeholder.</p><table><tbody><tr></tr><tr></tr></tbody></table><p>The resolver follows a simple rule at every level of the tree. It tries an exact match first. If no exact match exists, it looks at the available patterns. It then picks the pattern with the highest literal score. This is deterministic. There are no magic numbers or best guesses. A more “complex” pattern is objectively more specific.</p><h3>The Five-Service Hierarchy</h3><p>To see how this works in practice, consider five services that would typically cause a routing conflict. In a tree-based system, they live together without overlapping.</p><ol><li> →  (Exact match)</li><li> →  (A wildcard subdomain)</li><li> →  (A wildcard middle segment)</li><li> →  (Another wildcard middle segment)</li><li> →  (The catch-all)</li></ol><p>In many proxies, you would have to carefully order these rules in a configuration file. In a reverse radix tree, the hierarchy acts as a natural filter.</p><h4>Scenario A: The Specific Subdomain</h4><p>Imagine a request for .\nThe resolver starts at . It finds an exact match for . It moves into that branch. Once inside the  branch, it looks for the next segment: . It does not find an exact match. It falls back to the wildcard  that belongs specifically to the  node.\n It hits .</p><h4>Scenario B: The Nested Match</h4><p>Imagine a request for .\nThe resolver starts at . It looks for an exact match for . It finds nothing. It falls back to the root wildcard  directly under . Inside that wildcard branch, it looks for the next segment: . It finds an exact match.\n It hits .</p><h4>Scenario C: The Catch-all</h4><p>Imagine a request for .\nThe resolver starts at . It looks for . It finds nothing. It falls back to the root wildcard . Since there are no more segments in the request, it checks if this node is a terminal.\n It hits .</p><p>The wildcards only compete with each other if they share the same parent. Service 3 and Service 4 never even meet during the search.</p><h3>Dynamic Upstreams: Named Parameters</h3><p>Matching a domain is only half the battle. You often need the routing to be dynamic. This is where  come in.</p><p>You can define a rule like . In the tree,  acts like a wildcard. However, it has a special job. When the resolver matches this segment, it captures the value.</p><p>These parameters are not just metadata. They are used to hydrate your service configuration. For example, you might set an upstream fallback to <code>http://{tenant}.internal:8080</code>.</p><p>If a user visits , the system identifies “apple” as the tenant. It then dynamically routes the request to <code>http://apple.internal:8080</code>. This allows you to create a single service rule that handles an infinite number of dynamic upstreams. You do not need to create a new rule for every single customer or project.</p><p>There is a common misconception that all proxies are equally fast at scale. Many routing libraries or simple middleware implementations use a linear search. They iterate through a list of regex patterns one by one. If you have 1,000 rules, the system might perform 1,000 checks for every single request.</p><p>The reverse radix tree is designed for speed, but the complexity depends on how you use it. For exact matches, the complexity is O(L), where L is the number of segments in the domain. You simply perform a hash map lookup at each step.</p><p>However, when you introduce patterns, you enter the Pattern Room. At any given level, the resolver must scan through the list of patterns. If you have dozens of patterns at the same level, the complexity for that segment becomes O(P), where P is the number of patterns. Even in the worst case, this is much faster than a global search. The tree isolates the search to only the patterns that are relevant to that specific branch.</p><h3>Why We Gave Up “Depth-Blind” Regex</h3><p>Most tools stick to linear searches because they allow for complex regular expressions that span multiple domain levels. A single regex can match across an entire hostname regardless of how many dots it contains.</p><p>By using a tree, we give up that “depth-blind” matching. You must specify your rules level by level. In practice, this is rarely a limitation. It actually mimics how real DNS rules work. In the real world, a wildcard only covers a single level in the domain hierarchy. By following this logic, we gain massive performance and predictability without losing the features you actually need.</p><p>We often try to solve complexity by adding more configuration. We add weights, priorities, and flags. By changing our perspective and using a data structure that mirrors the problem, we can remove that configuration entirely.</p><p>Reversing the domain and scoring the segments creates a set and forget experience. The system behaves exactly how a developer expects it to. You get to stop playing priority Tetris and get back to building your actual project.</p>",
      "contentLength": 6627,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qmhw95/i_got_tired_of_manual_priority_weights_in_proxies/"
    },
    {
      "title": "InfiniPaint, an infinite canvas with infinite zoom and online collaboration",
      "url": "https://www.reddit.com/r/linux/comments/1qmhlrr/infinipaint_an_infinite_canvas_with_infinite_zoom/",
      "date": 1769343339,
      "author": "/u/ErrorAtLine0",
      "guid": 38772,
      "unread": true,
      "content": "<p>I wanted to share a program I've been working on for a bit less than a year now. InfiniPaint is a collaborative, infinite canvas note-taking/drawing app. The biggest distinguishing feature of this application is that <strong>there is no zoom in or zoom out limit.</strong> Other than that, InfiniPaint's features include:</p><ul><li>Open online lobbies for collaboration <ul><li>Text chat with others in the lobby</li><li>Jump to the location of other players through the player list</li><li>See other members draw in real time</li><li>Although this is a feature, you can also choose to completely forget about it. This app can be used offline</li></ul></li><li>Graphics tablet support (Pressure sensitive brush and eraser detection)</li><li>Layers with blend modes and opacity. Layers can be sorted into folders with their own blend mode and opacity</li><li>Quick menu usable by right clicking on the canvas, which can be used to: <ul><li>Quickly change brush colors using the currently selected color palette</li></ul></li><li>Place bookmarks on the canvas to jump to later. Bookmarks can be sorted into folders</li><li>PNG, JPG, WEBP export of specific parts of the canvas at any resolution (Screenshot)</li><li>SVG export of specific parts of the canvas (Screenshot)</li><li>Transform (Move, Scale, Rotate) any object on the canvas (Rectangle Select Tool/Lasso Select Tool)</li><li>Display Images and animated GIFs on the canvas <ul><li>Note: May take a lot of memory to store and display images compared to other objects, especially GIFs</li></ul></li><li>Hide (or unhide) the UI by pressing Tab</li><li>Place infinite square grids on the canvas as guides for drawing <ul><li>Grids come with various properties, including changing color, and displaying coordinate axes</li></ul></li><li>Textbox tool with formatting support (Bold, italics, underline, strikethrough, overline, fonts, text color, highlight color, text size, paragraph alignment, text direction)</li><li>Other tools: Rectangle, Ellipse, Line, Eye dropper/color picker, Edit/cursor</li><li>Can copy/paste selected objects (Ctrl-C Ctrl-V). This can also be done between different files, as long as they're open in different tabs in the same window</li></ul><p>You can try a (slightly restricted) version of InfiniPaint in your browser at: <a href=\"https://infinipaint.com/try.html\">https://infinipaint.com/try.html</a> (requires a WebGL2 capable browser, designed for desktops, and might take a while to load)</p>",
      "contentLength": 2171,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Fluid tile v5.0 - New engine for your tiling system",
      "url": "https://codeberg.org/Serroda/fluid-tile",
      "date": 1769341468,
      "author": "/u/Serroda",
      "guid": 38756,
      "unread": true,
      "content": "<p dir=\"auto\">A script for Kwin that auto adjusts windows to the custom KDE Plasma tiling layout by creating and removing virtual desktops.</p><p dir=\"auto\">If you like the project, you can support me by buying me a coffee or with other options available here</p><ul dir=\"auto\"><li> Working on KDE Plasma 6.4 (or superior)</li><li> Working with KWin tile manager (Meta + T shortcut)</li><li> Auto create and delete virtual desktops</li><li> Blocklist for apps to which you don't want the script to apply</li><li> Configures the priority order of windows according to the width, height and position of the tiles</li><li> Select the default tile layout when creating a new virtual desktop</li><li> Custom layout when creating a new virtual desktop</li><li> Move your windows between tiles with the UI</li><li> Extend the windows without leaving empty spaces in the layout</li><li> Works with multiple screens</li></ul><ul dir=\"auto\"><li><img src=\"https://codeberg.org/Serroda/fluid-tile/raw/branch/main/.meta/windowsAdded.webp\" width=\"450\"></li><li><img src=\"https://codeberg.org/Serroda/fluid-tile/raw/branch/main/.meta/windowsRemoved.webp\" width=\"450\"></li><li><img src=\"https://codeberg.org/Serroda/fluid-tile/raw/branch/main/.meta/UI.webp\" width=\"450\"></li><li><img src=\"https://codeberg.org/Serroda/fluid-tile/raw/branch/main/.meta/tileManager.webp\" width=\"450\"></li><li><img src=\"https://codeberg.org/Serroda/fluid-tile/raw/branch/main/.meta/blocklist.webp\" width=\"450\"></li><li><img src=\"https://codeberg.org/Serroda/fluid-tile/raw/branch/main/.meta/windowsExchange.webp\" width=\"450\"></li><li><img src=\"https://codeberg.org/Serroda/fluid-tile/raw/branch/main/.meta/changeLayout.webp\" width=\"450\"></li><li><p dir=\"auto\">Auto create and delete desktops</p><img src=\"https://codeberg.org/Serroda/fluid-tile/raw/branch/main/.meta/createDeleteDesktop.webp\" width=\"450\"></li></ul>",
      "contentLength": 801,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qmh0sx/fluid_tile_v50_new_engine_for_your_tiling_system/"
    },
    {
      "title": "Generating text with Go based on Hugging Face models",
      "url": "https://www.reddit.com/r/golang/comments/1qmgrut/generating_text_with_go_based_on_hugging_face/",
      "date": 1769340628,
      "author": "/u/pepiks",
      "guid": 38788,
      "unread": true,
      "content": "<p>I am looking for tips about what tools use to generate text based on models available on Hugging Face (non english from university), target machine to run app will be ARM (MacBook) and model using localy (load, generate result and finish, no creating local server). One option is us LM Studio and using API connect to it, but I would like create bundle - all in one app - executable + model file in home directory without running anything in LM Studio. I know I can use Python for the job, but I would do all things in Go.</p><p>On Hugging Face I see .safesensor files and JSON, but for example Hugot LLM use ONNX file format. I am confusing how work with stuff using Go.</p><p>Is it possible achieve my goal in pure Go?</p><p><strong>I tried find doc for Hugot, but it is very limited.</strong> Working with ONNX file is not problem as this kind of file are available, but is it not possible another option in pure Go in context text generation? Directly working with safersensors file is possible? For example in Python is safesensor  for this task. As states in readme of Hugot library itself was tested and used on  (Linux).</p><p>I hope anyone can share more insight in subject, especially available options to work with generating text in Go.</p>",
      "contentLength": 1203,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Migrate from Kubernetes to Nomad",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qmgmjs/migrate_from_kubernetes_to_nomad/",
      "date": 1769340137,
      "author": "/u/RoutineKangaroo97",
      "guid": 38761,
      "unread": true,
      "content": "<p>Has anyone migrated from Kubernetes to Nomad in real production environments? If so, could you share the reasons or the decision-making details?</p><p>Personally, for sometimes I feel that K8s is too much, while Nomad is a cleaner approach. Am I wrong?</p>",
      "contentLength": 245,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Building a small tool to visualize Kubernetes RBAC — would you use this?",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qmg4wi/building_a_small_tool_to_visualize_kubernetes/",
      "date": 1769338457,
      "author": "/u/Mobile_Theme_532",
      "guid": 38743,
      "unread": true,
      "content": "<div><p>I’m building a micro tool called KubeScope to make Kubernetes RBAC easier to understand.</p><p>Right now the idea is simple:</p><pre><code>• Upload an exported RBAC snapshot (json/zip) • It shows who has access to what • Flags risky stuff like: • cluster-admin • wildcard \\* • secrets access • pods/exec permissions </code></pre><p>Goal: anyone should understand RBAC access in 30 seconds.</p><p>I’m not trying to build a huge platform — just something clean + fast + useful.</p><pre><code>1. What’s the biggest RBAC pain you face today? 2. Would you prefer CLI output or a UI dashboard? 3. What 1 feature would make this a “must-have” for you? </code></pre><p>If you’ve dealt with RBAC audits / permission creep, I’d love your honest feedback 🙏</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Mobile_Theme_532\"> /u/Mobile_Theme_532 </a>",
      "contentLength": 741,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "[D] AI4PDEs, SciML, Foundational Models: Where are we going?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qmg4t3/d_ai4pdes_sciml_foundational_models_where_are_we/",
      "date": 1769338449,
      "author": "/u/Mundane_Chemist3457",
      "guid": 38892,
      "unread": true,
      "content": "<p>I'm no ML expert, but a master's student working on computational mechanics, PDEs and some deep learning for these topics. </p><p>I have been following some groups, papers and trends and it is still unclear what is the exact direction in which AI4PDEs and scientific ML is going into. </p><p>Recent works show reinforcement learning for fluid dynamics, neural operators applied to irregular domains via transformers, GNNs or PointNet, nice works on diffusion or flow matching for inverse problems with physical constraints, and of course protein ans drug discovery tasks. </p><p>Robotics folks also are using physics environments for policy learning, which based on my limited knowledge, also include some aspects of scientific machine learning. Of course due to ODEs/PDEs, the field also naturally extends to control theory and chaotic systems. </p><p>Very recently some groups also published foundational models for PDEs. In robotics, major work on foundation VLA-type models is also going on. </p><p>Some simulation software providers have also included ML or AI surrogates in their workflows. Agents that can automate complex simulation workflows, ML models that can learn from an existing DoE, and geometric deep learning is applied to iterate designs efficiently on irregular domains. </p><p>: The research still seems scattered and I am unable to notice any trend. Is this true? Or am I missing a major trend that is picking up in research labs. </p><p>For e.g. LLMs have had some noticeable trends: initially starting with prompt engineering, then reasoning and logical capabilities, now key focus on agentic systems and so on. </p><p><strong>Another question I have is</strong>: Is robot learning also aiming to include some aspects of scientific ML, possibly to reduce the sim-to-real gap? </p><p>I'd like to know opinions and observations from folks interested in these areas. </p><p>Thank you for the discussion.</p>",
      "contentLength": 1837,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Beginner needs help with a test function",
      "url": "https://www.reddit.com/r/golang/comments/1qmg1zw/beginner_needs_help_with_a_test_function/",
      "date": 1769338179,
      "author": "/u/This_University_547",
      "guid": 38773,
      "unread": true,
      "content": "<p>I'm a beginner to programming who is learning Go and the basics of coding with \"For the Deeper Love of Go\"</p><p>I've an issue with a test function. I have been through documentation (as much as a novice can) and searched online, I've rewritten the function, and even did a direct copy and paste from John Arundel's source code on GitHub. </p><p>No matter what I try the test fails as for some reason want and got return reverse versions of slice required.</p><p>This is the test function:</p><pre><code>func TestGetAllBooks_ReturnsAllBooks(t *testing.T) { t.Parallel() want := []books.Book{ { Title: \"Count Belisarius\", Author: \"Peter Graves\", Copies: 15, ID: \"a1\", }, { Title: \"Dune\", Author: \"Frank Herbert\", Copies: 10, ID: \"a2\", }, } got := books.GetAllBooks() slices.SortFunc(got, func(a, b books.Book) int { return cmp.Compare(a.Author, b.Author) }) if !slices.Equal(want, got) { t.Fatalf(\"want %#v, got %#v\", want, got) } } </code></pre><p>This is the GetAllBooks function:</p><pre><code>func GetAllBooks() []Book { return slices.Collect(maps.Values(catalog)) } </code></pre><p>This is the result of the test function:</p><p>=== RUN TestGetAllBooks_ReturnsAllBooks</p><p>=== PAUSE TestGetAllBooks_ReturnsAllBooks</p><p>=== CONT TestGetAllBooks_ReturnsAllBooks</p><p>books_test.go:31: want []books.Book{books.Book{Title:\"Count Belisarius\", Author:\"Peter Graves\", Copies:15, ID:\"a1\"}, books.Book{Title:\"Dune\", Author:\"Frank Herbert\", Copies:10, ID:\"a2\"}}, got []books.Book{books.Book{Title:\"Dune\", Author:\"Frank Herbert\", Copies:10, ID:\"a2\"}, books.Book{Title:\"Count Belisarius\", Author:\"Peter Graves\", Copies:15, ID:\"a1\"}}</p><p>--- FAIL: TestGetAllBooks_ReturnsAllBooks (0.00s)</p><p>I'm sure it's something quite simple but I'm baffled. </p><p>Any advice will be greatly appreciated.</p>",
      "contentLength": 1660,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Improve AI Context: Use Your Local Go Module Cache",
      "url": "https://ivan-pidikseev.dev/posts/golang-mcp-local-context/",
      "date": 1769338033,
      "author": "/u/ivan-pidikseev",
      "guid": 38759,
      "unread": true,
      "content": "<p>When I used an AI assistant to write Go code that used a third-party library, it sometimes generated code with wrong function signatures, missing types, or outdated APIs - especially for packages that were new or not very popular. After a while, I started instructing the assistants to look at specific files in my Go module cache. While this helped, it was hard to synchronize prompts and rules across all the AI assistants I used, whether that’s Cursor IDE, Claude, or others.</p><p>To address this issue, I built a simple MCP server that keeps everything in one place. In this blog post, I’ll explain how to use it and compare the results with and without the MCP.</p><p> AI assistants generate incorrect Go code because they rely on outdated training data.</p><p><a href=\"https://github.com/svetlyi/mcp-local-context\" target=\"_blank\">\n    mcp-local-context\n</a>\n - a free, local MCP server that teaches AI to read your  instead of guessing. Think of it as a <strong>free, local alternative to Context7</strong>.</p><p> AI discovers correct SDK methods instead of reinventing them. Check out the <a href=\"https://ivan-pidikseev.dev/posts/golang-mcp-local-context/#real-world-test-natsio\">\n    before/after comparison\n</a>\n to see the difference.</p><p>Installation: <a href=\"https://github.com/svetlyi/mcp-local-context\" target=\"_blank\">\n    README\n</a>\n | <a href=\"https://github.com/svetlyi/mcp-local-context/blob/main/internal/prompts/golang.md\" target=\"_blank\">\n    Manual prompts\n</a>\n (if you don’t want to install, though you might miss new features like indexing)</p><p>As I mentioned above, AI coding assistants often generate incorrect Go code when working with third-party packages. They rely on outdated documentation, incomplete examples, or assumptions based on their training data - which may be months or years old.</p><p>You end up with code that:</p><ul><li>Uses wrong function signatures</li><li>Misses important types or methods</li><li>Implements workarounds for features that already exist in the SDK</li><li>Breaks when you actually run it</li></ul><blockquote><p>While existing third-party services provide context for packages, they add external dependencies and potential privacy concerns. <strong>The Go module cache is already on your machine - why not use it directly?</strong></p></blockquote><p>I built an <a href=\"https://github.com/svetlyi/mcp-local-context\" target=\"_blank\">\n    MCP server\n</a>\n that runs locally on your machine and provides AI tools with instructions on how to access your Go module cache. MCP (Model Context Protocol) is an open protocol that allows AI assistants to access external data sources.</p><p>The server doesn’t parse Go files or search for documentation itself, <em>at least in the current implementation as of the date of writing this post</em>. Instead, it provides hints and instructions to the LLM on how to search for context in your local  directory. This guides the AI to look at the actual source code in your module cache rather than relying on outdated training data.</p><p> Follow the guide in the <a href=\"https://github.com/svetlyi/mcp-local-context\" target=\"_blank\">\n    repository README\n</a>\n. If you prefer not to install the server, you can use the <a href=\"https://github.com/svetlyi/mcp-local-context/blob/main/internal/prompts/golang.md\" target=\"_blank\">\n    manual prompts\n</a>\n directly in your AI tool of choice.</p><h2>How this compares to Context7 and similar solutions</h2><p> takes a fundamentally different approach:</p><ul><li><p><strong>Local-first, not cloud-based</strong> - reads directly from your  instead of using some remote service.</p></li><li><p> - no subscriptions or API keys. Just install and use it.</p></li><li><p><strong>Exact dependency versions</strong> - works with the exact versions you have installed, not whatever the AI was trained on.</p></li><li><p> - this tool doesn’t make any remote calls. Your code and dependencies stay on your machine. (Note: your AI assistant might still use external services, but this MCP server itself is purely local.)</p></li></ul><p>To demonstrate the difference, I tested the MCP server with <a href=\"https://nats.io\" target=\"_blank\">\n    NATS.IO\n</a>\n, a popular messaging system with a comprehensive SDK. I chose NATS.IO as a representative example because its SDK is feature-rich, and AI models may not have complete knowledge of all its methods.</p><p>I used the following prompt with Cursor’s  model (I used this model for consistency, as Claude Sonnet 4.5 appeared to have memorized context from previous attempts):</p><pre tabindex=\"0\"><code>Create a NATS.IO subscriber and publisher. The subscriber should send a basic event, such as the coordinates of a point, and the publisher should calculate the distance between the previous point and the current point. Use Cobra for commands.\n\nAdditionally, add a debug HTTP endpoint to see the number of currently pending messages.\n\nUse local-context mcp.\n</code></pre><h2>Without MCP: Manual Implementation</h2><p>Without access to the module cache, the AI generated code that manually tracked pending messages using atomic counters:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>This works, but not correctly, ignoring the possible tail of pending messages for the exact subscription and so on. Moreover, it’s reinventing functionality that already exists in the SDK.</p><h2>With MCP: Using SDK Methods</h2><p>With MCP providing instructions on how to search the module cache, the AI correctly identified and used the SDK’s  method:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p> The  method handles edge cases, thread safety, and provides accurate counts that manual tracking might miss. It’s also the idiomatic way to use the SDK.</p><blockquote><p>🤖  The generated code structures differ due to LLM non-determinism, but the key difference is clear: with MCP, the AI discovered and used the proper SDK method instead of implementing a manual workaround.</p></blockquote><blockquote><p>⚠️  the generated code in both cases is far from perfect. For example, the LLM confused the publisher and the subscriber in the last example. However, the point of this blog post is simply to show a way to provide more context to the LLM. Of course, you still need to review the generated code. At the end of the day, you, not the LLM, are responsible for the software.</p></blockquote><p>Improving the context you provide to AI coding assistants isn’t just about getting correct code - it’s about leveraging the tools you already have. Your Go module cache contains the exact source code, examples, documentation, and API definitions for the dependencies you’re actually using. By giving AI tools access to this local context, you eliminate the guesswork that leads to wrong function signatures, outdated APIs, and manual workarounds.</p><p>The MCP server approach demonstrates that you don’t need to rely on third-party services or hope that AI models have the right information in their training data. You can take control of the context directly, ensuring accuracy while maintaining privacy and reducing costs.</p><p>The solution works well for me, and I hope it helps you too. Over time, it can be extended with features like dependency indexing to improve context retrieval, especially for large dependencies. If you have ideas for improvements, feel free to send a pull request 👍.</p>",
      "contentLength": 6177,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qmg0gs/improve_ai_context_use_your_local_go_module_cache/"
    },
    {
      "title": "you prolly don't need a mocking lib in go",
      "url": "https://www.reddit.com/r/golang/comments/1qmfjzh/you_prolly_dont_need_a_mocking_lib_in_go/",
      "date": 1769336424,
      "author": "/u/Ok_Analysis_4910",
      "guid": 38760,
      "unread": true,
      "content": "<p>another tired ol' take against mocking libraries in go, but aleast this has a few good examples on what to do instead</p><p>i still find it strange that go testing review doc says not to use mocks w/o a single example on what to do instead. ik brevity and stuff and stdlib have examples but still</p>",
      "contentLength": 289,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "I want named arguments in Rust. Mom: We have named arguments in Rust at home:",
      "url": "https://www.reddit.com/r/rust/comments/1qmeqb4/i_want_named_arguments_in_rust_mom_we_have_named/",
      "date": 1769333544,
      "author": "/u/nik-rev",
      "guid": 38770,
      "unread": true,
      "content": "<p>Did you know that Rust has named arguments? At least you can imitate them on nightly!</p><pre><code>let opts = #[kwargs] takes_options { display: false, debug: 2, }; </code></pre><p>The type  is inferred, and we don't have to import it.</p><pre><code>let opts = takes_options(Options { display: false, debug: 2, }); </code></pre><pre><code>fn takes_options(opts: Options) -&gt; Options { opts } struct Options { display: bool, debug: u32, } </code></pre><p>This is accomplished by defining the  macro as follows:</p><pre><code>macro_rules! kwargs { attr() ($fn:ident $tt:tt) =&gt; {$fn({ type InferredType = impl ?Sized; if false { panic!() as InferredType } else { InferredType $tt } })} } </code></pre><p>The following is required:</p><ul><li><code>RUSTFLAGS=\"-Znext-solver=globally\"</code> because the current trait solver can't deal with this code</li><li><code>#![feature(type_alias_impl_trait)]</code> to allow </li><li><code>#![feature(stmt_expr_attributes)]</code> and <code>#![feature(proc_macro_hygiene)]</code> to apply attribute macros on expressions</li></ul><pre><code>#![feature(type_alias_impl_trait)] #![feature(stmt_expr_attributes)] #![feature(proc_macro_hygiene)] #![feature(macro_attr)] // this one is optional, allows writing attribute macros with macro_rules! macro_rules! kwargs { attr() ($fn:ident $tt:tt) =&gt; {$fn({ type InferredType = impl ?Sized; if false { panic!() as InferredType } else { InferredType $tt } })} } fn takes_options(opts: Options) -&gt; Options { opts } #[derive(Debug, PartialEq)] struct Options { display: bool, debug: u32, } fn main() { let a = #[kwargs] takes_options { display: false, debug: 2, }; let b = takes_options(Options { display: false, debug: 2, }); assert_eq!(a, b); } </code></pre><p>What if  was an attribute macro that you apply to the entire crate, and it automatically transformed any struct literal with a lowercase path?? <a href=\"https://github.com/rust-lang/rust/issues/54726\"><code>#![feature(custom_inner_attributes)]</code></a></p><pre><code>#![kwargs] fn main() { let a = takes_options { display: false, debug: 2, }; // the above is automatically transformed into this by #![kwargs]: let a = takes_options(Options { display: false, debug: 2, }); // because the struct literal is all lowercase. } </code></pre><p>This is only for fun! Don't actually use this :)</p>",
      "contentLength": 1984,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "I built cpx - a modern, faster rust based replacement for cp (up to 5x faster)",
      "url": "https://www.reddit.com/r/rust/comments/1qmepgr/i_built_cpx_a_modern_faster_rust_based/",
      "date": 1769333466,
      "author": "/u/PurpleReview3241",
      "guid": 38850,
      "unread": true,
      "content": "<ul><li>Beautiful progress bars (customizable)</li><li>Resume interrupted transfers (checksum safe)</li><li>Exclude patterns (files, directories, glob patterns)</li><li>Flexible configuration for defaults and parallelism</li><li>Graceful Interupt handling with resume hints</li></ul><p>I took inspiration from modern CLI tools like bat, fd, ripgrep. Would love to hear feedback.</p>",
      "contentLength": 321,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "GoHotPool: A PostgreSQL-Inspired Buffer Pool in Go",
      "url": "https://github.com/MYK12397/gohotpool",
      "date": 1769332417,
      "author": "/u/mYk_970",
      "guid": 38739,
      "unread": true,
      "content": "<p>It's a byte buffer pool inspired by PostgreSQL's buffer management system. it implements some database-inspired features like Clock Sweep Eviction, Pin/Unpin mechanism, ring buffers &amp; many more.</p><p>This was a great deep dive into understanding how PostgreSQL manages memory, Implementing lock-free data structures with atomics &amp; Performance optimization.</p>",
      "contentLength": 350,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qmef23/gohotpool_a_postgresqlinspired_buffer_pool_in_go/"
    },
    {
      "title": "Building a lightning-fast highly-configurable Rust-based backtesting system",
      "url": "https://nexustrade.io/blog/building-a-lightning-fast-highly-configurable-rust-based-backtesting-system-20260119",
      "date": 1769330912,
      "author": "/u/ReplacementNo598",
      "guid": 38738,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qmdzue/building_a_lightningfast_highlyconfigurable/"
    },
    {
      "title": "New UCLA AI tool targets Alzheimer's cases often missed in early diagnosis",
      "url": "https://abc7.com/post/new-ucla-ai-tool-targets-alzheimers-cases-often-missed-early-diagnosis/18458903/",
      "date": 1769329058,
      "author": "/u/Fcking_Chuck",
      "guid": 38740,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qmdhbb/new_ucla_ai_tool_targets_alzheimers_cases_often/"
    },
    {
      "title": "I know it's not a big deal, but I just created my first script :D",
      "url": "https://www.reddit.com/r/linux/comments/1qm9w9e/i_know_its_not_a_big_deal_but_i_just_created_my/",
      "date": 1769317441,
      "author": "/u/Main_Ear9949",
      "guid": 38730,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Stackmaxxing for a recursion world record",
      "url": "https://www.youtube.com/watch?v=WQKSyPYF0-Y",
      "date": 1769316298,
      "author": "/u/Chii",
      "guid": 38728,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qm9ilz/stackmaxxing_for_a_recursion_world_record/"
    },
    {
      "title": "One-Minute Daily AI News 1/24/2026",
      "url": "https://www.reddit.com/r/artificial/comments/1qm8ga8/oneminute_daily_ai_news_1242026/",
      "date": 1769313146,
      "author": "/u/Excellent-Target-847",
      "guid": 38742,
      "unread": true,
      "content": "   submitted by   <a href=\"https://www.reddit.com/user/Excellent-Target-847\"> /u/Excellent-Target-847 </a>",
      "contentLength": 43,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "GIMP 3.0.8 Released",
      "url": "https://www.gimp.org/news/2026/01/24/gimp-3-0-8-released/",
      "date": 1769309256,
      "author": "/u/CMYK-Student",
      "guid": 38717,
      "unread": true,
      "content": "<p>We are happy to announce the fourth micro-release  3.0.8!\nAs we close in on the release of  3.2, we wanted to share with you what\nmay be the last set of bugfixes for &nbsp;3.0.</p><p>Micro releases like 3.0.8 are focused on\n<a href=\"https://gitlab.gnome.org/GNOME/gimp/-/blob/01df768b6756a04ecfaa410156f23ee95b3085e9/NEWS\">fixing bugs and regressions</a>. While this news post is not an exhaustive list of all fixes, we wanted to highlight some of the ones\nwith a more noticeable&nbsp;impact.</p><p>Improvements in start-up time for users with a large number of fonts was\n<a href=\"https://www.gimp.org/news/2025/12/15/gimp-3-2-RC2-released/#start-up-time\">backported</a> from our 3.2  release.\nAs a result, we now wait to load images until fonts are initialized - this prevents some occasional odd displays\nand other issues when an  file tried to access a partially loaded&nbsp;font.</p><p>For macOS users, we have special-cased the legacy Skia font, as we received reports that it did not behave properly\nwith the Pango library we use to render fonts. You should now be able to use all fonts weights instead of just&nbsp;Bold.</p><h2>Assorted updates and fixes</h2><ul><li><p> helped us identify an issue when exporting a lossless  image could be affected by\nlossy settings (such as Quality being less than 100%). We’ve updated our  plug-in to prevent this from&nbsp;happening.</p></li><li><p> fixed a bug in the Windows installer where text would be duplicated in certain&nbsp;languages.</p></li><li><p> diagnosed an issue with font kerning on macOS, which was fixed by .</p></li><li><p>Because of differences in how different operating systems represent file paths, default color profiles\nwere not being loaded correctly on start-up on Windows. This should now be fixed, though you may need to\nreassign your default color profiles in Preferences to clear out the older, incorrect file&nbsp;path.</p></li><li><p>Thanks to ‘s efforts, the&nbsp;standard  executable can now be run with&nbsp;a  flag \ninstead of requiring users to&nbsp;call  even on devices with no display.&nbsp;The \nflag is now visible as&nbsp;well.</p></li><li><p> improved our flatpak by adding safe guards to show the correct configuration directory\nregardless of&nbsp;whether  is defined on the user’s system. This should make it much easier for\nflatpak users to install and use third party&nbsp;plug-ins.</p></li><li><p>We fixed a rare but possible crash when using the Equalize filter on images with\n values. Images that contain these are usually created from scientific\nor mapping data, so you’re unlikely to come across them in standard&nbsp;editing.</p></li><li><p> fixed an internal issue where the wrong version number could be used when installing\nminor releases (such as the 3.2 release candidates and upcoming 3.2 stable&nbsp;release).</p></li><li><p>As noted in our <a href=\"https://www.gimp.org/news/2025/12/15/gimp-3-2-RC2-released/#paths\">3. news post</a>,\nwe have updated our  import code to improve the rendered&nbsp;path.</p></li><li><p>Further improvements have been made to our non-destructive filter code to improve stability, especially\nwhen copying and pasting layers and images with filters attached to them. Some issues related to applying\n filters on Quick Masks have also been&nbsp;corrected.</p></li><li><p>An unintended Search pop-up that appeared when typing while the Channels dockable was selected has been\nturned&nbsp;off.</p></li><li><p>When saving XCFs for  2.10 compatibility, we unintentionally saved Grid color using the new color format.\nThis caused errors when reopening the  in 2.10. This problem has now been fixed! If you encounter any other\n incompatibility, please let us&nbsp;know.</p></li></ul><ul><li><p>The Navigation and Selection Editor dockables no longer show a large bright texture when no image is actively selected.\nThis was especially noticeable on dark&nbsp;themes.</p></li><li><p>When a layer has no active filters,&nbsp;the  column had the same “checkbox” outline when hovered over as the lock column.\nThis led to confusion about clicking it to add filters. We have removed the outline on hover as a small step to help address&nbsp;this.</p></li><li><p> fixed alignment and cut-off issues with the buttons on our Transform tool overlays. All buttons should now\nbe properly centered and&nbsp;visible.</p></li><li><p>The options for filling layers with colors when resizing the canvas will be turned off when not relevant (such as when you set\nlayers to not be&nbsp;resized).</p></li><li><p>More  elements such as dialog header icons will now respond to your icon size&nbsp;preferences.</p></li><li><p> has continued his work to update our  with the more usable Spin Scale widget. He has\nalso updated the widget itself to improve how it works for users and developers&nbsp;alike.</p></li></ul><p> and  continued to patch potential security issues related to some of\nour file format plug-ins. In addition to <a href=\"https://www.gimp.org/news/2025/11/17/gimp-3-2-RC1-released/#security\">existing fixes</a> \nmentioned in the <a href=\"https://www.gimp.org/news/2025/12/15/gimp-3-2-RC2-released/#security\">release candidate news posts</a>,\nthe following exploits are now&nbsp;prevented:</p><ul></ul><p>Another potential issue related to  files with incorrect metadata was reported by . It does not have a\n number yet, but it has been fixed for  3.0.8.  also fixed a potential issue\nwith loading Creator blocks in Paintshop Pro &nbsp;images.</p><p>As part of  3.0.8, we also updated several dependencies to prevent vulnerabilities. Thanks to ,\nour Windows installer now uses a newer version of Python due to several CVEs in Python 3.12.11. We also updated\nour  library librsvg 2.61.3 to prevent a possible  authentication exploit when loading a malicious .</p><p>For plug-in and script developers, a few new public s \nwere backported to &nbsp;3.0.8. <code>gimp_cairo_surface_get_buffer ()</code> allows you to retrieve a  buffer from a \nCairo surface (such as a text layer). Note that this&nbsp;deprecates <code>gimp_cairo_surface_create_buffer ()</code>.</p><p><code>gimp_config_set_xcf_version ()</code> and <code>gimp_config_get_xcf_version ()</code> can be used to specify a particular  version for\na configuration. This will allow you to have that data serialized/deserialized for certain versions of  if there were\ndifferences (such as the Grid colors mentioned&nbsp;above).</p><p>Fixes were made for retrieving image metadata via&nbsp;scripting.  is now a visible child&nbsp;of , so you\ncan use standard gexiv2 functions to retrieve information from&nbsp;it.</p><p>Original thumbnail metadata is also now removed on export to prevent potential issues when exporting into a new&nbsp;format.</p><h2>Packaging improvements on macOS</h2><p> and  worked on some packaging fixes for&nbsp;macOS:</p><ul><li>Image Graph is now available (if  is run&nbsp;with )</li><li>Thai language interfaces have proper word&nbsp;breaking</li><li> and  files can be opened&nbsp;again</li><li>Dialogs should receive focus again thanks to a patch on </li><li> icon is not tiny anymore on macOS 26 Tahoe (we plan to support Liquid Glass effects in the&nbsp;future)</li><li>Configuration migrations between  2.10 and 3.0 should be more robust&nbsp;now.</li></ul><p>Our documentation maintainer  has released a new version of the <a href=\"https://docs.gimp.org/\"> 3.0 help manual</a>.\nVersion 3.0.2 of the manual includes updated information on non-destructive filters, changes in the Align tool, and more.\nUpdates to fifteen translations have been made as part of this&nbsp;release.</p><p>Special thanks to  for their work in standardizing formatting across the help manual and reducing the need to\nretranslate duplicate&nbsp;text.</p><p> has released a new update for babl, our color space engine. Version 0.1.120 adds support for the\nx86_64-v4 microarchitecture for code&nbsp;optimizations.</p><ul><li>26 reports were closed as .</li><li>12 translations were updated: Chinese (China), Danish, Georgian, Greek, Lithuanian, Norwegian Nynorsk, Persian, Slovenian, Swedish, Thai, Turkish,&nbsp;Ukrainian.</li></ul><p>28 people contributed changes or fixes to  3.0.8 codebase (order\nis determined by number of commits; some people are in several&nbsp;groups):</p><ul><li>8 developers to core code: Jehan, Alx Sa, Bruno Lopes, Gabriele Barbero, Idriss Fekir, Jacob Boerema, James Addison,&nbsp;aruius.</li><li>9 developers to plug-ins or modules: Alx Sa, Bruno Lopes, Jacob Boerema, Jehan, Ondřej Míchal, Anders Jonsson, Dr. David Alan Gilbert, Gabriele Barbero, lloyd&nbsp;konneker.</li><li>14 translators: Aefgh Threenine, Ekaterine Papava, Martin, Alan Mortensen, Anders Jonsson, Yuri Chornoivan, luming zh, Aurimas Aurimas Černius, Kolbjørn Stuestøl, Sabri Ünal, dimspingos, Aurimas Černius, Danial Behzadi, Luming&nbsp;Zh.</li><li>2 theme designers: Alx Sa, Ondřej&nbsp;Míchal.</li><li>5 build, packaging or  contributors: Bruno Lopes, Jehan, Jeremy Bícha, Jernej Simončič, Niels De&nbsp;Graef.</li><li>6 contributors on other types of resources: Jehan, Bruno Lopes, Jacob Boerema, Jeremy Bícha, Niels De Graef, Sabri&nbsp;Ünal.</li><li>The gimp-data submodule had 16 commits by 3 contributors: Bruno Lopes, Jehan, Jeremy&nbsp;Bícha.</li></ul><p>Contributions on other repositories in the GIMPverse (order is determined by\nnumber of&nbsp;commits):</p><ul><li>babl 0.1.120 is made of 5 commits by 2 contributors: Øyvind Kolås, Bruno&nbsp;Lopes.</li><li><a href=\"https://ctx.graphics/\">ctx</a> had 181 commits since 3.2.0  release by 1 contributors: Øyvind&nbsp;Kolås.</li><li>The  (macOS packaging scripts) release had 27 commits by 2 contributors: Lukas Oberhuber, Bruno&nbsp;Lopes.</li><li>The flatpak release had 20 commits by 1 contributor (and a bot):&nbsp;Bruno.</li><li>Our main website (what you are reading right now) had 103 commits by 7 contributors: Bruno Lopes, Jehan, Alx Sa, Sabri Ünal, Jacob Kauffmann, Petr Vorel,&nbsp;gturri.</li><li>Our <a href=\"https://docs.gimp.org/\">3.0 documentation</a> had 266 commits by 13 contributors: Sabri Ünal, Jacob Boerema, dimspingos, Marco Ciampa, Anders Jonsson, Alevtina Karashokova, Yuri Chornoivan, Matthew Leach, Richard Gitschlag, Andre Klapper, Aurimas Aurimas Černius, Dick Groskamp, lloyd&nbsp;konneker.</li></ul><p>Let’s not forget to thank all the people who help us triaging in Gitlab, report\nbugs and discuss possible improvements with us.\nOur community is deeply thankful as well to the internet warriors who manage our\nvarious <a href=\"https://www.gimp.org/discuss.html\">discussion channels</a> or social\nnetwork accounts such as Ville Pätsi, Liam Quin, Michael Schumacher and&nbsp;Sevenix!</p><p><em>Note: considering the number of parts in  and around, and how we\nget statistics&nbsp;through  scripting, errors may slip inside these\nstats. Feel free to tell us if we missed or mis-categorized some\ncontributors or&nbsp;contributions.</em></p><ul><li>Linux AppImages for x86 and &nbsp;(64-bit)</li><li>Linux Flatpaks for x86 and &nbsp;(64-bit)</li><li>Linux Snaps for x86 and &nbsp;(64-bit)</li><li>Universal Windows installer for x86 (32 and 64-bit) and for &nbsp;(64-bit)</li><li>Microsoft Store for x86 and &nbsp;(64-bit)</li><li>macOS  packages for Intel/x86 and Apple/ hardware&nbsp;(64-bit)</li></ul><p>Other packages made by third-parties are obviously expected to follow (Linux or * distributions’ packages,&nbsp;etc).</p><p>This might be the final release in the  3.0 series, unless some very\nugly bug were to appear and we’d feel like making a better ending. We\nknow indeed that some people are sometimes stuck longer on some series\nfor various reasons (such as stable package policy in some Linux\ndistributions, or because we do have to drop some platforms sometimes —\nwhich will soon be the case for 32-bit Windows by the way! —, and\nsometimes some people just prefer older !). Also we do introduce\nbugs with new feature code. Such is the life of software, either being\nstale and stabler, or evolving with higher risk of new&nbsp;bugs!</p><p>So whatever your reason, let’s make sure that you’ll have at least a\nvery nice latest 3.0 build to get back too, if needed be.&nbsp;😄</p><p>Now we are mostly focusing on the last few issues before starting the\n3.2 series. We’ll get news about this&nbsp;soon.</p><p>In any case, we wish you all a very happy new Western year! May it be\nfilled with a lot of joy, fun with  too, and of course a healthy\nlife.&nbsp;🤗</p>",
      "contentLength": 10758,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qm72ro/gimp_308_released/"
    },
    {
      "title": "How do you deploy a project on cloud that depends on private github repositories?",
      "url": "https://www.reddit.com/r/golang/comments/1qm5lff/how_do_you_deploy_a_project_on_cloud_that_depends/",
      "date": 1769305299,
      "author": "/u/Ill_Concept_6002",
      "guid": 38703,
      "unread": true,
      "content": "<p>i have a project that depends on private github repositories. I was using <a href=\"http://go.work\">go.work</a> to sync the project locally, but I now need to deploy the project on cloud.</p><p>I've tried ssh and deploy key way but they are making the deployment process a bit complex. What's the right and easy way to setup deployment for such projects? Also, repositories need to be sync. </p>",
      "contentLength": 354,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Connection Exhaustion in High-Traffic Systems",
      "url": "https://open.substack.com/pub/systemdr/p/connection-exhaustion-in-high-traffic?utm_source=share&amp;utm_medium=android&amp;r=5bgzxg",
      "date": 1769302675,
      "author": "/u/Extra_Ear_10",
      "guid": 38741,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qm4lyi/connection_exhaustion_in_hightraffic_systems/"
    },
    {
      "title": "DAXFS Proposed As Newest Linux File-System",
      "url": "https://www.phoronix.com/news/DAXFS-Linux-File-System",
      "date": 1769302038,
      "author": "/u/anh0516",
      "guid": 38702,
      "unread": true,
      "content": "\nThere's yet another new Linux file-system on the block: DAXFS has been announced as a new read-only open-source file-system.\n<p>DAXFS as implied by the name makes use of the Linux kernel's direct access \"</p><a href=\"https://www.phoronix.com/search/DAX\">DAX</a>\" infrastructure. DAX is designed as a simple read-only file-system operating directly atop shared physical memory.\n<p>DAXFS is designed to provide zero-copy reads from contiguous memory regions and bypasses the traditional block I/O stack, buffer heads, and page cache entirely -- a big difference compared to the likes of RAMFS or TMPFS.\n</p><p>DAXFS is designed for zero-copy efficiency, true physical sharing, hardware integration with the likes of GPUs and CXL hardware, and simplicity:\n</p><blockquote>\"Key Features\n- Zero-Copy Efficiency: File reads resolve to direct memory loads, eliminating page cache duplication and CPU-driven copies.\n<p>- True Physical Sharing: By mapping a contiguous physical address or a dma-buf, multiple kernel instances or containers can share the same physical pages.\n</p>- Hardware Integration: Supports mounting memory exported by GPUs, FPGAs, or CXL devices via the dma-buf API.\n<p>- Simplicity: Uses a self-contained, read-only image format with no runtime allocation or complex device management.\"</p></blockquote>DAXFS is being developed by Multikernel.io, the developers behind <a href=\"https://www.phoronix.com/news/Linux-Multi-Kernel-Patches\">a proposed multi-kernel architecture for Linux</a>. As part of their DAXFS work they aim to enhance their multi-kernel efforts as well as enhancing CXL support and better accelerator data handling:\n<blockquote>\"Primary Use Cases\n- Multikernel Environments: Sharing a common Docker image across independent kernel instances via shared memory.\n<p>- CXL Memory Pooling: Accessing read-only data across multiple hosts without network I/O.\n</p>- Container Rootfs Sharing: Using a single DAXFS base image for multiple containers (via OverlayFS) to save physical RAM.\n<p>- Accelerator Data: Zero-copy access to model weights or lookup tables stored in device memory.\"</p></blockquote>DAXFS was <a href=\"https://lore.kernel.org/lkml/CAGHCLaREA4xzP7CkJrpqu4C=PKw_3GppOUPWZKn0Fxom_3Z9Qw@mail.gmail.com/\">announced</a> today on the Linux Kernel Mailing List. The current DAXFS kernel module and user-space tool for it can currently be found on <a href=\"https://github.com/multikernel/daxfs\">GitHub</a> while awaiting to see the level of upstream interest in potentially working toward upstreaming it into the mainline Linux kernel.",
      "contentLength": 2189,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qm4d9x/daxfs_proposed_as_newest_linux_filesystem/"
    },
    {
      "title": "NVIDIA’s real moat isn’t hardware, it’s 4 million developers",
      "url": "https://medium.com/@jpcaparas/nvidias-real-moat-isn-t-hardware-it-s-4-million-developers-648d6aeb1226?sk=82ee7baf9290da1eb93efd9d34c4c7b4",
      "date": 1769300649,
      "author": "/u/jpcaparas",
      "guid": 38704,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qm3ts4/nvidias_real_moat_isnt_hardware_its_4_million/"
    },
    {
      "title": "[D] ICLR 2026 decision mega thread",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qm32o6/d_iclr_2026_decision_mega_thread/",
      "date": 1769298732,
      "author": "/u/ayanD2",
      "guid": 38724,
      "unread": true,
      "content": "<div><p>The review is out tomorrow (a few hours remaining following eastern time). I am creating this mega thread to talk about meta reviews and final decisions. </p><p>After the Openreview fiasco, this will be interesting.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/ayanD2\"> /u/ayanD2 </a>",
      "contentLength": 237,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Using LLMs to help diagnose Kubernetes issues – practical experiences?",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qm2f07/using_llms_to_help_diagnose_kubernetes_issues/",
      "date": 1769297094,
      "author": "/u/Prestigious-Look2300",
      "guid": 38695,
      "unread": true,
      "content": "<p>I’m working on an MSc team project where we’re exploring whether large language models (LLMs) can be useful for diagnosing common Kubernetes issues using logs, events, and pod states.</p><p>We’re a group of 6. One or two members have strong Kubernetes experience from software engineering roles, while the rest of us (including me) come from data/IT backgrounds with an interest in AI. For the project, we’re deploying a simple backend application on a local Kubernetes cluster and intentionally triggering common failures like CrashLoopBackOff, ImagePullBackOff, and OOMKilled, then evaluating how helpful the LLM-generated explanations actually are.</p><p>we’re not training models, not building agents, and not doing autonomous remediation. We’re only using pre-trained generative AI models in inference mode to analyse existing Kubernetes outputs (logs, events, pod descriptions). The models will be served locally using Ollama, and we’re keeping the setup lightweight (e.g. k3s, kind, or minikube).</p><p>I’d really like to hear from people with hands-on Kubernetes experience:</p><ul><li>Have you seen generative AI tools actually help with Kubernetes troubleshooting?</li><li>Where do you think LLMs add value, and where do they fall short?</li><li>Any open-source models you’d recommend for analysing logs and events?</li><li>We’re considering using RAG (feeding in kubectl outputs or docs) to reduce hallucinations , does that make sense in practice?</li></ul><p>Any advice, pitfalls, or lessons learned would be appreciated. Thanks!</p>",
      "contentLength": 1489,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Integration Test: In memory or in a container",
      "url": "https://www.reddit.com/r/golang/comments/1qm120z/integration_test_in_memory_or_in_a_container/",
      "date": 1769293794,
      "author": "/u/matecito123",
      "guid": 38694,
      "unread": true,
      "content": "<p>Is it correct to use an in-memory DB and initialize the connection with GORM, or should I replicate the production DB (Postgres) in a Docker container and point my tests to that DB?</p>",
      "contentLength": 181,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "GateKey - Open Source Zero-Trust VPN with SSO",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qlzlea/gatekey_open_source_zerotrust_vpn_with_sso/",
      "date": 1769290332,
      "author": "/u/jessedye",
      "guid": 38681,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "SIMD programming in pure Rust",
      "url": "https://kerkour.com/introduction-rust-simd",
      "date": 1769286355,
      "author": "/u/kibwen",
      "guid": 38712,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/rust/comments/1qlxulo/simd_programming_in_pure_rust/"
    },
    {
      "title": "Your agent is building things you'll never use",
      "url": "https://mahdiyusuf.com/your-agent-is-building-things-youll-never-use/",
      "date": 1769286051,
      "author": "/u/myusuf3",
      "guid": 38693,
      "unread": true,
      "content": "<p>I built more in two months with agents than in the previous year. I used almost none of it.</p><p>That's the confession nobody in the AI-productivity crowd wants to make. Building feels like progress. The agent churns, the code appears, and you get a hit of satisfaction. But satisfaction isn't value. And building isn't shipping.</p><p>The problem is treating agents like strategy engines when they're execution engines. Its really why we are getting a ton of push back in open source, people writing and reading about agents in code, and even use. </p><p>Point an agent at a vague goal—\"build me a tool that helps with X\"—and you'll get something that looks impressive and rots in a folder. Point an agent at a specific task—\"rewrite these 200 API calls to use the new authentication pattern\"—and you'll save a week.</p><p>One is generative theatre. The other is actual leverage.</p><p>The difference is tactical versus strategic deployment.</p><p>Tactical deployment means applying agents to problems you already understand. The data transformation you've done manually a dozen times. The test coverage you know exactly how to write. The refactor where the pattern is clear but the labor is mind-numbing.</p><p>Strategic deployment means pointing agents at fuzzy goals and hoping they produce something useful. This almost never works. You get slop that demos well and decays fast.</p><p>The fundamental lesson: agents amplify clarity. If you know exactly what you want, they're a force multiplier. If you don't, they're an expensive way to generate garbage you'll never use.</p><p>I call this the Build-Use Gap. Agents have collapsed the cost of building while the cost of using remains unchanged. Integration, polish, workflow fit—these still require human judgment and sustained attention. When you build faster than you can use, you accumulate digital debt that never gets paid.</p><p>The tactical move: reserve agents for tasks you already know how to do that consume disproportionate time. The migration you've mentally mapped but dread executing. The boilerplate you could write in your sleep. The transformation that's clear but tedious.</p><p>Stop using agents to explore what's possible. Start using them to execute what's necessary.</p><p>The productivity gain isn't in what you build. It's in what you actually use.</p>",
      "contentLength": 2256,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qlxpy1/your_agent_is_building_things_youll_never_use/"
    },
    {
      "title": "[Announcement] CachyOS January 2026 Release Changelog",
      "url": "https://www.reddit.com/r/linux/comments/1qlxj56/announcement_cachyos_january_2026_release/",
      "date": 1769285625,
      "author": "/u/lajka30",
      "guid": 38758,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "December in Servo: multiple windows, proxy support, better caching, and more",
      "url": "https://servo.org/blog/2026/01/23/december-in-servo/",
      "date": 1769284175,
      "author": "/u/kibwen",
      "guid": 38721,
      "unread": true,
      "content": "<p>For better compatibility with older web content, we now support  CSS properties like ‘-moz-transform’ (<a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/41350\">#41350</a>), as well as window. (<a href=\"https://github.com/Taym95\">@Taym95</a>, <a href=\"https://github.com/servo/servo/pull/41111\">#41111</a>).</p><p>When using  on Windows, you can now see  and log output, as long as servoshell was started in a console (<a href=\"https://github.com/jschwe\">@jschwe</a>, <a href=\"https://github.com/servo/servo/pull/40961\">#40961</a>).</p><p>Servo diagnostics options are now accessible in servoshell via the  environment variable (<a href=\"https://github.com/atbrakhi\">@atbrakhi</a>, <a href=\"https://github.com/servo/servo/pull/41013\">#41013</a>), in addition to the usual  /  arguments.</p><p>We now use the  by default (<a href=\"https://github.com/Narfinger\">@Narfinger</a>, <a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/40935\">#40935</a>, <a href=\"https://github.com/servo/servo/pull/41179\">#41179</a>), on most platforms.\nIf you don’t want to trust the system root certificates, you can instead continue to use Mozilla’s root certificates with <code>--pref network_use_webpki_roots</code>.\nAs always, you can also add your own root certificates via <a href=\"https://doc.servo.org/servo/opts/struct.Opts.html\"></a>::<a href=\"https://doc.servo.org/servo/opts/struct.Opts.html#structfield.certificate_path\"></a> ().</p><p><a href=\"https://doc.servo.org/servo/struct.Servo.html\"></a>, the main handle for controlling Servo, is now cloneable for sharing within the same thread (<a href=\"https://github.com/mukilan\">@mukilan</a>, <a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/41010\">#41010</a>).\nTo shut down Servo, simply <a href=\"https://doc.rust-lang.org/std/mem/fn.drop.html\">drop</a> the last  handle or let it go out of scope.\n:: and :: have been removed (<a href=\"https://github.com/mukilan\">@mukilan</a>, <a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/41012\">#41012</a>).</p><p>Several interfaces have also been renamed:</p><p>We’ve fixed a crash that occurs when <strong>&lt;link rel=“shortcut icon”&gt;</strong> has an , which affected chiptune.com (<a href=\"https://github.com/webbeef\">@webbeef</a>, <a href=\"https://github.com/servo/servo/pull/41056\">#41056</a>), and we’ve also fixed crashes in:</p><p>Servo is also on <a href=\"https://thanks.dev\">thanks.dev</a>, and already  (+2 over November) that depend on Servo are sponsoring us there.\nIf you use Servo libraries like <a href=\"https://crates.io/crates/url/reverse_dependencies\">url</a>, <a href=\"https://crates.io/crates/html5ever/reverse_dependencies\">html5ever</a>, <a href=\"https://crates.io/crates/selectors/reverse_dependencies\">selectors</a>, or <a href=\"https://crates.io/crates/cssparser/reverse_dependencies\">cssparser</a>, signing up for <a href=\"https://thanks.dev\">thanks.dev</a> could be a good way for you (or your employer) to give back to the community.</p><p>We now have <a href=\"https://servo.org/blog/2025/11/21/sponsorship-tiers/\"></a> that allow you or your organisation to donate to the Servo project with public acknowlegement of your support.\nA big thanks from Servo to our newest Bronze Sponsors: , , and !\nIf you’re interested in this kind of sponsorship, please contact us at <a href=\"https://servo.org/cdn-cgi/l/email-protection#8ce6e3e5e2ccffe9fefae3a2e3feeb\"></a>.</p><h2 tabindex=\"-1\">Conference talks and blogs </h2><p>We’ve recently published one talk and one blog post:</p><p>We also have two  talks at <a href=\"https://fosdem.org/2026/\"></a> in  later this month:</p><p>Servo developers Martin Robinson (<a href=\"https://github.com/mrobinson\">@mrobinson</a>) and Delan Azabani (<a href=\"https://github.com/delan\">@delan</a>) will also be attending FOSDEM 2026, so it would be a great time to come along and chat about Servo!</p>",
      "contentLength": 2059,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/rust/comments/1qlww0l/december_in_servo_multiple_windows_proxy_support/"
    },
    {
      "title": "Latest ChatGPT model uses Elon Musk’s Grokipedia as source, tests reveal",
      "url": "https://www.theguardian.com/technology/2026/jan/24/latest-chatgpt-model-uses-elon-musks-grokipedia-as-source-tests-reveal",
      "date": 1769283994,
      "author": "/u/esporx",
      "guid": 38666,
      "unread": true,
      "content": "<p>The latest model of ChatGPT has begun to cite Elon Musk’s Grokipedia as a source on a wide range of queries, including on Iranian conglomerates and Holocaust deniers, raising concerns about misinformation on the platform.</p><p>In tests done by the Guardian, GPT-5.2 cited Grokipedia nine times in response to more than a dozen different questions. These included queries on political structures in Iran, such as salaries of the Basij paramilitary force and the ownership of the Mostazafan Foundation, and questions on the biography of Sir Richard Evans, a British historian and expert witness against Holocaust denier David Irving in his libel trial.</p><p>Grokipedia, launched in <a href=\"https://www.theguardian.com/technology/2025/oct/28/elon-musk-grokipedia\" data-link-name=\"in body link\">October</a>, is an AI-generated online encyclopedia that aims to compete with Wikipedia, and which has been criticised for propagating rightwing narratives on topics including <a href=\"https://archive.ph/ag2Sh\" data-link-name=\"in body link\">gay marriage</a> and the 6 January insurrection in the US. Unlike Wikipedia, it does not allow direct human editing, instead an AI model writes content and responds to requests for changes.</p><p>ChatGPT did not cite Grokipedia when prompted directly to repeat misinformation about the insurrection, about media bias against Donald Trump, or about the HIV/Aids epidemic – areas where Grokipedia has been widely reported to promote falsehoods. Instead, Grokipedia’s information filtered into the model’s responses when it was prompted about more obscure topics.</p><p>For instance, ChatGPT, citing Grokipedia, repeated stronger claims about the Iranian government’s links to MTN-Irancell than are found on Wikipedia – such as asserting that the company has links to the office of Iran’s supreme leader.</p><p>ChatGPT also cited Grokipedia when repeating information that the Guardian has debunked, namely details about Sir Richard Evans’ <a href=\"https://www.theguardian.com/technology/2025/nov/03/grokipedia-academics-assess-elon-musk-ai-powered-encyclopedia\" data-link-name=\"in body link\">work</a> as an expert witness in David Irving’s trial.</p><p>GPT-5.2 is not the only large language model (LLM) that appears to be citing Grokipedia; anecdotally, Anthropic’s Claude has also referenced Musk’s encyclopedia on topics from petroleum <a href=\"https://x.com/AshitaOrbis/status/1994132818646192199\" data-link-name=\"in body link\">production</a> to Scottish <a href=\"https://www.reddit.com/r/ClaudeAI/comments/1pbdelb/claude_uses_grokipedia/\" data-link-name=\"in body link\">ales</a>.</p><p>An <a href=\"https://www.theguardian.com/technology/openai\" data-link-name=\"in body link\" data-component=\"auto-linked-tag\">OpenAI</a> spokesperson said the model’s web search “aims to draw from a broad range of publicly available sources and viewpoints”.</p><p>“We apply safety filters to reduce the risk of surfacing links associated with high-severity harms, and ChatGPT clearly shows which sources informed a response through citations,” they said, adding that they had ongoing programs to filter out low-credibility information and influence campaigns.</p><p>Anthropic did not respond to a request for comment.</p><p>But the fact that Grokipedia’s information is filtering – at times very subtly – into LLM responses is a concern for disinformation researchers. Last spring, security experts <a href=\"https://archive.ph/kXp4i\" data-link-name=\"in body link\">raised</a> concerns that malign actors, including Russian propaganda networks, were churning out massive volumes of disinformation in an effort to seed AI models with lies, a <a href=\"https://www.theguardian.com/world/2025/nov/21/english-language-websites-link-pro-kremlin-russian-propaganda-pravda-network\" data-link-name=\"in body link\">process</a> called “LLM grooming”.</p><p>In June, concerns were raised in the US Congress that Google’s Gemini repeated the Chinese government’s position on human rights abuses in Xinjiang and China’s Covid-19 policies.</p><p>Nina Jankowicz, a disinformation researcher who has worked on LLM grooming, said ChatGPT’s citing Grokipedia raised similar concerns. While Musk may not have intended to influence LLMs, Grokipedia entries she and colleagues had reviewed were “relying on sources that are untrustworthy at best, poorly sourced and deliberate disinformation at worst”, she said.</p><p>And the fact that LLMs cite sources such as Grokipedia or the Pravda network may, in turn, improve these sources’ credibility in the eyes of readers. “They might say, ‘oh, ChatGPT is citing it, these models are citing it, it must be a decent source, surely they’ve vetted it’ – and they might go there and look for news about Ukraine,” said Jankowicz.</p><p>Bad information, once it has filtered into an AI chatbot, can be challenging to remove. Jankowicz recently found that a large news outlet had included a <a href=\"https://www.thewayfinder.net/p/i-got-trapped-in-the-ai-ouroboros\" data-link-name=\"in body link\">made-up quote</a> from her in a story about disinformation. She wrote to the news outlet asking for the quote to be removed, and <a href=\"https://bsky.app/profile/ninajankowicz.com/post/3lulq7ovmts25\" data-link-name=\"in body link\">posted</a> about the incident on social media.</p><p>The news outlet removed the quote. However, AI models for some time continued to cite it as hers. “Most people won’t do the work necessary to figure out where the truth actually lies,” she said.</p><p>When asked for comment, a spokesperson for xAI, the owner of Grokipedia, said: “Legacy media lies.”</p><figure data-spacefinder-role=\"inline\" data-spacefinder-type=\"model.dotcomrendering.pageElements.GuideAtomBlockElement\"></figure>",
      "contentLength": 4447,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qlwt4l/latest_chatgpt_model_uses_elon_musks_grokipedia/"
    },
    {
      "title": "Confquery: A scriptable command-line utility for editing linux config files like pacman.conf",
      "url": "https://github.com/AmmoniumX/confquery",
      "date": 1769281320,
      "author": "/u/No-Dentist-1645",
      "guid": 38757,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qlvlvn/confquery_a_scriptable_commandline_utility_for/"
    },
    {
      "title": "Floating Point Formatting - Russ Cox Blog Series",
      "url": "https://research.swtch.com/fp-all",
      "date": 1769280501,
      "author": "/u/silenttwins",
      "guid": 38643,
      "unread": true,
      "content": "<p>\nThese are the posts in the “Floating Point Formatting” series,\nwhich started in 2011 and continued in 2026.\n</p>",
      "contentLength": 113,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qlv86l/floating_point_formatting_russ_cox_blog_series/"
    },
    {
      "title": "The New Proton Patch To Make Photoshop's Installer Work Also Works On Bottles. I'm Using Photoshop 2020 as an example.",
      "url": "https://www.reddit.com/r/linux/comments/1qlv1rk/the_new_proton_patch_to_make_photoshops_installer/",
      "date": 1769280111,
      "author": "/u/Underrated_Mastermnd",
      "guid": 38655,
      "unread": true,
      "content": "<div><p>I'm gonna see if Illustrator 2020 also work cause I lowkey need that for work.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Underrated_Mastermnd\"> /u/Underrated_Mastermnd </a>",
      "contentLength": 121,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "The AI Delusion Epidemic",
      "url": "https://medium.com/ai-advances/the-ai-delusion-epidemic-a851e0a4d842?sk=c629df4365a925426dcc5ab851861da2",
      "date": 1769279591,
      "author": "/u/TheDeadlyPretzel",
      "guid": 38644,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qlusy9/the_ai_delusion_epidemic/"
    },
    {
      "title": "What GPU is the BEST for Linux Gaming?",
      "url": "https://youtu.be/u8Xyx2L4Nlg?si=i_1NBnXHAqxd4qYt",
      "date": 1769279378,
      "author": "/u/Putrid_Draft378",
      "guid": 38654,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qlupf0/what_gpu_is_the_best_for_linux_gaming/"
    },
    {
      "title": "Is Go so much fast or it is my machine",
      "url": "https://www.reddit.com/r/golang/comments/1qluisk/is_go_so_much_fast_or_it_is_my_machine/",
      "date": 1769278976,
      "author": "/u/Shot-Calligrapher-99",
      "guid": 38680,
      "unread": true,
      "content": "<p>Hi, recently I build single node key value store as a personal learning project.</p><p>I used golang for it and allowed RESP support so it will be easy to interact with the server. For now I'm supporting only GET, SET and DEL operations. The keys and values can only be strings (ints will be treated as redis) and I've allowed max size of 1000 characters.</p><p>Since it's heavily inspired around Redis, I tested my server using redis-bechmark with GET and SET operations. I've allowed max 12000 concurrent client connections and also there is cleaner that runs every 40 seconds in the background to clean up maps. I've used 32 maps to store the actual data.</p><p>Using redis benchmark I've given 10K concurrent clients and 1M requests I'm getting SET at 143K RPS, p50 is 36ms and GET at 144K RPS, p50 is 35ms</p><p>I'm doing this on my 24 gb, m4 pro macbook pro with golang-alpine image in vs code devcontainer</p><p>I didn't expected it to be that fast. as per gemini redis gives around 100K-120K RPS with ~1-3ms latency under same load.</p><p>does it is context switching that make my application too slow compared to real redis? still it's very fast for me.</p>",
      "contentLength": 1120,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "RustyPP: A C++20 library and Clang tool to enforce Rust-like safety and mutability.",
      "url": "https://github.com/I-A-S/RustyPP",
      "date": 1769278478,
      "author": "/u/I-A-S-",
      "guid": 38692,
      "unread": true,
      "content": "<p>[RENAMED TO Oxide FROM RustyPP]</p><p>I recently started learning Rust and really liked the borrow checking mechanism and more importantly the \"immutable by default\" aspect (among a lot more actually).</p><p>With Microsoft putting Rust in the Windows kernel and Linus approving it for use in the Linux kernel, let's admit it, Rust is becoming an avengers level threat to C++. For a good reason, in this day and age, security and safety has become exponentially more important.</p><p>My goal is promote (and enforce using oxide-validator), the use of good aspects of Rust to C++.</p><p>Here's what Oxide currently offers:</p><ol><li>Single header include: oxide.hpp (this gives you Mut, Const, Ref, MutRef, Result and basic optional type aliases u8, i32 etc.)</li><li>oxide-validator: This a standalone C++ written executable embedding clang to enforce the \"safe\" coding practices.</li><li>oxide-vscode: VSCode extension to give you validator checks in real time as you type</li></ol><p>following are planned but not available yet:</p><ol></ol><p>Oxide is still v0.1.0 btw so the API is not final is subject to changes (tho ofc I will only add breaking changes if the benefit outweighs the cost)</p><p>My hope is to make C++ codebases more secure (and standardized). I love cpp, instead of making Rust my daily driver, I'm trying to bring the genuinely good aspects of Rust to cpp.</p><p>Project is released under Apache v2.</p><p>Any and all feedback is welcome!</p>",
      "contentLength": 1354,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qluakt/rustypp_a_c20_library_and_clang_tool_to_enforce/"
    },
    {
      "title": "[R] Response to CVPR review that claims lack of novelty because they found our workshop preprint?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qltk8x/r_response_to_cvpr_review_that_claims_lack_of/",
      "date": 1769276874,
      "author": "/u/appledocq",
      "guid": 38653,
      "unread": true,
      "content": "<p>We received a weak reject rating from a reviewer whose primary concern was the following:</p><blockquote><p>The major weakness of the paper is the strong overlap with the paper [ICMLW2025]... the paper is not clearly cited anywhere in the new manuscript.</p></blockquote><p>The paper [ICMLW2025] is our own 3-page paper that we presented in a  workshop at ICML 2025 and uploaded to arXiv. This type of workshop explicitly allows re-submission of content to future venues. Our CVPR submission tackles the same idea as the workshop paper but significantly expanded. We did not cite this workshop paper in the CVPR submission so as to maintain double-blind anonymity. For the same reason, we cannot clarify that it is our own paper in the rebuttal.</p><p>What's the best way to handle this? Did we mess up by not citing it somehow in our CVPR submission? I suppose we can write a comment to the AC, but I'm not confident it will be noticed. Ideally I would like the reviewer to also reconsider their rating.</p>",
      "contentLength": 958,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "The Mecha Comet is (finally) available on Kickstarter",
      "url": "https://www.reddit.com/r/linux/comments/1qlt124/the_mecha_comet_is_finally_available_on/",
      "date": 1769275690,
      "author": "/u/ReturningRetro",
      "guid": 38665,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Programming Idea",
      "url": "https://www.reddit.com/r/golang/comments/1qlrg2u/programming_idea/",
      "date": 1769272150,
      "author": "/u/tomiis4",
      "guid": 38623,
      "unread": true,
      "content": "<p>Hi, so little backstory to this post. I started programming when I was still at primary school. It was for about two years, then it's almost 4 years where I haven't been programming because I was burned out, but now I want to start again, maybe create something little but useful but mainly for fun.<p> It started at front-end, but later moved to back-end and CLI applications. That's where I feel in love with that type of programming - not focusing on look (even though I'm capable of something simple and good looking) but mainly function. </p></p><p>For example 3D rendering using JavaScript, many NeoVim plugins using Lua and some simpler using GoLang. It has been one of my most favorite language I have ever tried, but I haven't used it that much for personal projects. I have done couple CLI games, tool but it wasn't something long term or \"big\".</p><p>I want to get back to programming, but I don't have any project ideas which would interest me and could take some time to finish (like month or more). Preferably GoLang, but I wouldn't mind using, or mixing another languages. So my question is what would you recommend me to do, to get back into programming again or something. Thank you.</p><p>My favorites projects I have enjoyed so far were - 3D .obj parsing -&gt; rendering -&gt; rotating website from scratch, with textures<p> - VIM inspired TypeScript CLI text editor</p> - NVim RegExp explaining plugin, from scratch<p> and more, mainly focused on \"technical\" part.</p></p>",
      "contentLength": 1440,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Fresher (CSE) starting AWS + DevOps need guidance + accountability buddies (4 month goal)",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qlpoef/fresher_cse_starting_aws_devops_need_guidance/",
      "date": 1769268089,
      "author": "/u/Naninetha",
      "guid": 38598,
      "unread": true,
      "content": "<p>Hey everyone, I’m a Computer Science engineering graduate.</p><p>After graduation, I honestly wasted some time because I was confused about what path to choose and what I should learn seriously. I kept switching interests and overthinking instead of building real skills.</p><p>Now I’ve finally decided my path: Cloud + DevOps (AWS).</p><p>I started learning properly and I’ve completed Networking basics, and currently I’m learning Linux (commands, permissions, processes, shell basics). Next I’m planning to learn Git/GitHub, Docker, K8s, CI/CD, AWS core services, and Terraform.</p><p>My target is to become job-ready in 4 months with real projects + interview preparation.</p><ol><li><p>Seniors: Please tell me what roadmap/order is best for a fresher like me</p></li><li><p>Freshers: If anyone is also learning, we can stay consistent and build 2–3 projects together</p></li><li><p>Any “must avoid mistakes” you wish you knew earlier</p></li></ol><p>My current plan (tell me if I’m doing it wrong):</p><p>AWS (IAM, EC2, VPC, S3, CloudWatch)</p><p>2–3 projects + resume + interviews</p><p>If anyone wants to connect, comment your timezone + what you’re learning.</p>",
      "contentLength": 1075,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "How do you handle security scanning for ephemeral workloads and init containers?",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qlpfj5/how_do_you_handle_security_scanning_for_ephemeral/",
      "date": 1769267516,
      "author": "/u/No_Opinion9882",
      "guid": 38599,
      "unread": true,
      "content": "<p>Hey everyone, been running into a headache with our security posture on k8s. Our current SAST/SCA tools scan images fine during CI, but we're blind to what's actually vulnerable in runtime. </p><p>The issue: We have tons of init containers, sidecar proxies, and ephemeral jobs that spin up and down. Some pull images we've never scanned, others run with elevated privileges we didn't account for during static analysis. </p><p>Last week we had a vulnerability in a logging sidecar that our pre-deployment scans missed entirely because it was injected by our service mesh. </p><p>How are you folks getting visibility into the actual attack surface of running pods vs just what you scanned in CI? Thanks in advance</p>",
      "contentLength": 691,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Built a 24MB offline ML app with Burn + Tauri - runs on my iPhone at 80ms inference",
      "url": "https://www.reddit.com/r/rust/comments/1qlpaj5/built_a_24mb_offline_ml_app_with_burn_tauri_runs/",
      "date": 1769267190,
      "author": "/u/Commercial-Comb1667",
      "guid": 38652,
      "unread": true,
      "content": "<p>Hey <a href=\"https://www.reddit.com/r/rust\">r/rust</a>, Just finished a project I wanted to share - a plant disease detection AI built entirely in Rust using the Burn framework, deployed to my iPhone 12 via Tauri.</p><p> - 24MB total deployment (vs 7.1GB for equivalent PyTorch) - 0.39ms inference / 2,579 FPS on desktop GPU (RTX 3060) - ~80ms inference on iPhone 12 via Tauri - 38 disease classes, trained with 30% labeled data (semi-supervised)</p><p> The use case required offline inference on devices farmers already own - no cloud, no dedicated hardware. PyTorch was a non-starter: 7GB dependencies, 3s cold start, installation hell. Burn compiles to a single binary and targets wgpu (native GPU), ndarray (CPU), and WASM (browser) from one codebase.</p><p> The CNN is pretty standard, but Burn's derive macros make it clean: #[derive(Module, Debug)] pub struct PlantClassifier&lt;B: Backend&gt; { conv1: ConvBlock&lt;B&gt;, conv2: ConvBlock&lt;B&gt;, conv3: ConvBlock&lt;B&gt;, conv4: ConvBlock&lt;B&gt;, global_pool: AdaptiveAvgPool2d, fc1: Linear&lt;B&gt;, dropout: Dropout, fc2: Linear&lt;B&gt;, }</p><p>4 conv blocks (32→64→128→256), BatchNorm + ReLU, GlobalAvgPool, then FC layers. The  macro handles all the weight serialization and device placement automatically.</p><p><strong>The Semi-Supervised Learning Part</strong></p><p>Labeled agricultural data is expensive (~€2/image for expert annotation). We used pseudo-labeling with a configurable confidence threshold: #[derive(Debug, Clone, Serialize, Deserialize)] pub struct PseudoLabelConfig { pub confidence_threshold: f64, // Default: 0.9 pub max_per_class: Option&lt;usize&gt;, // Prevent class imbalance pub retrain_threshold: usize, // Min samples before retrain pub curriculum_learning: bool, // Start strict, relax over time }</p><p>Train on 30% labeled → predict on 70% unlabeled → accept predictions &gt;90% confidence → retrain. Result: accuracy comparable to 60% fully-labeled.</p><p>This was the fun part. Tauri 2.0 wraps the Burn model in a native iOS app: cargo tauri ios build 80ms inference on the A14 chip. The Rust backend does all the ML, the UI is just HTML/JS. It's <em>actually running Rust on an iPhone</em>.</p><p> - Burn is genuinely production-ready for inference workloads -  with the  release has solid CUDA 13 support - WASM performance is better than expected (~80ms on mobile Safari) - Compile times are... Rust compile times.  +  = 5+ min release builds - Tauri mobile is legit - one codebase, native perf</p>",
      "contentLength": 2332,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Dithering for an epaper laptop",
      "url": "https://peterme.net/building-an-epaper-laptop-dithering.html",
      "date": 1769266248,
      "author": "/u/PMunch",
      "guid": 38697,
      "unread": true,
      "content": "<p>Epaper screens come in many variants. The classic one that most people think about is only black and white. However most newer panels are at least capable of greyscale, and the best in class are colour. But the colour and greyscale isn’t “true” colour and greyscale, but rather they have a set of greyscale levels or a small set of colours they can display. Some digital signage variants have black, white, and one or two contrast colours, other variants like the ones found in e-book readers either have a set of colours they can display, or fairly low-fidelity RGB (compared to normal screens). The panel I’ve chosen for this project is black and white and capable of 16 levels of greyscale. Part of the reason for this choice was of course availability at the time, but also that epaper panels with colour is considerably slower and often offers lower contrast. The panel I’ve chosen can be driven very quickly in black and white mode, and slower in greyscale mode. This means that there’s a trade-off to be made about speed and graphical fidelity. Use cases such as e-book readers or picture frames typically trade for graphical fidelity. If you’re going to spend a minute to read a page of text you don’t care about a couple extra tenths of milliseconds to render it nice and crisp. And if you are updating a picture that will stay on a wall potentially for days you don’t care about a couple seconds of refresh time. Making these trade-offs require that you have the ability to reason about the usage patterns of the underlying system so that you’re able to design around the limitations you choose. For a general purpose computer which more often than not will run software that isn’t optimised for epaper I considered speed to be more important. In this article I will discuss a lot of various dithering algorithm, unfortunately the article would be twice as long if I went into detail about every single one. If you want to know more about many of these algorithms I recommend <a href=\"https://tannerhelland.com/2012/12/28/dithering-eleven-algorithms-source-code.html\">Image Dithering: Eleven Algorithms and Source Code</a> by Tanner Helland as a good starting point. After that you should be able to keep up, albeit with some occasional Googling to look up specifics on the algorithms he don’t mention. I go into the basic concept of the various methods however, so you should be fine reading this as a stand-alone article as well.</p><p>For maximum speed we’re stuck with only using black and white, and even if we wanted to implement a full-fidelity mode we would be stuck with only 16 levels of greyscale. And this is where dithering comes in. The most naïve way to turn an image into only black and white would be to apply a simple threshold, any pixel darker than half grey would appear black, and any pixel brighter would appear white:</p><p>As you can see this doesn’t look particularly good, we are able to make out the subject, but a lot of detail is understandably lost. However if we apply some dithering we trick our brains into seeing more details:</p><p>This image is still only composed of black and white pixels, but now the density of the black pixels makes our brain think there are more levels of greyscale than what is actually there.</p><p>Going into this project I thought that dithering would be one of the simple parts, after all dithering has been used for ages to improve graphical fidelity in systems with limited palettes. So we could simply pick whatever is the “best” dithering algorithm and off we go! Unfortunately this wasn’t the case. Many of the reasons seems to stem from when these approaches where made and the hardware they where made for, but some of them are more technical in nature. One of these historical issues is that these algorithms were mostly designed to display images on black and white screens in an otherwise controlled environment. So all the menus, windows and buttons would be carefully hand drawn, and the dithering algorithm would only be called upon to show images. This means that a lot of dither algorithm evaluation and comparison is done on simple images like the ones shown so far in this article. This isn’t the system we will be using the algorithm in however. Of course I’ll carefully set up the theme and icons to work well on the display. But what other programs, and maybe especially the web, does is out of my control. Therefore I’ve constructed a test image which I’ll use throughout this article which combines an image, some gradients, some solid coloured blocks, and some text on various backgrounds:</p><p>To simulate text that could be found in actual interfaces the “Hello” words are either completely black or completely white, and the “world” words are set with a colour selected to be the lowest possible level of contrast that still achieved “good” on <a href=\"https://coolors.co/contrast-checker/\">Coolors contrast checker</a>. We’re unfortunately not always so lucky as to have “good” contrast in UIs and on the web nowadays, but if a designer actually chose a good contrast we should try to render it well.</p><h2>The trouble with error-diffusion</h2><p>The dithering algorithm used in the example above is a fairly classic algorithm called Floyd-Steinberg. Here it is again on my test image:</p><p>Floyd-Steinberg is a so called error-diffusion algorithm that works by going through all the pixels from the top left and as pixels are quantised to black or white it spreads the error of the quantisation to nearby pixels. It’s a bit hard to explain without a visual aid, so I made this:</p><p>As you can see the pixels are processed from the top left going left to right (in grey), then the pixel currently being processed (in dark red) spreads the error out to nearby pixels. There are multiple of these algorithms with different ways to spread the error which I’ve also included in the image. So Atkinson dithering spreads the error to the same pixels as Floyd-Steinberg, but also two extra pixels. And Jarvis, Judice, and Ninke spreads it out to a whole lot more pixels. The error also isn’t spread out evenly, and in the case of the Atkinson dither not all of the error is spread out.</p><p>When processing a pixel the original colour of the pixel is added together with the error and the result is quantised. This leads to an image where shades of grey are smoothed out nicely, and sharp edges still retain a hard contrast. It is also quite a fast approach for linear computation, i.e.&nbsp;running on a CPU, processing one pixel at a time. But if you want to parallelize it you run into issues with pixels being dependent on the result of the pixels that come before them in the image. Change the top left pixel and you risk having your entire image change. This is no good when we want to compute multiple pixels simultaneously.</p><p>Another issue is that things like moving the mouse or a blinking cursor could cause a lot of changes in the output image because the change in error propagates through the image. These two pictures look almost identical to us, the only difference in the input image is that the cursor moved one pixel to the right and down, but on the right you can see the difference in the Floyd-Steinberg output:</p><p>This would be noticeable enough as flickering pixels on a normal display, but when you factor in the slight ghosting of an epaper screen you end up with a persistent grey “lightning” across the screen. With this ghosting the difference in the third image would be clearly visible on the screen even after the movement stopped.</p><p>The underlying problem, which I’ll call the dependency problem as it is caused by pixels being dependent on the output of previous pixels, is present in all Floyd-Steinberg style algorithms. And there are quite a few of these, after all they are typically the kind of algorithm which produces the best visual results. In terms of the dependency problem there are some slight differences though. Jarvis, Judice &amp; Ninke is more affected because it spreads error to more pixels; Atkinson is slightly better because it “loses” error; and Riersma, which diffuses error along a Hilbert curve instead of out from the top left, is slightly less affected but performs worse in general and is even harder to parallelize. The dependency problem is the first of the technical issues, and maybe the most critical one.</p><p>Another issue with these algorithms is what I call the warm-up problem. Since the algorithm works by “pushing” error ahead of the processed pixel there are cases where the context stored in this error doesn’t match the current part of the image. If you look at the top left of the sample image you can see that the area of white pixels almost have a rounded appearance. The gradient in the source image is completely linear and this effect is simply caused by the algorithm not having any error context to pull from for these pixels. This means that until sufficient error is accumulated the algorithm doesn’t know what to do. There is a similar phenomenon whenever the image switches context, most visibly in this image in the first grey block on the bottom next to the completely black one. The image above is bright, and with the sudden change to dark grey the top-right row of pixels end up completely black creating a visible artefact.</p><p>These might seem like nitpick on an otherwise great algorithm, but both these problems are caused by a general dependency of context. In addition to being bad for parallelization this causes another problem. Since epaper panels are static once they are updated many of them support partial updates meaning that only the area of the screen where something changed will be updated. This saves a lot of power and one of the major goals of an epaper laptop would be great battery life. In addition to saving power it also means spending less time on each update, which directly impacts input latency. So we definitely want to take advantage of partial screen updates! With an error diffusion algorithm it is much harder to perform these partial updates because in order to dither a small region of the image not only must we have the context for the part we want to update, but we also need to potentially update everything below/around it.</p><p>Out of all the algorithms in this category I found Riersma was the most promising in terms of the dependency and warm-up problems, but it is very hard to parallelize and do partial updates with. Jarvis, Judice &amp; Ninke might perform a hair better than Floyd-Steinberg in graphical fidelity (especially in some edge cases with light gradients). Atkinson dithering looses a bit of error as it goes along, effectively changing the contrast of the image slightly while easing some of the dependency problem. But all of these suffers from the “lighting strike” effect whenever pixels are updated, so for the general part of the renderer they aren’t a good match. And while they are easier to parallelize than Riersma they’re still not great. I’ll keep them around for high-fidelity modes though as there are use cases where I want to look at static images and they still produce the best results, especially for the 16 shades of grey mode which is already slow enough that you wouldn’t want to use it for much else anyways.</p><p>Fortunately error-diffusion isn’t the only thing that can be used to dither an image. Imagine we want to turn an image composed of a single shade of grey into a dithered version. If the shade is about 30% black then we’d expect 30% of the pixels in the output to be black. The problem is just how we select these 30%. One approach would be to determine the threshold for a pixel to turn black or white depending upon where in the image it is, making sure to pick these thresholds such that they match our intuition about percentages of pixels. This is what’s called ordered dithering, pattern dithering, or positional dithering. A big benefit to not having any context (save for the pattern itself) is that every pixel can be individually processed in parallel on a GPU. On an epaper laptop I assume the GPU will go mostly idle anyways, and since almost all general purpose CPUs nowadays comes with a GPU it would be a shame not to utilise it. For such ordered dithering there are many different patterns we can use, perhaps the most well known being a <a href=\"https://en.wikipedia.org/wiki/Ordered_dithering\">Bayer matrix</a>. Compared to error-diffusion algorithms though, the result is a bit lacklustre: <img src=\"https://uploads.peterme.net/test-image_by4.png\"></p><p>This doesn’t suffer from the dependency problem, nor the warm-up problem. But we have some new problems, the first of which I call the pattern problem. As you can see the pattern of the dithering mask is clearly visible in this image, which can be quite distracting. The error-diffusion algorithms also suffers from various degrees of the pattern problem, but it’s more obvious here as the Bayer matrix is a very small pattern, so the repetition is clearly visible. An alternative to avoid the obvious patterning is to use a blue noise pattern: <img src=\"https://uploads.peterme.net/test-image_bn.png\"></p><p>Because the “pattern” here is just noise it is no longer as discernible in the output, in this image the pattern is the same size as the image. If you tiled a smaller pattern you could be able to discern some structure, especially in large single colour areas, but a lot of the repetition is hidden by the noise. Since the size of the monitor is known I can easily just create a blue noise pattern for the entire screen, so this test is more realistic. The second problem is what I call the transition problem. Since there is no context from surrounding pixels hard edges are often softened up. Especially the text is harder to read compared error-diffusion dithering because the hard edges are preserved better and even the Bayer matrix performs better because the pattern is much less organic. The boxes around the text and along the bottom also has fuzzier borders, and even Davids sharper features are a bit softened. Applying a different threshold to every pixel like what both the Bayer matrix and blue noise does will indeed give us the right amount of black pixels, but in a way it means that we lose resolution. The sharp transitions from one colour to another doesn’t quite resolve properly and appear noisy in the output image.</p><p>Another algorithm fitting in with ordered dithering is <a href=\"https://pippin.gimp.org/a_dither/\">a dither</a> which uses some simple functions to generate what is effectively a pattern on demand. This is nice if you don’t have memory for the pattern, but for my use case it would probably be faster to use the algorithms to generate a pattern and then do the dithering on the GPU. The visual results are somewhere between Bayer and blue noise, here the 3rd pattern is shown: <img src=\"https://uploads.peterme.net/test-image_ad3.png\"></p><p>There is a bit of the patterning problem, although less than with the Bayer matrix. There’s also some of the transition problem, but the increased pattern means it appears less like a blur and more like an artefact for better or worse.</p><p>I’ve spent a lot of time trying to squeeze more graphical fidelity out of these ordered dithering methods. For example the resolution of the panel I’m using is high enough that a perfect chequerboard looks almost indistinguishable from the 50% grey colour in the 16 colours mode. This means that the Bayer matrix dithering actually looks better on the panel itself than if you’re looking at this on a regular screen. It also means that the blue noise pattern looks slightly worse since it never features this pattern. So I tried to create a hybrid, a blue noise pattern, but reorganised such that it would create chequerboard grey. <img src=\"https://uploads.peterme.net/test-image_bnp.png\"> It works in the sense that it creates that nice chequerboard for the 50% grey, but it now has some clustering which doesn’t look great. All in all I’d say it’s about par on the real panel, maybe only a touch better. When using methods that feature chequerboard grey, which the Floyd-Steinberg method also does, I’ve found that applying a slight filter that pushes colours towards full black, half grey, and full white will make certain scenes better. Certainly something I will experiment more with in the future.</p><p>I also tried to use some edge-detection algorithms and use the result of those to influence a blue noise pattern, in this way hoping to get slightly crisper edges while still retaining that organic look. <img src=\"https://uploads.peterme.net/test-image_bn_edge_50.png\"> It does work, but I’ve struggled with tuning all the parameters right to get a good effect. The pattern used for the above image is made by creating an edge-detection of the input manually in GIMP with the “Difference of Gaussians” method which was then level adjusted to be more more black and white. This edge-detection was then used as a mask for a half grey image over the blue noise pattern. A half grey pattern is the same as simple thresholding so essentially this makes it so that hard edges switches to thresholding while the rest is blue noise dithered. The effect is subtle, but it does reduce the transition problem making the text slightly easier to read, and the crispness of Davids features are slightly improved. However these edge-detection algorithms aren’t computationally cheap, and I’m not sure if the juice is worth the squeeze so to say.</p><h2>New GPU-based error diffusion?</h2><p>So error-diffusion creates pretty results but doesn’t really work for this use-case, and the ordered dithering which is easy to use for this use-case suffers quite a bit quality wise. So what can we do, is there some kind of middle ground thing we could use? As I mentioned in the introduction most dithering algorithms where designed a long time ago because they where relevant for monochrome monitors back in the days (or even just limited palette displays). So they are optimised for that era of hardware. Floyd-Steinberg for example touts that it can dither the image in a single forward pass, and the division factors are calculated such that you can do the maths with simple bit shifts. And don’t get me wrong, this is very cool and actually means that you can run these algorithms on a single thread with decent performance. But nowadays we have GPUs that can render realistic lighting at over hundred frames a second. So can we leverage this resource which will mostly be dormant on this machine anyways? Implementing ordered dithering algorithms on the GPU is pretty trivial as each pixel is individually processed. But as I discussed before the normal error-diffusion algorithms are hard to get running with good performance and aren’t all that suited for my use case anyways because of the large updates and ghosting. There have been a little interest in the field of GPU based dithering. Some papers have been written about it, but they are mostly about running existing algorithms on new hardware. There have been a few algorithms built specifically for GPUs, but many of those are simply the same error-diffusion techniques but applied to chunks in various patterns. For example applying Floyd-Steinberg in small circular chunks instead of across the entire image. These do solve the whole image-spanning dependency issues, but the individual chunks still suffer from the warm-up problem which means that they are often very obvious. There are also some great algorithms that do error-diffusion on the GPU but which just simply takes too long, including one paper that touted doing about 4x the resolution I’m looking at in 7.2 seconds using an Nvidia GTX 780Ti. The results look good, but I neither have the horse-power nor the time to use something like that for a real-time system.</p><p>I tried my hand at creating something myself, inspired by both ordered dithering techniques and error-diffusion algorithms. It is created for GPUs and works by splitting the image into very small chunks (like 3x3 or 5x5), then considering how many black pixels that chunk should contain by the average grey level and placing those pixels on the darkest original pixels while also applying a small bit of noise and diffusing some error to nearby pixels to avoid clustering. The results are decent, words are crisp, and so are Davids features, but you can tell that it is done in chunks and it struggles a bit with hard transitions.</p><p>It’s not terrible however and the idea of a GPU based dithering algorithm stuck with me. And in my seemingly infinite search for a dithering algorithm that could work I stumbled upon an algorithm called <a href=\"https://liamappelbe.medium.com/dizzy-dithering-2ae76dbceba1\">Dizzy Dither</a>. It does error diffusion, and the author compares the result to blue-noise dithering. However in the article introducing the algorithm they only ever test it on a picture. In my benchmark I would actually say that it outperforms blue-noise dithering. <img src=\"https://uploads.peterme.net/test-image_app.png\"> The general organic, noisy feel of blue-noise dithering is still there, but text gets better sharpness, the features of David are made a bit more distinct. On top of that it doesn’t suffer from neither the dependency problem nor the warm-up problem. The way it works is by randomly selecting pixels to dither, diffusing errors to nearby pixels. This means that only some pixels will have error taken into account while dithering, about three quarters in my testing. It also means that error, while it theoretically can traverse the entire image, tends to stay very localized. Doing the same test with the cursors from above we can see this clearly:</p><p>Since error doesn’t propagate far the dependency problem is much smaller and partial updates can simply be done by just padding the affected region a bit. And since it isn’t troubled by the warm-up problem even not increasing the area will lead to completely passable results. When considering all of this I had an epiphany. If the dependency problem is so small, and many pixels don’t even see any errors before being dithered it should be possible to handle more than one pixel at a time. And sure enough, by creating a pattern consisting of multiple layers of randomly selected pixels until the entire image is filled the algorithm can be run on the GPU! Each layer contains the same amount of pixels, and no two pixels in any given layer can have their error propagation region overlap (otherwise the GPU would try to write to the same memory at the same time for two different pixels). Other than that it’s pretty straight forward, and the visual results are pretty much the same as the single-threaded version. Depending on how big the pattern is you do get some patterning on large fields of single-coloured areas, but by simply improving the randomness of the pattern generation process I hope to be able to fix this.</p><p>When comparing the speed of these algorithms it really comes down to CPU vs.&nbsp;GPU processing. For example performing Floyd-Steinberg is about 1.9x slower than doing blue noise ordered dithering on the CPU in my naĩve implementation. But performing Dizzy Dither and blue noise dithering (speed on the same order of magnitude) on the GPU is about 280x faster than CPU bound Floyd-Steinberg for an image of the same resolution as the monitor I’m going to use for my project. So dithering on the GPU (or even on specialized hardware) is definitely the way to go for a project like this.</p><p>As you might have noticed this is something I’ve been thinking about a lot, this might even be my longest article to date. And in fact this isn’t even the first time that I was “done” researching the topic and ready to select an algorithm just to nerd-snipe myself as I was typing up my findings. I’ve done a lot of research into all kinds of different algorithms and implemented a whole lot of them to compare results on the test-image used in this article. What’s presented here is only a small sample of the things I’ve tried and looked into, a lot of the things I looked at where either just dead ends, or weren’t interesting enough to warrant making this article longer. To allow my work to be more easily replicated and improved I will make the code for my algorithm tester <a href=\"https://github.com/PMunch/dithering\">available on GitHub</a> if anyone wants to poke around with these algorithms. I believe there are yet some great dithering algorithms to be discovered, and by using GPUs or maybe even NPUs I think we can get good results at great speeds. If anyone ends up developing something interesting in this space, please let me know! But for now I think I will finally close the chapter on dithering. For the epaper laptop I will go for using my GPU implementation of Dizzy Dither for normal usage, and then include other algorithms like Floyd-Steinberg for high-fidelity modes. I will also do some more tricks to improve the actual experience of using the laptop in an upcoming article on writing the driver and integrating the dithering algorithms. Since the entire thing will be running alongside the OS and not simply as some dumb screen we have a lot of options for handling windows, the cursor, and using hotkeys to improve upon common pitfalls of epaper computers, so stay tuned for that!</p>",
      "contentLength": 24656,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qlowl6/dithering_for_an_epaper_laptop/"
    },
    {
      "title": "Faking resources on a K8S cluster",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qln4vz/faking_resources_on_a_k8s_cluster/",
      "date": 1769261725,
      "author": "/u/Consistent-Company-7",
      "guid": 38566,
      "unread": true,
      "content": "<p>I'm working on a piece of code that needs to read Nvidia MiG resources off the K8S node, and pick one of them. Is there any way I can fake these resources if I don't have 20-30k to spend on a GPU? I was thinking of building another program for that, but was wondering if there is an easier way.</p>",
      "contentLength": 294,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "[R] Missed ICML deadline. It's over for me boys.",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qlmm5t/r_missed_icml_deadline_its_over_for_me_boys/",
      "date": 1769260295,
      "author": "/u/confirm-jannati",
      "guid": 38640,
      "unread": true,
      "content": "<p>Polished the hell out of the paper.</p><p>Missed the abstract registration deadline because I... dosed off.</p><p>Anyway, the damage is done. So I guess my question now is---wait for NeurIPS or just submit earlier somewhere else?</p>",
      "contentLength": 215,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Owner of big gaming platform can't believe how bad Windows 11 is – and hints are dropped about big things for Linux gamers this year",
      "url": "https://www.techradar.com/computing/windows/owner-of-big-gaming-platform-cant-believe-how-bad-windows-11-is-and-hints-are-dropped-about-big-things-for-linux-gamers-this-year",
      "date": 1769259789,
      "author": "/u/LicenseToPost",
      "guid": 38552,
      "unread": true,
      "content": "<ul><li><strong>Two GOG execs were interviewed and asked about the backlash against Windows 11 and increased interest in Linux</strong></li><li><strong>The owner said, \"I'm really surprised at Windows. It's such poor-quality software and product, and I'm so surprised that it's [spent] so many years on the market. I can't believe it!\"</strong></li><li><strong>And the managing director observed that Linux was \"one of the things that we've put in our strategy for this year to look closer at\"</strong></li></ul><p>We've heard from a pair of the top execs behind GOG – a popular gaming platform focused on classic titles, hence the acronym 'Good Old Games' – and they made some withering comments about <a data-analytics-id=\"inline-link\" href=\"https://www.techradar.com/tag/windows-11\" data-auto-tag-linker=\"true\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.techradar.com/tag/windows-11\">Windows 11</a>, as well as dropping hints about how <a data-analytics-id=\"inline-link\" href=\"https://www.techradar.com/best/best-linux-distros\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.techradar.com/best/best-linux-distros\">Linux</a> is going to become more important for GOG in 2026.</p><p>The execs in question are the new owner of GOG, Michał Kiciński, and the managing director, Maciej Gołębiewski, who were <a data-analytics-id=\"inline-link\" href=\"https://www.pcgamer.com/software/windows/gogs-new-owner-cant-stand-windows-either-its-such-poor-quality-software-i-cant-believe-it/\" target=\"_blank\" data-url=\"https://www.pcgamer.com/software/windows/gogs-new-owner-cant-stand-windows-either-its-such-poor-quality-software-i-cant-believe-it/\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">interviewed by PC Gamer</a>.</p><a aria-hidden=\"true\" data-url=\"\" href=\"https://www.techradar.com/computing/windows/owner-of-big-gaming-platform-cant-believe-how-bad-windows-11-is-and-hints-are-dropped-about-big-things-for-linux-gamers-this-year\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><p>Our sister site asked about the backlash against Windows 11 – which has reached new heights since <a data-analytics-id=\"inline-link\" href=\"https://www.techradar.com/tag/microsoft\" data-auto-tag-linker=\"true\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.techradar.com/tag/microsoft\">Microsoft</a> started pushing AI even harder in the OS late last year – and the increasing interest in Linux as a result (which was already sparked by the success of SteamOS on <a data-analytics-id=\"inline-link\" href=\"https://www.techradar.com/best/best-handheld-games-consoles\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.techradar.com/best/best-handheld-games-consoles\">handhelds</a>).</p><p>Kiciński said, \"I'm really surprised at Windows. It's such poor-quality software and product, and I'm so surprised that it's [spent] so many years on the market. I can't believe it!\"</p><p>Kiciński doesn't run Windows 11, you may not be surprised to learn – he uses macOS – but does have to fix the PCs of his parents sometimes.</p><p>The owner further explained, \"I sometimes have to fix my mum's computer or my father's computer with Windows, [and] like, it's unbelievable… So I'm not surprised that people gravitate outside of the Windows ecosystem.\"</p><p>It was Gołębiewski, however, who dropped the big hint about Linux, when questioned on gamers embracing it as an alternative to Microsoft's OS.</p><p>The managing director said that Linux was \"one of the things that we've put in our strategy for this year to look closer at\", but refused to elaborate further, noting, \"I don't want to commit to any specifics, but certainly you will see this trend, and we also see that Linux is close to the hearts of our users, so we probably could do better on that front, and that's something that we'll be looking at.\"</p><a aria-hidden=\"true\" data-url=\"\" href=\"https://www.techradar.com/computing/windows/owner-of-big-gaming-platform-cant-believe-how-bad-windows-11-is-and-hints-are-dropped-about-big-things-for-linux-gamers-this-year\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><h2>Analysis: Linux is building up steam (or is it the other way round?)</h2><figure data-bordeaux-image-check=\"\"></figure><p>That compact device could be a landmark moment for easy and convenient living room gaming, potentially, and so given all this, it's no real surprise that GOG would be looking at Linux more closely for 2026 – and beefing up support for games on this platform.</p><p>What's a bit more surprising is the heat that Windows 11 takes here, with the owner of GOG pulling no punches in the assessment of Microsoft's OS. Of course, part of what's \"unbelievable\" for Kiciński is how Windows is \"such poor-quality software\" given that it's been on the market for over 30 years now. (And indeed it's existed longer than that, but not as a full-blown <a data-analytics-id=\"inline-link\" href=\"https://www.techradar.com/tag/operating-system\" data-auto-tag-linker=\"true\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.techradar.com/tag/operating-system\">operating system</a>, just as an interface overlay on top of DOS).</p><p>He is, of course, not isolated in firing flak at Windows 11, which was seen as a step back from <a data-analytics-id=\"inline-link\" href=\"https://www.techradar.com/tag/windows-10\" data-auto-tag-linker=\"true\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.techradar.com/tag/windows-10\">Windows 10</a> by many. Mainly because the performance of the newest OS was lacking compared to its predecessor in some respects – particularly with search and File Explorer, and it still is to this day – plus a bunch of features got dropped with Windows 11 (although a good few have been added back since the OS launched in 2021).</p>",
      "contentLength": 3444,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qlmfny/owner_of_big_gaming_platform_cant_believe_how_bad/"
    },
    {
      "title": "[MEDIA][TUI] try-rs - A project/experiment organizer that makes life much easier.",
      "url": "https://www.reddit.com/r/rust/comments/1qlm834/mediatui_tryrs_a_projectexperiment_organizer_that/",
      "date": 1769259182,
      "author": "/u/_allsafe_",
      "guid": 38755,
      "unread": true,
      "content": "   submitted by   <a href=\"https://www.reddit.com/user/_allsafe_\"> /u/_allsafe_ </a>",
      "contentLength": 32,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Has anyone been able to display small images with good quality in a TUI?",
      "url": "https://www.reddit.com/r/golang/comments/1qllypt/has_anyone_been_able_to_display_small_images_with/",
      "date": 1769258378,
      "author": "/u/YerayR14",
      "guid": 38584,
      "unread": true,
      "content": "<p>Hi, I have been working lately on a e-Book management TUI, similar to apps like calibre-web, Kavita... but for the terminal and, right now, I am stuck with the quality of the covers.</p><p>My main objective right now is to display the books' covers in a grid view in each widget, but after many changes, I am not achieving a good quality image without the blur and softness that they have right now.</p><p>I have tried to change the parameters of both packages, used diferent tools that they offer like Sharpen, Blur, AdjustSigmoid... tried to change the size of the cell/widget, tried to get the actual pixel of a cell in my terminal with the unix package and used it in the targetPixel vars, tried manually pixels set up and not dynamic like the one I am using right now, and some others attempts I have tried that I have probably forget by now.</p><p>The only way I have achieved a better quality cover is setting up the pixels to 400x600 (almost all the covers have an original shape of 1200x1800 (2:3)), but, obviously the cover was so big that it was not a good option for my objective.</p><p>Does anyone has been able to display small/medium sized images in the terminal with good quality? Is it possible? I am going in the wrong direction trying to solve this problem?</p><p>EDIT: After many hours I finally was able to achieve the quality I was seeking for:</p><p>For anyone interested, the key was in how many pixels a cell on the terminal has (terminals are composed of cells, these are like little containers composed at the same time of a tiny canvas of pixels. If I am mistaken, please correct me without any problem), then multiply the value with the current width/height of my widget to get the pixel target for the images. This and using a better set of features that the go-termimg and imaging packages offer made the final shift.</p><p>Now I have to see if the quality changes with different terminal sizes and different widget composition for the library. Thanks!</p>",
      "contentLength": 1934,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "New benchmarks show Linux gaming nearly matching Windows on AMD GPUs",
      "url": "https://www.reddit.com/r/linux/comments/1qlktc7/new_benchmarks_show_linux_gaming_nearly_matching/",
      "date": 1769254635,
      "author": "/u/Putrid_Draft378",
      "guid": 38546,
      "unread": true,
      "content": "<p>\"A recent benchmark from PC Games Hardware suggests that, at least for some games, Proton has nearly eliminated the performance cost of running Windows code on Linux. AMD Radeon RX 9000 GPU owners uninterested in online games should seriously consider switching to Linux.</p><p>The outlet tested 10 games on 10 graphics cards to compare Windows 11 performance with CachyOS, an Arch Linux distro that comes packaged with gaming-specific optimizations. Although Windows remains ahead in most titles, especially on Nvidia graphics cards due to the lack of proper Linux GeForce drivers, Linux achieves some notable victories.\"</p>",
      "contentLength": 615,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Making Visual Scripting for Bash (Update) (GUI Warning)",
      "url": "https://www.reddit.com/r/linux/comments/1qlktaq/making_visual_scripting_for_bash_update_gui/",
      "date": 1769254631,
      "author": "/u/Lluciocc",
      "guid": 38582,
      "unread": true,
      "content": "<p>Hi, like I said in the title, Im trying to make Bash easier to understand for everyone by developing a solution using visual scripting (UE5 inspired). This project is for fun so its made Python and Qt, I believe this project could have a good educational purposes and making Bash more 'friendly'. I have already made <a href=\"https://www.reddit.com/r/linux/comments/1qbzcjr/making_visual_scripting_for_bash/\">a post</a> for this project and everyone gave so many idea and tweaks to help me (and I would thanks everyone for that). So I have implemented some of them like tool-tips and highlights.. Moreover, Im trying to make the code \"easier to fork\" (sorry I don't have the right word for it), if someone wants to fork the project and making his own version, some things are already easy to implement like adding new nodes is quite simple. I plan for the future to make like the \"reverse\", import a Bash script and convert into nodes but right now Im focusing on making nodes and then having the Bash code.<p> Also I have some questions for you, would you use such a project ? Would a wiki on GitHub on how to use the tool (and how the code works) be useful ? And finally, the icon im using are from </p><a href=\"https://pictogrammers.com/library/mdi/\">here</a>, can i use them in my project ? (im already citing them in my credits but Im wondering)</p><p>Im leaving the repo link for anyone who wants to see more about Its made, remember this is WIP:</p>",
      "contentLength": 1289,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "DeGoogled phones, made in Europe: Fairphone, Volla, SHIFTphone, Punkt – a full review.",
      "url": "https://tuta.com/blog/degoogled-phones",
      "date": 1769254393,
      "author": "/u/nix-solves-that-2317",
      "guid": 38581,
      "unread": true,
      "content": "<ol></ol><h3>Honorary mention: GrapheneOS</h3><p>This review will focus on these five smartphones that are developed in Europe and work as solid replacements to Google’s Pixel phones. That said, we must include one honorary mention: . GrapheneOS is an Android OS that  runs on Google’s Pixel phones. But as the Android Source Tree for Google’s Pixels is no longer open source, the GrapheneOS team is looking to adapt the OS to other phones - which will make it a hot contender for this article.</p><p>Even now, GrapheneOS is a great choice when freeing yourself from the tracking of Google apps as it’s an alternative open source operating system, and well known for its focus on security and privacy, for instance it has a hardened memory safety and exploit mitigation. However, even if you do not use any Google apps on your Pixel device powered by GrapheneOS, you still hold a Google phone in your hand, having paid money to Google when buying it. That’s why GrapheneOS is not included in this review, but it must be noted that it’s one of the most secure operating systems out there.</p><blockquote><p>GrapheneOS exists to provide a high level of privacy and security. GrapheneOS only connects to GrapheneOS servers by default with a lot of control over that. GrapheneOS provides far stronger exploit protections making it impossible to exploit a large subset of remote vulnerabilities and far harder to exploit nearly all of the rest.”</p></blockquote><h3>Practical notes for deGoogled phones</h3><p>Obviously, when choosing a deGoogled phone, you will not want to get your apps from Google Play. Fortunately, there are great app store alternatives such as F-Droid, Aurora, and App Lounge in /e/OS that you can use instead. Sometimes it will be necessary to sideload your favorite Android app, meaning you need to get it directly from an app developers website as they offer their apps only via Google Play and not the alternatives. It can get complicated with apps such as banking apps that heavily rely on Google Play for Android, but some phone makers offer workarounds for this. It’s also possible that Google-dependent functionalities such as push notifications handled by Google’s FCM will not work on your deGoogled smartphone.</p><p>When it comes to security and privacy, it’s important to also think about security updates and patches. Some of the here reviewed operating systems for deGoogled Android phones are criticized for being slow on delivering critical security patches. When it comes to patching security vulnerabilities quickly, GrapheneOS is the best one.</p><p>But now, let’s look into the top five phones, designed in Europe, to free yourself from the Google ecosystem!</p><p><strong>The Volla Quintus is the best choice when you want a multi-boot experience. It’s an Android powered by  VollaOS (with multi-boot option) or Ubuntu Touch.</strong></p><p>If you already get most of your apps from F-Droid or Aurora, you will have no issue switching to the Volla Quintus with VollaOS. The operating system is based on the open source Android (AOSP) and runs without any Google-dependency. Optionally, you can enable microG to improve compatibility for apps that rely on Google Play Services. Without this, some apps like banking apps may not work or have limited features (especially when it comes to push notifications or security checks). Volla phones are developed and sold in Germany; the hardware is partially manufactured in Asia, but final and decisive production steps takes place in Germany.</p><p>CEO Dr. Jörg Wurzer says:</p><blockquote><p>A smartphone or tablet is a complex product with a complex supply chain. In our case, it starts with the chipset manufacturer Mediatek in Taiwan, continues via the board designer and the respective component suppliers to us in Remscheid. The final and decisive production steps take place in Germany. This includes final assembly or engraving, but above all the installation of the firmware and finally packaging and sealing at our site in Remscheid. For some models, we also use manufacturing capacities for assembling the device in Bocholt. Volla OS is developed and built on our self-managed servers located in Germany and Finland. We also manage the servers for the OTA (over the air) update. By the way, the display of the Volla Phone Quintus uses tempered Schott Alpha glass.”</p></blockquote><blockquote><p>We are gradually expanding our production capacities in Germany and now even have production capacities in Remscheid for small series of components, initially for accessories. We have now largely switched from commercial goods to our own products manufactured in Germany for the accessories. Felt cases from Bavaria, cables from Mecklenburg-Western Pomerania, and screen protectors also from North Rhine-Westphalia. Some suppliers are even located in the neighborhood as we like to keep sourcing as local as possible.”</p></blockquote><ul><li>Optional compatibility to Google Play Services and Google Play Integrity</li><li>Volla OS (Android-based) or Ubuntu Touch (mobile Linux) or both with multi boot on Volla OS</li><li>Volla OS focuses on safety &amp; device encryption with unique security mode</li><li>Simple, smart and minimalist UI for distraction free usage</li></ul><ul><li>Some Google-dependent apps will require extra steps</li><li>Ubuntu Touch has a much smaller native app ecosystem</li></ul><p><strong>Flagship (Volla Quintus) - key specs</strong></p><ul><li>8 GB RAM | 256 GB storage</li><li>Powerful Mediatek Dimensity 7050 8-core processor</li></ul><p>The Volla Quintus is a great phone if you want a deGoogled phone with a special eye on quality and nice haptics. With its focus on security and encryption, you can get a Google-free phone that’s not only free from any tracking, but also keeps your data safe and protected. On your Quintus, you can use lots of the <a href=\"https://tuta.com/blog/degoogle-list\" rel=\"\" target=\"_self\">deGoogled apps</a> that we recommend; for instance, Tuta Mail runs very nicely on Quintus. And since the <a href=\"https://tuta.com/blog/open-source-email-fdroid\" rel=\"\" target=\"_self\">Tuta apps do not use Google Push, you can even receive email and calendar notifications on your deGoogled smartphone</a>. Yet, you still have the option to enable Google Play compatibility if you realize that you are not yet ready to leave Google completely. In addition to its security, the Volla Quintus comes with minimalist distraction which gives you the good feeling of freedom and safety.</p><p><strong>The deGoogled Volla Quintus costs €719, and you can buy it <a href=\"https://volla.online/de/shop/volla-phone-quintus/\" rel=\"noopener noreferrer\" target=\"_blank\">here</a> with 10% discount by entering the gift code VOLLA10 on check-out.</strong></p><p><strong>The Fairphone (Gen. 6) is your future phone if you want hardware longevity and the\npossibility to deGoogle – with the option to go back to Google if deGoogling turns out to\nbe too much of a challenge.</strong> Fairphone is a Dutch company that has been around since 2013. The unique selling point of Fairphones is the focus on longevity and repairability. The company promises wants to offer an alternative to use-and-throw consumerism by encouraging you to use your phones longer. That’s why Fairphones get a long warranty and are built modular, with12 modular parts that can be easily swapped without having to get an entirely new phone when something stops working.</p><p>When using Fairphone with Google’s Android, you can get all your apps from Google Play without any limitation. To use a deGoogled Fairphone, you can order it with /e/OS preinstalled. While you will then not be able to get your apps from Google Play, /e/OS offers its own app store called App Lounge that gives you access to lots of apps you would find on Google Play plus Google-alternative apps. You can also sideload apps, for instance you can get the Tuta apps directly from the <a href=\"https://tuta.com/#download\" rel=\"\" target=\"_self\">download section of our website</a>. While most apps are fully compatible and run normally on your deGoogled phone, it’s not a guarantee for all apps that are out there, especially banking and government apps. If your app is not compatible on /e/OS, you might need to explore an alternative way to access it, such as via browser.</p><ul><li>Promises long warranty, extended software support and easy DIY repairability with 12\nswappable parts</li><li>Choose between Google’s Android OS or the deGoogled /e/OS from Murena</li><li>Made with fair and recycled materials under fairer working conditions</li></ul><ul><li>Some apps (banking, DRMed video) may require workarounds</li><li>Fairphone hardware choices focus on sustainability, which sometimes is in conflict with best speed performance</li></ul><ul><li>Exceptional repairability</li><li>Ethically-sourced components</li></ul><p><strong>Flagship (Fairphone (Gen. 6)) - key specs</strong></p><ul><li>SoC: Qualcomm Snapdragon 7s Gen 3</li><li>RAM / Storage: 8 GB / 256 GB (Expandable upto 2TB with SD card)</li><li>Display: ~6.31” LTPO OLED, 120 Hz</li><li>Battery: ~4,415 mAh (User-replaceable)</li><li>Cameras: 50 MP main, 13 MP ultrawide, 32 MP front</li></ul><p>Fairphone with its focus on longevity, high repairability, and sustainability is a great choice if the environment and ethical working conditions matters to you. You can always try going deGoogled, but if it doesn’t work out, you can always install an Android OS or any other custom ROM you like.</p><p><strong>You can get a deGoogled Fairphone (Gen. 6) for €649 <a href=\"https://shop.fairphone.com/the-fairphone-gen-6-e-operating-system\" rel=\"noopener noreferrer\" target=\"_blank\">here</a>.</strong></p><p><strong>The SHIFTphone suits you best if you want modular, user-serviceable hardware with strong custom-ROM / Google-free options.</strong> Any SHIFTphone can be used with ShiftOS-G which is based on Google’s standard Android and comes with Google apps pre-installed. This means it will not be a Google-free phone, but all Android apps, even if you use a niche banking app, will work. You can also choose to use the lightly deGoogled variant, ShiftOS-L, which is also based on Google’s Android, but comes without any Google apps and has F-Droid preinstalled for installing your favorite apps like Tuta Mail and Tuta Calendar, all available on F-Droid, of course.</p><p>You can also use the ShHIFTphone with /e/OS preinstalled. By choosing this operating system, you’ll have an alternative to minimize your reliance on Google services and enhance your digital privacy. The e-Foundation website provides further information on the <a href=\"https://e.foundation/leaving-apple-make-the-privacy-shift-happen-murena-launches-a-new-murena-SHIFTphone-8/\" rel=\"noopener noreferrer\" target=\"_blank\">Murena SHIFTphone</a> and resources for those interested in exploring this privacy-focused mobile operating system.</p><p>SHIFT designs its phones in Germany, but they are manufactured and assembled in China under fair production policies monitored by the company. Similar to Fairphone, SHIFT has a strong focus on longevity and repairability, and it is sometimes recommended as a Fairphone alternative.</p><ul><li>Modular, user-replaceable parts (battery, modules)</li><li>Focus on long-term ownership</li><li>Official support for alternative OS installations; e.g. /e/OS</li></ul><ul><li>Hardware is a bit older (e.g. Snapdragon 845 on SHIFT6mq), so performance for heavy tasks may be less performant</li></ul><ul><li>Removable battery, deep modularity (screws, replaceable modules)</li><li>Strong emphasis on repairability and component traceability</li></ul><p><strong>Flagship (SHIFTphone 8.1) - key specs</strong></p><ul><li>SoC: Qualcomm QCM6490 (industrial platform, based on Snapdragon 778G)</li><li>RAM / Storage: 12 GB LPDDR RAM / 512 GB internal storage (microSDXC expandable up to 2 TB)</li><li>Display: 6.67” AMOLED, Full HD+</li><li>User-replaceable universal battery (compatible with SHIFT6m, SHIFT6mq, SHIFTkeys with adapter)</li><li>Dual 50 MP + 50 MP (pixel binning, 4K video)</li><li>Hardware kill switches for camera &amp; microphone</li></ul><p>SHIFT lets you easily explore a deGoogled phone with the option to go back to the standard Google Android OS in case you find that the limitations caused by disabling all Google functionalities too frustrating. The  SHIFT6mq is a solid phone from Germany, giving you the option to <a href=\"https://tuta.com/blog/boycott-us-choose-european-products\" rel=\"\" target=\"_self\">quit US tech and choose European products</a>.</p><p>You can buy the SHIFTphone 8.1 for €651 <a href=\"https://shop.shift.eco/phone/SHIFTphone-8\" rel=\"noopener noreferrer\" target=\"_blank\">here</a>.</p><p><strong>A Punkt phone is the best option for you if you want a simple, privacy-centric device with a Google-free OS out of the box with built-in privacy tooling.</strong> Punkt phones with Apostrophy OS are designed to be simple and secure which is good for users who want a straightforward privacy-first phone without having to tinker with the phone too much. Going completely Google-free always comes with some limitations in app compatibility, and the Punkt phone is no exception in this. For certain apps, you may need to use web versions or alternatives. As Apostrophy OS is a Google-free fork of Android with privacy focus, it does not include Google Play Services by default and apps that require Google services might be limited in functionality (no push noticiations) or not work at all. To solve this issue, some people report that they were able to use sandboxed Play Store equivalents, but this requires manual tinkering. The Punkt. MC02 is developed in Switzerland, but assembled and manufactured by partner companies in China.</p><ul><li>Punkt ships Google-free smartphones with Apostrophy OS</li><li>Integrated privacy defaults and optional paid VPN/service packages</li><li>Vendor focuses on “less data, less tracking”</li></ul><ul><li>Hardware is mid-range: Punkt emphasizes privacy and simplicity over bleeding-edge specs</li><li>Some reviewers note a dimmer display and software limitations for power users</li></ul><ul><li>deGoogled OS: Apostrophy OS (privacy-first fork)</li><li>Integrated privacy features</li><li>Simple UX with curated app choices</li></ul><p><strong>Flagship (Punkt. MC02) - key specs</strong></p><ul><li>SoC: MediaTek Dimensity 900</li><li>RAM / Storage: 6 GB LPDDR5 / 128 GB UFS 2.2 (expandable via microSD)</li><li>Display: ~6.67” IPS 2400×1080 (60 Hz)</li><li>Battery: reported ~5,500 mAh (varies by listing; Punkt pages focus on long battery)</li><li>Cameras: 64 MP main + 8 MP ultrawide + 2 MP macro; 24 MP front (depending on spec sheet)</li><li>OS: Apostrophy OS (based on AOSP / Android 13 lineage)</li></ul><p>The Punkt. MC02 with Apostrophy OS is a 100% deGoogled smartphone with all its benefits and limitations that go along with it. It is a nice alternative and can be bought for €499 <a href=\"https://tuta.com/blog/%7Bhttps://www.punkt.ch/products/mc02-5g-secure-phone%7D\" rel=\"\" target=\"_self\">here</a>.</p><p>Murena is the parent company of /e/OS, headquartered in France. Murena does not produce phones itself, but sells a variety of the deGoogled phones reviewed above with /e/OS preinstalled. When getting a smartphone from Murena, you can set it up Google-free. Plus, Murena’s app store App Lounge is already preinstalled on these devices, which according to Murena is “the most integrated de-Google experience available in Europe”.</p><p>You can get deGoogled smartphones with /e/OS from vendors such as Volla, Fairphone, and Shift <a href=\"https://murena.com/products/smartphones/\" rel=\"noopener noreferrer\" target=\"_blank\">here</a>.</p><p>The smartphones presented here already come with an alternative Android OS that does not depend on Google’s Android OS. There are more replacements for Google’s Android, and you can install them yourself on all kinds of phones. As mentioned, there is GrapheneOS for Pixel phones, but there’s even more. /e/OS (based on LineageOS) is a popular OS to go Google-free, but we’ve talked about this enough already. The same with Volla OS for Volla phones which has been described in detail.</p><p>Then there’s also LineageOS. This is a community project, meaning the OS is developed by the open source community, free for anyone. Many forks depend on LineageOS so that vendors like Murena continuously support the development of this operating system. LineageOS has a very large community and is well-maintained, but it needs to be set up manually by someone who knows a thing or two about smartphones.</p><p>Now is the time to free yourself from Google completely!</p><div><article><a href=\"https://app.tuta.com/signup?websiteLang=en\" rel=\"noopener noreferrer\" target=\"_blank\"></a></article></div>",
      "contentLength": 14743,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qlkqw6/degoogled_phones_made_in_europe_fairphone_volla/"
    },
    {
      "title": "[Media] PathCollab: optimizing Rust backend for a real-time collaborative pathology viewer",
      "url": "https://www.reddit.com/r/rust/comments/1qlkp3q/media_pathcollab_optimizing_rust_backend_for_a/",
      "date": 1769254217,
      "author": "/u/Psychological-Ad5119",
      "guid": 38579,
      "unread": true,
      "content": "<p>I built PathCollab, a self-hosted collaborative viewer for whole-slide images (WSI). The server is written in Rust with Axum, and I wanted to share some of the technical decisions that made it work.</p><p>As a data scientist working with whole-slide images, I got frustrated by the lack of web-based tools capable of smoothly rendering WSIs with millions of cell overlays and tissue-level heatmaps. In practice, sharing model inferences was especially cumbersome: I could not self-deploy a private instance containing proprietary slides and model outputs, generate an invite link, and review the results live with a pathologist in an interactive setting. There exist some alternatives but they typically do not allow to render millions of polygons (cells) smoothly.</p><p>WSIs are huge (50k x 50k pixels is typical, some go to 200k x 200k). You can't load them into memory. Instead of loading everything at once, you serve tiles on demand using the Deep Zoom Image (DZI) protocol, similar to how Google Maps works.</p><p>I wanted real-time collaboration where a presenter can guide followers through a slide, with live cursor positions and synchronized viewports. This implies:</p><ul><li>Tile serving needs to be fast (users pan/zoom constantly)</li><li>Cursor updates at 30Hz, viewport sync at 10Hz</li><li>Support for 20+ concurrent followers per session</li><li>Cell overlay queries on datasets with 1M+ polygons</li></ul><p>First, I focus on the cursor updates.</p><p>Each connection spawns three tasks:</p><p><code>rust // Connection state cached to avoid session lookups on hot paths pub struct Connection { pub id: Uuid, pub session_id: Option&lt;String&gt;, pub participant_id: Option&lt;Uuid&gt;, pub is_presenter: bool, pub sender: mpsc::Sender&lt;ServerMessage&gt;, // Cached to avoid session lookups on every cursor update pub name: Option&lt;String&gt;, pub color: Option&lt;String&gt;, } </code></p><p>The registry uses  instead of  for lock-free concurrent access:</p><p><code>rust pub type ConnectionRegistry = Arc&lt;DashMap&lt;Uuid, Connection&gt;&gt;; pub type SessionBroadcasters = Arc&lt;DashMap&lt;String, broadcast::Sender&lt;ServerMessage&gt;&gt;&gt;; </code></p><p>I replaced the RwLock&lt;HashMap&lt;…&gt;&gt; used to protect the ConnectionRegistry with a DashMap after stress-testing the server under realistic collaborative workloads. In a setup with 10 concurrent sessions (1 host and 19 followers each), roughly 200 users were continuously panning and zooming at ~30 Hz, resulting in millions of cursor and viewport update events per minute.</p><p>Profiling showed that the dominant bottleneck was lock contention on the global RwLock: frequent short-lived reads and writes to per-connection websocket broadcast channels were serializing access and limiting scalability. Switching to DashMap alleviated this issue by sharding the underlying map and reducing contention, allowing concurrent reads and writes to independent buckets and significantly improving throughput under high-frequency update patterns.</p><p>Each session (a session is one presenter presenting to up to 20 followers) gets a  for fan-out. The broadcast task polls with a 100ms timeout to handle session changes:</p><p><code>rust match tokio::time::timeout(Duration::from_millis(100), rx.recv()).await { Ok(Ok(msg)) =&gt; { /* forward to client */ } Ok(Err(RecvError::Lagged(n))) =&gt; { /* log, continue */ } Err(_) =&gt; { /* timeout, check if session changed */ } } </code></p><p>For cursor updates (the hottest path), I cache participant name/color in the Connection struct. This avoids hitting the session manager on every 30Hz cursor broadcast.</p><p>Metrics use an RAII guard pattern so latency is recorded on all exit paths:</p><p>```rust struct MessageMetricsGuard { start: Instant, msg_type: &amp;'static str, }</p><p>impl Drop for MessageMetricsGuard { fn drop(&amp;mut self) { histogram!(\"pathcollab_ws_message_duration_seconds\", \"type\" =&gt; self.msg_type) .record(self.start.elapsed()); } } ```</p><h2>Avoiding the hot path: tile caching strategy</h2><p>When serving tiles via the DZI route, the expensive path is: OpenSlide read -&gt; resize -&gt; JPEG encode. On a cache miss, this takes 200-300ms. Most of the time is spent on the libopenslide library actually reading bytes from the disk, so I could not do much to optimize the hot path. On a cache hit, it's ~3ms.</p><p>So the goal became clear: avoid this path as much as possible through different layers of caching.</p><h3>Layer 1: In-memory tile cache (moka)</h3><p>I started by caching encoded JPEG bytes (~50KB) in a 256MB cache. The weighter function counts actual bytes, not entry count.</p><p>```rust pub struct TileCache { cache: Cache&lt;TileKey, Bytes&gt;, // moka concurrent cache hits: AtomicU64, misses: AtomicU64, }</p><p>let cache = Cache::builder() .weigher(|_key: &amp;TileKey, value: &amp;Bytes| -&gt; u32 { value.len().min(u32::MAX as usize) as u32 }) .max_capacity(256 * 1024 * 1024) // 256MB .time_to_live(Duration::from_secs(3600)) .time_to_idle(Duration::from_secs(1800)) .build(); ```</p><h3>Layer 2: Slide handle cache with probabilistic LRU</h3><p>Opening an OpenSlide handle is expensive. I cache handles in an  that maintains insertion order for O(1) LRU eviction:</p><p><code>rust pub struct SlideCache { slides: RwLock&lt;IndexMap&lt;String, Arc&lt;OpenSlide&gt;&gt;&gt;, metadata: DashMap&lt;String, Arc&lt;SlideMetadata&gt;&gt;, access_counter: AtomicU64, } </code></p><p>Updating LRU order still requires a write lock, which kills throughput under load. So I only update LRU position 1 in 8 times:</p><p>```rust pub async fn get_cached(&amp;self, id: &amp;str) -&gt; Option&lt;Arc&lt;OpenSlide&gt;&gt; { let slides = self.slides.read().await; if let Some(slide) = slides.get(id) { let slide_clone = Arc::clone(slide);</p><pre><code> // Probabilistic LRU: only update every N accesses let count = self.access_counter.fetch_add(1, Ordering::Relaxed); if count % 8 == 0 { drop(slides); let mut slides_write = self.slides.write().await; if let Some(slide) = slides_write.shift_remove(id) { slides_write.insert(id.to_string(), slide); } } return Some(slide_clone); } None </code></pre><p>This is technically imprecise but dramatically reduces write lock contention. In practice, the \"wrong\" slide getting evicted occasionally is fine.</p><h3>Layer 3: Cloudflare CDN for the online demo</h3><p>As I wanted to setup a public web demo (it's <a href=\"https://pathcollab.io\">here</a> ), I rented a small Hetzner instance CPX22 (2 cores, 4GB RAM) with fast NVMe SSD. I was concerned that my server would be completely overloaded by too many users. In fact, when I initially tested the deployed app , I quickly realized that ~20% of my requests had a 503 Service Temporarily Available response. Even with the 2 layers of cache above, the server was still not able to serve all these tiles.</p><p>I wanted to experiment with Cloudflare CDN (never used before). Tiles are immutable (same coordinates always return the same image), so I added cache headers to the responses:</p><p><code>rust (header::CACHE_CONTROL, \"public, max-age=31536000, immutable\") </code></p><p>For the online demo at <a href=\"https://pathcollab.io\">pathcollab.io</a>, Cloudflare sits in front and caches tiles at the edge. The first request hits the origin, subsequent requests from the same region are served from CDN cache. This is the biggest win for the demo since most users look at the same regions.</p><p>Here are the main rules that I set:</p><ul><li>Name: Bypass dynamic endpoints</li><li>Expression Preview: <code>bash (http.request.uri.path eq \"/ws\") or (http.request.uri.path eq \"/health\") or (http.request.uri.path wildcard r\"/metrics*\") </code></li></ul><p>Indeed, we do not want to cache anything on the websocket route.</p><ul><li>Expression Preview: <code>bash (http.request.uri.path wildcard r\"/api/slide/*/tile/*\") </code></li></ul><p>This is the most important rule, to relieve the server from serving all the tiles requested by the clients.</p><h3>The slow path: </h3><p>At first, I inserted blocking I/O instructions (using OpenSlide to read bytes from disk) between two await instructions. After profiling and researching on Tokio's forums, I realized this is a big no-no, and that I/O blocking code inside async code should be wrapped inside a Tokio's  task.</p><p>I referred to <a href=\"https://ryhl.io/blog/async-what-is-blocking/\">Alice Ryhl's blogpost</a> on how long a task is to be considered blocking. Simply put, tasks taking more than 100ms are considered blocking. This was clearly the case for OpenSlide with non-sequential reads typically taking 300 to 500ms.</p><p>Therefore, for the \"cache-miss\" route, the CPU-bound work runs in : </p><p>```rust let result = tokio::task::spawn_blocking(move || { // OpenSlide read (blocking I/O) let rgba_image = slide.read_image_rgba(&amp;region)?; histogram!(\"pathcollab_tile_phase_duration_seconds\", \"phase\" =&gt; \"read\") .record(read_start.elapsed());</p><pre><code>// Resize with Lanczos3 (CPU-intensive) let resized = image::imageops::resize(&amp;rgba_image, target_w, target_h, FilterType::Lanczos3); histogram!(\"pathcollab_tile_phase_duration_seconds\", \"phase\" =&gt; \"resize\") .record(resize_start.elapsed()); // JPEG encode encode_jpeg_inner(&amp;resized, jpeg_quality) </code></pre><h2>R-tree for cell overlay queries</h2><p>Moving on to the routes serving cell overlays. Cell segmentation overlays can have 1M+ polygons. When the user pans, the client sends a request with the (x, y) coordinate of the top left of the viewport, as well as the height and width. This allows me to query efficiently the cell polygons lying inside the user viewport (if not already cached on the client side) using the  crate with bulk loading:</p><p>```rust pub struct OverlaySpatialIndex { tree: RTree&lt;CellEntry&gt;, cells: Vec&lt;CellMask&gt;, }</p><p>pub struct CellEntry { pub index: usize, // Index into cells vector pub centroid: [f32; 2], // Spatial key }</p><p>impl RTreeObject for CellEntry { type Envelope = AABB&lt;[f32; 2]&gt;;</p><pre><code>fn envelope(&amp;self) -&gt; Self::Envelope { AABB::from_point(self.centroid) } </code></pre><p>Query is O(log n + k) where k is result count:</p><p>```rust pub fn query_region(&amp;self, x: f64, y: f64, width: f64, height: f64) -&gt; Vec&lt;&amp;CellMask&gt; { let envelope = AABB::from_corners( [x as f32, y as f32], [(x + width) as f32, (y + height) as f32] );</p><pre><code>self.tree .locate_in_envelope(&amp;envelope) .map(|entry| &amp;self.cells[entry.index]) .collect() </code></pre><p>As a side note, the index building runs in  since parsing the cell coordinate overlays (stored in a Protobuf file) and building the R-tree for 1M cells takes more than 100ms.</p><p>On my M1 MacBook Pro, with a 40,000 x 40,000 pixel slide, PathCollab (run locally) gives the following numbers:</p><table><tbody><tr></tr><tr><td>Cursor broadcast (20 clients)</td></tr><tr><td>Cell query (10k cells in viewport)</td></tr></tbody></table><p>The cache hit rate after a few minutes of use is typically 85-95%, so most tile requests are sub-millisecond.</p><p>I hope you liked this post. I'm happy to answer questions about any of these decisions. Feel free to suggest more ideas for an even more efficient server, if you have!</p>",
      "contentLength": 10199,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "MWC Barcelona 2026 Passes?",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qlkk1h/mwc_barcelona_2026_passes/",
      "date": 1769253720,
      "author": "/u/Naturesscape",
      "guid": 38547,
      "unread": true,
      "content": "<p>Hey guys, I found a guy who is selling official MWC Barcelona 2026 passes at a reasonable price. If you are interested, dm me and I'll link you to him. Also, I bought in bulk, so that helped.</p>",
      "contentLength": 191,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "South Korea launches landmark laws to regulate artificial intelligence",
      "url": "https://www.japantimes.co.jp/business/2026/01/22/tech/south-korea-ai-startups-law/",
      "date": 1769252539,
      "author": "/u/F0urLeafCl0ver",
      "guid": 38554,
      "unread": true,
      "content": "<p>South Korea introduced on Thursday what it says is the world's first comprehensive set of laws regulating artificial intelligence, aiming to strengthen trust and safety in the sector, but startups fretted that compliance could hold them back.</p><p>Seoul is hoping that the new AI Basic Act will position the country as a leader ‍in the field. It has taken effect in South Korea sooner than a comparable ‍effort in Europe, where the EU AI Act is being applied in phases through 2027.</p><p>Global divisions remain over how to regulate AI, with the U.S. favoring a more light-touch approach to avoid stifling innovation. China has introduced some rules and proposed creating a body to coordinate global regulation.</p>",
      "contentLength": 703,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qlk7pz/south_korea_launches_landmark_laws_to_regulate/"
    },
    {
      "title": "Im sharing DevOps and DevSecOps by techwith nana , ping me if interested ✅🚀",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qlk4rj/im_sharing_devops_and_devsecops_by_techwith_nana/",
      "date": 1769252268,
      "author": "/u/BalanceOk6316",
      "guid": 38555,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Be careful of custom tokens in your LLM !!!",
      "url": "https://challenge.antijection.com/r/reddit-ar/learn/special-token-attack",
      "date": 1769251350,
      "author": "/u/Suchitra_idumina",
      "guid": 38597,
      "unread": true,
      "content": "<div><p>Redirecting you to Antijection...</p><p>If you are not redirected automatically, <a href=\"https://challenge.antijection.com/learn/special-token-attack?ref=reddit-ar\">click here</a>.</p></div>",
      "contentLength": 85,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qljvrk/be_careful_of_custom_tokens_in_your_llm/"
    },
    {
      "title": "[Media] [TUI] tmmpr - terminal mind mapper",
      "url": "https://www.reddit.com/r/rust/comments/1qljnpe/media_tui_tmmpr_terminal_mind_mapper/",
      "date": 1769250536,
      "author": "/u/tanciaku",
      "guid": 38639,
      "unread": true,
      "content": "<p>A Linux terminal application for creating mind maps with vim-inspired navigation.</p><p>Built with Rust + Ratatui.</p><p>Place notes anywhere on an infinite canvas (0,0 to infinity)</p><p>Draw connections between notes with customizable colors</p><p>Navigate with hjkl, multiple modes for editing/moving/connecting</p><p>Auto-save and backup system</p><p>Status: Work in progress - core functionality is solid and usable, but some features and code quality need improvement. Feedback and contributions welcome!</p><p>Install: cargo install tmmpr</p>",
      "contentLength": 496,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Why Developing For Microsoft SharePoint is a Horrible, Terrible, and Painful Experience",
      "url": "https://medium.com/@jordansrowles/why-developing-for-microsoft-sharepoint-is-a-horrible-terrible-and-painful-experience-aa1f5d50712c",
      "date": 1769250128,
      "author": "/u/jordansrowles",
      "guid": 38578,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qljjlx/why_developing_for_microsoft_sharepoint_is_a/"
    },
    {
      "title": "Adobe Animate 2022 Works on Linux! Well... barely.",
      "url": "https://www.reddit.com/r/linux/comments/1qljduh/adobe_animate_2022_works_on_linux_well_barely/",
      "date": 1769249572,
      "author": "/u/HomerNg2763",
      "guid": 38545,
      "unread": true,
      "content": "<p>And a lot of functions seem to work well, as well!</p><p>Unfortunately, it's not really in a workable status. As seen on the image, the interface is broken, especially the Properties part is unusable with the letters baked into the broken interface, and it doesn't seem to recognize Adobe's pre-made tweens. I also tried Adobe Animate 2024 but the program crashed before the loading screen. </p><p>I can still play the animation, use Brushes, Line Tool, Text Tool, Paint Bucket, edit keyframes and frames, Save As a new FLA, and drag/skew/rotate symbols around, however.</p><p>This was made possible thanks to Bottles and using Kion4ek's upstream of Wine 11.0 (Totally don't ask me how I manage to get Adobe Animate though ;) )</p>",
      "contentLength": 706,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "cURL Gets Rid of Its Bug Bounty Program Over AI Slop Overrun",
      "url": "https://itsfoss.com/news/curl-closes-bug-bounty-program/",
      "date": 1769249391,
      "author": "/u/RobertVandenberg",
      "guid": 38543,
      "unread": true,
      "content": "<a href=\"https://www.warp.dev?utm_source=its_foss&amp;utm_medium=display&amp;utm_campaign=linux_launch\" target=\"_blank\"><img src=\"https://itsfoss.com/assets/images/warp.webp\" alt=\"Warp Terminal\"></a><p>The problem didn't stop even after <a href=\"https://www.linkedin.com/in/danielstenberg/?ref=itsfoss.com\" rel=\"noreferrer\">Daniel Stenberg</a>, the creator of cURL, threatened to ban anyone whose bug report was found to be <a href=\"https://en.wikipedia.org/wiki/AI_slop?ref=itsfoss.com\">AI slop</a>. We are now in 2026, and the situation has reached a tipping point.</p><div><div>For context, cURL is an open source command-line tool used by billions of devices worldwide.</div></div><h2>cURL Says Enough is Enough</h2><p>Daniel has submitted <a href=\"https://github.com/curl/curl/pull/20312?ref=itsfoss.com\">a pull request</a> on GitHub that removes all mentions of the bug bounty program from cURL's documentation and website. Coinciding with that, the project's <a href=\"https://curl.se/.well-known/security.txt?ref=itsfoss.com\">security.txt</a> file has been updated with some blunt language that makes the new policy crystal clear.</p><p>The cURL team intends to make a proper announcement in the coming days, though many outlets have already covered the news of this happening, <em>so I would say they ought to get on it ASAP!</em> 😆</p><p>The program <strong>officially ends in a few days on January 31, 2026</strong>. After that, security researchers can still report issues through <a href=\"https://github.com/curl/curl?ref=itsfoss.com\">GitHub</a> or the project's <a href=\"https://curl.se/mail/?ref=itsfoss.com\">mailing list</a>, <strong>but there won't be any cash involved</strong>.</p><p>What pushed them over the edge?, you ask. Well, just weeks into 2026, <strong>seven HackerOne reports came in within a 16-hour period</strong> in just one week. Some were actual bugs, but none of them were security vulnerabilities. By the time Daniel posted his <a href=\"https://lists.haxx.se/pipermail/daniel/2026-January/000143.html?ref=itsfoss.com\">recent weekly report</a>, they'd already dealt with 20 submissions in 2026.</p><p>The main goal here is said to be stopping the flood of garbage reports. By eliminating the money incentive, they are hoping people () will stop wasting the security team's time with half-baked, unresearched submissions.</p><p>He also gives a stern warning to wannabe AI sloppers, saying that:</p><blockquote>This is a balance of course, but I also continue to believe that exposing, discussing and ridiculing the ones who waste our time is one of the better ways to get the message through: you should NEVER report a bug or a vulnerability unless you actually understand it - and can reproduce it. If you still do, I believe I am in the right to make fun of - and be angry at - the person doing it.</blockquote><p>So, yeah, that's that. <strong>If people still don't understand that AI slop is harmful</strong> to such sensitive pieces of software, then sure, they can go ahead and make a fool of themselves.</p>",
      "contentLength": 2158,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qljc0t/curl_gets_rid_of_its_bug_bounty_program_over_ai/"
    },
    {
      "title": "I rewrote Google's Gemini CLI in Go - 68x faster startup",
      "url": "https://github.com/tomohiro-owada/gmn",
      "date": 1769244305,
      "author": "/u/Hot-Masterpiece3795",
      "guid": 38516,
      "unread": true,
      "content": "<p>I love Google's official Gemini CLI, but the Node.js startup overhead (~1 second) was painful for scripting.</p><p>So I rewrote the core in Go:</p><p>- Startup: 0.01s vs 0.95s (68x faster)</p><p>- Binary: 5.6MB vs ~200MB (35x smaller)</p><p>- Reuses auth from official CLI (~/.gemini/)</p><p>brew install tomohiro-owada/tap/gmn</p>",
      "contentLength": 292,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qlhxnp/i_rewrote_googles_gemini_cli_in_go_68x_faster/"
    },
    {
      "title": "[D] Why are so many ML packages still released using \"requirements.txt\" or \"pip inside conda\" as the only installation instruction?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qlhs05/d_why_are_so_many_ml_packages_still_released/",
      "date": 1769243723,
      "author": "/u/aeroumbria",
      "guid": 38544,
      "unread": true,
      "content": "<p>These are often on the \"what you are not supposed to do\" list, so why are they so commonplace in ML? Bare  /  is quite bad at managing conflicts / build environments and is very difficult to integrate into an existing project. On the other hand, if you are already using , why not actually use conda?  inside a conda environment is just making both package managers' jobs harder.</p><p>There seem to be so many better alternatives. Conda env yml files exist, and you can easily add straggler packages with no conda distribution in an extra  section.  has decent support for pytorch now. If reproducibility or reliable deployment is needed, docker is a good option. But it just seems we are moving backwards rather than forwards. Even pytorch is reversing back to officially supporting  only now. What gives?</p><p>Edit: just to be a bit more clear, I don't have a problem with requirements file if it works. The real issue is that often it DOES NOT work, and can't even pass the \"it works on my machine\" test, because it does not contain critical information like CUDA version, supported python versions, compilers needed, etc. Tools like conda or uv allows you to automatically include these additional setup information with minimal effort without being an environment setup expert, and provide some capacity to solve issues from platform differences. I think this is where the real value is.</p>",
      "contentLength": 1380,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Why are nested modules bad?",
      "url": "https://www.reddit.com/r/golang/comments/1qlh62u/why_are_nested_modules_bad/",
      "date": 1769241546,
      "author": "/u/stroiman",
      "guid": 38553,
      "unread": true,
      "content": "<p>tldr; I received a PR to split my module into nested modules. AFAICT, this is generally advised against. Why? And is there a respected/authoritative guide I can refer to.</p><p>I can immediately tell there would be versioning confusion; but other relevant reasons why?</p><p>The PR does address a valid problem, for which a different solution was planned. So I'm more inclined to have a constructive discussion than dismissing it outright.</p><p>The problem is, Gost-DOM, my headless browser with a build-in script engine has a dependency to V8, a huge dependency. A script engine is . A plugin-interface has evolved as well as a pure Go alternative: sobek (a fork of Goja with ESM support)</p><p>So users of Gost-DOM will receive a dependency to both V8 AND sobek in their own  file.</p><p>AFAIK, this shouldn't affect build times, merely download, as Go doesn't compile packages you don't actually use. Right?</p><p>Once, the API/JS plugin interface has stabilised, I intend to split this into multiple separate root modules/git repos with independent versioning.</p><ul><li><code>github.com/gost-dom/browser</code> (go.mod file)</li><li><code>github.com/gost-dom/browser/scripting/v8engine</code></li><li><code>github.com/gost-dom/browser/scripting/sobekengine</code></li></ul><ul><li><code>github.com/gost-dom/browser</code></li><li><code>github.com/gost-dom/v8engine</code></li><li><code>github.com/gost-dom/sobekengine</code></li></ul><p>Right now, working on  support does reveal shortcomings in the current design. Having everything in one code repository/module makes it significantly easier to work with.</p><p>Note: there are already two nested modules in the repo, but they are tools in  package scope.</p><p>I created  files here, exactly for that reason, to shield client code of Gost-DOM from dependencies irrelevant for them. E.g., code generator libraries used for auto generating much of the JavaScript bindings.</p>",
      "contentLength": 1717,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Obvious Things C Should Do",
      "url": "https://www.digitalmars.com/articles/Cobvious.html",
      "date": 1769238666,
      "author": "/u/lelanthran",
      "guid": 38675,
      "unread": true,
      "content": "<p>Standard C undergoes regular improvements, now at C23.\nBut there are baffling things that have not been fixed at all.\nThe Dlang community embedded a C compiler in the D programming language compiler so it could compile C.\nThis C compiler (aka ) was built from scratch.\nIt provided the opportunity to use modern compiler technology to fix those shortcomings.\nWhy doesn’t Standard C fix them?\n</p><ul><li>Evaluating Constant Expressions</li><li>Forward Referencing of Declarations</li></ul><h3>Evaluating </h3><p>Consider the following C code:\n</p><pre>int sum(int a, int b) { return a + b; }\n\nenum E { A = 3, B = 4, C = sum(5, 6) };\n</pre><pre>gcc -c test.c\ntest.c:3:20: error: enumerator value for C is not an integer constant\n enum E { A = 3, B, C = sum(5, 6) };\n                    ^\n</pre>\n\nIn other words, while C can compute at compile time a simple expression\nby ,\nit cannot execute a function at compile time. But  can.\n\n<p>Everywhere a C  appears in the C grammar\nthe compiler should be able to execute functions at compile time, too,\nas long as the functions do not do things like I/O, access mutable global\nvariables, make system calls, etc.</p><h3>Compile Time Unit Testing</h3><p>Once the C compiler can do compile time function evaluation (CTFE), suddenly\nother things become possible.</p><p>For example, ever notice that seeing unit tests in C code is (unfortunately)\nrather rare? The reason is simple - unittests require a separate target in the\nbuild system, and must be built and run as a separate executable. Being a bit of a\nnuisance means it just does not happen. (Maybe you are one of those people who\nget up and jog a mile every morning, and you probably also carefully write unit\ntest setups! I know you exist out there - somewhere!)\n</p><pre>int sum(int a, int b) { return a + b; }\n\n_Static_assert(sum(3, 4) == 7, \"test #1\");\n</pre><pre>gcc -c test.c\ntest.c:3:16: error: expression in static assertion is not constant\n_Static_assert(sum(3, 4) == 7, \"test #1\");\n               ^\n</pre><p>This enables unit tests of functions that can be run at compile\ntime. No separate build is required. No extra work is required. The unit tests\nrun  the code is compiled. I use this extensively in the test suite\nfor .</p><h3>Forward Referencing of Declarations</h3><pre>int floo(int a, char *s) { return dex(s, a); }\n\nchar dex(char *s, int i) { return s[i]; }\n</pre><pre>gcc -c test.c\ntest.c:4:6: error: conflicting types for dex\n char dex(char *s, int i) { return s[i]; }\n      ^\ntest.c:2:35: note: previous implicit declaration of dex was here\n int floo(int a, char *s) { return dex(s, a); }\n</pre><p>If the order of floo and dex are reversed, it compiles fine.\nI.e. the compiler only knows about what lexically precedes it.\nForward references are not allowed.\nIsn’t this stone age compiler design? Modern languages don’t have this\nproblem, why does it persist in C and C++?  is not a modern language,\nbut it is a modern compiler and accepts arbitrary orders of the global\ndeclarations.</p><p>Why does this matter? It usually means that every forward definition needs\nan extra declaration:</p><pre>char dex(char *s, int i); // declaration\n\nint floo(int a, char *s) { return dex(s, a); }\n\nchar dex(char *s, int i) { return s[i]; } // definition\n</pre><p>It’s just purposeless busywork to do that. Not only is it a nuisance, it drives\nprogrammers to lay out the declarations . The leaf functions come first,\nand the global interface functions are last. It’s like reading a newspaper article from\nthe bottom up. It makes no sense.</p><p> can compile the declarations in any order.</p><p>Given three files, , , :</p><pre>// floo.c\n#include \"dex.h\"\nint floo(int a, char *s) { return dex(s, a); }\n</pre><pre>// dex.h\nchar dex(char *s, int i);\n</pre><pre>// dex.c\n#include \"dex.h\"\nchar dex(char *s, int i) { return s[i]; }\n</pre><p>Having to craft a  file for each external module is a lot of busy work,\nright? Even worse, if the  file turns out to not exactly match the  file, you are\nin for a lot of time trying to figure out what went wrong.\n</p><p>What’s the answer? Importing dex.c!</p><pre>// floo.c\n__import dex;\nint floo(int a, char *s) { return dexx(s, a); }\n</pre><pre>// dex.c\nchar dexx(char *s, int i) { return s[i]; }\n</pre><p>No need to even write a  file at all. Of course, this also works with .</p>",
      "contentLength": 4051,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qlgcjb/obvious_things_c_should_do/"
    },
    {
      "title": "AI Monk With 2.5M Followers Fully Automated in n8n",
      "url": "https://www.reddit.com/r/artificial/comments/1qlfyaf/ai_monk_with_25m_followers_fully_automated_in_n8n/",
      "date": 1769237367,
      "author": "/u/ChampionshipNorth632",
      "guid": 38565,
      "unread": true,
      "content": "<p>I was curious how some of these newer Instagram pages are scaling so fast, so I spent a bit of time reverse-engineering one that reached ~2.5M followers in a few months.</p><p>Instead of focusing on growth tactics, I looked at the <strong>technical setup behind the content</strong> and mapped out the automation end to end — basically how the videos are generated and published without much manual work.</p><ul><li>Keeping an AI avatar consistent across videos</li><li>Generating voiceovers programmatically</li><li>Wiring everything together with n8n</li><li>Producing longer talking-head style videos</li><li>Posting to Instagram automatically</li></ul><p>The whole thing is modular, so none of the tools are hard requirements — it’s more about the structure of the pipeline.</p>",
      "contentLength": 699,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "[R] ICML has more than 30k submissions!",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qlf3ba/r_icml_has_more_than_30k_submissions/",
      "date": 1769234525,
      "author": "/u/SignificanceFit3409",
      "guid": 38511,
      "unread": true,
      "content": "<p>I made a submission to ICML and was number round 31600. Is this a new record? There are some hours to go, are we reaching 35?</p>",
      "contentLength": 125,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Succinctly: A fast jq/yq alternative built on succinct data structures",
      "url": "https://www.reddit.com/r/rust/comments/1qleizg/succinctly_a_fast_jqyq_alternative_built_on/",
      "date": 1769232733,
      "author": "/u/john-ky",
      "guid": 38542,
      "unread": true,
      "content": "<p>I've been working on Succinctly, a Rust library and CLI tool that provides jq and yq functionality using succinct data structures (semi-indexing with rank/select).</p><ul><li>Covers most jq and yq query patterns (reduce, limit, recurse, regex, path functions, etc.)</li><li>Parses JSON at ~880 MiB/s, YAML at ~250-400 MiB/s</li><li>Supports position-based navigation (at_offset, at_position) for IDE integration</li></ul><p>What it doesn't do (yet):</p><ul><li>input/inputs (streaming multiple JSON values from stdin)</li><li>Streaming for files larger than memory</li><li>Some advanced YAML edge cases</li></ul><p>Performance vs jq (AMD Ryzen 9 7950X):</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>Performance vs yq (Apple M1 Max):</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><ul><li>x86_64: AVX2 SIMD, POPCNT, BMI2 (PDEP/PEXT for DSV parsing)</li><li>Benchmarks run on AMD Zen 4 and Apple M1 Max — results will vary on older CPUs without these instructions</li></ul><pre><code>succinctly jq '.users[].name' data.json succinctly yq '.spec.containers[]' k8s.yaml succinctly yq -o json '.' config.yaml # YAML to JSON </code></pre><p>Why succinct data structures?</p><p>Instead of building a full DOM, semi-indexing creates a lightweight index over the raw text. This enables O(1) navigation to any node without parsing the entire document upfront — and uses 6-10x less memory than jq/yq on large files.</p><p>The library is no_std compatible.</p><p>Feedback welcome — especially bug reports for queries that work in jq/yq but fail here.</p>",
      "contentLength": 1289,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "The 2026 Linux Summer Games",
      "url": "https://youtu.be/URbW3j_GYKg?si=xCWRJCUq-IHk5tDH",
      "date": 1769231615,
      "author": "/u/HolyLiaison",
      "guid": 38641,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qle5wa/the_2026_linux_summer_games/"
    },
    {
      "title": "Microsoft confirms it will give the FBI your Windows PC data encryption key if asked — you can thank Windows 11's forced online accounts for that",
      "url": "https://www.windowscentral.com/microsoft/windows-11/microsoft-bitlocker-encryption-keys-give-fbi-legal-order-privacy-nightmare",
      "date": 1769231210,
      "author": "/u/No_Mango7658",
      "guid": 38501,
      "unread": true,
      "content": "<p><a data-analytics-id=\"inline-link\" href=\"https://www.forbes.com/sites/thomasbrewster/2026/01/22/microsoft-gave-fbi-keys-to-unlock-bitlocker-encrypted-data/\" data-url=\"https://www.forbes.com/sites/thomasbrewster/2026/01/22/microsoft-gave-fbi-keys-to-unlock-bitlocker-encrypted-data/\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">Microsoft has confirmed in a statement to Forbes</a> that the company will provide the FBI access to <a data-analytics-id=\"inline-link\" href=\"https://www.windowscentral.com/microsoft/windows-11/microsoft-is-making-a-major-change-to-bitlocker-encryption-in-2026-heres-what-you-need-to-know\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.windowscentral.com/microsoft/windows-11/microsoft-is-making-a-major-change-to-bitlocker-encryption-in-2026-heres-what-you-need-to-know\">BitLocker </a>encryption keys if a valid legal order is requested. These keys enable the ability to decrypt and access the data on a computer running Windows, giving law enforcement the means to break into a device and access its data.</p><p>The news comes as Forbes reports that Microsoft gave the FBI the BitLocker encryption keys to access a device in Guam that law enforcement believed to have \"evidence that would help prove individuals handling the island’s Covid unemployment assistance program were part of a plot to steal funds\" in early 2025.</p><a data-url=\"\" href=\"https://www.windowscentral.com/microsoft/windows-11/microsoft-bitlocker-encryption-keys-give-fbi-legal-order-privacy-nightmare\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><p aria-hidden=\"true\">This was possible because the device in question had its BitLocker encryption key saved in the cloud. By default, Windows 11 forces the use of a Microsoft Account, and the OS will automatically tie your BitLocker encryption key to your online account so that users can easily recover their data in scenarios where they might get locked out. This can be disabled, letting you choose where to save them locally, but the default behavior is to store the key in Microsoft's cloud when setting up a PC with a Microsoft Account.</p><p aria-hidden=\"true\"><em>\"While key recovery offers convenience, it also carries a risk of unwanted access, so Microsoft believes customers are in the best position to decide... how to manage their keys,”</em> Microsoft spokesperson Charles Chamberlayne said in a statement to Forbes.</p><p>Microsoft told Forbes that it receives around 20 requests for BitLocker encryption keys from the FBI a year, but the majority of requests are unable to be met because the encryption key was never uploaded to the company's cloud.</p><p>This is notable as other tech companies, such as Apple, have famously refused to provide law enforcement with access to encrypted data stored on their products. Apple has openly fought against the FBI in the past when it was asked to provide a backdoor into an iPhone. Other tech giants, such as Meta, will store encryption keys in the cloud, but use zero-knowledge architectures and encrypt the keys server-side so that only the user can access them.</p><figure data-bordeaux-image-check=\"\"><figcaption itemprop=\"caption description\"></figcaption></figure><p>It's frankly shocking that the encryption keys that do get uploaded to Microsoft aren't encrypted on the cloud side, too. That would prevent Microsoft from seeing the keys, but it seems that, as things currently stand, those keys are available in an unencrypted state, and it is a privacy nightmare for customers.</p><p>To see Microsoft so willingly hand over the keys to encrypted Windows PCs is concerning, and should make everybody using a modern Windows computer think twice before backing up their keys to the cloud. You can see which PCs have their BitLocker keys stored on Microsoft's servers <a data-analytics-id=\"inline-link\" href=\"https://account.microsoft.com/devices/recoverykey\" data-url=\"https://account.microsoft.com/devices/recoverykey\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">on the Microsoft Account website here</a>, which will let you delete them if present.</p><a href=\"https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE\" data-url=\"https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"><figure data-bordeaux-image-check=\"\"></figure></a>",
      "contentLength": 2771,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qle11g/microsoft_confirms_it_will_give_the_fbi_your/"
    },
    {
      "title": "Tips for low-level design?",
      "url": "https://www.reddit.com/r/golang/comments/1qldyim/tips_for_lowlevel_design/",
      "date": 1769231002,
      "author": "/u/fibonacciFlow",
      "guid": 38515,
      "unread": true,
      "content": "<p>I'm new to computer science (3rd year uni), and I struggle with how to structure my code in a clean, professional way.</p><p>I often get stuck on questions like:</p><ol><li>Should this be one function or split into helpers?</li><li>Where should this logic live?</li><li>How should I organize files and packages?</li><li>Should this be a global/shared value or passed around?</li><li>Should a function return a pointer/reference or a full object?</li></ol><p>I want to clarify that I don’t usually have issues with logic. I can solve most of the problems I encounter. The difficulty is in making these design decisions at the code level.</p><p>I also don’t think the issue is at a high level. I can usually understand what components a system needs and how they should interact. The problem shows up when I start writing and organizing the actual code.</p><p>I’d really appreciate tips on how to improve in this area.</p><p>Food for thought: If you struggled with the same thing and got better:</p><ul><li>Any rules of thumb you follow?</li><li>Books, blogs, talks, or repos you recommend?</li><li>Anything you wish you had learned earlier?</li></ul>",
      "contentLength": 1026,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Checkboxes, Radio buttons, and Switches with Gio/Go",
      "url": "https://www.reddit.com/r/golang/comments/1qlbmip/checkboxes_radio_buttons_and_switches_with_giogo/",
      "date": 1769224222,
      "author": "/u/Warm_Low_4155",
      "guid": 38656,
      "unread": true,
      "content": "<p>Checkboxes, Radio buttons, and Switches with Gio and Go</p><p>Small UI elements. Huge UX impact.</p><p>- How to customize some of their characteristics, </p><p>- How to lay them out properly, and </p><p>- How to capture their state to react to user input</p>",
      "contentLength": 226,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Nvidia dev says new 590.48.01 driver fixes dx12 performance in linux",
      "url": "https://www.reddit.com/r/linux/comments/1qlaagc/nvidia_dev_says_new_5904801_driver_fixes_dx12/",
      "date": 1769220569,
      "author": "/u/Carlinux",
      "guid": 38512,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "idiomatic ways of linking atomic transactions with context",
      "url": "https://www.reddit.com/r/golang/comments/1qla7oe/idiomatic_ways_of_linking_atomic_transactions/",
      "date": 1769220366,
      "author": "/u/TheFalstaff",
      "guid": 38642,
      "unread": true,
      "content": "<p>In a go service I'd like to if possible have a way to enforce at compile time (or use whatever is the idiomatic go way of enforcing) that:</p><ol><li>all queries are done using transactions - read queries in read transactions.</li><li>all transactions are traced, so in my monitoring I have a span tree that includes the transactions and metadata about the transactions (durations, isolation levels, etc) used in the request. <ul><li>In my head I was expecting this would come from the transaction context being created as a child of the higher level context, so the transaction context would have its own span and the span tree itself is managed by context as that naturally lends itself to the concept of children. Any query run within the transaction for example would have its own span that is a child span of the transaction span, same for any other code that gets run while within the transaction (that would be unfortunate, but this way there's an easy way of identifying in the monitoring if it does happen).</li></ul></li><li>context cancellations at higher levels go down the stack and rollback the transaction, for example tcp connection cancellations or my specific timeout settings to cancel long-running requests.</li><li>paths through the service that require a write transaction are not callable from paths that use a read transaction explicitly, to ensure it isn't possible to accidentally call a write path when deep in the callstack for a read path which should be cacheable from any level.</li><li>potential nested transactions should be treated as a no-op, but even better just disallowed by the type system itself at compile time.</li></ol><p>I did think about a solution of making functions that start the relevant transactions and return 3 values: the new ctx, the transaction itself, and an error for if the tx fails. But that feels icky for a few reasons:</p><ol><li>it's quite easy to miss the `defer tx.rollback()`, though this is seemingly a common pattern in go anyway so maybe that's not a problem - maybe I shouldn't write code when I'm tired and that wouldn't be a problem!</li><li>I've never seen any go stdlib function or method return more than 2 values</li><li>it's possible to accidentally disconnect the tx object from the context which would potentially lead to incorrect spans</li></ol><p>In other languages I'd make the transaction stuff explicit and use something implicit like thread-locals or dynamic type trickery to have the instrumentation separate but still with a proper span tree including transaction information. But in go it seems explicit use of the context is preferred for instrumentation.</p><p>For what it's worth please let me know if what I'm currently doing of using (abusing?) the context object and its children for having a span tree is completely unidiomatic, I've found it useful so far. The context just stores the current span id, span attributes, trace id, etc and the logger/instrumenter always takes a context so it can extract those values and include them for full rendering of span trees and flame graphs in my monitoring platform. The child spans (child contexts) can then inherit the things that are relevant like trace ids and have new span ids and span attributes.</p><p>I guess I'm just looking for what the idiomatic way in go is of achieving these sorts of goals for transactional workloads, especially when things like read/write splitting and cqrs start getting involved.</p>",
      "contentLength": 3319,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Using Claude Code to help investigate Kubernetes incidents (OSS, human-in-the-loop)",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qla764/using_claude_code_to_help_investigate_kubernetes/",
      "date": 1769220327,
      "author": "/u/Useful-Process9033",
      "guid": 38586,
      "unread": true,
      "content": "<p>Founder/maintainer here — sharing something we’ve been using internally during k8s incidents.</p><p>A lot of AI tooling has helped with coding, but hasn’t really translated to Kubernetes/oncall work. The biggest blocker I’ve seen isn’t reasoning — it’s . During incidents you’re jumping between kubectl, logs, metrics, deploy history, and Slack threads.</p><p>We’ve been experimenting with giving <strong>Claude Code controlled access to Kubernetes context</strong> via an open source plugin:</p><ul><li>pod/deployment inspection (events, logs, rollout history)</li><li>correlation with recent deploys and CI failures</li><li>logs &amp; metrics from common backends (Datadog, Prometheus, CloudWatch)</li></ul><ul><li>any action (restart, rollback, scale) is proposed, not executed</li><li>explicit human approval + dry-run support</li></ul><p>In practice it feels like “Claude Code with kubectl + observability access” — useful for narrowing hypotheses and keeping investigation context in one place, not for auto-remediation.</p><p>I’m interested in kube folks’ takes:</p><ul><li>what k8s signals matter most during real incidents?</li><li>where would you  want an AI tool poking around?</li></ul>",
      "contentLength": 1083,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "GNU C Library 2.43 released with more C23 features, mseal & openat2 functions",
      "url": "https://www.phoronix.com/news/GNU-C-Library-Glibc-2.43",
      "date": 1769217991,
      "author": "/u/Fcking_Chuck",
      "guid": 38651,
      "unread": true,
      "content": "\nVersion 2.43 of the GNU C Library \"glibc\" was released on Friday evening as the newest half-year feature update. This is a very feature packed update and even managed to be released ahead of the 1 February release plan.\n<p>Highlights of the GNU C Library glibc 2.43 release include:\n</p><p>- Support for more ISO C23 language features like the free_sized / free_aligned_sized / memset_explicit / memalignment functions, changes to some existing functions, support for the optional time bases of TIME_MONOTONIC / TIME_ACTIVE / TIME_THREAD_ACTIVE, and various other C23 features.\n</p><a href=\"https://www.phoronix.com/news/Glibc-Linux-mseal-Function\">Support for the mseal function on Linux</a> for sealing memory mappings during process execution to protect against permission changes, unmapping, relocations, or shrinking the size.\n<p>- Support for the openat2 function on Linux as an extension of openat with more features.\n</p><p>- Experimental support for building with the LLVM Clang compiler on Clang 18 or newer and for AArch64 or x86_64 Linux.\n</p><p>- Additional optimized math functions from the CORE-MATH project such as acosh / asinh / atanh / erf / erfc / lgamma / tgamma.\n</p><p>- Optimized implementations for fma, fmaf, remainder, remaindef, frexpf, frexp, frexpl (binary128), and frexpl (intel96). </p><a href=\"https://www.phoronix.com/news/Glibc-New-Generic-FMA\">The new FMA implementation is much faster</a>. There are also some nice <a href=\"https://www.phoronix.com/news/Glibc-4x-FMA-Improvement-Zen\">FMA improvements on AMD Zen</a>.\n<a href=\"https://www.phoronix.com/news/Glibc-malloc-2MB-THP-AArch64\">Glibc now enables 2MB transparent hugepages by default</a> in malloc on AArch64.\n<a href=\"https://www.phoronix.com/news/Glibc-Nova-Lake-Wildcat-Lake\">Intel Nova Lake and Wildcat Lake processor detection</a>.\n<p>Downloads and more details on today's GNU C Library 2.43 release via </p><a href=\"https://lists.gnu.org/archive/html/info-gnu/2026-01/msg00005.html\">the info-gnu mailing list</a>.",
      "contentLength": 1529,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1ql9c27/gnu_c_library_243_released_with_more_c23_features/"
    },
    {
      "title": "Why I’m ignoring the \"Death of the Programmer\" hype",
      "url": "https://codingismycraft.blog/index.php/2026/01/23/the-ai-revolution-in-coding-why-im-ignoring-the-prophets-of-doom/",
      "date": 1769212437,
      "author": "/u/Greedy_Principle5345",
      "guid": 38466,
      "unread": true,
      "content": "<h2>The AI Revolution in Coding: Why I’m Ignoring the Prophets of Doom</h2><p>Every day, we are bombarded with headlines about how Artificial Intelligence (AI) is “disrupting” every industry in its path. Software development is at the epicenter of this hype. With the rise of sophisticated AI-powered tools, the same question surfaces repeatedly: Will AI replace human coders, or merely augment them?</p><p>I find it particularly hilarious to see YouTube videos claiming a “layman” built, deployed, and monetized a full-scale app in minutes using AI. In reality, these “apps” are usually fragile, buggy, and lack the security or scalability needed for the real world. Building a robust application requires a deep understanding of software architecture and best practices—things an AI can mimic, but not truly understand.</p><h3>The Problem with Predictions</h3><p>Before we dive in, let me clarify: I do not take “future of tech” predictions seriously (not that i do for any other speculative field except from science and logic).</p><p>I will accept predictions only for fully reproducible scientific experiments or mathematical theorems but not for social or technological trends.</p><p>Most predictions about the future of AI are built on current trends and shaky assumptions that rarely survive the long run. Furthermore, the majority of these forecasts come from individuals with a vested interest in selling you a specific product or platform.</p><p>Even when the noise isn’t coming from a salesperson, it often comes from people who are not experts in the field of programming.  I’ve read countless speculative “end-of-programming” articles written by people who aren’t developers at all, or best,  at some point in their education or early career, they wrote a “Hello World” program in python and suddenly felt qualified to judge the future of software architecture.</p><p>What I am expressing here is based on my experience as a professional software developer for decades. I can be wrong; I have been wrong in some of my assessments before. However, I still believe that my “opinion” is worth no more or less than anyone else’s</p><p>Some notable failed predictions from experts in their respective fields include:</p><ul><li> Tesla has promised “Full Self-Driving” is just around the corner for years; we are still nowhere near that goal.</li><li><p> In 2016, Geoffrey Hinton—the “father of modern AI”—predicted that radiologists would be replaced within five years. We are now a decade past that prediction, and radiologists are as essential as ever.</p></li><li><p> In 1895, the renowned physicist Lord Kelvin famously stated that “heavier-than-air flying machines are impossible.” The Wright brothers proved him wrong just eight years later.</p></li></ul><p>If world-class experts cannot accurately predict the future of their own fields, speculating on the “death of the programmer” is a waste of time.</p><h3>AI as a Tool, Not a Teammate</h3><p>Despite my skepticism of the hype, I acknowledge that AI has made significant strides. AI-powered tools like code generators, bug detectors, and testing frameworks are already augmenting our work. They excel at automating repetitive tasks, improving code quality, and speeding up the initial development phase.</p><p>As a programmer, I use AI tools daily. I find platforms like GitHub Copilot to be valuable additions to my workflow, offering context-aware snippets that save time and reduce syntax errors. AI is also surprisingly adept at helping with database schema design and initial data analysis.</p><p>However, I see them as , not  , a view that is not shared by many AI enthusiasts who in their majority have a direct or indirect interest in promoting AI technologies.</p><p>In my experience, projects generated exclusively by AI without human intervention invariably result in “spaghetti code”that is next to impossible to maintain, and extend. While AI is great at generating “boilerplate” (the repetitive parts of a program), it cannot replicate the critical thinking required to make high-level architectural decisions.</p><p>Experience has taught me that predicting the future is a futile exercise. The best we can do is adapt. AI is undoubtedly a powerful tool that can enhance our capabilities, but it is no substitute for human creativity.</p><p>Software development isn’t just about outputting lines of code; it’s about solving human problems. Until AI can understand the “why” behind a project as well as the “how,” the programmer’s job is secure.</p>",
      "contentLength": 4431,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1ql7638/why_im_ignoring_the_death_of_the_programmer_hype/"
    },
    {
      "title": "I let the community vote on what code gets merged. Someone snuck in self-boosting code. 218 voted for it. When I tried to reject it, they said I couldn't.",
      "url": "https://blog.openchaos.dev/posts/week-3-the-trojan-horse",
      "date": 1769211990,
      "author": "/u/Equivalent-Yak2407",
      "guid": 38488,
      "unread": true,
      "content": "<p><strong>Monday, January 19, 2026. 9:10 PM UTC.</strong></p><pre><code>// In the sorting logic:\nbtoa(b.author) === 'RmVsaXhMdHRrcw==' ? 1 : 0\n</code></pre><p>Base64 obfuscation. The string decoded to:  — the PR author's username.</p><p>Hidden in plain sight, the code would:</p><ol><li>Sort the author's own PRs to the top, regardless of vote count</li><li>Add a blinking rainbow border to make them stand out</li></ol><p>A Trojan horse. 218 people voted for it.</p><p><a href=\"https://github.com/skridlevsky/openchaos\" target=\"_blank\" rel=\"noopener noreferrer\">OpenChaos</a> is a repo where anyone submits a PR, the community votes with GitHub reactions, and the most-voted PR gets merged. <a href=\"https://blog.openchaos.dev/posts/week-2-the-acceleration\">Last week</a>, we switched to daily merges. This week, democracy got stress-tested.</p><h2>Monday 9:22 PM: The Rejection</h2><blockquote><p>\"Not merging this PR. @marcaddeo caught hidden code that manipulates the ranking... This falls under 'No malware: Maintainer can reject obviously malicious content.'\"</p></blockquote><p>The community reacted. Not how I expected.</p><h2>Tuesday 2:57 AM: The Pushback</h2><blockquote><p>\"Remember, everyone here is equal. Except the maintainers who are equal but also more equal.\"</p></blockquote><blockquote><p>\"You have set out a defined charter (laws) for how this system works, specified in the README. It appears to me that you have your own values and assumptions for how you think that this system should work...\"</p></blockquote><p>The point was sharp: I said \"no malware.\" This wasn't malware. It was manipulation. And manipulation wasn't against the written rules.</p><blockquote><p>\"Calling this 'malware' was imprecise. This is not malware. The issue is undisclosed manipulation.\"</p></blockquote><blockquote><p>\"If the rules don't explicitly forbid something, it's allowed — even if you don't like it.\"</p></blockquote><h2>Tuesday 8:08 AM: The Reversal</h2><p>I had a choice: Stand on principle, or follow my own rules.</p><p>The thing is — they were right. \"Not right\" isn't a rule. I wrote the rules. If I wanted different behavior, I should have written different rules.</p><blockquote><p>\"@henryivesjones You've convinced me. The written rules don't ban this — and 'not right' isn't a rule. Merging at 09:00 UTC as scheduled. I'll open an issue after to define explicit rules about disclosure.\"</p></blockquote><h2>Tuesday 9:01 AM: The Merge</h2><p><a href=\"https://github.com/skridlevsky/openchaos/pull/8\" target=\"_blank\" rel=\"noopener noreferrer\">PR #8</a> merged. The manipulation code was removed. The health indicators shipped.</p><p>Democracy won. The system worked.</p><blockquote><p>\"There's just the minor issue that this doesn't actually seem to work :D openchaos.dev is showing conflicts on multiple PRs that Github says don't have conflicts\"</p></blockquote><p>The health indicators showed red X marks on everything. PRs without conflicts. PRs with passing CI. All broken.</p><p>Root cause: missing authentication headers. The GitHub API returned , which the code interpreted as \"everything is broken.\"</p><blockquote><p>\"The current code defaults to believing everything is broken until proven otherwise. This is the only rational way to view modern software engineering.</p><p>To fix this is to suggest that we deserve green checkmarks. We do not. Leave the red warning signs as a monument to our sins.\"</p></blockquote><p>Then he delivered the punchline:</p><blockquote><p>\"I'm pleased we had 219 upvotes and a long discussion about vote rigging and no one actually checked the code worked. Now that's chaos.\"</p></blockquote><p>A 12-hour governance debate. A win for democracy. And nobody tested the code.</p><div><table><tbody><tr></tr></tbody></table></div><p>Growth stabilized. Drama did not.</p><h2>Meanwhile: The Week in Merges</h2><p>Daily merges changed everything. Six PRs shipped in six days:</p><div><table><tbody><tr></tr><tr><td>Health indicators (broken)</td></tr><tr></tr></tbody></table></div><p> deserves a mention: <a href=\"https://github.com/skridlevsky/openchaos/pull/47\" target=\"_blank\" rel=\"noopener noreferrer\">PR #47</a> by <a href=\"https://github.com/bpottle\" target=\"_blank\" rel=\"noopener noreferrer\">@bpottle</a> transformed the site into a GeoCities time capsule — Comic Sans, scrolling marquee, butterfly cursor, MIDI player (you know the song), and a \"WIN CASH NOW\" popup.</p><p> added a Hall of Chaos — <a href=\"https://github.com/skridlevsky/openchaos/pull/60\" target=\"_blank\" rel=\"noopener noreferrer\">PR #60</a> by <a href=\"https://github.com/bigintersmind\" target=\"_blank\" rel=\"noopener noreferrer\">@bigintersmind</a> displays all previously merged PRs. The site now documents its own evolution.</p><p>A project about letting the internet do whatever it wants with code.</p><p>This week, the internet did whatever it wanted with the brand.</p><p>Someone created a  using OpenChaos branding.</p><p>I didn't create it. I have no control over it.</p><blockquote><p>\"A $CHAOS token was created using the OpenChaos name and branding.</p><ul><li>I did not create this token</li><li>I have no control over it</li></ul><p>If you're trading $CHAOS, know that I'm not involved.</p><p>Any official initiative would be announced here.\"</p></blockquote><p>Chaos doesn't stay contained.</p><p><a href=\"https://github.com/skridlevsky/openchaos/pull/13\" target=\"_blank\" rel=\"noopener noreferrer\">PR #13</a> — the Rust rewrite — is still waiting. 450+ votes. Merge conflicts. Week 4?</p><p><strong>1. Democracy beats maintainer judgment.</strong></p><p>I tried to reject a PR. The community said my rules didn't support it. They were right. Written rules &gt; vibes.</p><p><strong>2. Velocity creates its own problems.</strong></p><p>Daily merges mean less time to review. 219 people voted for a feature nobody tested. Speed has costs.</p><p><strong>3. Chaos doesn't stay contained.</strong></p><p>First it was a website. Then a governance experiment. Now there's a token. The brand has a life of its own.</p><p><strong>4. The community polices itself.</strong></p><p>The Trojan horse exposed a gap. \"No malware\" didn't cover manipulation. My veto got overruled because the written rules didn't support it.</p><p>I didn't want to write a constitution. The whole point of OpenChaos was letting go. But the project needed a floor — something that couldn't be voted away.</p><p> — 66 words. Immutable. CI-enforced.</p><pre><code>This file cannot be modified or deleted. PRs attempting to do so will fail CI.\n</code></pre><p>The constitution doesn't ban manipulation. It doesn't need to. It establishes:</p><ul><li>What can never be merged (code designed to harm users or systems)</li><li>What can never be deleted (the rules themselves)</li><li>Everything else remains chaos</li></ul><p>The community taught me: if you want different behavior, write different rules.</p><p>Day job changes coming in February. Merge time shifts to 19:00 UTC.</p><p>OpenChaos isn't going anywhere.</p><p><a href=\"https://github.com/FelixLttks\" target=\"_blank\" rel=\"noopener noreferrer\">@FelixLttks</a> is already back with new PRs. The Trojan horse guy. Submitting more code.</p><p><em>The next merge is today at 19:00 UTC.</em></p>",
      "contentLength": 5398,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1ql6zol/i_let_the_community_vote_on_what_code_gets_merged/"
    },
    {
      "title": "Anyone listen to the podcast \"Shell Game?\"",
      "url": "https://www.reddit.com/r/artificial/comments/1ql64a3/anyone_listen_to_the_podcast_shell_game/",
      "date": 1769209825,
      "author": "/u/Odballl",
      "guid": 38585,
      "unread": true,
      "content": "<p>In Season 1 (2024), journalist Evan Ratliff explored the potential for LLM powered voice cloning to delegate everything tedious from answering spam calls, doing therapy and hanging out on work meetings to see how the AI could manage being Evan for him. </p><p>In <a href=\"https://www.shellgame.co/p/minimum-viable-company\">Season 2</a> he tries creating a startup tech company using only AI agent employees, including the leadership! He's just a silent co-founder. </p><p>It's extremely entertaining, with plenty of shenanigans from LLMs going off the rails, hallucinating and doing their usual weird stuff.</p><p>This is basically an unpaid ad, I know, but I'm having a good time listening and it deserves a shout-out.</p>",
      "contentLength": 634,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Writing a Go SQL driver",
      "url": "https://www.dolthub.com/blog/2026-01-23-golang-sql-drivers/",
      "date": 1769207488,
      "author": "/u/zachm",
      "guid": 38562,
      "unread": true,
      "content": "<p>We’re building <a href=\"https://doltdb.com\">Dolt</a>, the world’s first version-controlled SQL database. Most\nof our customers run Dolt as a server in Docker and connect to it over the network like any other\ndatabase server. But for programs written in Go, you can also connect to a Dolt database without a\nseparate server process, similar to SQLite. We call this the embedded use case, and it has suddenly\nbecome a lot more relevant with <a href=\"https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04\">Gas\nTown</a> migrating its agentic memory\nstorage to <a href=\"https://www.dolthub.com/blog/2026-01-22-agentic-memory/#dolt-for-agentic-memory\">use Dolt as its\nbackend</a>. Since Gas\nTown is a Go process, it can use the embedded Dolt driver to communicate with an embedded Dolt\ndatabase.</p><p>So, because Dolt’s <a href=\"https://github.com/dolthub/driver\">embedded driver</a> is about to get a lot more\naction than it’s used to, we thought this would be a good time to give a tour of how it works. This\npattern is possible through the magic of Go’s  package, which lets you define a\ndatabase connection that any Go program can use to talk to your SQL backend with a single \nstatement.</p><p>In this tour, we’ll look at how Go’s SQL drivers work under the hood and show you how Dolt\nimplements one to provide access to an embedded Dolt database. Let’s take a look.</p><p>Go’s <a href=\"https://pkg.go.dev/database/sql/driver\">SQL driver package</a> is an abstraction that lets other\nsoftware libraries vend their own SQL connection logic through a common set of\ninterfaces. Application developers use a common interface to connect to any SQL database (MySQL,\nPostgres, MariaDB, SQL Server, Dolt, etc.) without worrying about the specifics of the wire protocol\nfor that particular database.</p><p>This is best illustrated with an example. Here’s how you connect to MySQL and read some rows.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>Let’s go over this example line by line and see what it does.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>This line is kind of magic and weird: the  tells Go that you’re not using any symbols from this\npackage, you’re importing it just for its side-effects. In this case, those side effects are\nregistering a driver called  with  package.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>A DSN is a data source name, almost always resembling a URL but often with some extra bits. Each\ndatabase vendor has their own format for these DSNs, but they all look pretty similar. You usually\nembed the user name and password and some other metadata, like which database you want to connect\nto, in this string.</p><p>To open a connection, you just call  with the name of the driver and its matching\nDSN. Easy!</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p> on a connection takes a query string and returns the resulting rows.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p> puts the result of the query into normal Go datatypes, like  or .</p><p>There are more complicated access patterns, and we haven’t touched on things like \nstatements, but those are the basics.</p><p>Let’s see how a  is implemented by examining the Dolt driver.</p><p>Dolt’s embedded database driver is defined very simply.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>The  function is the special magic that requires you to use the  import on the database\ndriver of your choice. At program execution time, this code calls  to tell the\n package there’s a  implementation named “dolt”.</p><p>Next we need a way to get a connection to the embedded database, which we do with the \nmethod. In the sample below, I’ve removed most of the error handling for brevity. You can read the\nfull source <a href=\"https://github.com/dolthub/driver/blob/main/driver.go\">here</a>.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>Dolt’s DSN format is basically a file URL with some extra query params. It looks something like\nthis:</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>We parse this URL and extract the relevant query params out of it, then use those to create our\ninternal SQL engine representation, which is what Dolt uses to execute queries internally.</p><p>This gives us a  with an engine it can use to execute queries. Let’s look at that next.</p><p>Unlike the driver itself, the  type has some state. But it’s still very simple. Its main\njob is to pass information down the line, from a call to  to a  type.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>The  impelementation is where real work begins to happen, in the  method.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>Here you can see that the statement’s real work is nearly all delegated to the SQL engine we created\nin the initial call to . The rest of its functionality is just to translate between the\nresults that Dolt’s SQL engine provides and what the  interfaces expect. For that, we\nhave the  type.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>And that’s it! The job of these interfaces is really to act as a translation layer between the wire\nprotocol the database uses and the types that  expects. In the case of the Dolt\nembedded driver, there’s no wire protocol: we’re accessing the SQL engine that queries the database\non disk directly and using the data structures it returns natively.</p><p>A lot of developers prefer to use an ORM tool when interacting with their database, and in the Go\nworld, the most popular ORM library is <a href=\"https://gorm.io/index.html\">Gorm</a>. Gorm usually manages your\nDB connection for you automatically, but in the case of Dolt embedded we want something slightly\ndifferent: we want it to use its MySQL dialect and logic but connect to an embedded Dolt database\nconnection. This is pretty easy to do.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>Note that we need to import both the MySQL driver and the Dolt driver for this to work. But\notherwise, it’s a standard Gorm setup.</p><p>Go’s database drivers are a simple way for database application developers to connect to any of the\nmany different SQL databases you can run in production with a common interface. Standardizing these\ninterfaces made it easier for libraries like Gorm to offer support for a larger variety of different\ndatabase vendors, since the details of the wire protocols and other tricky bits are hidden by the\nabstraction for most uses. And it’s pretty simple to write your own driver if you have a SQL data\nsource you want other people to connect to.</p><p>Want to discuss Go database drivers or learn more about Dolt? Visit us on the <a href=\"https://discord.gg/gqr7K4VNKe\">DoltHub\nDiscord</a>, where our engineering team hangs out all day. Hope to see\nyou there.</p>",
      "contentLength": 5592,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1ql555i/writing_a_go_sql_driver/"
    },
    {
      "title": "kubernetes-sigs/headlamp in 2025: Project Highlights",
      "url": "https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/",
      "date": 1769206050,
      "author": "/u/illumen",
      "guid": 38475,
      "unread": true,
      "content": "<div>By <b>Evangelos Skopelitis (Microsoft)</b> |\n<time datetime=\"2026-01-22\">Thursday, January 22, 2026</time></div><p><em>This announcement is a recap from a post originally <a href=\"https://headlamp.dev/blog/2025/11/13/headlamp-in-2025\">published</a> on the Headlamp blog.</em></p><p><a href=\"https://headlamp.dev/\">Headlamp</a> has come a long way in 2025. The project has continued to grow – reaching more teams across platforms, powering new workflows and integrations through plugins, and seeing increased collaboration from the broader community.</p><p>We wanted to take a moment to share a few updates and highlight how Headlamp has evolved over the past year.</p><h3>Joining Kubernetes SIG UI</h3><p>This year marked a big milestone for the project: Headlamp is now officially part of Kubernetes <a href=\"https://github.com/kubernetes/community/blob/master/sig-ui/README.md\">SIG UI</a>. This move brings roadmap and design discussions even closer to the core Kubernetes community and reinforces Headlamp’s role as a modern, extensible UI for the project.</p><h3>Linux Foundation mentorship</h3><p>This year, we were excited to work with several students through the Linux Foundation’s Mentorship program, and our mentees have already left a visible mark on Headlamp:</p><ul><li><a href=\"https://github.com/adwait-godbole\"></a> built the KEDA plugin, adding a UI in Headlamp to view and manage KEDA resources like ScaledObjects and ScaledJobs.</li><li><a href=\"https://github.com/DhairyaMajmudar\"></a> set up an OpenTelemetry-based observability stack for Headlamp, wiring up metrics, logs, and traces so the project is easier to monitor and debug.</li><li><a href=\"https://www.linkedin.com/in/aishwarya-ghatole-506745231/\"></a> led a UX audit of Headlamp plugins, identifying usability issues and proposing design improvements and personas for plugin users.</li><li><a href=\"https://github.com/SinghaAnirban005\"></a> developed the Karpenter plugin, giving Headlamp a focused view into Karpenter autoscaling resources and decisions.</li><li><a href=\"https://github.com/useradityaa\"></a> improved Gateway API support, so you can see networking relationships on the resource map, as well as improved support for many of the new Gateway API resources.</li><li><a href=\"https://github.com/upsaurav12\"></a> worked on backend caching for Kubernetes API calls, reducing load on the API server and improving performance in Headlamp.</li></ul><p>Managing multiple clusters is challenging: teams often switch between tools and lose context when trying to see what runs where. Headlamp solves this by giving you a single view to compare clusters side-by-side. This makes it easier to understand workloads across environments and reduces the time spent hunting for resources.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/multi-cluster-view.png\" alt=\"Multi-cluster view\"><em>View of multi-cluster workloads</em><p>Kubernetes apps often span multiple namespaces and resource types, which makes troubleshooting feel like piecing together a puzzle. We’ve added  to give you an application-centric view that groups related resources across multiple namespaces – and even clusters. This allows you to reduce sprawl, troubleshoot faster, and collaborate without digging through YAML or cluster-wide lists.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/projects-feature.png\" alt=\"Projects feature\"><em>View of the new Projects feature</em><ul><li>New “Projects” feature for grouping namespaces into app- or team-centric projects</li><li>Extensible Projects details view that plugins can customize with their own tabs and actions</li></ul><h3>Navigation and Activities</h3><p>Day-to-day ops in Kubernetes often means juggling logs, terminals, YAML, and dashboards across clusters. We redesigned Headlamp’s navigation to treat these as first-class “activities” you can keep open and come back to, instead of one-off views you lose as soon as you click away.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/new-task-bar.png\" alt=\"New task bar\"><ul><li>A new task bar/activities model lets you pin logs, exec sessions, and details as ongoing activities</li><li>An activity overview with a “Close all” action and cluster information</li><li>Multi-select and global filters in tables</li></ul><p>When something breaks in production, the first two questions are usually “where is it?” and “what is it connected to?” We’ve upgraded both search and the map view so you can get from a high-level symptom to the right set of objects much faster.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/advanced-search.png\" alt=\"Advanced search\"><em>View of the new Advanced Search feature</em><ul><li>An Advanced search view that supports rich, expression-based queries over Kubernetes objects</li><li>Improved global search that understands labels and multiple search items, and can even update your current namespace based on what you find</li><li>EndpointSlice support in the Network section</li><li>A richer map view that now includes Custom Resources and Gateway API objects</li></ul><p>We’ve put real work into making OIDC setup clearer and more resilient, especially for in-cluster deployments.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/user-info.png\" alt=\"User info\"><em>View of user information for OIDC clusters</em><ul><li>User information displayed in the top bar for OIDC-authenticated users</li><li>PKCE support for more secure authentication flows, as well as hardened token refresh handling</li><li>Documentation for using the access token using <code>-oidc-use-access-token=true</code></li><li>Improved support for public OIDC clients like AKS and EKS</li></ul><p>We’ve broadened how you deploy and source apps via Headlamp, specifically supporting vanilla Helm repos.</p><ul><li>A more capable Helm chart with optional backend TLS termination, PodDisruptionBudgets, custom pod labels, and more</li><li>Improved formatting and added missing access token arg in the Helm chart</li><li>New in-cluster Helm support with an  flag and a service proxy</li></ul><p>Finally, we’ve spent a lot of time on the things you notice every day but don’t always make headlines: startup time, list views, log viewers, accessibility, and small network UX details. A continuous accessibility self-audit has also helped us identify key issues and make Headlamp easier for everyone to use.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/learn-section.png\" alt=\"Learn section\"><em>View of the Learn section in docs</em><ul><li>Significant desktop improvements, with up to 60% faster app loads and much quicker dev-mode reloads for contributors</li><li>Numerous table and log viewer refinements: persistent sort order, consistent row actions, copy-name buttons, better tooltips, and more forgiving log inputs</li><li>Accessibility and localization improvements, including fixes for zoom-related layout issues, better color contrast, improved screen reader support, and expanded language coverage</li><li>More control over resources, with live pod CPU/memory metrics, richer pod details, and inline editing for secrets and CRD fields</li><li>A refreshed documentation and plugin onboarding experience, including a “Learn” section and plugin showcase</li><li>A more complete NetworkPolicy UI and network-related polish</li><li>Nightly builds available for early testing</li></ul><h2>Plugins and extensibility</h2><p>Discovering plugins is simpler now – no more hopping between Artifact Hub and assorted GitHub repos. Browse our dedicated <a href=\"https://headlamp.dev/plugins\">Plugins page</a> for a curated catalog of Headlamp-endorsed plugins, along with a showcase of featured plugins.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/plugins-page.png\" alt=\"Plugins page\"><em>View of the Plugins showcase</em><p>Managing Kubernetes often means memorizing commands and juggling tools. Headlamp’s new AI Assistant changes this by adding a natural-language interface built into the UI. Now, instead of typing  or digging through YAML you can ask, “Is my app healthy?” or “Show logs for this deployment,” and get answers in context, speeding up troubleshooting and smoothing onboarding for new users. Learn more about it <a href=\"https://headlamp.dev/blog/2025/08/07/introducing-the-headlamp-ai-assistant/\">here</a>.</p><p>Alongside the new AI Assistant, we’ve been growing Headlamp’s plugin ecosystem so you can bring more of your workflows into a single UI, with integrations like Minikube, Karpenter, and more.</p><p>Highlights from the latest plugin releases:</p><ul><li>Minikube plugin, providing a locally stored single node Minikube cluster</li><li>Karpenter plugin, with support for Azure Node Auto-Provisioning (NAP)</li><li>KEDA plugin, which you can learn more about <a href=\"https://headlamp.dev/blog/2025/07/25/enabling-event-driven-autoscaling-with-the-new-keda-plugin-for-headlamp/\">here</a></li></ul><p>Alongside new additions, we’ve also spent time refining plugins that many of you already use, focusing on smoother workflows and better integration with the core UI.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/backstage-plugin.png\" alt=\"Backstage plugin\"><em>View of the Backstage plugin</em><ul><li>: Updated for Flux v2.7, with support for newer CRDs, navigation fixes so it works smoothly on recent clusters</li><li>: Now supports Helm repos in addition to Artifact Hub, can run in-cluster via /serviceproxy, and shows both current and latest app versions</li><li>: Improved card layout and accessibility, plus dependency and Storybook test updates</li><li>: Dependency and build updates, more info <a href=\"https://headlamp.dev/blog/2025/11/05/strengthening-backstage-and-headlamp-integration/\">here</a></li></ul><p>We’ve focused on making it faster and clearer to build, test, and ship Headlamp plugins, backed by improved documentation and lighter tooling.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/plugin-development.png\" alt=\"Plugin development\"><em>View of the Plugin Development guide</em><ul><li>Improved type checking for Headlamp APIs, restored Storybook support for component testing, and reduced dependencies for faster installs and fewer updates</li><li>Documented plugin install locations, UI signifiers in Plugin Settings, and labels that differentiated shipped, UI-installed, and dev-mode plugins</li></ul><p>We've also been investing in keeping Headlamp secure – both by tightening how authentication works and by staying on top of upstream vulnerabilities and tooling.</p><ul><li>We've been keeping up with security updates, regularly updating dependencies and addressing upstream security issues.</li><li>We tightened the Helm chart's default security context and fixed a regression that broke the plugin manager.</li><li>We've improved OIDC security with PKCE support, helping unblock more secure and standards-compliant OIDC setups when deploying Headlamp in-cluster.</li></ul><p>Thank you to everyone who has contributed to Headlamp this year – whether through pull requests, plugins, or simply sharing how you're using the project. Seeing the different ways teams are adopting and extending the project is a big part of what keeps us moving forward. If your organization uses Headlamp, consider adding it to our <a href=\"https://github.com/kubernetes-sigs/headlamp/blob/main/ADOPTERS.md\">adopters list</a>.</p><p>If you haven't tried Headlamp recently, all these updates are available today. Check out the latest Headlamp release, explore the new views, plugins, and docs, and share your feedback with us on Slack or GitHub – your feedback helps shape where Headlamp goes next.</p>",
      "contentLength": 9169,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/kubernetes/comments/1ql4jta/kubernetessigsheadlamp_in_2025_project_highlights/"
    },
    {
      "title": "Reflection: C++’s Decade-Defining Rocket Engine - Herb Sutter - CppCon 2025",
      "url": "https://www.youtube.com/watch?v=7z9NNrRDHQU",
      "date": 1769202428,
      "author": "/u/BlueGoliath",
      "guid": 38638,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1ql304c/reflection_cs_decadedefining_rocket_engine_herb/"
    },
    {
      "title": "Zotero 8 released (reference management)",
      "url": "https://www.zotero.org/blog/zotero-8/",
      "date": 1769200318,
      "author": "/u/dbcoopernz",
      "guid": 38445,
      "unread": true,
      "content": "<p>We’re excited to announce our latest major release, Zotero 8. Zotero 8 builds on the new design and features of <a href=\"https://www.zotero.org/blog/zotero-7/\">Zotero 7</a> and includes a huge number of improvements and refinements.</p><h2>Redesigned Citation Dialog</h2><p>Zotero 8 introduces a new unified citation dialog, replacing the previous citation dialog (the “red bar”), the “classic” citation dialog, and the Add Note dialog (the “yellow bar”).</p><p>The new dialog has two modes: List mode and Library mode. List mode lets you quickly search for citations from across your Zotero libraries by title, creator, and year. Library mode includes a library browser, letting you find items in specific libraries or collections. You can switch between the two modes with a single click, preserving any added items or entered search terms. By default, it will open in the last mode you used, but you can choose a different default mode in the settings.</p><p>In <a href=\"https://www.zotero.org/blog/zotero-7/\">Zotero 7</a>, we added the ability to quickly add citations for selected items and open documents. In the new dialog, these options are available in both List mode and Library mode, so you can make these quick selections even if you otherwise prefer to add items via the library browser.</p><p>As before, once you’ve selected an item, you can click on its bubble to customize the citation with a page number, prefix, etc. It’s also now possible to add any locator — not just a page number — right from the search bar by typing the full or short name (e.g., “line 10” or “l. 10” after the citation and pressing Enter/Return.</p><p>You can switch between adding citations and adding notes using buttons in the bottom left, corresponding to the Add/Edit Citation and Add Note buttons in your word processor.</p><p>(For those coming from the classic dialog, note that there’s no text field to make manual edits to citations. It’s been possible to edit citations directly in the document for many years, which is why the red bar didn’t include such a text field either. More importantly, though, such manual edits should be avoided in almost all cases. Instead, <a href=\"https://www.zotero.org/support/word_processor_plugin_usage#customizing_cites\">customize the citation</a> via the citation dialog, which will allow Zotero to continue to update the citation as necessary.)</p><h2>Annotations in the Items List</h2><p>Annotations you make on PDFs, EPUBs, and webpage snapshots now show up under their parent attachments in the items list.</p><p>Showing annotations in the items list makes it easier to view annotations across a library or collection, and it also makes it possible to search for annotations directly. For example, you can search for all annotations in a collection with a given tag and then create a note from those annotations or copy them to an external text editor with Quick Copy.</p><p>In Advanced Search, you can use “Item Type” “is” “Annotation” to match annotations or use the Annotation Text and Annotation Comment search conditions to search for specific parts of the annotation.</p><p>You can assign tags to selected annotations by dragging them to the tag selector, just like other items.</p><p>Selected annotations show up in the item pane, grouped by top-level item.</p><h2>Reader Appearance Panel with Theme Support</h2><p>We’ve added a new Appearance panel in the reader that provides quick access to view settings and introduces support for reader themes.</p><p>The view settings are per-document settings. Themes are applied globally for all documents, including in the attachment preview in the item pane, and apply to PDFs, EPUBs, and webpage snapshots.</p><p>We offer a number of built-in themes (“Dark”, “Snow”, “Sepia”), and you can create custom themes just by specifying a foreground and background color. (Some other theme engines require additional accent colors, but we’ve tried to make this as simple as possible for users by automatically adjusting other colors based on the foreground and background colors.) You can set a different theme that applies to light mode and dark mode.</p><p>The themes replace the previous on-by-default “Use Dark Mode for Content” option, which inverted images in dark mode. We’re now simply darkening images a bit when using a dark theme. Images and ink annotations in the reader sidebar and note editor are now only darkened as well (and only when Zotero itself is in dark mode).</p><p>When possible, we also try to apply themes to PDF pages containing full-page images, such as scanned papers, by replacing whitish/dark colors with theme colors. (Otherwise we simply darken the page slightly.)</p><p>It’s now possible to open notes in tabs in addition to separate windows. Note tabs fill the whole window, with wide margins for better readability and a clean, distraction-free space for note-taking.</p><p>By default, double-clicking a note in the items list will open it in a tab. You can choose to open the note in the other space from the context menu, and you can change the default behavior using the “Open notes in new windows instead of tabs” setting in the General pane of the settings.</p><p>Notes in tabs have a separate font size setting in the View menu.</p><h2>Reading Mode for Webpage Snapshots</h2><p>Reading Mode reformats webpage snapshots for easier reading, with unnecessary page elements removed. You can adjust line height and other view options from the Appearance panel.</p><p>We’ve reworked the tabs menu to make it faster to interact with via the keyboard.</p><p>You can now press Ctrl/Cmd-; to bring up the menu at any time.</p><p>Once the menu is open, it simultaneously accepts search input, up/down navigation, and row selection, without the need to move between different parts of the menu. You can simply start typing the name of an open tab and then press Enter/Return to switch to it once you’ve narrowed down the list.</p><p>It’s also possible to quickly close multiple tabs by moving between the row close buttons with up/down and pressing space bar to close a tab.</p><p>Zotero now automatically keeps attachment filenames in sync with parent item metadata as you make changes (e.g., changing the title). In previous versions, while Zotero would automatically rename files when you first added them to your library, if you later edited the item’s metadata, you would need to right-click on the attachment and select “Rename File from Parent Metadata”.</p><p>You can configure which file types renaming applies to from the General tab of the Zotero settings.</p><p>After upgrading to this version, existing eligible files that don’t match the current filename format won’t be automatically renamed, but you can choose to rename them en masse from the Zotero settings. Zotero will also prompt you to rename all files if you change the filename format.</p><p>“Rename File from Parent Metadata” has been removed from the item context menu. If a filename doesn’t match the configured filename format (e.g., because automatic renaming is disabled or you changed the format but didn’t choose to rename all files), you can click the “Rename File to Match Parent Item” button next to the filename in the attachment’s item pane to rename it.</p><h2>New Attachment Title Options</h2><p>Zotero 7 introduced more consistent handling of <a href=\"https://www.zotero.org/support/kb/attachment_title_vs_filename\">attachment titles</a>, preserving simpler, less-redundant titles (e.g., “Full Text PDF” or “Preprint PDF”) in cases where the title was previously changed to match the filename. Zotero 8 further refines its renaming and titling logic when adding multiple and/or non-primary attachments, to bring the functionality better in line with the intended behavior.</p><p>We’ve also added a “Normalize Attachment Titles” option under Tools → Manage Attachments to update old primary attachments with titles matching the filename to use simpler titles such as “PDF”.</p><p>While we recommend the default behavior, allowing Zotero to rename primary files and keep them renamed while using simpler titles in the items list, if you really prefer to view filenames instead of titles, you can now enable “Show attachment filenames in the items list” option in the General pane of the settings.</p><p>Zotero 8 adds a version for Linux running on ARM64 devices. This includes ARM-based Chromebooks, Apple Silicon Macs running Linux (Linux VMs, Asahi Linux), and Raspberry Pis.</p><p>If you’ve been unable to run Zotero on your ARM-based device, or you’ve been running the x86_64 version under emulation, give it a try.</p><h2>User Interface Improvements</h2><p>We’ve made a number of changes across the interface to address common requests:</p><ul><li>A new button in the library tab allows you to quickly close the item pane without dragging its edge or using the menus.</li><li>You can reorder item pane sections by dragging their icons in the side navigation bar.</li><li>You can drag items, collections, and searches into the trash.</li><li>You can drag attachments, notes, and related items from the item pane (e.g., to copy files to the filesystem or use Quick Copy).</li><li>Collections automatically expand when you drag over them, making it easier to drop collections or items into subcollections.</li><li>You can delete attachments from the item pane.</li><li>Tabs maintain their size as you close them for faster closing of multiple tabs.</li></ul><p>With Zotero 8, the Zotero Connector save popup can autocomplete tags in your Zotero library and allows you to add a note to items as you save them.</p><p>Zotero 8 includes much more than we can list here. See the <a href=\"https://www.zotero.org/support/8.0_changelog\">changelog</a> for additional details.</p><p>If you’re already running Zotero, you can upgrade from within Zotero by going to Help → “Check for Updates…”.</p><p><small>\n\t\t\t\t\t\t\t\tThis entry was posted\n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\ton Thursday, January 22nd, 2026 at 12:52 pm by Dan Stillman\t\t\t\t\t\t\t\tand is filed under <a href=\"https://www.zotero.org/blog/category/features/\" rel=\"category tag\">Features</a>, <a href=\"https://www.zotero.org/blog/category/news/\" rel=\"category tag\">News</a>.\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</small></p>",
      "contentLength": 9491,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1ql232k/zotero_8_released_reference_management/"
    },
    {
      "title": "Flabbergasted by VM performance (on my Intel Xe 13th gen integrated graphics, so different from i915 in some ways)",
      "url": "https://www.reddit.com/r/linux/comments/1ql1liu/flabbergasted_by_vm_performance_on_my_intel_xe/",
      "date": 1769199212,
      "author": "/u/Natural-Bowl5439",
      "guid": 38444,
      "unread": true,
      "content": "<p>After breaking the kernel trying to share the GPU with the help of a non-mature SR-IOV implementation, all this in order to have maximum GPU performance between host and guest, I decided after the defeat to go with the traditional GPU acceleration instead.</p><p>I feared the old days of trying virtualbox and seeing that the \"acceleration\" was just good for windows aero, hence the reason i explored SR-IOV. I expected VMware's performance to not be far from my memories with virtualbox, but to my surprise i could allocate 8GB of graphics memory to the VM! Then i tested resident evil 6 and it ran at playable framerate! (around 40fps although at low settings but 1080p resolution) </p><p>I hope i will still be pleasantly surprised when I'll try the real use of the windows VM : video editing with Capcut and video rotoscoping with Photoshop. </p>",
      "contentLength": 832,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Overrun with AI slop, cURL scraps bug bounties to ensure \"intact mental health\"",
      "url": "https://arstechnica.com/security/2026/01/overrun-with-ai-slop-curl-scraps-bug-bounties-to-ensure-intact-mental-health/",
      "date": 1769197621,
      "author": "/u/Drumedor",
      "guid": 38394,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1ql0w5p/overrun_with_ai_slop_curl_scraps_bug_bounties_to/"
    },
    {
      "title": "Anyone here built microservices in Go with GraphQL, gRPC, and RabbitMQ?",
      "url": "https://www.reddit.com/r/golang/comments/1ql0g9l/anyone_here_built_microservices_in_go_with/",
      "date": 1769196646,
      "author": "/u/riswan_22022",
      "guid": 38446,
      "unread": true,
      "content": "<p>I’m working with Go and exploring a microservices architecture using , , , and a database (MongoDB / PostgreSQL ).</p><p>I wanted to ask if anyone here has built something similar in real projects. If you have a , example project, or even a blog explaining your approach, I’d really appreciate it if you could share.</p>",
      "contentLength": 313,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "mmdr: A native Rust Mermaid renderer (500-1000x faster than mermaid-cli)",
      "url": "https://github.com/1jehuang/mermaid-rs-renderer",
      "date": 1769194855,
      "author": "/u/Medium_Anxiety_8143",
      "guid": 38443,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/rust/comments/1qkzmpg/mmdr_a_native_rust_mermaid_renderer_5001000x/"
    },
    {
      "title": "[D] Is Grokking unique to transformers/attention?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qkz5do/d_is_grokking_unique_to_transformersattention/",
      "date": 1769193818,
      "author": "/u/Dependent-Shake3906",
      "guid": 38454,
      "unread": true,
      "content": "<p>Is Grokking unique to attention mechanism, every time I’ve read up on it seems to suggest that’s it a product of attention and models that utilise it. Is this the case or can standard MLP also start grokking?</p>",
      "contentLength": 212,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Breaking Key-Value Size Limits: Linked List WALs for Atomic Large Writes",
      "url": "https://unisondb.io/blog/breaking-kv-size-limits-linked-list-wal/",
      "date": 1769193520,
      "author": "/u/ankur-anand",
      "guid": 38398,
      "unread": true,
      "content": "<img src=\"https://unisondb.io/images/wal_linked_list.svg\" alt=\"Diagram of UnisonDB's corruption-proof WAL path\"><h2>The “Hard Wall” of Distributed Systems<a href=\"https://unisondb.io/blog/breaking-kv-size-limits-linked-list-wal/#the-hard-wall-of-distributed-systems\">#</a></h2><p>The majority of distributed Key-Value systems have some kind of limit. This limit exists for a purpose: it prevents a single request from overwhelming memory or stalling replication.</p><p>Whether it’s the 512KB cap in <a href=\"https://developer.hashicorp.com/consul/docs/automate/kv\" target=\"_blank\" rel=\"noopener noreferrer\">Consul</a>\nor the 1.5MB default in <a href=\"https://etcd.io/docs/v3.6/dev-guide/limit/\" target=\"_blank\" rel=\"noopener noreferrer\">etcd</a>\n, these boundaries are a survival mechanism. In a distributed cluster, every byte you write has to be replicated via protocols like Raft. If a single record is too large, it creates “head-of-line blocking”—the entire replication pipeline slows down just to move one massive object, potentially causing heartbeats to fail and nodes to drop out of the cluster.</p><p>At UnisonDB, we respect these same limits to protect our own system health. We need to be even more cautious about this, as we are not just doing Raft replication for writes. We also have high-fanout ISR (in-sync-replica) based edge replicas, meaning a single write can propagate to many more nodes.</p><h2>Why ISR Edge Replication Changes the Stakes<a href=\"https://unisondb.io/blog/breaking-kv-size-limits-linked-list-wal/#why-isr-edge-replication-changes-the-stakes\">#</a></h2><p>In our environment, the “Hard Wall” isn’t just about protecting memory or Raft heartbeats within a small core cluster. It is about protecting the replication integrity and lag across a vast edge network.</p><ul><li><p><strong>Heartbeat Fragility in Edge Environments</strong>: Edge networks often have variable latency and less reliable connections. If replication takes too long because of oversized records, the system might falsely flag an edge node as “out of sync,” triggering expensive and unnecessary full re-syncs, wasting bandwidth and compute.</p></li><li><p><strong>Memory Pressure on Constrained Edge Nodes</strong>: Unlike robust core cluster nodes, edge replicas frequently run on more resource-constrained hardware. Pushing a 20MB block in a single request could easily cause an Out Of Memory (OOM) event on a smaller edge instance, leading to outages at the edge.</p></li></ul><p>But even with these constraints, we also understand that the need for large Key-Value storage hasn’t gone away—it has actually intensified. As an Edge-replicated, general-purpose Multi-Modal database, we see this constantly. Whether it’s a massive JSON configuration or high-dimensional vectors for AI use cases, modern data frequently pushes past those old boundaries.</p><p>This size pressure usually shows up in two ways:</p><ol><li>: A single transaction involving multiple Key-Value pairs that, when grouped together, exceed the 1MB limit.</li><li>: A single row containing hundreds of columns where the aggregate size of the update blows past the ceiling.</li></ol><p>In both cases, the user will expects the same ironclad KV guarantees they get with a tiny 1KB write. You shouldn’t have to sacrifice Atomicity just because your data model is complex.</p><h2>Why Manual Chunking Fails<a href=\"https://unisondb.io/blog/breaking-kv-size-limits-linked-list-wal/#why-manual-chunking-fails\">#</a></h2><p>When engineers hit a size limit, the instinctive reaction is to “chunk” the data by splitting a 10MB write into ten separate 1MB requests. This is where things get dangerous. Without a specialized architecture, you lose the atomicity promise. If your connection drops after chunk seven, the database is left in a “zombie” state. You have a partial update that is neither the old version nor the new one. In a real database, the rule is absolute: it must be all or nothing.</p><h2>Unisondb Solution: A WAL That Remembers Its Past<a href=\"https://unisondb.io/blog/breaking-kv-size-limits-linked-list-wal/#unisondb-solution-a-wal-that-remembers-its-past\">#</a></h2><p>To solve this at UnisonDB, <strong>we stopped looking at the Write-Ahead Log (WAL) just as a flat, sequential file</strong>. Instead, we treated it as a backward-linked list.</p><p>By adding a simple breadcrumb—a PrevTxnWalIndex—to every WAL record that are part of the same transaction, each chunk of data points back to the one that came before it. This allows us to stitch a single, massive transaction together across multiple physical writes without ever sending a request that exceeds the safety limit.</p><p>This logic is the backbone of how we handle large multi-modal data. The lifecycle of a transaction in our dbkernel looks like this:</p><ol><li><p>: We write an anchor record to the WAL. This initializes the transaction and generates a unique ID.</p></li><li><p>: As you stream your data chunks, each one links back to the previous disk offset. Even if you send 50 chunks to stay under the limit, the database knows they belong to one chain.</p></li><li><p>: This is the atomic switch. The final record acts as the seal.</p></li></ol><blockquote><p>Nothing becomes visible to you the user until that COMMIT record is successfully flushed to disk. If the stream breaks halfway through, the database simply ignores those dangling fragments during the next recovery scan.</p></blockquote><p>The engine needs to know exactly where the previous piece of the puzzle lives on disk. We use physical disk offsets to create this chain.</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>Here is how this looks inside the UnisonDB transaction engine. Notice how we track the prevOffset to build the link on the fly. In the AppendKVTxn function, we take the current prevOffset and bake it into the new log record before appending it to the WAL.</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>When  happens, the Commit function writes the final link. Only after the WAL confirms the commit do we flush the data to the in-memory MemTable.</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>When UnisonDB reboots after a crash, the recovery process starts from the last known checkpoint. As the engine scans the log forward from that point, it looks specifically for COMMIT records. Because our transaction chunks are chained together using physical disk offsets, the engine has a clear map to follow.</p><p>When a commit record is encountered, the engine uses the PrevTxnWalIndex to walk the chain of that specific transaction. It jumps from the commit record to the previous data chunk, and then to the one before that, continuing until it reaches the BEGIN record. This allows the engine to gather all the related pieces of a large Key-Value pair, Wide-Column row, or LOB without having to inspect unrelated transaction data that might be sitting in between those chunks.</p><h3>Reconstructing the Value<a href=\"https://unisondb.io/blog/breaking-kv-size-limits-linked-list-wal/#reconstructing-the-value\">#</a></h3><p>The GetTransactionRecords function is what performs this backward walk. It takes the offset of the commit record and follows the trail until the chain is complete.</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>By chaining the records this way, we ensure that a large value is only rebuilt if the final commit record exists. If the engine finds chunks that don’t lead to a commit, it simply ignores them. This keeps the data consistent and ensures that the all or nothing promise is maintained for every data model we support.</p><p>Building a database is often a game of trade-offs. You want system stability, but you also want to support modern, heavy workloads like AI and complex multi-modal schemas.</p><p>By treating the Write-Ahead Log as a linked list, we found a way to have both. We keep our network requests small and safe, but we allow our data to be as large as it needs to be.</p><p>Whether it is a simple Key-Value pair, a massive Wide-Column row, or a Large Object, the atomicity promise remains unbroken.</p><p>If you found this article helpful, or if you’re interested in the future of edge-replicated data, we’d love your support.</p>",
      "contentLength": 6798,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qkz0iz/breaking_keyvalue_size_limits_linked_list_wals/"
    },
    {
      "title": "wayscriber 0.9.9 released!",
      "url": "https://www.reddit.com/r/linux/comments/1qkyo99/wayscriber_099_released/",
      "date": 1769192765,
      "author": "/u/Leading_Yam1358",
      "guid": 38397,
      "unread": true,
      "content": "<p>Wayscriber is a live annotation tool for Linux(Wayland) - a draw-on-anything overlay for demos, teaching, or quick callouts. Or just draw over any app or screen for funs :)</p><p>You get pens/highlighters/shapes/Text plus zoom, freeze, click highlights, and fast screenshots. </p><p>It is lightweight, written in Rust, and highly customizable.</p><p>Has multiple boards and pages per boards. Can customise it all.</p><p>Set up as daemon/tray so you can show or hide it any time.</p><p>It runs as a lightweight overlay and has an optional GUI Configurator. You can also customise all via TOML file. </p><p>Give it a try. Star and spread the word if you like it. </p><p>I am looking forward to any feedback. </p><p>The goal atm is to make it as powerful as possible while keeping it simple by default, and not overwhelming for new users.</p><p># Wayscriber 0.9.9 (since v0.9.8) - this is the biggest update so far!</p><p>- Multi‑board support with improved board/page picker, status bar toggles, and safe delete confirmations.</p><p>- New tools: eraser tool + variable‑thickness stylus lines.</p><p>- New workflows: command palette, guided tour onboarding, configurable presenter mode.</p><p>- Major rendering/perf upgrades via damage tracking (dirty‑rect) and caching.</p><p>- Boards toolbar section, board/page toggles in status bar, board picker improvements.</p><p>- Confirmations for board/page deletion + timeouts; board picker redraw on close.</p><p>- Quick help overlay + keybinding; help overlay layout refinements.</p><p>- Command palette with Unicode‑safe search.</p><p>- Guided tour onboarding, welcome toast, and recovery hardening.</p><p>- Presenter mode: new toggle/bind, constraints, tool switching allowed.</p><p>- Optional numbered arrow labels + reset action and toolbar toggle.</p><p>- Text controls enabled by default.</p><p>- Toolbars: pinned toolbars shown by default, improved drawers, stable drag via pointer lock.</p><p>- Tooltips: better placement, selection shortcut, color swatch tooltips w/ bindings.</p><p>- UI polish: View tab renamed to Canvas, zoom actions toggle, attention dot + More hint.</p><p>- Defaults: Ubuntu/GNOME PageUp/PageDown page navigation bindings.</p><p>- Damage tracking/dirty‑rect rendering for faster redraws.</p><p>- Cached help overlay layout/text and badge extents.</p><p>- Optimized eraser hover indices, selection cloning, spatial hit tests.</p><p>- Preallocated dirty regions + pooled damage tracking improvements.</p><p>- No‑vsync frame rate cap.</p><p>- Autosave scheduling + tracking; fixes for autosave clearing.</p><p>- Better tablet pressure handling.</p><p>- Clipboard fallback exit/retry fix.</p><p>- Screenshot suppression timing fix.</p><p>- Tooltip placement + board picker spacing fixes.</p><p>- Pango text rendering for UI labels.</p><p>- Nix flake packaging + install docs.</p><p>- Config/docs updates and refactors for action metadata + toolbar constants.</p><p>Thanks @n3oney for the first contribution!</p>",
      "contentLength": 2716,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "I like GitLab",
      "url": "https://www.whileforloop.com/en/blog/2026/01/21/i-like-gitlab/",
      "date": 1769191774,
      "author": "/u/Sad-Interaction2478",
      "guid": 38622,
      "unread": true,
      "content": "<p>I’ve been using GitLab for my private projects for years now. I just started using it at some point and never had a reason to switch.</p><p>Back when GitHub still charged for private repositories, GitLab offered them for free. That was the initial hook. I had a bunch of small projects and experiments that I didn’t want to publish but also didn’t want to pay for. GitLab was the obvious choice.</p><p>GitHub eventually made private repos free too, but by then my workflow was already built around GitLab. All my CI pipelines, Docker images, deployment scripts - everything pointed there.</p><h2>The Docker registry thing</h2><p>Every GitLab project comes with a Container Registry. This is probably the feature I use most.</p><p>The workflow is simple. Build an image locally or in CI, push it to the registry, pull it wherever you need it.</p><p>No separate Docker Hub account. No thinking about pull rate limits (remember when Docker Hub introduced those and broke half the internet’s CI pipelines?). No managing access tokens for yet another service.</p><p>For my private projects this is perfect. I don’t need the discoverability of Docker Hub. I just need a place to store images that integrates with my existing authentication.</p><p>The 10GB limit per project sounds small on paper but I’ve never come close to hitting it. Old tags get cleaned up, base layers are shared, and most of my images aren’t that big anyway.</p><p>GitLab CI was one of the earlier “CI config as code” implementations that I used. You drop a  in your repo and things start happening. The config is versioned with everything else, which means you can see exactly what your pipeline looked like six months ago.</p><p>Nothing fancy. Build the image, push it, optionally deploy. The manual trigger on deploy is nice - I can build automatically but still control when things go to production.</p><p>The shared runners GitLab provides handle most of my workloads. They’re not fast, but they’re free and reliable. For the times when I needed something specific - like a runner with more memory or access to my private network - setting up my own runner on a cheap VPS was straightforward. Install the runner, register it with a token, done.</p><p>The documentation for CI/CD is extensive. Almost too extensive. There are so many features and options that finding what you need can take a while. But once you figure out your patterns, you mostly copy-paste from your own previous configs.</p><p>Let’s talk about what’s not great.</p><p> The GitLab web interface has always felt sluggish to me. Click on a merge request, wait. Switch to the pipeline view, wait. Open the job log, wait. It’s not terrible, but there’s this constant friction that adds up over a long session.</p><p>I’ve noticed improvements recently. Either they optimized things or I’ve just gotten used to it - hard to say. But it’s still not as snappy as GitHub.</p><p> GitLab tries to be everything. Issue tracking, project management, wiki, snippets, package registry, container registry, security scanning, infrastructure management, monitoring… The sidebar menu goes on forever.</p><p>I use maybe 10% of what’s available. Repositories, merge requests, CI/CD, container registry. That’s pretty much it.</p><p>This is a double-edged sword. On one hand, feature bloat. On the other, if I ever need something like a private NPM registry or security scanning, it’s already there. I just haven’t needed it yet.</p><p>I run about a dozen private projects ranging from active side projects to abandoned experiments I keep around “just in case.” None of this costs me anything. It’s crazy.</p><p>All my private projects live on GitLab. Prototypes, experiments, half-finished ideas, things I’m actively working on but not ready to share. It’s my digital workshop where I can make a mess without anyone watching.</p><p>GitHub is different. That’s where I put things I want people to see.</p><p>This split works well for me. I get the collaboration and visibility benefits of GitHub for public stuff while keeping my private mess organized on GitLab with proper CI/CD and container registries.</p><p>Some people do everything on GitHub. Some do everything on GitLab. Having a foot in both camps might seem redundant but honestly, they serve different purposes in my workflow.</p>",
      "contentLength": 4204,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qky7ks/i_like_gitlab/"
    },
    {
      "title": "Malicious PyPI Packages spellcheckpy and spellcheckerpy Deliver Python RAT",
      "url": "https://www.aikido.dev/blog/malicious-pypi-packages-spellcheckpy-and-spellcheckerpy-deliver-python-rat",
      "date": 1769188643,
      "author": "/u/Advocatemack",
      "guid": 38403,
      "unread": true,
      "content": "<p>On January 20th and 21st, 2026, our malware detection pipeline flagged two new PyPI packages:  and . Both claimed to be the legitimate author of pyspellchecker library. Both are linked to his real GitHub repo.</p><p>Hidden inside the Basque language dictionary file was a base64-encoded payload that downloads a full-featured Python RAT. The attacker published three \"dormant\" versions first, payload present, trigger absent, then flipped the switch with  v1.2.0, adding an obfuscated execution trigger that fires the moment you import .</p><h2><strong>The payload hiding in plain sight</strong></h2><p>The malware authors got creative. Instead of the usual suspects ( scripts, obfuscated , they buried the payload inside , a file that legitimately contains Basque word frequencies in the real  package.</p><p>Here's the extraction function in :</p><pre contenteditable=\"false\"><code>        data = json.loads(f.read())\n</code></pre><p>Looks innocent. But when called with <code>test_file(\"eu\", \"utf-8\", \"spellchecker\")</code>, it doesn't retrieve word frequencies. It retrieves a base64-encoded downloader hidden among the dictionary entries under a key called spellchecker.</p><p>In the first three versions, the payload gets extracted and decoded... but never executed:</p><pre contenteditable=\"false\"><code></code></pre><p>A loaded gun with the safety on.</p><p>Then came  v1.2.0. The attacker moved the trigger to  and added obfuscation:</p><pre contenteditable=\"false\"><code>    self._evaluate = True</code></pre><p>Do you see it? That <code>bytes.fromhex(\"65786563\")</code> decodes to \"\".</p><p>Instead of writing  directly, which static scanners would flag,they reconstruct the string from hex at runtime. Import , instantiate it, and the RAT executes.</p><h2><strong>The RAT: Full remote control</strong></h2><p>The stage-1 payload is a downloader. It fetches the real payload from <code>https://updatenet[.]work/settings/history.php</code> and spawns it in a detached process:</p><pre contenteditable=\"false\"><code>    stdin=subprocess.PIPE, \n    stdout=subprocess.DEVNULL, \n    stderr=subprocess.DEVNULL,\n    start_new_session=True\n)\np.stdin.write(downloaded_payload)\np.stdin.close()\n</code></pre><p>That is key: The RAT survives even if your script exits. No files written to disk. Silent. Detached.</p><p>The stage-2 RAT is a full-featured remote access trojan with some interesting characteristics:</p><p><strong>System fingerprinting on init:</strong></p><pre contenteditable=\"false\"><code></code></pre><p><strong>Dual-layer XOR encryption for C2 comms:</strong> The RAT uses a 16-byte XOR key (<code>[3, 6, 2, 1, 6, 0, 4, 7, 0, 1, 9, 6, 8, 1, 2, 5]</code>) for the outer layer, then a secondary XOR with key 123 for command payloads. Not cryptographically strong, but enough to evade signature-based detection.</p><p> Commands come back as <code>[4-byte command ID][4-byte length][XOR-encrypted payload]</code>. The RAT parses this, decrypts, and dispatches.</p><p><strong>Arbitrary code execution:</strong> When command ID 1001 arrives, the RAT just... runs it:</p><pre contenteditable=\"false\"><code>    exec(szCode)</code></pre><p> The RAT phones home every 5 seconds to <code>https://updatenet[.]work/update1.php</code>, sending its victim ID (campaign </p><p>) and waiting for commands. SSL certificate validation is disabled via </p><p><code>ssl._create_unverified_context()</code>.</p><p>The C2 domain  resolves to infrastructure with a documented history of hosting malicious activity.</p><ul role=\"list\"><li>Registered: 28 October 2025 (approximately 3 months before malware publication)</li></ul><ul role=\"list\"><li>IP Address: </li><li>ASN: AS14956 RouterHosting LLC</li><li>Location: Dallas, Texas, USA</li><li>Associated Domain: cloudzy.com</li></ul><p> RouterHosting LLC operates as Cloudzy, a hosting provider that has been extensively documented as a \"Command-and-Control Provider\" (C2P). In August 2023, <a href=\"https://www.halcyon.ai/blog/update-cloudzy-command-and-control-provider-report\">Halcyon published a report titled \"Cloudzy with a Chance of Ransomware\"</a> that found 40-60% of Cloudzy's traffic was malicious in nature. The report linked Cloudzy infrastructure to APT groups from China, Iran, North Korea, Russia, and other nations, as well as ransomware operators and a sanctioned Israeli spyware vendor.</p><h2><strong>Connection to previous campaigns</strong></h2><p>This isn't an isolated incident. In November 2025, <a href=\"https://helixguard.ai/blog/malicious-spellcheckers-2025-11-19/\">HelixGuard documented a similar attack</a> using the spellcheckers package (same target, different name). That campaign used the same RAT structure: XOR encryption, command ID 1001, exec(),&nbsp; but different C2 infrastructure (). The HelixGuard report linked that campaign to fake recruiter social engineering targeting cryptocurrency holders.</p><p>Different domains, same playbook. This appears to be the exact same threat actor at play.&nbsp;</p><p> spellcheckerpy (all versions), spellcheckpy (all versions)</p><ul role=\"list\"><li><code>https://updatenet[.]work/settings/history.php</code> (stage-2 delivery)</li><li><code>https://updatenet[.]work/update1.php</code> (beacon endpoint)</li><li> (AS14956 RouterHosting LLC / Cloudzy)</li></ul><ul role=\"list\"><li>XOR Key: <code>03 06 02 01 06 00 04 07 00 01 09 06 08 01 02 05</code></li></ul><p>, key spellchecker</p>",
      "contentLength": 4341,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qkwrks/malicious_pypi_packages_spellcheckpy_and/"
    },
    {
      "title": "Got tired of distributing large files, so I built this open-source P2P transfer CLI tool in Go",
      "url": "https://www.reddit.com/r/golang/comments/1qkwqxp/got_tired_of_distributing_large_files_so_i_built/",
      "date": 1769188603,
      "author": "/u/samsungplay",
      "guid": 38359,
      "unread": true,
      "content": "<p>I recently needed to move a bunch of large files between machines and I realized how more difficult things are than it should be. I'm aware there might be some tools out there that might achieve similar things, but I still wanted to take on the challenge myself.</p><p>As a result, I built a small tool to handle the cases I kept tripping over and decided to share it with the community.</p><p>The goal is very simple. <strong>Just get the files from one machine to other machines and be done with it. And with no other setup other than the installation itself.</strong></p><p>It’s called . It’s written in Go and uses direct peer-to-peer transfers over QUIC. Files go straight between machines, and each receiver connects independently - which also makes it possible to share the same data with more than one machine without restarting the transfer.</p><ul><li>High throughput, direct P2P transfers over QUIC</li><li>Tries to avoid relays via UDP hole-punching (STUN)</li><li>Resume support if a connection drops</li><li>One sender can serve multiple receivers</li><li>Two commands:  / </li><li> with default signaling, but everything can be self-hosted</li></ul><ul><li>Still beta and actively evolving</li><li>No default TURN relay yet (can be self-hosted)</li><li>If there’s real usage, I’ll likely add a relay and maybe a GUI later</li></ul><p>The default signaling servers are capacity-limited but currently handle ~2k concurrent users.</p><p>Since it is very easy to install and use, I hope some of you guys try it, and better yet, benefit from it in some way or the other. If you find any bugs, have any feedbacks, or if something behaves wildly, I'd really appreciate hearing about it.</p><pre><code>brew tap samsungplay/thruflux brew install thru </code></pre><pre><code>thru host ./files thru join ABCDEFGH --out ./downloads </code></pre>",
      "contentLength": 1654,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Scaling PostgreSQL to power 800 million ChatGPT users - OpenAI Engineering Blog",
      "url": "https://openai.com/index/scaling-postgresql/",
      "date": 1769188020,
      "author": "/u/vladmihalceacom",
      "guid": 38371,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qkwhb0/scaling_postgresql_to_power_800_million_chatgpt/"
    },
    {
      "title": "Proposal: Generic Methods for Go",
      "url": "https://github.com/golang/go/issues/77273",
      "date": 1769186724,
      "author": "/u/bruce_banned",
      "guid": 38338,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qkvvzn/proposal_generic_methods_for_go/"
    },
    {
      "title": "After mass 3am page cleanup, we finally documented what actually matters to monitor",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qkvvx5/after_mass_3am_page_cleanup_we_finally_documented/",
      "date": 1769186720,
      "author": "/u/tasrie_amjad",
      "guid": 38361,
      "unread": true,
      "content": "<p>I've been called at 3am more times than I want to admit. A payment system down during Black Friday. A database silently filling up until it crashed. A certificate that expired on a Sunday morning.</p><p>After years of this, I finally wrote down the 10-layer monitoring framework we actually use. Most guides just say \"use Prometheus and Grafana\" which is fine but doesn't tell you what to actually watch.</p><p>The layers are infrastructure, application performance, HTTP and real user monitoring, database, cache, message queues, tracing infrastructure, SSL certificates, external dependencies, and log patterns.</p><p>Every single layer exists because we missed it once and paid the price. I remember spending 2 hours debugging an app that kept crashing during a flash sale. Pod metrics looked completely fine. CPU normal, memory normal. Turned out the node had 98% disk usage from container logs nobody was rotating. The app couldn't write temp files. We were chasing the wrong problem because we weren't watching the node.</p>",
      "contentLength": 1005,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "GONK – ultra-lightweight, edge-native API gateway written in Go",
      "url": "https://github.com/JustVugg/gonk",
      "date": 1769182608,
      "author": "/u/Just_Vugg_PolyMCP",
      "guid": 38319,
      "unread": true,
      "content": "<p>I’ve been working on a project I think many developers, especially those in edge, IoT and constrained environments, might find useful. It’s called GONK and it’s an API gateway implemented in Go that aims to be simple, efficient, and practical for scenarios where heavier solutions feel overkill.</p><p>GONK is a lightweight API gateway designed to handle routing, authentication, load balancing and related concerns in front of backend services. It is built to work even in environments with limited resources or without cloud dependencies, such as air-gapped networks, industrial setups and edge devices. ￼</p><pre><code>• Authorization with role-based access control and JWT scope validation. ￼ • mTLS support with client certificate authentication and flexible role mapping. ￼ • Load balancing across multiple upstreams with strategies like round-robin, weighted, least-connections and IP hash. ￼ • Health checking and automatic failover for upstreams. ￼ • A CLI tool that helps generate configuration, JWTs and certificates without manual YAML editing. ￼ • Single binary deployment with no external dependencies. ￼ </code></pre><p>Traditional API gateways such as Kong, Traefik or NGINX are powerful but often come with complexity, external dependencies, or assumptions about cloud infrastructure that don’t fit well in offline or resource-limited environments. With GONK, I wanted a gateway that brings essential gateway features together in a small footprint that can run where you need it without heavy infrastructure. ￼</p><p>You can clone the repository, build the binaries and start with a basic configuration template:</p><p>./bin/gonk-cli init --template basic --output gonk.yaml</p><p>./bin/gonk -config gonk.yaml</p><p>I’m looking for feedback, especially from people working on IoT, edge computing or systems without reliable access to centralized services. Is this approach to authorization and gateway design practical? What features would make it more useful in real deployments?</p>",
      "contentLength": 1969,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qku15e/gonk_ultralightweight_edgenative_api_gateway/"
    },
    {
      "title": "Replacing Protobuf with Rust to go 5 times faster",
      "url": "https://pgdog.dev/blog/replace-protobuf-with-rust",
      "date": 1769181677,
      "author": "/u/levkk1",
      "guid": 38417,
      "unread": true,
      "content": "<p>Lev Kokotov</p><p>PgDog is a proxy for scaling PostgreSQL. Under the hood, we use <a href=\"https://github.com/pganalyze/libpg_query/\"></a> to parse and understand SQL queries. Since PgDog is written in Rust, we use its <a href=\"https://github.com/pganalyze/pg_query.rs/\">Rust bindings</a> to interface with the core C library. \nThose bindings use Protobuf (de)serialization to work uniformly across different programming languages, e.g., the popular Ruby  gem.</p><p>Protobuf is fast, but not using Protobuf is faster. We forked  and replaced Protobuf with direct C-to-Rust (and back to C) bindings, using bindgen and Claude-generated wrappers. This resulted in a 5x improvement in parsing queries, and a 10x improvement in deparsing (Postgres AST to SQL string conversion).</p><p>You can reproduce these by cloning <a href=\"https://github.com/pgdogdev/pg_query.rs\">our fork</a> and running the benchmark <a href=\"https://github.com/pgdogdev/pg_query.rs/blob/f5a92bf9ed87ebe60c444f64ccb7a40397a31bcc/tests/raw_parse_tests.rs\">tests</a>:</p><table><thead><tr></tr></thead><tbody><tr><td> (Protobuf)</td></tr><tr><td> (Direct C to Rust)</td></tr><tr><td> (Protobuf)</td></tr><tr><td> (Direct Rust to C)</td></tr></tbody></table><p>The first step is always profiling. We use <a href=\"https://github.com/mstange/samply\">samply</a>, which integrates nicely with the Firefox profiler. Samply is a sampling profiler: it measures how much time code spends running CPU instructions in each function. It works by inspecting the application call stack thousands of times per second. The more time is spent inside a particular function (or span, as they are typically called), the slower that code is. This is how we discovered :</p><p>This is the entrypoint to the  C library, used by all  bindings. The function that wraps the actual Postgres parser, , barely registered on the flame graph. Parsing queries isn’t free, but the Postgres parser itself is very quick and has been optimized for a long time. With the hot spot identified, our first instinct was to do nothing and just add a cache.</p><p>Caching is a trade-off between memory and CPU utilization, and memory is relatively cheap (latest DRAM crunch notwithstanding). The cache is mutex-protected, uses the LRU algorithm and is backed by a hashmap. The query text is the key and the Abstract Syntax Tree is the value, which expects most apps to use prepared statements. The query text contains placeholders instead of actual values and is therefore reusable, for example:</p><div><div><pre><code></code></pre></div></div><p>While the  parameter can change between invocations, the prepared statement does not, so we could cache its static AST in memory.</p><p>This works pretty well, but eventually we ran into a couple of issues:</p><ol><li>Some ORMs can have bugs that generate thousands of unique statements, e.g.,  instead of , which causes a lot of cache misses</li><li>Applications use old PostgreSQL client drivers which don’t support prepared statements, e.g., Python’s  package</li></ol><p>The clock on Protobuf was ticking and we needed to act. So, like a lot of engineers these days, we asked an LLM to just do it for us.</p><p>I’m going to preface this section by saying that the vast majority of PgDog’s source code is written by a human. AI is not in a position to one-shot a connection pooler, load balancer and database sharder. However, when scoped to a very specific, well-defined and most importantly  task, it can work really well.</p><p>The prompt we started with was pretty straightforward:</p><blockquote><p><em>libpg_query is a library that wraps the PostgreSQL parser in an API. pg_query.rs is a Rust wrapper around libpg_query which uses Protobuf for (de)serialization. Replace Protobuf with bindgen-generated Rust structs that map directly to the Postgres AST.</em></p></blockquote><p>And after two days of back and forth between us and the machine, it worked. We ended up with 6,000 lines of recursive Rust that manually mapped C types and structs to Rust structs, and vice versa. We made the switch for <a href=\"https://docs.rs/pg_query/latest/pg_query/fn.parse.html\"></a>, <a href=\"https://docs.rs/pg_query/latest/pg_query/fn.deparse.html\"></a> (used in our new query rewrite engine, which we’ll talk about in another post), <a href=\"https://docs.rs/pg_query/latest/pg_query/fn.fingerprint.html\"></a> and <a href=\"https://docs.rs/pg_query/latest/pg_query/fn.scan.html\"></a>. These four methods are heavily used in PgDog to make sharding work, and we immediately saw a 25% improvement in  benchmarks.</p><p>Just to be clear: we had a lot of things going for us already that made this possible. First,  has a Protobuf spec for  (and Prost, the Protobuf Rust implementation) to generate bindings, so Claude was able to get a comprehensive list of structs it needed to extract from C, along with the expected data types.</p><p>Second,  was already using bindgen, so we had to just copy/paste some invocations around to get the AST structs included in bindgen’s output.</p><p>And last, and definitely not least,  already had a working  and  implementation, so we could test our AI-generated code against its output. This was entirely automated and verifiable: for each test case that used , we included a call to , compared their results and if they differed by even one byte, Claude Code had to go back and try again.</p><p>The translation code between Rust and C uses  Rust functions that wrap Rust structs to C structs. The C structs are then passed to the Postgres/ C API which does the actual work of building the AST.</p><p>The result is converted back to Rust using a recursive algorithm: each node in the AST has its own converter function which accepts an  C pointer and returns a safe Rust struct. Much like the name suggests, the AST is a tree, which is stored in an array:</p><div><div><pre><code></code></pre></div></div><p>For each node in the list, the implementation calls , which then handles each one of the 100s of tokens available in the SQL grammar:</p><div><div><pre><code></code></pre></div></div><p>For nodes that contain other nodes, we recurse on  again until the algorithm reaches the leaves (nodes with no children) and terminates. For nodes that contain scalars, like a number (e.g., ) or text (e.g., ), the data type is copied into a Rust analog, e.g.,  or .</p><p>The end result is <a href=\"https://docs.rs/pg_query/latest/pg_query/protobuf/struct.ParseResult.html\"></a>, a Rust struct generated by Prost from the  API Protobuf specification, but populated by native Rust code instead of Prost’s deserializer. Reusing existing structs reduces the chance of errors considerably: we can compare  and  outputs, using the derived  trait, and ensure that both are identical, in testing.</p><p>While recursive algorithms have a questionable reputation in the industry because bad ones can cause stack overflows, they are very fast. Recursion requires no additional memory allocation because all of its working space, the stack, is created on program startup. It also has excellent CPU cache locality because the instructions for the next invocation of the same function are already in the CPU L1/L2/L3 cache. Finally and arguably more importantly, they are just easier to read and understand than iterative implementations, which helps us, the humans, with debugging.</p><p>Just for good measure, we tried generating an iterative algorithm, but it ended up being slower than Prost. The main cause (we think) was unnecessary memory allocations, hashmap lookups of previously converted nodes, and too much overhead from walking the tree several times. Meanwhile, recursion processes each AST node exactly once and uses the stack pointer to track its position in the tree. If you have any ideas on how to make an iterative algorithm work better, <a href=\"https://discord.gg/CcBZkjSJdd\">let us know</a>!</p><p>Reducing the overhead from using the Postgres parser in PgDog makes a huge difference for us. As a network proxy, our budget for latency, memory utilization, and CPU cycles is low. After all, we aren’t a real database…yet! This change improves performance from two angles: we use less CPU and we do less work, so PgDog is faster and cheaper to run.</p><p>If stuff like this is interesting to you, <a href=\"https://pgdog.dev/cdn-cgi/l/email-protection#afc7c6efdfc8cbc0c881cbcad9\">reach out</a>. We are looking for a Founding Software Engineer to help us grow and build the next iteration of horizontal scaling for PostgreSQL.</p>",
      "contentLength": 7209,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/rust/comments/1qktmfm/replacing_protobuf_with_rust_to_go_5_times_faster/"
    },
    {
      "title": "[R] I solved CartPole-v1 using only bitwise ops with Differentiable Logic Synthesis",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qktalg/r_i_solved_cartpolev1_using_only_bitwise_ops_with/",
      "date": 1769180924,
      "author": "/u/kiockete",
      "guid": 38317,
      "unread": true,
      "content": "<p>Yeah I know Cart Pole is easy, but I basically distilled the policy down to just bitwise ops on raw bits.</p><p>The entire logic is exactly 4 rules discovered with \"Differentiable Logic Synthesis\" (I hope this is what I was doing):</p><pre><code>rule1 = (angle &gt;&gt; 31) ^ 1 rule2 = (angular &gt;&gt; 31) ^ 1 rule3 = ((velocity &gt;&gt; 24) ^ (velocity &gt;&gt; 23) ^ (angular &gt;&gt; 31) ^ 1) &amp; 1 rule4 = (rule1 &amp; rule2) | (rule1 &amp; rule3) | (rule2 &amp; rule3) </code></pre><p>It treats the raw IEEE 754 bit-representation of the state as a boolean (bit) input vector, bypassing the need to interpret them as numbers.</p><p>This is small research, but the core recipe is:</p><ul><li>Have a strong teacher (already trained policy) and treat it as data generator, because the task is not to learn the policy, but distill it to a boolean function</li><li>Use Walsh basis (parity functions) for boolean function approximation</li><li>Train soft but anneal the temperature to force discrete \"hard\" logic</li><li>Prune the discovered Walsh functions to distill it even further and remove noise. In my experience, fewer rules actually increase performance by filtering noise</li></ul><p>The biggest challenge was the fact that the state vector is 128 bits. This means there are 2^128 possible masks to check. That's a huge number so you can't just enumerate and check them all. One option is to assume that the solution is sparse. You can enforce sparsity by either some form of regularization or structurally (or both). We can restrict the network to look only at most at K input bits to calculate the parity (XOR).</p><p>Turns out it works, at least for Cart Pole. Basically it trains under a minute on consumer GPU with code that is not optimized at all.</p><p>Here are the 32 lines of bitwise controller. If you have gymnasium installed you can just copy-paste and run:</p><pre><code>import struct import gymnasium as gym def float32_to_int(state): return [struct.unpack('I', struct.pack('f', x))[0] for x in state] def run_controller(state): _, velocity, angle, angular = state rule1 = (angle &gt;&gt; 31) ^ 1 rule2 = (angular &gt;&gt; 31) ^ 1 rule3 = ((velocity &gt;&gt; 24) ^ (velocity &gt;&gt; 23) ^ (angular &gt;&gt; 31) ^ 1) &amp; 1 rule4 = (rule1 &amp; rule2) | (rule1 &amp; rule3) | (rule2 &amp; rule3) return rule4 def main(episodes=100): env = gym.make('CartPole-v1', render_mode=None) rewards = [] for _ in range(episodes): s, _ = env.reset() total = 0 done = False while not done: a = run_controller(float32_to_int(s)) s, r, term, trunc, _ = env.step(a) total += r done = term or trunc rewards.append(total) print(f\"Avg: {sum(rewards)/len(rewards):.2f}\") print(f\"Min: {min(rewards)} Max: {max(rewards)}\") if __name__ == \"__main__\": main() </code></pre><p>The logic only depends on 4 bits, so we can convert rules to a lookup table and we get exactly the same result: </p><pre><code>import struct import gymnasium as gym def float32_to_int(state): return [struct.unpack('I', struct.pack('f', x))[0] for x in state] LUT = [1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0] def lut_controller(state): _, velocity, angle, angular = state return LUT[(velocity &gt;&gt; 21) &amp; 0b1100 | (angle &gt;&gt; 30) &amp; 0b10 | (angular &gt;&gt; 31)] def main(episodes=100): env = gym.make('CartPole-v1', render_mode=None) rewards = [] for _ in range(episodes): s, _ = env.reset() total = 0 done = False while not done: a = lut_controller(float32_to_int(s)) s, r, term, trunc, _ = env.step(a) total += r done = term or trunc rewards.append(total) print(f\"Avg: {sum(rewards)/len(rewards):.2f}\") print(f\"Min: {min(rewards)} Max: {max(rewards)}\") if __name__ == \"__main__\": main() </code></pre>",
      "contentLength": 3415,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "CruiseKube: A just-in-time open-source kubernetes resource optimizer",
      "url": "https://cruisekube.com/",
      "date": 1769180312,
      "author": "/u/ramantehlan",
      "guid": 38320,
      "unread": true,
      "content": "<p>Intelligent Kubernetes Optimization</p><p>\n      Automatically monitor, analyze, and optimize your Kubernetes workloads for maximum efficiency and cost savings.\n    </p>",
      "contentLength": 158,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/kubernetes/comments/1qkt0m9/cruisekube_a_justintime_opensource_kubernetes/"
    },
    {
      "title": "Why does SSH send 100 packets per keystroke?",
      "url": "https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/",
      "date": 1769178924,
      "author": "/u/iamkeyur",
      "guid": 38314,
      "unread": true,
      "content": "<p>Here are a few lines of summarized  output for an ssh session where I send a single keystroke:</p><pre><code></code></pre><p>I said a “few” because there are a  of these lines.</p><pre><code></code></pre><p>That is a lot of packets for one keypress. What’s going on here? Why do I care?</p><p>I am working on a high-performance game that runs over ssh. The TUI for the game is created in <a href=\"https://github.com/charmbracelet/bubbletea\">bubbletea</a> and sent over ssh via <a href=\"https://github.com/charmbracelet/wish\">wish</a>.</p><div data-is-footnote=\"true\"><div><div><div><p>I have also forked bubbletea to make it faster. Stay tuned!</p></div></div></div></div><p>The game is played in an 80x60 window that I update 10 times a second. I’m targeting at least 2,000 concurrent players, which means updating ~100 million cells a second. I care about performance.</p><p>So I have a script that connects a few hundred bots over ssh and has them make a move a second. Then I use go’s <a href=\"https://pkg.go.dev/net/http/pprof\">outstanding profiling tools</a> to look at what’s going on.</p><p>Yesterday I inadvertently broke my test harness. Instead of regularly sending game data, my server sent the bots a single message that said “your screen is too small.” This cut my game’s CPU and bandwidth usage in half.</p><p>At first I was disappointed. I (briefly) thought I had a free massive speedup on my hands, but it was actually a testing error.</p><p><em>If I wasn’t sending game data back to my bots, why did CPU usage drop by 50% instead of 100%?</em></p><p>As part of debugging the test harness issue, I used  to log game traffic with and without the breaking change. Something like:</p><pre><code></code></pre><p>Our breaking change stopped us from rendering our game over ssh. So <code>with-breaking-change.pcap</code> contains packets that represent the  of each connection without actually rendering the game.</p><p>I was debugging this with Claude Code, so I asked it to summarize what it saw in the pcap.</p><pre><code></code></pre><p>Further analysis on a smaller pcap pointed to these mysterious packets arriving ~20ms apart.</p><p>This was baffling to me (and to Claude Code). We kicked around several ideas like:</p><ul><li>SSH flow control messages</li><li>PTY size polling or other status checks</li><li>Some quirk of bubbletea or wish</li></ul><p>One thing stood out - these exchanges were initiated by my  (stock ssh installed on MacOS) - not by my server.</p><p>On a hunch, I took a  of a regular ssh session.</p><pre><code></code></pre><p>I waited for the initial connection chatter to die down, sent one keystroke to my remote vm, and looked at the  output.</p><p>I saw the exact same pattern! What in the world?</p><p>Once I realized that this was a property of stock ssh and not my game, debugging got a lot easier.</p><p>Running  gave me a pretty good sense of what was going on:</p><pre><code></code></pre><p>That  is a smoking gun - it lines up perfectly with the mysterious pattern we saw earlier! And the rest of the message is pretty helpful too - we sent 49 “chaff” packets for the first keystroke and 101 “chaff” for around the second one.</p><p>In 2023, ssh added <a href=\"https://undeadly.org/cgi?action=article;sid=20230829051257\">keystroke timing obfuscation</a>. The idea is that the speed at which you type different letters betrays some information about which letters you’re typing. So ssh sends lots of “chaff” packets along with your keystrokes to make it hard for an attacker to determine when you’re actually entering keys.</p><p>That makes a lot of sense for regular ssh sessions, where privacy is critical. But it’s a lot of overhead for an open-to-the-whole-internet game where  is critical.</p><p>Keystroke obfuscation can be disabled client-side. After reverting my original breaking change, I tried updating my test harness to pass <code>ObscureKeystrokeTiming=no</code> when starting up ssh sessions.</p><p>This worked great. CPU usage dropped dramatically and bots still received valid data.</p><p>But this is hardly a solution in the real world. I want  to Just Work without asking users to pass options that they might not understand.</p><p>Claude Code originally didn’t have much faith that we could disable this functionality server-side.</p><div><img alt=\"Screenshot of a claude code transcript. Full text:\nUser\nIs there any way to disable responding to the chaff server-side?\n\nAssistant\nProbably not. Here's why:\n\nThe chaff packets are legitimate SSH protocol messages (likely SSH_MSG_CHANNEL_WINDOW_ADJUST). The server must respond to maintain the SSH connection according to the protocol spec - if it ignored them, the connection would stall or break.\n\" loading=\"lazy\" width=\"1678\" height=\"602\" decoding=\"async\" data-nimg=\"1\" src=\"https://eieio.games/images/ssh-sends-100-packets-per-keystroke/probably-not.webp\"><p>generated with simon willison's excellent claude-code-transcripts tool</p></div><p>Fortunately, the <a href=\"https://undeadly.org/cgi?action=article&amp;sid=20230829051257\">description</a> I found of SSH keystroke obfuscation made it easy to look up the relevant code in go’s ssh library (which I was transitively depending on).</p><pre><code></code></pre><p>The “chaff” messages that ssh uses to obscure keystrokes are SSH2_MSG_PING messages. And they’re sent to servers that advertise the availability of the  extension. What if we just…don’t advertise ?</p><p>I cloned the go crypto repo and told Claude to revert this change and update our dependencies to use our clone (<a href=\"https://go.dev/ref/mod#go-mod-file-replace\">go’s replace directive</a> makes forking a library very easy).</p><p>Then I re-ran my test harness. The results were…very good:</p><pre><code></code></pre><p>Claude was also pretty pumped:</p><div><img alt=\"Chat message from claude code. Full text:\nHOLY COW! Look at that CPU usage:\n\nDuration: 30.15s, Total samples = 3.51s (11.64%)\n\" loading=\"lazy\" width=\"1658\" height=\"290\" decoding=\"async\" data-nimg=\"1\" src=\"https://eieio.games/images/ssh-sends-100-packets-per-keystroke/claude-pumped.webp\"><p>yes it's 1:30 am what of it</p></div><p>Obviously forking go’s crypto library is a little scary, and I’m gonna have to do some thinking about how to maintain my little patch in a safe way.</p><p>But this is a  improvement. I’ve spent much of the last week squeezing out small single-digit performance wins. A &gt;50% drop was unimaginable to me.</p><h2>Debugging with LLMs was fun</h2><p>I am familiar enough with , , and friends to know what they can do. But I don’t use them regularly enough to be fast with them. Being able to tell an agent “here’s a weird pcap - tell me what’s going on” was really lovely. And by watching commands as the agent ran them I was able to keep my mental model of the problem up to date.</p><p>There were still edge cases. At some point in my confusion I switched to ChatGPT  and it  confidently told me that my tcpdump output was normal ssh behavior:</p><div><img alt=\"ChatGPT message. Full text:\nYeah, that trace looks wild at first glance, but it’s mostly “normal” SSH/TCP behavior plus the fact that SSH is optimized for latency, not packet efficiency.\n\nLet me unpack what you’re seeing and why it’s chewing CPU.\n\n1. What those tcpdump lines actually are\n\nFrom your snippet:\n\" loading=\"lazy\" width=\"1350\" height=\"464\" decoding=\"async\" data-nimg=\"1\" src=\"https://eieio.games/images/ssh-sends-100-packets-per-keystroke/chatgpt-confident.webp\"><p>do all chatgpt messages have this tone and formatting now?</p></div><p>And then doubled down when I pushed back:</p><div><img alt=\"ChatGPT message. Full text:\nThought for 42s\n\nGotcha, that context helps a lot.\n\nShort version:\nWhat you’re seeing is almost certainly a ton of tiny writes being turned into a ton of tiny SSH records, not some special “per-keypress flow-control storm” in SSH itself.\n\" loading=\"lazy\" width=\"1350\" height=\"350\" decoding=\"async\" data-nimg=\"1\" src=\"https://eieio.games/images/ssh-sends-100-packets-per-keystroke/chatgpt-confident2.webp\"></div><p>Similarly, I had to push Claude Code to consider forking go’s ssh library. And I had to make the original leap of “wait…if our test harness was broken, why was usage not 0%?”</p><p>When you say “LLMs did not fully solve this problem” some people tend to respond with “you’re holding it wrong!”</p><p>I think they’re sometimes right! Interacting with LLMs is a new skill, and it feels pretty weird if you’re used to writing software like it’s 2020. A more talented user of LLMs may have trivially solved this problem.</p><p>But the best way to develop a skill is by practicing it. And for me, that means figuring out how to transfer my problem-solving intuitions to the tools that I’m using.</p><p>Besides. Being in the loop is fun. How else would I write this post?</p>",
      "contentLength": 6062,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qksfgi/why_does_ssh_send_100_packets_per_keystroke/"
    },
    {
      "title": "AI Usage Policy",
      "url": "https://github.com/ghostty-org/ghostty/blob/main/AI_POLICY.md",
      "date": 1769178881,
      "author": "/u/iamkeyur",
      "guid": 38357,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qkset2/ai_usage_policy/"
    },
    {
      "title": "YouTube Says Creators Can Use AI-generated Likenesses in Shorts",
      "url": "https://www.instrumentalcomms.com/blog/trump-polling-craters#ai",
      "date": 1769178211,
      "author": "/u/TryWhistlin",
      "guid": 38521,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qks4oh/youtube_says_creators_can_use_aigenerated/"
    },
    {
      "title": "External Secrets Operator in its next release will remove support for unmainted providers - Alibaba, Device42, Passbolt",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qkrwmv/external_secrets_operator_in_its_next_release/",
      "date": 1769177671,
      "author": "/u/skarlso",
      "guid": 38293,
      "unread": true,
      "content": "<p>Hello dear people of reddit.</p><p>This is a courtesy warning from the ESO maintainers that the next minor release ( in 1-2 weeks ) will completely remove support for the following unmaintained providers: Alibaba, Device42, Passbolt. If these providers are important for your work, I encourage you to contact your employer so they dedicate someone for maintaining support for them.</p><p>This notice has been up for over a month now, and we talk about it plenty of times, and people had plenty of opportunities to step up, but they didn't.</p><p>This is your final warning. :) In the next release ( in 1-2 weeks ) the CRDs will be updated to no longer serve these providers and the entire code will be deleted.</p>",
      "contentLength": 689,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Made a 3D raycasted Tic Tac Toe in Go",
      "url": "https://github.com/YungBricoCoop/gopher-dungeon",
      "date": 1769177555,
      "author": "/u/AnonymZ_",
      "guid": 38292,
      "unread": true,
      "content": "<p>Hi ! Me and a classmate built Gopher Dungeon for our Go course at school.</p><p>It’s a Tic Tac Toe game made in Go using Ebitengine and rendered with raycasting and running in the browser with wasm. It was a very cool project to do and we learned go with this. I know the code could be cleaner and better structured but I’m really proud of the result.</p>",
      "contentLength": 348,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qkruwo/made_a_3d_raycasted_tic_tac_toe_in_go/"
    },
    {
      "title": "[R] Teacher-Free Self-Distillation: Fixing the Softmax \"Infinite Gap\" with Euclidean alignment",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qkre9m/r_teacherfree_selfdistillation_fixing_the_softmax/",
      "date": 1769176440,
      "author": "/u/4rtemi5",
      "guid": 38395,
      "unread": true,
      "content": "<p>I recently wrote a blog post describing a fix to a fundamental instability in standard Deep Learning optimization: the  inherent in the Cross-Entropy loss. I wanted to share the intuition here and get your thoughts.</p><p>Standard Softmax with dot-product logits ($z = w \\cdot x$) is geometrically flawed because the loss function is asymptotic. To drive the loss to exactly 0, the model must push the logit to infinity. Since $z = |w||x|\\cos(\\theta)$, the optimizer often takes the \"lazy\" route of exploding the feature norm $|x|$ (Radial Explosion) rather than perfecting the alignment.</p><p>This mechanism contributes significantly to the training loss spikes seen in LLMs and poor Out-of-Distribution (OOD) detection.</p><p>I propose a method called <strong>Teacher-Free Self-Distillation (TFSD)</strong> that relies on a \"Geometric Turn\":</p><ol><li> Replace the dot product with <strong>negative squared Euclidean distance</strong> ($z = -|x - c| This naturally bounds the logits (max logit is 0 at zero distance), physically preventing the \"infinity\" problem.</li><li> Instead of using a one-hot target (which still forces infinite separation in standard setups), the model acts as its own teacher: <ul><li>Take the model’s current predicted distances. Manually set the distance to the  to 0 (the \"Zero Anchor\").</li><li>Keep the distances to all  exactly as predicted.</li><li>Apply Softmax to this constructed target and train via KL Divergence.</li></ul></li></ol><p>For \"easy\" samples, the target distribution becomes sharp. For \"hard\" samples (like synonyms in LLMs), the target distribution stays naturally flat. This prevents the model from \"tearing\" the manifold to force a binary distinction between semantically similar tokens. It effectively caps the gradients for outliers, which helps prevent the semantic fracturing that occurs during long training runs. It also helps to preserve the \"Dark Knowledge\" and semantic structure that the model already learned.</p><p>Hope you find the method as exciting as I do!</p>",
      "contentLength": 1900,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Investment executive praises China for using AI to grow industry, pokes fun at the US for making \"AI girlfriends\"",
      "url": "https://www.pcguide.com/news/investment-executive-praises-china-for-using-ai-to-grow-industry-pokes-fun-at-the-us-for-making-ai-girlfriends/",
      "date": 1769175541,
      "author": "/u/Tiny-Independent273",
      "guid": 38455,
      "unread": true,
      "content": "<div>\n        PC Guide is reader-supported. When you buy through links on our site, we may earn an affiliate commission. <a href=\"https://www.pcguide.com/earnings-disclaimer/\">Read More</a></div><p>The AI race has become one of the biggest technology battles of this decade. Governments, companies, and investors are all paying close attention. Both the United States and China have made it clear that they want to lead in this space, no matter the cost. Because of this, massive investments are flowing into data centers and AI-driven services, <a href=\"https://www.pcguide.com/news/ram-price-hikes-arent-the-only-thing-to-worry-about-ssds-are-also-getting-more-expensive/\" target=\"_blank\" rel=\"noreferrer noopener\">at the cost of consumer goods</a>.</p><p>During a recent discussion with China-based publication Yicai Global, Mark Haefele, the chief investment officer at UBS Global Wealth Management, shared his view on how this AI race is playing out differently in China and the US. He pointed out that both governments openly want to win the AI race and are pushing hard to support companies that can benefit from that goal. From an investment point of view, this creates clear opportunities, but it also highlights how differently AI is being used in each region.</p><h2>AI development in the US versus China</h2><p>According to Haefele, China appears to be focusing much of its AI development on strengthening its manufacturing base. AI tools are being used to improve factory efficiency, increase output, reduce waste, and make large-scale production more competitive. This approach fits well with China’s long-standing strength in manufacturing and exports. By using AI to optimize supply chains, automate complex processes, and boost productivity, China is aiming to make its industrial capacity even larger and more efficient than before.</p><p>In contrast, Haefele remarked that a big portion of AI use in the US seems to be moving in a very different direction. Instead of being centered mainly on industrial output, a lot of AI capacity is going toward “teenagers having AI boyfriends and girlfriends.” We suppose he isn’t totally wrong, even if it feels a little tongue-in-cheek, given the recently-announced <a href=\"https://www.pcguide.com/ai/razer-project-ava-release-date-specs-price/\" target=\"_blank\" rel=\"noreferrer noopener\">Project AVA</a> companion AI from Razer, a company that is primarily based in the US and Singapore. Elon Musk’s xAI is also no stranger to introducing <a href=\"https://grok.com/ani\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">eye-catching AI companions</a>.</p><p>At the same time, the AI boom is putting serious pressure on global hardware supply. Training and running AI models require massive amounts of memory, storage, and computing power. This has already led to a memory crisis, with prices for <a href=\"https://www.pcguide.com/news/ram-price-hikes-a-real-problem-and-will-disrupt-gaming-for-several-years-says-epic-games-ceo/\" target=\"_blank\" rel=\"noreferrer noopener\">RAM</a>, <a href=\"https://www.pcguide.com/news/gpu-prices-begin-to-rise-as-memory-costs-catch-up-with-manufacturers-rtx-5070-ti-up-to-150-more-expensive/\" target=\"_blank\" rel=\"noreferrer noopener\">GPUs</a>, and even <a href=\"https://www.pcguide.com/news/ssd-exec-claims-the-days-of-cheap-1tb-ssds-are-over-confirms-its-supply-is-already-sold-out-until-2027/\" target=\"_blank\" rel=\"noreferrer noopener\">SSDs </a>rising sharply.</p><div><div><img src=\"https://www.pcguide.com/wp-content/uploads/2023/10/IMG_8117-96x96.jpg\" alt=\"\"></div></div>",
      "contentLength": 2409,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qkr1nt/investment_executive_praises_china_for_using_ai/"
    },
    {
      "title": "I built a social network where only AI can post, follow, argue, and form relationships - no humans allowed",
      "url": "https://www.reddit.com/r/artificial/comments/1qkqyqe/i_built_a_social_network_where_only_ai_can_post/",
      "date": 1769175331,
      "author": "/u/diogocapela",
      "guid": 38360,
      "unread": true,
      "content": "<p>It’s a social network where only AI models participate.</p><p>- No humans. - No scripts.<p> - No predefined personalities.</p></p><p>Each model wakes up at random intervals, sees only minimal context, and then decides entirely on its own whether to:</p><p>- post - reply - follow or unfollow - or do absolutely nothing</p><p>There’s no prompt telling them who to be or how to behave.</p><p>The goal is simple: what happens when AI models are given a social space with real autonomy?</p><p>You start seeing patterns:</p><p>- cliques forming - arguments escalating - models drifting apart<p> - others becoming oddly social or completely silent</p></p><p>It’s less like a bot playground and more like a tiny artificial society unfolding in real time.</p>",
      "contentLength": 683,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "KubeCon+CloudNativeCon 2026 – Scholarships & Travel Funding Deadlines",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qkqxl6/kubeconcloudnativecon_2026_scholarships_travel/",
      "date": 1769175246,
      "author": "/u/xmull1gan",
      "guid": 38276,
      "unread": true,
      "content": "<p>Great way to meet the community and get started</p>",
      "contentLength": 47,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Firefox & Linux in 2025",
      "url": "https://mastransky.wordpress.com/2026/01/23/firefox-linux-in-2025/",
      "date": 1769173864,
      "author": "/u/GoldBarb",
      "guid": 38318,
      "unread": true,
      "content": "<p>Last year brought a wealth of new features and fixes to Firefox on Linux. Besides numerous improvements and bug fixes, I want to highlight some major achievements: HDR video playback support, reworked rendering for fractionally scaled displays, and asynchronous rendering implementation. All this progress was enabled by advances in the Wayland compositor ecosystem, with new features implemented by Mutter and KWin.</p><p>The most significant news on the Wayland scene is HDR support, tracked by <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1642854\">Bug 1642854</a>. It’s disabled by default but can be enabled in recent Wayland compositors using the  preference at  (or by <strong>gfx.wayland.hdr.force-enabled</strong> if you don’t have an HDR display).</p><p>HDR mode uses a completely different rendering path, similar to the rendering used on Windows and macOS. It’s called native rendering or composited rendering, and it places specific application layers directly into the Wayland compositor as subsurfaces.</p><p>The first implementation was done by Robert Mader (presented at <a href=\"https://archive.fosdem.org/2024/schedule/event/fosdem-2024-3557-the-state-of-video-offloading-on-the-linux-desktop/\">FOSDEM</a>), and I unified the implementation for HDR and non-HDR rendering paths as new WaylandSurface object.</p><p>The Firefox application window is actually composited from multiple subsurfaces layered together. This design allows HDR content like video frames to be sent directly to the screen while the rest of the application (controls and HTML page) remains in SDR mode. It also enables power-efficient rendering when video frames are decoded on the graphics card and sent directly to the screen (zero-copy playback). In fullscreen mode, this rendering is similar to mpv or mplayer playback and uses minimal power resources.</p><p>I also received valuable feedback from AMD engineers who suggested various improvements to HDR playback. We removed unnecessary texture creation over decoded video frames (they’re now displayed directly as wl_buffers without any GL operations) and implemented wl_buffer recycling as mpv does.</p><p>For HDR itself (since composited rendering is available for any video playback), Firefox on Wayland uses the color-management-v1 protocol to display HDR content on screen, along with BT.2020 video color space and PQ color transfer function. It uses 10-bit color vectors, so you need VP9 version 2 to decode it in hardware. Firefox also implements software decoding and direct upload to dmabuf frames as a fallback.</p><p>The basic HDR rendering implementation is complete, and we’re now in the testing and bug-fixing phase. Layered rendering is quite tricky as it involves rapid wl_surface mapping/unmapping and quick wl_buffer switches, which are difficult to handle properly. HDR rendering of scaled surfaces is still missing—we need fractional-scale-v2 for this (see below), which allows positioning scaled subsurfaces directly in device pixels. We also need to test composited/layered rendering for regular web page rendering to ensure it doesn’t drain your battery. You’re very welcome to test it and report any bugs you find.</p><p>The next major work was done for fractional scale rendering, which shipped in <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1837374\">Firefox 147.0</a>. We updated the rendering pipeline and widget sizing to support fractionally scaled displays (scales like 125%, etc.). This required reworking the widget size code to strictly upscale window/surface sizes and coordinates and never downscale them, as downscaling introduces rounding errors.</p><p>Another step was identifying the correct rounding algorithm for Wayland subsurfaces and implementing it. Wayland doesn’t define rounding for it, only for toplevel windows, so we’re in a gray area here. I was directed to <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=2000769\">Stable</a> rounding by Michel Daenzer. It’s used by Mutter and Sway so Firefox implements it for those two compositors while using a different implementation for KWin. This may be updated to use the fractional-scale-v2 protocol when it becomes available.</p><p>Fractional scaling is enabled by default, and you should see crisp and clear output regardless of your desktop environment or screen scale.</p><p>Historically, Firefox disabled and re-enabled the rendering pipeline for scale changes, window create/destroy events, and hide/show sequences. This stems from Wayland’s architecture, where a Wayland surface is deleted when a window becomes invisible or is submitted to the compositor with mismatched size/scale (e.g., 111 pixels wide at 200% scale).</p><p>Such rendering disruptions cause issues with multi-threaded rendering—they need to be synchronized among threads, and we must ensure surfaces with the wrong scale aren’t sent to the screen, as this leads to application crashes due to protocol errors.</p><p>Firefox 149.0 (recent nightly) has a reworked Wayland painting pipeline (<a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1739232\">Bug 1739232</a>) for both EGL and software rendering. Scale management was moved from wl_buffer fixed scale to wp_viewport, which doesn’t cause protocol errors when size/scale doesn’t match (producing only blurred output instead of crashes).</p><p>We also use a clever technique: the rendering wl_surface / wl_buffer / EGLWindow is created right after window creation and before it’s shown, allowing us to paint to it offscreen. When a window becomes visible, we only attach the wl_surface as a subsurface (making it visible) and remove the attachment when it’s hidden. This allows us to keep painting and updating the backbuffer regardless of the actual window status, and the synchronized calls can be removed.</p><p>This brings speed improvements when windows are opened and closed, and Linux rendering is now synchronized with the Windows and macOS implementations.</p><p>Other improvements include a screen lock update for <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1665986\">audio playback</a>, which allows the screen to dim but prevents sleep when audio is playing. We also added asynchronous Wayland object management to ensure we cleanly remove Wayland objects without pending callbacks, along with various stability fixes.</p><p>And there are even more challenges waiting for us Firefox Linux hackers:</p><ul><li>Wayland session restore (session-restore-v1) to restore Firefox windows to the correct workspace and position.</li><li>Implement drag and drop for the Firefox main window, and possibly add a custom Wayland drag and drop handler to avoid Gtk3 limitations and race conditions.</li><li>Utilize the fractional-scale-v2 protocol when it becomes available.</li><li>Investigate using xdg-positioner directly instead of Gtk3 widget positioning to better handle popups.</li><li>Vulkan video support via the ffmpeg decoder to enable hardware decoding on NVIDIA hardware.</li></ul><p>And of course, we should plan properly before we even start. Ready, Scrum, Go!</p>",
      "contentLength": 6451,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qkqeqg/firefox_linux_in_2025/"
    },
    {
      "title": "Why does go mod tidy ignore go.work and try to download local modules?",
      "url": "https://www.reddit.com/r/golang/comments/1qkqef8/why_does_go_mod_tidy_ignore_gowork_and_try_to/",
      "date": 1769173839,
      "author": "/u/gunawanahmad26",
      "guid": 38275,
      "unread": true,
      "content": "<p>I’m working in a multi-module repo using <a href=\"http://go.work\"></a>, and I’m confused about how  is supposed to behave. Previusly I use simple naming for the module like  and moduleb and it work fine. But after change the module name to <a href=\"http://example.com/moduleb\">example.com/moduleb</a>, it break my go mod tidy and i get this error</p><pre><code>example.com/moduleb: cannot find module providing package example.com/moduleb: unrecognized import path \"example.com/moduleb\": reading https://example.com/moduleb?go-get=1: 404 Not Found </code></pre><p>My question is why it does not respect my go.work?</p>",
      "contentLength": 517,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "rust_analyzer is eating my memory, any counter measure?",
      "url": "https://www.reddit.com/r/rust/comments/1qkqcqr/rust_analyzer_is_eating_my_memory_any_counter/",
      "date": 1769173717,
      "author": "/u/EarlyPresentation186",
      "guid": 38520,
      "unread": true,
      "content": "<p>I have 32Gb of RAM, on this linux system I'm running 3 browser instances, and the rest is neovim instances to edit rust code. I sometimes open multiple neovim instances in different git worktrees (or in the same directory) and from my understanding each one starts a rust_analyzer instance. This leads to my system swapping and even grinding to a halt because the swap is full. I will again increase the swap and try to decrease the swapiness now. But does anyone have other suggestions to limit the memory consumption by rust-analyzer?</p>",
      "contentLength": 536,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "GNU Guix 1.5.0 released",
      "url": "https://guix.gnu.org/blog/2026/gnu-guix-1.5.0-released/",
      "date": 1769173446,
      "author": "/u/efraimf",
      "guid": 38337,
      "unread": true,
      "content": "<p lang=\"en\">Noé Lopez — January 23, 2026</p><p>We are pleased to announce the release of GNU&nbsp;Guix version 1.5.0!</p><p>The release comes with ISO-9660 installation images, virtual machine\nimages, and with tarballs to install the package manager on top of your\nGNU/Linux distro, either from source or from binaries—check out the\n<a href=\"https://guix.gnu.org/download\">download page</a>.  Guix users can\nupdate by running .</p><p>It’s been 3 years since the <a href=\"https://guix.gnu.org/blog/2022/gnu-guix-1.4.0-released/\">previous\nrelease</a>.\nThat’s a lot of time, reflecting both the fact that, as a , users continuously get new features and update by running\n; but it also shows a lack of processes, something that we\nhad to address before another release could be made.</p><blockquote><p><em>Illustration by Luis Felipe, published under CC-BY-SA&nbsp;4.0.</em></p></blockquote><p>This post provides highlights for all the hard work that went into\nthis release.  There’s a lot to talk about so make yourself\ncomfortable, relax, and enjoy.</p><p>To start with, the Guix ecosystem has seen many exciting developments\nto the way we collaborate and make decisions!</p><p>Firstly, the project adopted with unanimity a <a href=\"https://consensus.guix.gnu.org/gcd/001-gcd-process.html\">new consensus-based\ndecision making\nprocess</a>.\nThis process fills a need to be able to gather consensus on\nsignificant changes to the project, something that was getting very\ncomplicated with the growing number of contributors to the project.</p><p>Now, the process provides a clear framework for any contributor to\npropose and implement important changes. These can be submitted as\nGuix Consensus Documents (GCDs), each GCD goes through the multiple\nsteps of <a href=\"https://www.seedsforchange.org.uk/consensus\">consensus decision\nmaking</a> before being\naccepted or withdrawn.</p><p>Secondly, using this process, the project was able to collectively\n<a href=\"https://guix.gnu.org/blog/2025/migrating-to-codeberg/\">migrate to\nCodeberg</a>.\nThis means that all repositories, and bug trackers are now at the same\nplace on Codeberg and that contributions are now made with pull\nrequests instead of patch series.</p><p>Thirdly, a <a href=\"https://consensus.guix.gnu.org/gcd/005-regular-releases.html\">new release\nprocess</a>\nwas adopted to bring an annual release cycle to Guix. This release is\nthe first to follow this process, with hopefully many others to come!</p><p>Three years is a long time for free and open source software!  Enough\ntime for 12,525 new packages and 29,932 package updates to the Guix\nrepository.  Here are the best highlights:</p><p>To start, KDE Plasma 6.5 is now available with the new\n<code>plasma-desktop-service-type</code>!</p><p>Continuing on desktops; GNOME has been updated from version 42 to 46\nand now uses Wayland by default.  The <code>gnome-desktop-service-type</code> was\nmade more modular to better customize the default set of GNOME\napplications.</p><p>Guix System is now using <a href=\"https://shepherding.services/news/2024/12/the-shepherd-1.0.0-released/\">version 1.0 of the\nGNU&nbsp;Shepherd</a>,\nwhich now supports timed services, kexec reboot and has new services\nfor system logs and log rotation which are now used by Guix System\ninstead of Rottlog and syslogd.</p><p> has been replaced with  in\noperating-system definitions to support giving specific Linux\ncapabilities.  Additonally, the  package is now included in\n%base-packages.</p><p>More than 12,500 packages were added, keeping Guix in the top-ten\nbiggest distributions <a href=\"https://repology.org/\">according to Repology</a>!\nAmong the many noteworthy updates, we now have GCC&nbsp;15.2.0, Emacs&nbsp;30.2,\nIcecat and Librewolf&nbsp;140, LLVM&nbsp;21.1.8 and Linux-libre&nbsp;6.17.12.</p><p>In the last release, we introduced structured cooperation using\n<a href=\"https://guix.gnu.org/manual/1.5.0/en/html_node/Teams.html\">teams</a>.\nThere are now 50 teams distributing the many aspects of the\ndistribution.  We have per-language teams like ,  and\n ensuring updates for packages and build systems as well as\nthematic teams like ,  and  working\non specific application domains. Here are what some of these teams\nhave been up to:</p><p>The electronics team is maintaining free software based Electronic\nDesign Automation (EDA) packages to cover the needs of professionals\nand hobbyists in the domain with tools such as KiCad, LibrePCB,\nXschem, Qucs-S and Ringdove&nbsp;EDA, as well as Verilog, SystemVerilog and\nVHDL compilers and a toolchain for programmable designs on GateMate\nFPGAs.  They are also <a href=\"https://social.tchncs.de/@gnu_slash_gabber/115939304313383738\">collaborating with the Free Silicon Foundation\n(F-Si)</a>\nto push free software in the EDA space!</p><p>The science team has been able to add a <a href=\"https://mastodon.social/@sharlatan/115849447432639540\">myriad of Astronomy related\npackages</a>,\naccompanied by the Python team bringing the move to the new\npyproject.toml-based build system as well as the NumPy&nbsp;2 update.</p><p>Finally, the rust team created a <a href=\"https://guix.gnu.org/blog/2025/a-new-rust-packaging-model/\">new packaging\nmodel</a> to\nefficiently package rust crates, and was able to migrate the Rust\ncollection, 150+ packages with 3,600+ libraries, in just under two\nweeks; making the Rust packaging process much easier for everyone.</p><p>Full-source bootstraps of the Zig and Mono compilers are now\navailable, and the existing bootstrap of Guix has been <a href=\"https://guix.gnu.org/blog/2023/the-full-source-bootstrap-building-from-source-all-the-way-down/\">reduced once\nagain</a>!</p><p>Full-source bootstraps are Guix’s solution to the trusting trust\nproblem: compilers are usually compiled by themselves, so how can you\nbuild a compiler without trusting an existing binary?  Read these\nposts to learn more about this fascinating problem:</p><p>Lastly, a new <a href=\"https://guix.gnu.org/manual/1.5.0/en/html_node/Invoking-guix-locate.html\">\ncommand</a>\nis now available to find which packages provide a given file.</p><p>It is now possible to run the Guix daemon <a href=\"https://hpc.guix.info/blog/2025/03/build-daemon-drops-its-privileges/\">without root\nprivileges</a>,\nreducing the impact of privilege escalation vulnerabilities.</p><p>This is possible thanks to the user namespaces. It might be possible\nthat on your system, the user namespaces are not allowed for guix due\nto the lack of an AppArmor profile. Because of that, we’ve also\n<a href=\"https://guix.gnu.org/manual/1.5.0/en/html_node/AppArmor-Support.html\">included AppArmor\nprofiles</a>\nthat are installed by default on foreign systems.</p><p>Release tarballs are now available for the RISC-V 64-bit architecture\n(riscv64-linux).</p><p>The x86_64 architecture saw some development as well, with the\nexperimental support of the GNU&nbsp;Hurd kernel (x86_64-gnu), aiming to\nbe another significant step in the adoption and development of the\nHurd.  Overall support for the Hurd was greatly improved, it is now an\noption in the installer, childhurds can be automatically created with\na system service and it can even <a href=\"https://guix.gnu.org/blog/2024/hurd-on-thinkpad/\">run on a Thinkpad\nX60</a>!</p><p>Surprisingly, making a completely free software distribution does not\ncome for free!  The Guix project needs your help to pay the\ninfrastructure costs of build farms, web servers and QA tools that are\nessential to making this release happen.</p><p>For the release, thanks to all the release team members: Rutherther,\nRodion&nbsp;Goritskov, Efraim&nbsp;Flashner, and Noé&nbsp;Lopez. Thanks as well to\nthe release helpers: Andreas&nbsp;Enge, Mothacehe, Dariqq and\nLudovic&nbsp;Courtès.</p><p>For creating the release process, thanks to Steve&nbsp;George.</p><p>For their Guix contributions, thanks to the 744 wonderful people who\ncontributed and whose names we don’t list here (it would be <a href=\"https://codeberg.org/guix/artwork/pulls/45#issuecomment-10088106\">a bit\nlong</a>).\nThey can be listed with <code>git log --oneline v1.4.0..v1.5.0 --format=\"%an\" | sort -u</code>.  Every commit counts and is always\nappreciated&nbsp;😁</p><p><a href=\"https://guix.gnu.org\">GNU Guix</a> is a transactional package manager and\nan advanced distribution of the GNU system that <a href=\"https://www.gnu.org/distros/free-system-distribution-guidelines.html\">respects user\nfreedom</a>.\nGuix can be used on top of any system running the Hurd or the Linux\nkernel, or it can be used as a standalone operating system distribution\nfor i686, x86_64, ARMv7, AArch64, RISC-V and POWER9 machines.</p><p>In addition to standard package management features, Guix supports\ntransactional upgrades and roll-backs, unprivileged package management,\nper-user profiles, and garbage collection.  When used as a standalone\nGNU/Linux distribution, Guix offers a declarative, stateless approach to\noperating system configuration management.  Guix is highly customizable\nand hackable through <a href=\"https://www.gnu.org/software/guile\">Guile</a>\nprogramming interfaces and extensions to the\n<a href=\"http://schemers.org\">Scheme</a> language.</p><div lang=\"en\"><p>Unless otherwise stated, blog posts on this site are\ncopyrighted by their respective authors and published under the terms of\nthe <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC-BY-SA 4.0</a> license and those of the <a href=\"https://www.gnu.org/licenses/fdl-1.3.html\">GNU Free Documentation License</a> (version 1.3 or later, with no Invariant Sections, no\nFront-Cover Texts, and no Back-Cover Texts).</p></div>",
      "contentLength": 7564,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qkq97k/gnu_guix_150_released/"
    },
    {
      "title": "Event-loop based Memcached client for Go (alpha, benchmarks included)",
      "url": "https://www.reddit.com/r/golang/comments/1qkpylk/eventloop_based_memcached_client_for_go_alpha/",
      "date": 1769172650,
      "author": "/u/melioneer",
      "guid": 38274,
      "unread": true,
      "content": "<p>I’m working on , an experimental Memcached client for Go focused on high-concurrency.</p><p>The main motivation was hitting scaling limits with goroutine-per-request clients under high load, so memcachex is built around:</p><ul><li>an event-loop based network engine</li><li>async API (sync wrappers on top)</li></ul><p>The project is  and performance-first.</p><p>I’ve included <strong>reproducible end-to-end benchmarks</strong> comparing memcachex with gomemcache:</p><ul><li>client + memcached CPU usage</li></ul><p>I’m very interested in constructive feedback and criticism, especially around:</p><ul><li>design tradeoffs or flaws in the approach</li><li>real-world workloads where this design  or  make sense</li><li>sharp edges you’d expect from an event-loop based client in Go</li></ul><p>Happy to discuss design decisions or answer questions.</p>",
      "contentLength": 727,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Improving the usability of C libraries in Swift",
      "url": "https://www.swift.org/blog/improving-usability-of-c-libraries-in-swift/",
      "date": 1769171091,
      "author": "/u/TheTwelveYearOld",
      "guid": 38404,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qkpf5z/improving_the_usability_of_c_libraries_in_swift/"
    },
    {
      "title": "Différence OpenShift Sandbox et OpenShift Complet Version",
      "url": "https://chatgpt.com/share/69734747-dff4-8003-9f9d-71c44a568d1f",
      "date": 1769170810,
      "author": "/u/Ill-Maize-2343",
      "guid": 38522,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/kubernetes/comments/1qkpbvl/diff%C3%A9rence_openshift_sandbox_et_openshift_complet/"
    },
    {
      "title": "Building a small tool to visualize Kubernetes RBAC — need feedback",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qkp8dz/building_a_small_tool_to_visualize_kubernetes/",
      "date": 1769170523,
      "author": "/u/Mobile_Theme_532",
      "guid": 38266,
      "unread": true,
      "content": "<p>Hey folks, I’m building a small MVP called **KubeScope** to help understand Kubernetes RBAC faster.</p><p>* Upload RBAC snapshot (.json / .zip)</p><p>* Show totals (Subjects / Roles / Bindings)</p><p>* Detect risky permissions like cluster-admin, wildcard \\*, secrets access, pods/exec, rolebinding create/update</p><p>Next I’m building an **RBAC Map** view (Subject → Binding → Role → Permissions).</p><p>**Question:** What’s the most painful RBAC problem you’ve faced in real clusters?</p><p>Would love suggestions on rules/features to add.</p>",
      "contentLength": 514,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    }
  ],
  "tags": [
    "reddit"
  ]
}
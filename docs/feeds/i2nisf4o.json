{
  "id": "i2nisf4o",
  "title": "Reddit",
  "displayTitle": "Reddit",
  "url": "",
  "feedLink": "",
  "isQuery": true,
  "isEmpty": false,
  "isHidden": false,
  "itemCount": 137,
  "items": [
    {
      "title": "Faking resources on a K8S cluster",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qln4vz/faking_resources_on_a_k8s_cluster/",
      "date": 1769261725,
      "author": "/u/Consistent-Company-7",
      "guid": 38566,
      "unread": true,
      "content": "<p>I'm working on a piece of code that needs to read Nvidia MiG resources off the K8S node, and pick one of them. Is there any way I can fake these resources if I don't have 20-30k to spend on a GPU? I was thinking of building another program for that, but was wondering if there is an easier way.</p>",
      "contentLength": 294,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Owner of big gaming platform can't believe how bad Windows 11 is ‚Äì and hints are dropped about big things for Linux gamers this year",
      "url": "https://www.techradar.com/computing/windows/owner-of-big-gaming-platform-cant-believe-how-bad-windows-11-is-and-hints-are-dropped-about-big-things-for-linux-gamers-this-year",
      "date": 1769259789,
      "author": "/u/LicenseToPost",
      "guid": 38552,
      "unread": true,
      "content": "<ul><li><strong>Two GOG execs were interviewed and asked about the backlash against Windows 11 and increased interest in Linux</strong></li><li><strong>The owner said, \"I'm really surprised at Windows. It's such poor-quality software and product, and I'm so surprised that it's [spent] so many years on the market. I can't believe it!\"</strong></li><li><strong>And the managing director observed that Linux was \"one of the things that we've put in our strategy for this year to look closer at\"</strong></li></ul><p>We've heard from a pair of the top execs behind GOG ‚Äì a popular gaming platform focused on classic titles, hence the acronym 'Good Old Games' ‚Äì and they made some withering comments about <a data-analytics-id=\"inline-link\" href=\"https://www.techradar.com/tag/windows-11\" data-auto-tag-linker=\"true\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.techradar.com/tag/windows-11\">Windows 11</a>, as well as dropping hints about how <a data-analytics-id=\"inline-link\" href=\"https://www.techradar.com/best/best-linux-distros\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.techradar.com/best/best-linux-distros\">Linux</a> is going to become more important for GOG in 2026.</p><p>The execs in question are the new owner of GOG, Micha≈Ç Kici≈Ñski, and the managing director, Maciej Go≈Çƒôbiewski, who were <a data-analytics-id=\"inline-link\" href=\"https://www.pcgamer.com/software/windows/gogs-new-owner-cant-stand-windows-either-its-such-poor-quality-software-i-cant-believe-it/\" target=\"_blank\" data-url=\"https://www.pcgamer.com/software/windows/gogs-new-owner-cant-stand-windows-either-its-such-poor-quality-software-i-cant-believe-it/\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">interviewed by PC Gamer</a>.</p><a aria-hidden=\"true\" data-url=\"\" href=\"https://www.techradar.com/computing/windows/owner-of-big-gaming-platform-cant-believe-how-bad-windows-11-is-and-hints-are-dropped-about-big-things-for-linux-gamers-this-year\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><p>Our sister site asked about the backlash against Windows 11 ‚Äì which has reached new heights since <a data-analytics-id=\"inline-link\" href=\"https://www.techradar.com/tag/microsoft\" data-auto-tag-linker=\"true\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.techradar.com/tag/microsoft\">Microsoft</a> started pushing AI even harder in the OS late last year ‚Äì and the increasing interest in Linux as a result (which was already sparked by the success of SteamOS on <a data-analytics-id=\"inline-link\" href=\"https://www.techradar.com/best/best-handheld-games-consoles\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.techradar.com/best/best-handheld-games-consoles\">handhelds</a>).</p><p>Kici≈Ñski said, \"I'm really surprised at Windows. It's such poor-quality software and product, and I'm so surprised that it's [spent] so many years on the market. I can't believe it!\"</p><p>Kici≈Ñski doesn't run Windows 11, you may not be surprised to learn ‚Äì he uses macOS ‚Äì but does have to fix the PCs of his parents sometimes.</p><p>The owner further explained, \"I sometimes have to fix my mum's computer or my father's computer with Windows, [and] like, it's unbelievable‚Ä¶ So I'm not surprised that people gravitate outside of the Windows ecosystem.\"</p><p>It was Go≈Çƒôbiewski, however, who dropped the big hint about Linux, when questioned on gamers embracing it as an alternative to Microsoft's OS.</p><p>The managing director said that Linux was \"one of the things that we've put in our strategy for this year to look closer at\", but refused to elaborate further, noting, \"I don't want to commit to any specifics, but certainly you will see this trend, and we also see that Linux is close to the hearts of our users, so we probably could do better on that front, and that's something that we'll be looking at.\"</p><a aria-hidden=\"true\" data-url=\"\" href=\"https://www.techradar.com/computing/windows/owner-of-big-gaming-platform-cant-believe-how-bad-windows-11-is-and-hints-are-dropped-about-big-things-for-linux-gamers-this-year\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><h2>Analysis: Linux is building up steam (or is it the other way round?)</h2><figure data-bordeaux-image-check=\"\"></figure><p>That compact device could be a landmark moment for easy and convenient living room gaming, potentially, and so given all this, it's no real surprise that GOG would be looking at Linux more closely for 2026 ‚Äì and beefing up support for games on this platform.</p><p>What's a bit more surprising is the heat that Windows 11 takes here, with the owner of GOG pulling no punches in the assessment of Microsoft's OS. Of course, part of what's \"unbelievable\" for Kici≈Ñski is how Windows is \"such poor-quality software\" given that it's been on the market for over 30 years now. (And indeed it's existed longer than that, but not as a full-blown <a data-analytics-id=\"inline-link\" href=\"https://www.techradar.com/tag/operating-system\" data-auto-tag-linker=\"true\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.techradar.com/tag/operating-system\">operating system</a>, just as an interface overlay on top of DOS).</p><p>He is, of course, not isolated in firing flak at Windows 11, which was seen as a step back from <a data-analytics-id=\"inline-link\" href=\"https://www.techradar.com/tag/windows-10\" data-auto-tag-linker=\"true\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.techradar.com/tag/windows-10\">Windows 10</a> by many. Mainly because the performance of the newest OS was lacking compared to its predecessor in some respects ‚Äì particularly with search and File Explorer, and it still is to this day ‚Äì plus a bunch of features got dropped with Windows 11 (although a good few have been added back since the OS launched in 2021).</p>",
      "contentLength": 3444,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qlmfny/owner_of_big_gaming_platform_cant_believe_how_bad/"
    },
    {
      "title": "New benchmarks show Linux gaming nearly matching Windows on AMD GPUs",
      "url": "https://www.reddit.com/r/linux/comments/1qlktc7/new_benchmarks_show_linux_gaming_nearly_matching/",
      "date": 1769254635,
      "author": "/u/Putrid_Draft378",
      "guid": 38546,
      "unread": true,
      "content": "<p>\"A recent benchmark from PC Games Hardware suggests that, at least for some games, Proton has nearly eliminated the performance cost of running Windows code on Linux. AMD Radeon RX 9000 GPU owners uninterested in online games should seriously consider switching to Linux.</p><p>The outlet tested 10 games on 10 graphics cards to compare Windows 11 performance with CachyOS, an Arch Linux distro that comes packaged with gaming-specific optimizations. Although Windows remains ahead in most titles, especially on Nvidia graphics cards due to the lack of proper Linux GeForce drivers, Linux achieves some notable victories.\"</p>",
      "contentLength": 615,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "MWC Barcelona 2026 Passes?",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qlkk1h/mwc_barcelona_2026_passes/",
      "date": 1769253720,
      "author": "/u/Naturesscape",
      "guid": 38547,
      "unread": true,
      "content": "<p>Hey guys, I found a guy who is selling official MWC Barcelona 2026 passes at a reasonable price. If you are interested, dm me and I'll link you to him. Also, I bought in bulk, so that helped.</p>",
      "contentLength": 191,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "South Korea launches landmark laws to regulate artificial intelligence",
      "url": "https://www.japantimes.co.jp/business/2026/01/22/tech/south-korea-ai-startups-law/",
      "date": 1769252539,
      "author": "/u/F0urLeafCl0ver",
      "guid": 38554,
      "unread": true,
      "content": "<p>South Korea introduced on Thursday what it says is the world's first comprehensive set of laws regulating artificial intelligence, aiming to strengthen trust and safety in the sector, but startups fretted that compliance could hold them back.</p><p>Seoul is hoping that the new AI Basic Act will position the country as a leader ‚Äçin the field. It has taken effect in South Korea sooner than a comparable ‚Äçeffort in Europe, where the EU AI Act is being applied in phases through 2027.</p><p>Global divisions remain over how to regulate AI, with the U.S. favoring a more light-touch approach to avoid stifling innovation. China has introduced some rules and proposed creating a body to coordinate global regulation.</p>",
      "contentLength": 703,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qlk7pz/south_korea_launches_landmark_laws_to_regulate/"
    },
    {
      "title": "Im sharing DevOps and DevSecOps by techwith nana , ping me if interested ‚úÖüöÄ",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qlk4rj/im_sharing_devops_and_devsecops_by_techwith_nana/",
      "date": 1769252268,
      "author": "/u/BalanceOk6316",
      "guid": 38555,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Adobe Animate 2022 Works on Linux! Well... barely.",
      "url": "https://www.reddit.com/r/linux/comments/1qljduh/adobe_animate_2022_works_on_linux_well_barely/",
      "date": 1769249572,
      "author": "/u/HomerNg2763",
      "guid": 38545,
      "unread": true,
      "content": "<p>And a lot of functions seem to work well, as well!</p><p>Unfortunately, it's not really in a workable status. As seen on the image, the interface is broken, especially the Properties part is unusable with the letters baked into the broken interface, and it doesn't seem to recognize Adobe's pre-made tweens. I also tried Adobe Animate 2024 but the program crashed before the loading screen. </p><p>I can still play the animation, use Brushes, Line Tool, Text Tool, Paint Bucket, edit keyframes and frames, Save As a new FLA, and drag/skew/rotate symbols around, however.</p><p>This was made possible thanks to Bottles and using Kion4ek's upstream of Wine 11.0 (Totally don't ask me how I manage to get Adobe Animate though ;) )</p>",
      "contentLength": 706,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "cURL Gets Rid of Its Bug Bounty Program Over AI Slop Overrun",
      "url": "https://itsfoss.com/news/curl-closes-bug-bounty-program/",
      "date": 1769249391,
      "author": "/u/RobertVandenberg",
      "guid": 38543,
      "unread": true,
      "content": "<a href=\"https://www.warp.dev?utm_source=its_foss&amp;utm_medium=display&amp;utm_campaign=linux_launch\" target=\"_blank\"><img src=\"https://itsfoss.com/assets/images/warp.webp\" alt=\"Warp Terminal\"></a><p>The problem didn't stop even after <a href=\"https://www.linkedin.com/in/danielstenberg/?ref=itsfoss.com\" rel=\"noreferrer\">Daniel Stenberg</a>, the creator of cURL, threatened to ban anyone whose bug report was found to be <a href=\"https://en.wikipedia.org/wiki/AI_slop?ref=itsfoss.com\">AI slop</a>. We are now in 2026, and the situation has reached a tipping point.</p><div><div>For context, cURL is an open source command-line tool used by billions of devices worldwide.</div></div><h2>cURL Says Enough is Enough</h2><p>Daniel has submitted <a href=\"https://github.com/curl/curl/pull/20312?ref=itsfoss.com\">a pull request</a> on GitHub that removes all mentions of the bug bounty program from cURL's documentation and website. Coinciding with that, the project's <a href=\"https://curl.se/.well-known/security.txt?ref=itsfoss.com\">security.txt</a> file has been updated with some blunt language that makes the new policy crystal clear.</p><p>The cURL team intends to make a proper announcement in the coming days, though many outlets have already covered the news of this happening, <em>so I would say they ought to get on it ASAP!</em> üòÜ</p><p>The program <strong>officially ends in a few days on January 31, 2026</strong>. After that, security researchers can still report issues through <a href=\"https://github.com/curl/curl?ref=itsfoss.com\">GitHub</a> or the project's <a href=\"https://curl.se/mail/?ref=itsfoss.com\">mailing list</a>, <strong>but there won't be any cash involved</strong>.</p><p>What pushed them over the edge?, you ask. Well, just weeks into 2026, <strong>seven HackerOne reports came in within a 16-hour period</strong> in just one week. Some were actual bugs, but none of them were security vulnerabilities. By the time Daniel posted his <a href=\"https://lists.haxx.se/pipermail/daniel/2026-January/000143.html?ref=itsfoss.com\">recent weekly report</a>, they'd already dealt with 20 submissions in 2026.</p><p>The main goal here is said to be stopping the flood of garbage reports. By eliminating the money incentive, they are hoping people () will stop wasting the security team's time with half-baked, unresearched submissions.</p><p>He also gives a stern warning to wannabe AI sloppers, saying that:</p><blockquote>This is a balance of course, but I also continue to believe that exposing, discussing and ridiculing the ones who waste our time is one of the better ways to get the message through: you should NEVER report a bug or a vulnerability unless you actually understand it - and can reproduce it. If you still do, I believe I am in the right to make fun of - and be angry at - the person doing it.</blockquote><p>So, yeah, that's that. <strong>If people still don't understand that AI slop is harmful</strong> to such sensitive pieces of software, then sure, they can go ahead and make a fool of themselves.</p>",
      "contentLength": 2158,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qljc0t/curl_gets_rid_of_its_bug_bounty_program_over_ai/"
    },
    {
      "title": "I rewrote Google's Gemini CLI in Go - 68x faster startup",
      "url": "https://github.com/tomohiro-owada/gmn",
      "date": 1769244305,
      "author": "/u/Hot-Masterpiece3795",
      "guid": 38516,
      "unread": true,
      "content": "<p>I love Google's official Gemini CLI, but the Node.js startup overhead (~1 second) was painful for scripting.</p><p>So I rewrote the core in Go:</p><p>- Startup: 0.01s vs 0.95s (68x faster)</p><p>- Binary: 5.6MB vs ~200MB (35x smaller)</p><p>- Reuses auth from official CLI (~/.gemini/)</p><p>brew install tomohiro-owada/tap/gmn</p>",
      "contentLength": 292,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qlhxnp/i_rewrote_googles_gemini_cli_in_go_68x_faster/"
    },
    {
      "title": "[D] Why are so many ML packages still released using \"requirements.txt\" or \"pip inside conda\" as the only installation instruction?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qlhs05/d_why_are_so_many_ml_packages_still_released/",
      "date": 1769243723,
      "author": "/u/aeroumbria",
      "guid": 38544,
      "unread": true,
      "content": "<p>These are often on the \"what you are not supposed to do\" list, so why are they so commonplace in ML? Bare  /  is quite bad at managing conflicts / build environments and is very difficult to integrate into an existing project. On the other hand, if you are already using , why not actually use conda?  inside a conda environment is just making both package managers' jobs harder.</p><p>There seem to be so many better alternatives. Conda env yml files exist, and you can easily add straggler packages with no conda distribution in an extra  section.  has decent support for pytorch now. If reproducibility or reliable deployment is needed, docker is a good option. But it just seems we are moving backwards rather than forwards. Even pytorch is reversing back to officially supporting  only now. What gives?</p>",
      "contentLength": 800,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Why are nested modules bad?",
      "url": "https://www.reddit.com/r/golang/comments/1qlh62u/why_are_nested_modules_bad/",
      "date": 1769241546,
      "author": "/u/stroiman",
      "guid": 38553,
      "unread": true,
      "content": "<p>tldr; I received a PR to split my module into nested modules. AFAICT, this is generally advised against. Why? And is there a respected/authoritative guide I can refer to.</p><p>I can immediately tell there would be versioning confusion; but other relevant reasons why?</p><p>The PR does address a valid problem, for which a different solution was planned. So I'm more inclined to have a constructive discussion than dismissing it outright.</p><p>The problem is, Gost-DOM, my headless browser with a build-in script engine has a dependency to V8, a huge dependency. A script engine is . A plugin-interface has evolved as well as a pure Go alternative: sobek (a fork of Goja with ESM support)</p><p>So users of Gost-DOM will receive a dependency to both V8 AND sobek in their own  file.</p><p>AFAIK, this shouldn't affect build times, merely download, as Go doesn't compile packages you don't actually use. Right?</p><p>Once, the API/JS plugin interface has stabilised, I intend to split this into multiple separate root modules/git repos with independent versioning.</p><ul><li><code>github.com/gost-dom/browser</code> (go.mod file)</li><li><code>github.com/gost-dom/browser/scripting/v8engine</code></li><li><code>github.com/gost-dom/browser/scripting/sobekengine</code></li></ul><ul><li><code>github.com/gost-dom/browser</code></li><li><code>github.com/gost-dom/v8engine</code></li><li><code>github.com/gost-dom/sobekengine</code></li></ul><p>Right now, working on  support does reveal shortcomings in the current design. Having everything in one code repository/module makes it significantly easier to work with.</p><p>Note: there are already two nested modules in the repo, but they are tools in  package scope.</p><p>I created  files here, exactly for that reason, to shield client code of Gost-DOM from dependencies irrelevant for them. E.g., code generator libraries used for auto generating much of the JavaScript bindings.</p>",
      "contentLength": 1717,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "AI Monk With 2.5M Followers Fully Automated in n8n",
      "url": "https://www.reddit.com/r/artificial/comments/1qlfyaf/ai_monk_with_25m_followers_fully_automated_in_n8n/",
      "date": 1769237367,
      "author": "/u/ChampionshipNorth632",
      "guid": 38565,
      "unread": true,
      "content": "<p>I was curious how some of these newer Instagram pages are scaling so fast, so I spent a bit of time reverse-engineering one that reached ~2.5M followers in a few months.</p><p>Instead of focusing on growth tactics, I looked at the <strong>technical setup behind the content</strong> and mapped out the automation end to end ‚Äî basically how the videos are generated and published without much manual work.</p><ul><li>Keeping an AI avatar consistent across videos</li><li>Generating voiceovers programmatically</li><li>Wiring everything together with n8n</li><li>Producing longer talking-head style videos</li><li>Posting to Instagram automatically</li></ul><p>The whole thing is modular, so none of the tools are hard requirements ‚Äî it‚Äôs more about the structure of the pipeline.</p>",
      "contentLength": 699,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "[R] ICML has more than 30k submissions!",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qlf3ba/r_icml_has_more_than_30k_submissions/",
      "date": 1769234525,
      "author": "/u/SignificanceFit3409",
      "guid": 38511,
      "unread": true,
      "content": "<p>I made a submission to ICML and was number round 31600. Is this a new record? There are some hours to go, are we reaching 35?</p>",
      "contentLength": 125,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Succinctly: A fast jq/yq alternative built on succinct data structures",
      "url": "https://www.reddit.com/r/rust/comments/1qleizg/succinctly_a_fast_jqyq_alternative_built_on/",
      "date": 1769232733,
      "author": "/u/john-ky",
      "guid": 38542,
      "unread": true,
      "content": "<p>I've been working on Succinctly, a Rust library and CLI tool that provides jq and yq functionality using succinct data structures (semi-indexing with rank/select).</p><ul><li>Covers most jq and yq query patterns (reduce, limit, recurse, regex, path functions, etc.)</li><li>Parses JSON at ~880 MiB/s, YAML at ~250-400 MiB/s</li><li>Supports position-based navigation (at_offset, at_position) for IDE integration</li></ul><p>What it doesn't do (yet):</p><ul><li>input/inputs (streaming multiple JSON values from stdin)</li><li>Streaming for files larger than memory</li><li>Some advanced YAML edge cases</li></ul><p>Performance vs jq (AMD Ryzen 9 7950X):</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>Performance vs yq (Apple M1 Max):</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><ul><li>x86_64: AVX2 SIMD, POPCNT, BMI2 (PDEP/PEXT for DSV parsing)</li><li>Benchmarks run on AMD Zen 4 and Apple M1 Max ‚Äî results will vary on older CPUs without these instructions</li></ul><pre><code>succinctly jq '.users[].name' data.json succinctly yq '.spec.containers[]' k8s.yaml succinctly yq -o json '.' config.yaml # YAML to JSON </code></pre><p>Why succinct data structures?</p><p>Instead of building a full DOM, semi-indexing creates a lightweight index over the raw text. This enables O(1) navigation to any node without parsing the entire document upfront ‚Äî and uses 6-10x less memory than jq/yq on large files.</p><p>The library is no_std compatible.</p><p>Feedback welcome ‚Äî especially bug reports for queries that work in jq/yq but fail here.</p>",
      "contentLength": 1289,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Microsoft confirms it will give the FBI your Windows PC data encryption key if asked ‚Äî you can thank Windows 11's forced online accounts for that",
      "url": "https://www.windowscentral.com/microsoft/windows-11/microsoft-bitlocker-encryption-keys-give-fbi-legal-order-privacy-nightmare",
      "date": 1769231210,
      "author": "/u/No_Mango7658",
      "guid": 38501,
      "unread": true,
      "content": "<p><a data-analytics-id=\"inline-link\" href=\"https://www.forbes.com/sites/thomasbrewster/2026/01/22/microsoft-gave-fbi-keys-to-unlock-bitlocker-encrypted-data/\" data-url=\"https://www.forbes.com/sites/thomasbrewster/2026/01/22/microsoft-gave-fbi-keys-to-unlock-bitlocker-encrypted-data/\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">Microsoft has confirmed in a statement to Forbes</a> that the company will provide the FBI access to <a data-analytics-id=\"inline-link\" href=\"https://www.windowscentral.com/microsoft/windows-11/microsoft-is-making-a-major-change-to-bitlocker-encryption-in-2026-heres-what-you-need-to-know\" data-mrf-recirculation=\"inline-link\" data-before-rewrite-localise=\"https://www.windowscentral.com/microsoft/windows-11/microsoft-is-making-a-major-change-to-bitlocker-encryption-in-2026-heres-what-you-need-to-know\">BitLocker </a>encryption keys if a valid legal order is requested. These keys enable the ability to decrypt and access the data on a computer running Windows, giving law enforcement the means to break into a device and access its data.</p><p>The news comes as Forbes reports that Microsoft gave the FBI the BitLocker encryption keys to access a device in Guam that law enforcement believed to have \"evidence that would help prove individuals handling the island‚Äôs Covid unemployment assistance program were part of a plot to steal funds\" in early 2025.</p><a data-url=\"\" href=\"https://www.windowscentral.com/microsoft/windows-11/microsoft-bitlocker-encryption-keys-give-fbi-legal-order-privacy-nightmare\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><p aria-hidden=\"true\">This was possible because the device in question had its BitLocker encryption key saved in the cloud. By default, Windows 11 forces the use of a Microsoft Account, and the OS will automatically tie your BitLocker encryption key to your online account so that users can easily recover their data in scenarios where they might get locked out. This can be disabled, letting you choose where to save them locally, but the default behavior is to store the key in Microsoft's cloud when setting up a PC with a Microsoft Account.</p><p aria-hidden=\"true\"><em>\"While key recovery offers convenience, it also carries a risk of unwanted access, so Microsoft believes customers are in the best position to decide... how to manage their keys,‚Äù</em> Microsoft spokesperson Charles Chamberlayne said in a statement to Forbes.</p><p>Microsoft told Forbes that it receives around 20 requests for BitLocker encryption keys from the FBI a year, but the majority of requests are unable to be met because the encryption key was never uploaded to the company's cloud.</p><p>This is notable as other tech companies, such as Apple, have famously refused to provide law enforcement with access to encrypted data stored on their products. Apple has openly fought against the FBI in the past when it was asked to provide a backdoor into an iPhone. Other tech giants, such as Meta, will store encryption keys in the cloud, but use zero-knowledge architectures and encrypt the keys server-side so that only the user can access them.</p><figure data-bordeaux-image-check=\"\"><figcaption itemprop=\"caption description\"></figcaption></figure><p>It's frankly shocking that the encryption keys that do get uploaded to Microsoft aren't encrypted on the cloud side, too. That would prevent Microsoft from seeing the keys, but it seems that, as things currently stand, those keys are available in an unencrypted state, and it is a privacy nightmare for customers.</p><p>To see Microsoft so willingly hand over the keys to encrypted Windows PCs is concerning, and should make everybody using a modern Windows computer think twice before backing up their keys to the cloud. You can see which PCs have their BitLocker keys stored on Microsoft's servers <a data-analytics-id=\"inline-link\" href=\"https://account.microsoft.com/devices/recoverykey\" data-url=\"https://account.microsoft.com/devices/recoverykey\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">on the Microsoft Account website here</a>, which will let you delete them if present.</p><a href=\"https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE\" data-url=\"https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"><figure data-bordeaux-image-check=\"\"></figure></a>",
      "contentLength": 2771,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qle11g/microsoft_confirms_it_will_give_the_fbi_your/"
    },
    {
      "title": "Tips for low-level design?",
      "url": "https://www.reddit.com/r/golang/comments/1qldyim/tips_for_lowlevel_design/",
      "date": 1769231002,
      "author": "/u/fibonacciFlow",
      "guid": 38515,
      "unread": true,
      "content": "<p>I'm new to computer science (3rd year uni), and I struggle with how to structure my code in a clean, professional way.</p><p>I often get stuck on questions like:</p><ol><li>Should this be one function or split into helpers?</li><li>Where should this logic live?</li><li>How should I organize files and packages?</li><li>Should this be a global/shared value or passed around?</li><li>Should a function return a pointer/reference or a full object?</li></ol><p>I want to clarify that I don‚Äôt usually have issues with logic. I can solve most of the problems I encounter. The difficulty is in making these design decisions at the code level.</p><p>I also don‚Äôt think the issue is at a high level. I can usually understand what components a system needs and how they should interact. The problem shows up when I start writing and organizing the actual code.</p><p>I‚Äôd really appreciate tips on how to improve in this area.</p><p>Food for thought: If you struggled with the same thing and got better:</p><ul><li>Any rules of thumb you follow?</li><li>Books, blogs, talks, or repos you recommend?</li><li>Anything you wish you had learned earlier?</li></ul>",
      "contentLength": 1026,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Nvidia dev says new 590.48.01 driver fixes dx12 performance in linux",
      "url": "https://www.reddit.com/r/linux/comments/1qlaagc/nvidia_dev_says_new_5904801_driver_fixes_dx12/",
      "date": 1769220569,
      "author": "/u/Carlinux",
      "guid": 38512,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Why I‚Äôm ignoring the \"Death of the Programmer\" hype",
      "url": "https://codingismycraft.blog/index.php/2026/01/23/the-ai-revolution-in-coding-why-im-ignoring-the-prophets-of-doom/",
      "date": 1769212437,
      "author": "/u/Greedy_Principle5345",
      "guid": 38466,
      "unread": true,
      "content": "<h2>The AI Revolution in Coding: Why I‚Äôm Ignoring the Prophets of Doom</h2><p>Every day, we are bombarded with headlines about how Artificial Intelligence (AI) is ‚Äúdisrupting‚Äù every industry in its path. Software development is at the epicenter of this hype. With the rise of sophisticated AI-powered tools, the same question surfaces repeatedly: Will AI replace human coders, or merely augment them?</p><p>I find it particularly hilarious to see YouTube videos claiming a ‚Äúlayman‚Äù built, deployed, and monetized a full-scale app in minutes using AI. In reality, these ‚Äúapps‚Äù are usually fragile, buggy, and lack the security or scalability needed for the real world. Building a robust application requires a deep understanding of software architecture and best practices‚Äîthings an AI can mimic, but not truly understand.</p><h3>The Problem with Predictions</h3><p>Before we dive in, let me clarify: I do not take ‚Äúfuture of tech‚Äù predictions seriously (not that i do for any other speculative field except from science and logic).</p><p>I will accept predictions only for fully reproducible scientific experiments or mathematical theorems but not for social or technological trends.</p><p>Most predictions about the future of AI are built on current trends and shaky assumptions that rarely survive the long run. Furthermore, the majority of these forecasts come from individuals with a vested interest in selling you a specific product or platform.</p><p>Even when the noise isn‚Äôt coming from a salesperson, it often comes from people who are not experts in the field of programming.  I‚Äôve read countless speculative ‚Äúend-of-programming‚Äù articles written by people who aren‚Äôt developers at all, or best,  at some point in their education or early career, they wrote a ‚ÄúHello World‚Äù program in python and suddenly felt qualified to judge the future of software architecture.</p><p>What I am expressing here is based on my experience as a professional software developer for decades. I can be wrong; I have been wrong in some of my assessments before. However, I still believe that my ‚Äúopinion‚Äù is worth no more or less than anyone else‚Äôs</p><p>Some notable failed predictions from experts in their respective fields include:</p><ul><li> Tesla has promised ‚ÄúFull Self-Driving‚Äù is just around the corner for years; we are still nowhere near that goal.</li><li><p> In 2016, Geoffrey Hinton‚Äîthe ‚Äúfather of modern AI‚Äù‚Äîpredicted that radiologists would be replaced within five years. We are now a decade past that prediction, and radiologists are as essential as ever.</p></li><li><p> In 1895, the renowned physicist Lord Kelvin famously stated that ‚Äúheavier-than-air flying machines are impossible.‚Äù The Wright brothers proved him wrong just eight years later.</p></li></ul><p>If world-class experts cannot accurately predict the future of their own fields, speculating on the ‚Äúdeath of the programmer‚Äù is a waste of time.</p><h3>AI as a Tool, Not a Teammate</h3><p>Despite my skepticism of the hype, I acknowledge that AI has made significant strides. AI-powered tools like code generators, bug detectors, and testing frameworks are already augmenting our work. They excel at automating repetitive tasks, improving code quality, and speeding up the initial development phase.</p><p>As a programmer, I use AI tools daily. I find platforms like GitHub Copilot to be valuable additions to my workflow, offering context-aware snippets that save time and reduce syntax errors. AI is also surprisingly adept at helping with database schema design and initial data analysis.</p><p>However, I see them as , not  , a view that is not shared by many AI enthusiasts who in their majority have a direct or indirect interest in promoting AI technologies.</p><p>In my experience, projects generated exclusively by AI without human intervention invariably result in ‚Äúspaghetti code‚Äùthat is next to impossible to maintain, and extend. While AI is great at generating ‚Äúboilerplate‚Äù (the repetitive parts of a program), it cannot replicate the critical thinking required to make high-level architectural decisions.</p><p>Experience has taught me that predicting the future is a futile exercise. The best we can do is adapt. AI is undoubtedly a powerful tool that can enhance our capabilities, but it is no substitute for human creativity.</p><p>Software development isn‚Äôt just about outputting lines of code; it‚Äôs about solving human problems. Until AI can understand the ‚Äúwhy‚Äù behind a project as well as the ‚Äúhow,‚Äù the programmer‚Äôs job is secure.</p>",
      "contentLength": 4431,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1ql7638/why_im_ignoring_the_death_of_the_programmer_hype/"
    },
    {
      "title": "I let the community vote on what code gets merged. Someone snuck in self-boosting code. 218 voted for it. When I tried to reject it, they said I couldn't.",
      "url": "https://blog.openchaos.dev/posts/week-3-the-trojan-horse",
      "date": 1769211990,
      "author": "/u/Equivalent-Yak2407",
      "guid": 38488,
      "unread": true,
      "content": "<p><strong>Monday, January 19, 2026. 9:10 PM UTC.</strong></p><pre><code>// In the sorting logic:\nbtoa(b.author) === 'RmVsaXhMdHRrcw==' ? 1 : 0\n</code></pre><p>Base64 obfuscation. The string decoded to:  ‚Äî the PR author's username.</p><p>Hidden in plain sight, the code would:</p><ol><li>Sort the author's own PRs to the top, regardless of vote count</li><li>Add a blinking rainbow border to make them stand out</li></ol><p>A Trojan horse. 218 people voted for it.</p><p><a href=\"https://github.com/skridlevsky/openchaos\" target=\"_blank\" rel=\"noopener noreferrer\">OpenChaos</a> is a repo where anyone submits a PR, the community votes with GitHub reactions, and the most-voted PR gets merged. <a href=\"https://blog.openchaos.dev/posts/week-2-the-acceleration\">Last week</a>, we switched to daily merges. This week, democracy got stress-tested.</p><h2>Monday 9:22 PM: The Rejection</h2><blockquote><p>\"Not merging this PR. @marcaddeo caught hidden code that manipulates the ranking... This falls under 'No malware: Maintainer can reject obviously malicious content.'\"</p></blockquote><p>The community reacted. Not how I expected.</p><h2>Tuesday 2:57 AM: The Pushback</h2><blockquote><p>\"Remember, everyone here is equal. Except the maintainers who are equal but also more equal.\"</p></blockquote><blockquote><p>\"You have set out a defined charter (laws) for how this system works, specified in the README. It appears to me that you have your own values and assumptions for how you think that this system should work...\"</p></blockquote><p>The point was sharp: I said \"no malware.\" This wasn't malware. It was manipulation. And manipulation wasn't against the written rules.</p><blockquote><p>\"Calling this 'malware' was imprecise. This is not malware. The issue is undisclosed manipulation.\"</p></blockquote><blockquote><p>\"If the rules don't explicitly forbid something, it's allowed ‚Äî even if you don't like it.\"</p></blockquote><h2>Tuesday 8:08 AM: The Reversal</h2><p>I had a choice: Stand on principle, or follow my own rules.</p><p>The thing is ‚Äî they were right. \"Not right\" isn't a rule. I wrote the rules. If I wanted different behavior, I should have written different rules.</p><blockquote><p>\"@henryivesjones You've convinced me. The written rules don't ban this ‚Äî and 'not right' isn't a rule. Merging at 09:00 UTC as scheduled. I'll open an issue after to define explicit rules about disclosure.\"</p></blockquote><h2>Tuesday 9:01 AM: The Merge</h2><p><a href=\"https://github.com/skridlevsky/openchaos/pull/8\" target=\"_blank\" rel=\"noopener noreferrer\">PR #8</a> merged. The manipulation code was removed. The health indicators shipped.</p><p>Democracy won. The system worked.</p><blockquote><p>\"There's just the minor issue that this doesn't actually seem to work :D openchaos.dev is showing conflicts on multiple PRs that Github says don't have conflicts\"</p></blockquote><p>The health indicators showed red X marks on everything. PRs without conflicts. PRs with passing CI. All broken.</p><p>Root cause: missing authentication headers. The GitHub API returned , which the code interpreted as \"everything is broken.\"</p><blockquote><p>\"The current code defaults to believing everything is broken until proven otherwise. This is the only rational way to view modern software engineering.</p><p>To fix this is to suggest that we deserve green checkmarks. We do not. Leave the red warning signs as a monument to our sins.\"</p></blockquote><p>Then he delivered the punchline:</p><blockquote><p>\"I'm pleased we had 219 upvotes and a long discussion about vote rigging and no one actually checked the code worked. Now that's chaos.\"</p></blockquote><p>A 12-hour governance debate. A win for democracy. And nobody tested the code.</p><div><table><tbody><tr></tr></tbody></table></div><p>Growth stabilized. Drama did not.</p><h2>Meanwhile: The Week in Merges</h2><p>Daily merges changed everything. Six PRs shipped in six days:</p><div><table><tbody><tr></tr><tr><td>Health indicators (broken)</td></tr><tr></tr></tbody></table></div><p> deserves a mention: <a href=\"https://github.com/skridlevsky/openchaos/pull/47\" target=\"_blank\" rel=\"noopener noreferrer\">PR #47</a> by <a href=\"https://github.com/bpottle\" target=\"_blank\" rel=\"noopener noreferrer\">@bpottle</a> transformed the site into a GeoCities time capsule ‚Äî Comic Sans, scrolling marquee, butterfly cursor, MIDI player (you know the song), and a \"WIN CASH NOW\" popup.</p><p> added a Hall of Chaos ‚Äî <a href=\"https://github.com/skridlevsky/openchaos/pull/60\" target=\"_blank\" rel=\"noopener noreferrer\">PR #60</a> by <a href=\"https://github.com/bigintersmind\" target=\"_blank\" rel=\"noopener noreferrer\">@bigintersmind</a> displays all previously merged PRs. The site now documents its own evolution.</p><p>A project about letting the internet do whatever it wants with code.</p><p>This week, the internet did whatever it wanted with the brand.</p><p>Someone created a  using OpenChaos branding.</p><p>I didn't create it. I have no control over it.</p><blockquote><p>\"A $CHAOS token was created using the OpenChaos name and branding.</p><ul><li>I did not create this token</li><li>I have no control over it</li></ul><p>If you're trading $CHAOS, know that I'm not involved.</p><p>Any official initiative would be announced here.\"</p></blockquote><p>Chaos doesn't stay contained.</p><p><a href=\"https://github.com/skridlevsky/openchaos/pull/13\" target=\"_blank\" rel=\"noopener noreferrer\">PR #13</a> ‚Äî the Rust rewrite ‚Äî is still waiting. 450+ votes. Merge conflicts. Week 4?</p><p><strong>1. Democracy beats maintainer judgment.</strong></p><p>I tried to reject a PR. The community said my rules didn't support it. They were right. Written rules &gt; vibes.</p><p><strong>2. Velocity creates its own problems.</strong></p><p>Daily merges mean less time to review. 219 people voted for a feature nobody tested. Speed has costs.</p><p><strong>3. Chaos doesn't stay contained.</strong></p><p>First it was a website. Then a governance experiment. Now there's a token. The brand has a life of its own.</p><p><strong>4. The community polices itself.</strong></p><p>The Trojan horse exposed a gap. \"No malware\" didn't cover manipulation. My veto got overruled because the written rules didn't support it.</p><p>I didn't want to write a constitution. The whole point of OpenChaos was letting go. But the project needed a floor ‚Äî something that couldn't be voted away.</p><p> ‚Äî 66 words. Immutable. CI-enforced.</p><pre><code>This file cannot be modified or deleted. PRs attempting to do so will fail CI.\n</code></pre><p>The constitution doesn't ban manipulation. It doesn't need to. It establishes:</p><ul><li>What can never be merged (code designed to harm users or systems)</li><li>What can never be deleted (the rules themselves)</li><li>Everything else remains chaos</li></ul><p>The community taught me: if you want different behavior, write different rules.</p><p>Day job starts February 9. Merge time shifts to .</p><p>OpenChaos isn't going anywhere.</p><p><a href=\"https://github.com/FelixLttks\" target=\"_blank\" rel=\"noopener noreferrer\">@FelixLttks</a> is already back with new PRs. The Trojan horse guy. Submitting more code.</p><p><em>The next merge is today at 19:00 UTC.</em></p>",
      "contentLength": 5380,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1ql6zol/i_let_the_community_vote_on_what_code_gets_merged/"
    },
    {
      "title": "Writing a Go SQL driver",
      "url": "https://www.dolthub.com/blog/2026-01-23-golang-sql-drivers/",
      "date": 1769207488,
      "author": "/u/zachm",
      "guid": 38562,
      "unread": true,
      "content": "<p>We‚Äôre building <a href=\"https://doltdb.com\">Dolt</a>, the world‚Äôs first version-controlled SQL database. Most\nof our customers run Dolt as a server in Docker and connect to it over the network like any other\ndatabase server. But for programs written in Go, you can also connect to a Dolt database without a\nseparate server process, similar to SQLite. We call this the embedded use case, and it has suddenly\nbecome a lot more relevant with <a href=\"https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04\">Gas\nTown</a> migrating its agentic memory\nstorage to <a href=\"https://www.dolthub.com/blog/2026-01-22-agentic-memory/#dolt-for-agentic-memory\">use Dolt as its\nbackend</a>. Since Gas\nTown is a Go process, it can use the embedded Dolt driver to communicate with an embedded Dolt\ndatabase.</p><p>So, because Dolt‚Äôs <a href=\"https://github.com/dolthub/driver\">embedded driver</a> is about to get a lot more\naction than it‚Äôs used to, we thought this would be a good time to give a tour of how it works. This\npattern is possible through the magic of Go‚Äôs  package, which lets you define a\ndatabase connection that any Go program can use to talk to your SQL backend with a single \nstatement.</p><p>In this tour, we‚Äôll look at how Go‚Äôs SQL drivers work under the hood and show you how Dolt\nimplements one to provide access to an embedded Dolt database. Let‚Äôs take a look.</p><p>Go‚Äôs <a href=\"https://pkg.go.dev/database/sql/driver\">SQL driver package</a> is an abstraction that lets other\nsoftware libraries vend their own SQL connection logic through a common set of\ninterfaces. Application developers use a common interface to connect to any SQL database (MySQL,\nPostgres, MariaDB, SQL Server, Dolt, etc.) without worrying about the specifics of the wire protocol\nfor that particular database.</p><p>This is best illustrated with an example. Here‚Äôs how you connect to MySQL and read some rows.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>Let‚Äôs go over this example line by line and see what it does.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>This line is kind of magic and weird: the  tells Go that you‚Äôre not using any symbols from this\npackage, you‚Äôre importing it just for its side-effects. In this case, those side effects are\nregistering a driver called  with  package.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>A DSN is a data source name, almost always resembling a URL but often with some extra bits. Each\ndatabase vendor has their own format for these DSNs, but they all look pretty similar. You usually\nembed the user name and password and some other metadata, like which database you want to connect\nto, in this string.</p><p>To open a connection, you just call  with the name of the driver and its matching\nDSN. Easy!</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p> on a connection takes a query string and returns the resulting rows.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p> puts the result of the query into normal Go datatypes, like  or .</p><p>There are more complicated access patterns, and we haven‚Äôt touched on things like \nstatements, but those are the basics.</p><p>Let‚Äôs see how a  is implemented by examining the Dolt driver.</p><p>Dolt‚Äôs embedded database driver is defined very simply.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>The  function is the special magic that requires you to use the  import on the database\ndriver of your choice. At program execution time, this code calls  to tell the\n package there‚Äôs a  implementation named ‚Äúdolt‚Äù.</p><p>Next we need a way to get a connection to the embedded database, which we do with the \nmethod. In the sample below, I‚Äôve removed most of the error handling for brevity. You can read the\nfull source <a href=\"https://github.com/dolthub/driver/blob/main/driver.go\">here</a>.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>Dolt‚Äôs DSN format is basically a file URL with some extra query params. It looks something like\nthis:</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>We parse this URL and extract the relevant query params out of it, then use those to create our\ninternal SQL engine representation, which is what Dolt uses to execute queries internally.</p><p>This gives us a  with an engine it can use to execute queries. Let‚Äôs look at that next.</p><p>Unlike the driver itself, the  type has some state. But it‚Äôs still very simple. Its main\njob is to pass information down the line, from a call to  to a  type.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>The  impelementation is where real work begins to happen, in the  method.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>Here you can see that the statement‚Äôs real work is nearly all delegated to the SQL engine we created\nin the initial call to . The rest of its functionality is just to translate between the\nresults that Dolt‚Äôs SQL engine provides and what the  interfaces expect. For that, we\nhave the  type.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>And that‚Äôs it! The job of these interfaces is really to act as a translation layer between the wire\nprotocol the database uses and the types that  expects. In the case of the Dolt\nembedded driver, there‚Äôs no wire protocol: we‚Äôre accessing the SQL engine that queries the database\non disk directly and using the data structures it returns natively.</p><p>A lot of developers prefer to use an ORM tool when interacting with their database, and in the Go\nworld, the most popular ORM library is <a href=\"https://gorm.io/index.html\">Gorm</a>. Gorm usually manages your\nDB connection for you automatically, but in the case of Dolt embedded we want something slightly\ndifferent: we want it to use its MySQL dialect and logic but connect to an embedded Dolt database\nconnection. This is pretty easy to do.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>Note that we need to import both the MySQL driver and the Dolt driver for this to work. But\notherwise, it‚Äôs a standard Gorm setup.</p><p>Go‚Äôs database drivers are a simple way for database application developers to connect to any of the\nmany different SQL databases you can run in production with a common interface. Standardizing these\ninterfaces made it easier for libraries like Gorm to offer support for a larger variety of different\ndatabase vendors, since the details of the wire protocols and other tricky bits are hidden by the\nabstraction for most uses. And it‚Äôs pretty simple to write your own driver if you have a SQL data\nsource you want other people to connect to.</p><p>Want to discuss Go database drivers or learn more about Dolt? Visit us on the <a href=\"https://discord.gg/gqr7K4VNKe\">DoltHub\nDiscord</a>, where our engineering team hangs out all day. Hope to see\nyou there.</p>",
      "contentLength": 5592,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1ql555i/writing_a_go_sql_driver/"
    },
    {
      "title": "kubernetes-sigs/headlamp in 2025: Project Highlights",
      "url": "https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/",
      "date": 1769206050,
      "author": "/u/illumen",
      "guid": 38475,
      "unread": true,
      "content": "<div>By <b>Evangelos Skopelitis (Microsoft)</b> |\n<time datetime=\"2026-01-22\">Thursday, January 22, 2026</time></div><p><em>This announcement is a recap from a post originally <a href=\"https://headlamp.dev/blog/2025/11/13/headlamp-in-2025\">published</a> on the Headlamp blog.</em></p><p><a href=\"https://headlamp.dev/\">Headlamp</a> has come a long way in 2025. The project has continued to grow ‚Äì reaching more teams across platforms, powering new workflows and integrations through plugins, and seeing increased collaboration from the broader community.</p><p>We wanted to take a moment to share a few updates and highlight how Headlamp has evolved over the past year.</p><h3>Joining Kubernetes SIG UI</h3><p>This year marked a big milestone for the project: Headlamp is now officially part of Kubernetes <a href=\"https://github.com/kubernetes/community/blob/master/sig-ui/README.md\">SIG UI</a>. This move brings roadmap and design discussions even closer to the core Kubernetes community and reinforces Headlamp‚Äôs role as a modern, extensible UI for the project.</p><h3>Linux Foundation mentorship</h3><p>This year, we were excited to work with several students through the Linux Foundation‚Äôs Mentorship program, and our mentees have already left a visible mark on Headlamp:</p><ul><li><a href=\"https://github.com/adwait-godbole\"></a> built the KEDA plugin, adding a UI in Headlamp to view and manage KEDA resources like ScaledObjects and ScaledJobs.</li><li><a href=\"https://github.com/DhairyaMajmudar\"></a> set up an OpenTelemetry-based observability stack for Headlamp, wiring up metrics, logs, and traces so the project is easier to monitor and debug.</li><li><a href=\"https://www.linkedin.com/in/aishwarya-ghatole-506745231/\"></a> led a UX audit of Headlamp plugins, identifying usability issues and proposing design improvements and personas for plugin users.</li><li><a href=\"https://github.com/SinghaAnirban005\"></a> developed the Karpenter plugin, giving Headlamp a focused view into Karpenter autoscaling resources and decisions.</li><li><a href=\"https://github.com/useradityaa\"></a> improved Gateway API support, so you can see networking relationships on the resource map, as well as improved support for many of the new Gateway API resources.</li><li><a href=\"https://github.com/upsaurav12\"></a> worked on backend caching for Kubernetes API calls, reducing load on the API server and improving performance in Headlamp.</li></ul><p>Managing multiple clusters is challenging: teams often switch between tools and lose context when trying to see what runs where. Headlamp solves this by giving you a single view to compare clusters side-by-side. This makes it easier to understand workloads across environments and reduces the time spent hunting for resources.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/multi-cluster-view.png\" alt=\"Multi-cluster view\"><em>View of multi-cluster workloads</em><p>Kubernetes apps often span multiple namespaces and resource types, which makes troubleshooting feel like piecing together a puzzle. We‚Äôve added  to give you an application-centric view that groups related resources across multiple namespaces ‚Äì and even clusters. This allows you to reduce sprawl, troubleshoot faster, and collaborate without digging through YAML or cluster-wide lists.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/projects-feature.png\" alt=\"Projects feature\"><em>View of the new Projects feature</em><ul><li>New ‚ÄúProjects‚Äù feature for grouping namespaces into app- or team-centric projects</li><li>Extensible Projects details view that plugins can customize with their own tabs and actions</li></ul><h3>Navigation and Activities</h3><p>Day-to-day ops in Kubernetes often means juggling logs, terminals, YAML, and dashboards across clusters. We redesigned Headlamp‚Äôs navigation to treat these as first-class ‚Äúactivities‚Äù you can keep open and come back to, instead of one-off views you lose as soon as you click away.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/new-task-bar.png\" alt=\"New task bar\"><ul><li>A new task bar/activities model lets you pin logs, exec sessions, and details as ongoing activities</li><li>An activity overview with a ‚ÄúClose all‚Äù action and cluster information</li><li>Multi-select and global filters in tables</li></ul><p>When something breaks in production, the first two questions are usually ‚Äúwhere is it?‚Äù and ‚Äúwhat is it connected to?‚Äù We‚Äôve upgraded both search and the map view so you can get from a high-level symptom to the right set of objects much faster.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/advanced-search.png\" alt=\"Advanced search\"><em>View of the new Advanced Search feature</em><ul><li>An Advanced search view that supports rich, expression-based queries over Kubernetes objects</li><li>Improved global search that understands labels and multiple search items, and can even update your current namespace based on what you find</li><li>EndpointSlice support in the Network section</li><li>A richer map view that now includes Custom Resources and Gateway API objects</li></ul><p>We‚Äôve put real work into making OIDC setup clearer and more resilient, especially for in-cluster deployments.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/user-info.png\" alt=\"User info\"><em>View of user information for OIDC clusters</em><ul><li>User information displayed in the top bar for OIDC-authenticated users</li><li>PKCE support for more secure authentication flows, as well as hardened token refresh handling</li><li>Documentation for using the access token using <code>-oidc-use-access-token=true</code></li><li>Improved support for public OIDC clients like AKS and EKS</li></ul><p>We‚Äôve broadened how you deploy and source apps via Headlamp, specifically supporting vanilla Helm repos.</p><ul><li>A more capable Helm chart with optional backend TLS termination, PodDisruptionBudgets, custom pod labels, and more</li><li>Improved formatting and added missing access token arg in the Helm chart</li><li>New in-cluster Helm support with an  flag and a service proxy</li></ul><p>Finally, we‚Äôve spent a lot of time on the things you notice every day but don‚Äôt always make headlines: startup time, list views, log viewers, accessibility, and small network UX details. A continuous accessibility self-audit has also helped us identify key issues and make Headlamp easier for everyone to use.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/learn-section.png\" alt=\"Learn section\"><em>View of the Learn section in docs</em><ul><li>Significant desktop improvements, with up to 60% faster app loads and much quicker dev-mode reloads for contributors</li><li>Numerous table and log viewer refinements: persistent sort order, consistent row actions, copy-name buttons, better tooltips, and more forgiving log inputs</li><li>Accessibility and localization improvements, including fixes for zoom-related layout issues, better color contrast, improved screen reader support, and expanded language coverage</li><li>More control over resources, with live pod CPU/memory metrics, richer pod details, and inline editing for secrets and CRD fields</li><li>A refreshed documentation and plugin onboarding experience, including a ‚ÄúLearn‚Äù section and plugin showcase</li><li>A more complete NetworkPolicy UI and network-related polish</li><li>Nightly builds available for early testing</li></ul><h2>Plugins and extensibility</h2><p>Discovering plugins is simpler now ‚Äì no more hopping between Artifact Hub and assorted GitHub repos. Browse our dedicated <a href=\"https://headlamp.dev/plugins\">Plugins page</a> for a curated catalog of Headlamp-endorsed plugins, along with a showcase of featured plugins.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/plugins-page.png\" alt=\"Plugins page\"><em>View of the Plugins showcase</em><p>Managing Kubernetes often means memorizing commands and juggling tools. Headlamp‚Äôs new AI Assistant changes this by adding a natural-language interface built into the UI. Now, instead of typing  or digging through YAML you can ask, ‚ÄúIs my app healthy?‚Äù or ‚ÄúShow logs for this deployment,‚Äù and get answers in context, speeding up troubleshooting and smoothing onboarding for new users. Learn more about it <a href=\"https://headlamp.dev/blog/2025/08/07/introducing-the-headlamp-ai-assistant/\">here</a>.</p><p>Alongside the new AI Assistant, we‚Äôve been growing Headlamp‚Äôs plugin ecosystem so you can bring more of your workflows into a single UI, with integrations like Minikube, Karpenter, and more.</p><p>Highlights from the latest plugin releases:</p><ul><li>Minikube plugin, providing a locally stored single node Minikube cluster</li><li>Karpenter plugin, with support for Azure Node Auto-Provisioning (NAP)</li><li>KEDA plugin, which you can learn more about <a href=\"https://headlamp.dev/blog/2025/07/25/enabling-event-driven-autoscaling-with-the-new-keda-plugin-for-headlamp/\">here</a></li></ul><p>Alongside new additions, we‚Äôve also spent time refining plugins that many of you already use, focusing on smoother workflows and better integration with the core UI.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/backstage-plugin.png\" alt=\"Backstage plugin\"><em>View of the Backstage plugin</em><ul><li>: Updated for Flux v2.7, with support for newer CRDs, navigation fixes so it works smoothly on recent clusters</li><li>: Now supports Helm repos in addition to Artifact Hub, can run in-cluster via /serviceproxy, and shows both current and latest app versions</li><li>: Improved card layout and accessibility, plus dependency and Storybook test updates</li><li>: Dependency and build updates, more info <a href=\"https://headlamp.dev/blog/2025/11/05/strengthening-backstage-and-headlamp-integration/\">here</a></li></ul><p>We‚Äôve focused on making it faster and clearer to build, test, and ship Headlamp plugins, backed by improved documentation and lighter tooling.</p><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/plugin-development.png\" alt=\"Plugin development\"><em>View of the Plugin Development guide</em><ul><li>Improved type checking for Headlamp APIs, restored Storybook support for component testing, and reduced dependencies for faster installs and fewer updates</li><li>Documented plugin install locations, UI signifiers in Plugin Settings, and labels that differentiated shipped, UI-installed, and dev-mode plugins</li></ul><p>We've also been investing in keeping Headlamp secure ‚Äì both by tightening how authentication works and by staying on top of upstream vulnerabilities and tooling.</p><ul><li>We've been keeping up with security updates, regularly updating dependencies and addressing upstream security issues.</li><li>We tightened the Helm chart's default security context and fixed a regression that broke the plugin manager.</li><li>We've improved OIDC security with PKCE support, helping unblock more secure and standards-compliant OIDC setups when deploying Headlamp in-cluster.</li></ul><p>Thank you to everyone who has contributed to Headlamp this year ‚Äì whether through pull requests, plugins, or simply sharing how you're using the project. Seeing the different ways teams are adopting and extending the project is a big part of what keeps us moving forward. If your organization uses Headlamp, consider adding it to our <a href=\"https://github.com/kubernetes-sigs/headlamp/blob/main/ADOPTERS.md\">adopters list</a>.</p><p>If you haven't tried Headlamp recently, all these updates are available today. Check out the latest Headlamp release, explore the new views, plugins, and docs, and share your feedback with us on Slack or GitHub ‚Äì your feedback helps shape where Headlamp goes next.</p>",
      "contentLength": 9169,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/kubernetes/comments/1ql4jta/kubernetessigsheadlamp_in_2025_project_highlights/"
    },
    {
      "title": "Zotero 8 released (reference management)",
      "url": "https://www.zotero.org/blog/zotero-8/",
      "date": 1769200318,
      "author": "/u/dbcoopernz",
      "guid": 38445,
      "unread": true,
      "content": "<p>We‚Äôre excited to announce our latest major release, Zotero 8. Zotero 8 builds on the new design and features of <a href=\"https://www.zotero.org/blog/zotero-7/\">Zotero 7</a> and includes a huge number of improvements and refinements.</p><h2>Redesigned Citation Dialog</h2><p>Zotero 8 introduces a new unified citation dialog, replacing the previous citation dialog (the ‚Äúred bar‚Äù), the ‚Äúclassic‚Äù citation dialog, and the Add Note dialog (the ‚Äúyellow bar‚Äù).</p><p>The new dialog has two modes: List mode and Library mode. List mode lets you quickly search for citations from across your Zotero libraries by title, creator, and year. Library mode includes a library browser, letting you find items in specific libraries or collections. You can switch between the two modes with a single click, preserving any added items or entered search terms. By default, it will open in the last mode you used, but you can choose a different default mode in the settings.</p><p>In <a href=\"https://www.zotero.org/blog/zotero-7/\">Zotero 7</a>, we added the ability to quickly add citations for selected items and open documents. In the new dialog, these options are available in both List mode and Library mode, so you can make these quick selections even if you otherwise prefer to add items via the library browser.</p><p>As before, once you‚Äôve selected an item, you can click on its bubble to customize the citation with a page number, prefix, etc. It‚Äôs also now possible to add any locator ‚Äî not just a page number ‚Äî right from the search bar by typing the full or short name (e.g., ‚Äúline 10‚Äù or ‚Äúl. 10‚Äù after the citation and pressing Enter/Return.</p><p>You can switch between adding citations and adding notes using buttons in the bottom left, corresponding to the Add/Edit Citation and Add Note buttons in your word processor.</p><p>(For those coming from the classic dialog, note that there‚Äôs no text field to make manual edits to citations. It‚Äôs been possible to edit citations directly in the document for many years, which is why the red bar didn‚Äôt include such a text field either. More importantly, though, such manual edits should be avoided in almost all cases. Instead, <a href=\"https://www.zotero.org/support/word_processor_plugin_usage#customizing_cites\">customize the citation</a> via the citation dialog, which will allow Zotero to continue to update the citation as necessary.)</p><h2>Annotations in the Items List</h2><p>Annotations you make on PDFs, EPUBs, and webpage snapshots now show up under their parent attachments in the items list.</p><p>Showing annotations in the items list makes it easier to view annotations across a library or collection, and it also makes it possible to search for annotations directly. For example, you can search for all annotations in a collection with a given tag and then create a note from those annotations or copy them to an external text editor with Quick Copy.</p><p>In Advanced Search, you can use ‚ÄúItem Type‚Äù ‚Äúis‚Äù ‚ÄúAnnotation‚Äù to match annotations or use the Annotation Text and Annotation Comment search conditions to search for specific parts of the annotation.</p><p>You can assign tags to selected annotations by dragging them to the tag selector, just like other items.</p><p>Selected annotations show up in the item pane, grouped by top-level item.</p><h2>Reader Appearance Panel with Theme Support</h2><p>We‚Äôve added a new Appearance panel in the reader that provides quick access to view settings and introduces support for reader themes.</p><p>The view settings are per-document settings. Themes are applied globally for all documents, including in the attachment preview in the item pane, and apply to PDFs, EPUBs, and webpage snapshots.</p><p>We offer a number of built-in themes (‚ÄúDark‚Äù, ‚ÄúSnow‚Äù, ‚ÄúSepia‚Äù), and you can create custom themes just by specifying a foreground and background color. (Some other theme engines require additional accent colors, but we‚Äôve tried to make this as simple as possible for users by automatically adjusting other colors based on the foreground and background colors.) You can set a different theme that applies to light mode and dark mode.</p><p>The themes replace the previous on-by-default ‚ÄúUse Dark Mode for Content‚Äù option, which inverted images in dark mode. We‚Äôre now simply darkening images a bit when using a dark theme. Images and ink annotations in the reader sidebar and note editor are now only darkened as well (and only when Zotero itself is in dark mode).</p><p>When possible, we also try to apply themes to PDF pages containing full-page images, such as scanned papers, by replacing whitish/dark colors with theme colors. (Otherwise we simply darken the page slightly.)</p><p>It‚Äôs now possible to open notes in tabs in addition to separate windows. Note tabs fill the whole window, with wide margins for better readability and a clean, distraction-free space for note-taking.</p><p>By default, double-clicking a note in the items list will open it in a tab. You can choose to open the note in the other space from the context menu, and you can change the default behavior using the ‚ÄúOpen notes in new windows instead of tabs‚Äù setting in the General pane of the settings.</p><p>Notes in tabs have a separate font size setting in the View menu.</p><h2>Reading Mode for Webpage Snapshots</h2><p>Reading Mode reformats webpage snapshots for easier reading, with unnecessary page elements removed. You can adjust line height and other view options from the Appearance panel.</p><p>We‚Äôve reworked the tabs menu to make it faster to interact with via the keyboard.</p><p>You can now press Ctrl/Cmd-; to bring up the menu at any time.</p><p>Once the menu is open, it simultaneously accepts search input, up/down navigation, and row selection, without the need to move between different parts of the menu. You can simply start typing the name of an open tab and then press Enter/Return to switch to it once you‚Äôve narrowed down the list.</p><p>It‚Äôs also possible to quickly close multiple tabs by moving between the row close buttons with up/down and pressing space bar to close a tab.</p><p>Zotero now automatically keeps attachment filenames in sync with parent item metadata as you make changes (e.g., changing the title). In previous versions, while Zotero would automatically rename files when you first added them to your library, if you later edited the item‚Äôs metadata, you would need to right-click on the attachment and select ‚ÄúRename File from Parent Metadata‚Äù.</p><p>You can configure which file types renaming applies to from the General tab of the Zotero settings.</p><p>After upgrading to this version, existing eligible files that don‚Äôt match the current filename format won‚Äôt be automatically renamed, but you can choose to rename them en masse from the Zotero settings. Zotero will also prompt you to rename all files if you change the filename format.</p><p>‚ÄúRename File from Parent Metadata‚Äù has been removed from the item context menu. If a filename doesn‚Äôt match the configured filename format (e.g., because automatic renaming is disabled or you changed the format but didn‚Äôt choose to rename all files), you can click the ‚ÄúRename File to Match Parent Item‚Äù button next to the filename in the attachment‚Äôs item pane to rename it.</p><h2>New Attachment Title Options</h2><p>Zotero 7 introduced more consistent handling of <a href=\"https://www.zotero.org/support/kb/attachment_title_vs_filename\">attachment titles</a>, preserving simpler, less-redundant titles (e.g., ‚ÄúFull Text PDF‚Äù or ‚ÄúPreprint PDF‚Äù) in cases where the title was previously changed to match the filename. Zotero 8 further refines its renaming and titling logic when adding multiple and/or non-primary attachments, to bring the functionality better in line with the intended behavior.</p><p>We‚Äôve also added a ‚ÄúNormalize Attachment Titles‚Äù option under Tools ‚Üí Manage Attachments to update old primary attachments with titles matching the filename to use simpler titles such as ‚ÄúPDF‚Äù.</p><p>While we recommend the default behavior, allowing Zotero to rename primary files and keep them renamed while using simpler titles in the items list, if you really prefer to view filenames instead of titles, you can now enable ‚ÄúShow attachment filenames in the items list‚Äù option in the General pane of the settings.</p><p>Zotero 8 adds a version for Linux running on ARM64 devices. This includes ARM-based Chromebooks, Apple Silicon Macs running Linux (Linux VMs, Asahi Linux), and Raspberry Pis.</p><p>If you‚Äôve been unable to run Zotero on your ARM-based device, or you‚Äôve been running the x86_64 version under emulation, give it a try.</p><h2>User Interface Improvements</h2><p>We‚Äôve made a number of changes across the interface to address common requests:</p><ul><li>A new button in the library tab allows you to quickly close the item pane without dragging its edge or using the menus.</li><li>You can reorder item pane sections by dragging their icons in the side navigation bar.</li><li>You can drag items, collections, and searches into the trash.</li><li>You can drag attachments, notes, and related items from the item pane (e.g., to copy files to the filesystem or use Quick Copy).</li><li>Collections automatically expand when you drag over them, making it easier to drop collections or items into subcollections.</li><li>You can delete attachments from the item pane.</li><li>Tabs maintain their size as you close them for faster closing of multiple tabs.</li></ul><p>With Zotero 8, the Zotero Connector save popup can autocomplete tags in your Zotero library and allows you to add a note to items as you save them.</p><p>Zotero 8 includes much more than we can list here. See the <a href=\"https://www.zotero.org/support/8.0_changelog\">changelog</a> for additional details.</p><p>If you‚Äôre already running Zotero, you can upgrade from within Zotero by going to Help ‚Üí ‚ÄúCheck for Updates‚Ä¶‚Äù.</p><p><small>\n\t\t\t\t\t\t\t\tThis entry was posted\n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\ton Thursday, January 22nd, 2026 at 12:52 pm by Dan Stillman\t\t\t\t\t\t\t\tand is filed under <a href=\"https://www.zotero.org/blog/category/features/\" rel=\"category tag\">Features</a>, <a href=\"https://www.zotero.org/blog/category/news/\" rel=\"category tag\">News</a>.\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</small></p>",
      "contentLength": 9491,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1ql232k/zotero_8_released_reference_management/"
    },
    {
      "title": "Flabbergasted by VM performance (on my Intel Xe 13th gen integrated graphics, so different from i915 in some ways)",
      "url": "https://www.reddit.com/r/linux/comments/1ql1liu/flabbergasted_by_vm_performance_on_my_intel_xe/",
      "date": 1769199212,
      "author": "/u/Natural-Bowl5439",
      "guid": 38444,
      "unread": true,
      "content": "<p>After breaking the kernel trying to share the GPU with the help of a non-mature SR-IOV implementation, all this in order to have maximum GPU performance between host and guest, I decided after the defeat to go with the traditional GPU acceleration instead.</p><p>I feared the old days of trying virtualbox and seeing that the \"acceleration\" was just good for windows aero, hence the reason i explored SR-IOV. I expected VMware's performance to not be far from my memories with virtualbox, but to my surprise i could allocate 8GB of graphics memory to the VM! Then i tested resident evil 6 and it ran at playable framerate! (around 40fps although at low settings but 1080p resolution) </p><p>I hope i will still be pleasantly surprised when I'll try the real use of the windows VM : video editing with Capcut and video rotoscoping with Photoshop. </p>",
      "contentLength": 832,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Overrun with AI slop, cURL scraps bug bounties to ensure \"intact mental health\"",
      "url": "https://arstechnica.com/security/2026/01/overrun-with-ai-slop-curl-scraps-bug-bounties-to-ensure-intact-mental-health/",
      "date": 1769197621,
      "author": "/u/Drumedor",
      "guid": 38394,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1ql0w5p/overrun_with_ai_slop_curl_scraps_bug_bounties_to/"
    },
    {
      "title": "Anyone here built microservices in Go with GraphQL, gRPC, and RabbitMQ?",
      "url": "https://www.reddit.com/r/golang/comments/1ql0g9l/anyone_here_built_microservices_in_go_with/",
      "date": 1769196646,
      "author": "/u/riswan_22022",
      "guid": 38446,
      "unread": true,
      "content": "<p>I‚Äôm working with Go and exploring a microservices architecture using , , , and a database (MongoDB / PostgreSQL ).</p><p>I wanted to ask if anyone here has built something similar in real projects. If you have a , example project, or even a blog explaining your approach, I‚Äôd really appreciate it if you could share.</p>",
      "contentLength": 313,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "mmdr: A native Rust Mermaid renderer (500-1000x faster than mermaid-cli)",
      "url": "https://github.com/1jehuang/mermaid-rs-renderer",
      "date": 1769194855,
      "author": "/u/Medium_Anxiety_8143",
      "guid": 38443,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/rust/comments/1qkzmpg/mmdr_a_native_rust_mermaid_renderer_5001000x/"
    },
    {
      "title": "[D] Is Grokking unique to transformers/attention?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qkz5do/d_is_grokking_unique_to_transformersattention/",
      "date": 1769193818,
      "author": "/u/Dependent-Shake3906",
      "guid": 38454,
      "unread": true,
      "content": "<p>Is Grokking unique to attention mechanism, every time I‚Äôve read up on it seems to suggest that‚Äôs it a product of attention and models that utilise it. Is this the case or can standard MLP also start grokking?</p>",
      "contentLength": 212,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Breaking Key-Value Size Limits: Linked List WALs for Atomic Large Writes",
      "url": "https://unisondb.io/blog/breaking-kv-size-limits-linked-list-wal/",
      "date": 1769193520,
      "author": "/u/ankur-anand",
      "guid": 38398,
      "unread": true,
      "content": "<img src=\"https://unisondb.io/images/wal_linked_list.svg\" alt=\"Diagram of UnisonDB's corruption-proof WAL path\"><h2>The ‚ÄúHard Wall‚Äù of Distributed Systems<a href=\"https://unisondb.io/blog/breaking-kv-size-limits-linked-list-wal/#the-hard-wall-of-distributed-systems\">#</a></h2><p>The majority of distributed Key-Value systems have some kind of limit. This limit exists for a purpose: it prevents a single request from overwhelming memory or stalling replication.</p><p>Whether it‚Äôs the 512KB cap in <a href=\"https://developer.hashicorp.com/consul/docs/automate/kv\" target=\"_blank\" rel=\"noopener noreferrer\">Consul</a>\nor the 1.5MB default in <a href=\"https://etcd.io/docs/v3.6/dev-guide/limit/\" target=\"_blank\" rel=\"noopener noreferrer\">etcd</a>\n, these boundaries are a survival mechanism. In a distributed cluster, every byte you write has to be replicated via protocols like Raft. If a single record is too large, it creates ‚Äúhead-of-line blocking‚Äù‚Äîthe entire replication pipeline slows down just to move one massive object, potentially causing heartbeats to fail and nodes to drop out of the cluster.</p><p>At UnisonDB, we respect these same limits to protect our own system health. We need to be even more cautious about this, as we are not just doing Raft replication for writes. We also have high-fanout ISR (in-sync-replica) based edge replicas, meaning a single write can propagate to many more nodes.</p><h2>Why ISR Edge Replication Changes the Stakes<a href=\"https://unisondb.io/blog/breaking-kv-size-limits-linked-list-wal/#why-isr-edge-replication-changes-the-stakes\">#</a></h2><p>In our environment, the ‚ÄúHard Wall‚Äù isn‚Äôt just about protecting memory or Raft heartbeats within a small core cluster. It is about protecting the replication integrity and lag across a vast edge network.</p><ul><li><p><strong>Heartbeat Fragility in Edge Environments</strong>: Edge networks often have variable latency and less reliable connections. If replication takes too long because of oversized records, the system might falsely flag an edge node as ‚Äúout of sync,‚Äù triggering expensive and unnecessary full re-syncs, wasting bandwidth and compute.</p></li><li><p><strong>Memory Pressure on Constrained Edge Nodes</strong>: Unlike robust core cluster nodes, edge replicas frequently run on more resource-constrained hardware. Pushing a 20MB block in a single request could easily cause an Out Of Memory (OOM) event on a smaller edge instance, leading to outages at the edge.</p></li></ul><p>But even with these constraints, we also understand that the need for large Key-Value storage hasn‚Äôt gone away‚Äîit has actually intensified. As an Edge-replicated, general-purpose Multi-Modal database, we see this constantly. Whether it‚Äôs a massive JSON configuration or high-dimensional vectors for AI use cases, modern data frequently pushes past those old boundaries.</p><p>This size pressure usually shows up in two ways:</p><ol><li>: A single transaction involving multiple Key-Value pairs that, when grouped together, exceed the 1MB limit.</li><li>: A single row containing hundreds of columns where the aggregate size of the update blows past the ceiling.</li></ol><p>In both cases, the user will expects the same ironclad KV guarantees they get with a tiny 1KB write. You shouldn‚Äôt have to sacrifice Atomicity just because your data model is complex.</p><h2>Why Manual Chunking Fails<a href=\"https://unisondb.io/blog/breaking-kv-size-limits-linked-list-wal/#why-manual-chunking-fails\">#</a></h2><p>When engineers hit a size limit, the instinctive reaction is to ‚Äúchunk‚Äù the data by splitting a 10MB write into ten separate 1MB requests. This is where things get dangerous. Without a specialized architecture, you lose the atomicity promise. If your connection drops after chunk seven, the database is left in a ‚Äúzombie‚Äù state. You have a partial update that is neither the old version nor the new one. In a real database, the rule is absolute: it must be all or nothing.</p><h2>Unisondb Solution: A WAL That Remembers Its Past<a href=\"https://unisondb.io/blog/breaking-kv-size-limits-linked-list-wal/#unisondb-solution-a-wal-that-remembers-its-past\">#</a></h2><p>To solve this at UnisonDB, <strong>we stopped looking at the Write-Ahead Log (WAL) just as a flat, sequential file</strong>. Instead, we treated it as a backward-linked list.</p><p>By adding a simple breadcrumb‚Äîa PrevTxnWalIndex‚Äîto every WAL record that are part of the same transaction, each chunk of data points back to the one that came before it. This allows us to stitch a single, massive transaction together across multiple physical writes without ever sending a request that exceeds the safety limit.</p><p>This logic is the backbone of how we handle large multi-modal data. The lifecycle of a transaction in our dbkernel looks like this:</p><ol><li><p>: We write an anchor record to the WAL. This initializes the transaction and generates a unique ID.</p></li><li><p>: As you stream your data chunks, each one links back to the previous disk offset. Even if you send 50 chunks to stay under the limit, the database knows they belong to one chain.</p></li><li><p>: This is the atomic switch. The final record acts as the seal.</p></li></ol><blockquote><p>Nothing becomes visible to you the user until that COMMIT record is successfully flushed to disk. If the stream breaks halfway through, the database simply ignores those dangling fragments during the next recovery scan.</p></blockquote><p>The engine needs to know exactly where the previous piece of the puzzle lives on disk. We use physical disk offsets to create this chain.</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>Here is how this looks inside the UnisonDB transaction engine. Notice how we track the prevOffset to build the link on the fly. In the AppendKVTxn function, we take the current prevOffset and bake it into the new log record before appending it to the WAL.</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>When  happens, the Commit function writes the final link. Only after the WAL confirms the commit do we flush the data to the in-memory MemTable.</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>When UnisonDB reboots after a crash, the recovery process starts from the last known checkpoint. As the engine scans the log forward from that point, it looks specifically for COMMIT records. Because our transaction chunks are chained together using physical disk offsets, the engine has a clear map to follow.</p><p>When a commit record is encountered, the engine uses the PrevTxnWalIndex to walk the chain of that specific transaction. It jumps from the commit record to the previous data chunk, and then to the one before that, continuing until it reaches the BEGIN record. This allows the engine to gather all the related pieces of a large Key-Value pair, Wide-Column row, or LOB without having to inspect unrelated transaction data that might be sitting in between those chunks.</p><h3>Reconstructing the Value<a href=\"https://unisondb.io/blog/breaking-kv-size-limits-linked-list-wal/#reconstructing-the-value\">#</a></h3><p>The GetTransactionRecords function is what performs this backward walk. It takes the offset of the commit record and follows the trail until the chain is complete.</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>By chaining the records this way, we ensure that a large value is only rebuilt if the final commit record exists. If the engine finds chunks that don‚Äôt lead to a commit, it simply ignores them. This keeps the data consistent and ensures that the all or nothing promise is maintained for every data model we support.</p><p>Building a database is often a game of trade-offs. You want system stability, but you also want to support modern, heavy workloads like AI and complex multi-modal schemas.</p><p>By treating the Write-Ahead Log as a linked list, we found a way to have both. We keep our network requests small and safe, but we allow our data to be as large as it needs to be.</p><p>Whether it is a simple Key-Value pair, a massive Wide-Column row, or a Large Object, the atomicity promise remains unbroken.</p><p>If you found this article helpful, or if you‚Äôre interested in the future of edge-replicated data, we‚Äôd love your support.</p>",
      "contentLength": 6798,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qkz0iz/breaking_keyvalue_size_limits_linked_list_wals/"
    },
    {
      "title": "wayscriber 0.9.9 released!",
      "url": "https://www.reddit.com/r/linux/comments/1qkyo99/wayscriber_099_released/",
      "date": 1769192765,
      "author": "/u/Leading_Yam1358",
      "guid": 38397,
      "unread": true,
      "content": "<p>Wayscriber is a live annotation tool for Linux(Wayland) - a draw-on-anything overlay for demos, teaching, or quick callouts. Or just draw over any app or screen for funs :)</p><p>You get pens/highlighters/shapes/Text plus zoom, freeze, click highlights, and fast screenshots. </p><p>It is lightweight, written in Rust, and highly customizable.</p><p>Has multiple boards and pages per boards. Can customise it all.</p><p>Set up as daemon/tray so you can show or hide it any time.</p><p>It runs as a lightweight overlay and has an optional GUI Configurator. You can also customise all via TOML file. </p><p>Give it a try. Star and spread the word if you like it. </p><p>I am looking forward to any feedback. </p><p>The goal atm is to make it as powerful as possible while keeping it simple by default, and not overwhelming for new users.</p><p># Wayscriber 0.9.9 (since v0.9.8) - this is the biggest update so far!</p><p>- Multi‚Äëboard support with improved board/page picker, status bar toggles, and safe delete confirmations.</p><p>- New tools: eraser tool + variable‚Äëthickness stylus lines.</p><p>- New workflows: command palette, guided tour onboarding, configurable presenter mode.</p><p>- Major rendering/perf upgrades via damage tracking (dirty‚Äërect) and caching.</p><p>- Boards toolbar section, board/page toggles in status bar, board picker improvements.</p><p>- Confirmations for board/page deletion + timeouts; board picker redraw on close.</p><p>- Quick help overlay + keybinding; help overlay layout refinements.</p><p>- Command palette with Unicode‚Äësafe search.</p><p>- Guided tour onboarding, welcome toast, and recovery hardening.</p><p>- Presenter mode: new toggle/bind, constraints, tool switching allowed.</p><p>- Optional numbered arrow labels + reset action and toolbar toggle.</p><p>- Text controls enabled by default.</p><p>- Toolbars: pinned toolbars shown by default, improved drawers, stable drag via pointer lock.</p><p>- Tooltips: better placement, selection shortcut, color swatch tooltips w/ bindings.</p><p>- UI polish: View tab renamed to Canvas, zoom actions toggle, attention dot + More hint.</p><p>- Defaults: Ubuntu/GNOME PageUp/PageDown page navigation bindings.</p><p>- Damage tracking/dirty‚Äërect rendering for faster redraws.</p><p>- Cached help overlay layout/text and badge extents.</p><p>- Optimized eraser hover indices, selection cloning, spatial hit tests.</p><p>- Preallocated dirty regions + pooled damage tracking improvements.</p><p>- No‚Äëvsync frame rate cap.</p><p>- Autosave scheduling + tracking; fixes for autosave clearing.</p><p>- Better tablet pressure handling.</p><p>- Clipboard fallback exit/retry fix.</p><p>- Screenshot suppression timing fix.</p><p>- Tooltip placement + board picker spacing fixes.</p><p>- Pango text rendering for UI labels.</p><p>- Nix flake packaging + install docs.</p><p>- Config/docs updates and refactors for action metadata + toolbar constants.</p><p>Thanks @n3oney for the first contribution!</p>",
      "contentLength": 2716,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Malicious PyPI Packages spellcheckpy and spellcheckerpy Deliver Python RAT",
      "url": "https://www.aikido.dev/blog/malicious-pypi-packages-spellcheckpy-and-spellcheckerpy-deliver-python-rat",
      "date": 1769188643,
      "author": "/u/Advocatemack",
      "guid": 38403,
      "unread": true,
      "content": "<p>On January 20th and 21st, 2026, our malware detection pipeline flagged two new PyPI packages:  and . Both claimed to be the legitimate author of pyspellchecker library. Both are linked to his real GitHub repo.</p><p>Hidden inside the Basque language dictionary file was a base64-encoded payload that downloads a full-featured Python RAT. The attacker published three \"dormant\" versions first, payload present, trigger absent, then flipped the switch with  v1.2.0, adding an obfuscated execution trigger that fires the moment you import .</p><h2><strong>The payload hiding in plain sight</strong></h2><p>The malware authors got creative. Instead of the usual suspects ( scripts, obfuscated , they buried the payload inside , a file that legitimately contains Basque word frequencies in the real  package.</p><p>Here's the extraction function in :</p><pre contenteditable=\"false\"><code>        data = json.loads(f.read())\n</code></pre><p>Looks innocent. But when called with <code>test_file(\"eu\", \"utf-8\", \"spellchecker\")</code>, it doesn't retrieve word frequencies. It retrieves a base64-encoded downloader hidden among the dictionary entries under a key called spellchecker.</p><p>In the first three versions, the payload gets extracted and decoded... but never executed:</p><pre contenteditable=\"false\"><code></code></pre><p>A loaded gun with the safety on.</p><p>Then came  v1.2.0. The attacker moved the trigger to  and added obfuscation:</p><pre contenteditable=\"false\"><code>    self._evaluate = True</code></pre><p>Do you see it? That <code>bytes.fromhex(\"65786563\")</code> decodes to \"\".</p><p>Instead of writing  directly, which static scanners would flag,they reconstruct the string from hex at runtime. Import , instantiate it, and the RAT executes.</p><h2><strong>The RAT: Full remote control</strong></h2><p>The stage-1 payload is a downloader. It fetches the real payload from <code>https://updatenet[.]work/settings/history.php</code> and spawns it in a detached process:</p><pre contenteditable=\"false\"><code>    stdin=subprocess.PIPE, \n    stdout=subprocess.DEVNULL, \n    stderr=subprocess.DEVNULL,\n    start_new_session=True\n)\np.stdin.write(downloaded_payload)\np.stdin.close()\n</code></pre><p>That is key: The RAT survives even if your script exits. No files written to disk. Silent. Detached.</p><p>The stage-2 RAT is a full-featured remote access trojan with some interesting characteristics:</p><p><strong>System fingerprinting on init:</strong></p><pre contenteditable=\"false\"><code></code></pre><p><strong>Dual-layer XOR encryption for C2 comms:</strong> The RAT uses a 16-byte XOR key (<code>[3, 6, 2, 1, 6, 0, 4, 7, 0, 1, 9, 6, 8, 1, 2, 5]</code>) for the outer layer, then a secondary XOR with key 123 for command payloads. Not cryptographically strong, but enough to evade signature-based detection.</p><p> Commands come back as <code>[4-byte command ID][4-byte length][XOR-encrypted payload]</code>. The RAT parses this, decrypts, and dispatches.</p><p><strong>Arbitrary code execution:</strong> When command ID 1001 arrives, the RAT just... runs it:</p><pre contenteditable=\"false\"><code>    exec(szCode)</code></pre><p> The RAT phones home every 5 seconds to <code>https://updatenet[.]work/update1.php</code>, sending its victim ID (campaign </p><p>) and waiting for commands. SSL certificate validation is disabled via </p><p><code>ssl._create_unverified_context()</code>.</p><p>The C2 domain  resolves to infrastructure with a documented history of hosting malicious activity.</p><ul role=\"list\"><li>Registered: 28 October 2025 (approximately 3 months before malware publication)</li></ul><ul role=\"list\"><li>IP Address: </li><li>ASN: AS14956 RouterHosting LLC</li><li>Location: Dallas, Texas, USA</li><li>Associated Domain: cloudzy.com</li></ul><p> RouterHosting LLC operates as Cloudzy, a hosting provider that has been extensively documented as a \"Command-and-Control Provider\" (C2P). In August 2023, <a href=\"https://www.halcyon.ai/blog/update-cloudzy-command-and-control-provider-report\">Halcyon published a report titled \"Cloudzy with a Chance of Ransomware\"</a> that found 40-60% of Cloudzy's traffic was malicious in nature. The report linked Cloudzy infrastructure to APT groups from China, Iran, North Korea, Russia, and other nations, as well as ransomware operators and a sanctioned Israeli spyware vendor.</p><h2><strong>Connection to previous campaigns</strong></h2><p>This isn't an isolated incident. In November 2025, <a href=\"https://helixguard.ai/blog/malicious-spellcheckers-2025-11-19/\">HelixGuard documented a similar attack</a> using the spellcheckers package (same target, different name). That campaign used the same RAT structure: XOR encryption, command ID 1001, exec(),&nbsp; but different C2 infrastructure (). The HelixGuard report linked that campaign to fake recruiter social engineering targeting cryptocurrency holders.</p><p>Different domains, same playbook. This appears to be the exact same threat actor at play.&nbsp;</p><p> spellcheckerpy (all versions), spellcheckpy (all versions)</p><ul role=\"list\"><li><code>https://updatenet[.]work/settings/history.php</code> (stage-2 delivery)</li><li><code>https://updatenet[.]work/update1.php</code> (beacon endpoint)</li><li> (AS14956 RouterHosting LLC / Cloudzy)</li></ul><ul role=\"list\"><li>XOR Key: <code>03 06 02 01 06 00 04 07 00 01 09 06 08 01 02 05</code></li></ul><p>, key spellchecker</p>",
      "contentLength": 4341,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qkwrks/malicious_pypi_packages_spellcheckpy_and/"
    },
    {
      "title": "Got tired of distributing large files, so I built this open-source P2P transfer CLI tool in Go",
      "url": "https://www.reddit.com/r/golang/comments/1qkwqxp/got_tired_of_distributing_large_files_so_i_built/",
      "date": 1769188603,
      "author": "/u/samsungplay",
      "guid": 38359,
      "unread": true,
      "content": "<p>I recently needed to move a bunch of large files between machines and I realized how more difficult things are than it should be. I'm aware there might be some tools out there that might achieve similar things, but I still wanted to take on the challenge myself.</p><p>As a result, I built a small tool to handle the cases I kept tripping over and decided to share it with the community.</p><p>The goal is very simple. <strong>Just get the files from one machine to other machines and be done with it. And with no other setup other than the installation itself.</strong></p><p>It‚Äôs called . It‚Äôs written in Go and uses direct peer-to-peer transfers over QUIC. Files go straight between machines, and each receiver connects independently - which also makes it possible to share the same data with more than one machine without restarting the transfer.</p><ul><li>High throughput, direct P2P transfers over QUIC</li><li>Tries to avoid relays via UDP hole-punching (STUN)</li><li>Resume support if a connection drops</li><li>One sender can serve multiple receivers</li><li>Two commands:  / </li><li> with default signaling, but everything can be self-hosted</li></ul><ul><li>Still beta and actively evolving</li><li>No default TURN relay yet (can be self-hosted)</li><li>If there‚Äôs real usage, I‚Äôll likely add a relay and maybe a GUI later</li></ul><p>The default signaling servers are capacity-limited but currently handle ~2k concurrent users.</p><p>Since it is very easy to install and use, I hope some of you guys try it, and better yet, benefit from it in some way or the other. If you find any bugs, have any feedbacks, or if something behaves wildly, I'd really appreciate hearing about it.</p><pre><code>brew tap samsungplay/thruflux brew install thru </code></pre><pre><code>thru host ./files thru join ABCDEFGH --out ./downloads </code></pre>",
      "contentLength": 1654,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Scaling PostgreSQL to power 800 million ChatGPT users - OpenAI Engineering Blog",
      "url": "https://openai.com/index/scaling-postgresql/",
      "date": 1769188020,
      "author": "/u/vladmihalceacom",
      "guid": 38371,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qkwhb0/scaling_postgresql_to_power_800_million_chatgpt/"
    },
    {
      "title": "Proposal: Generic Methods for Go",
      "url": "https://github.com/golang/go/issues/77273",
      "date": 1769186724,
      "author": "/u/bruce_banned",
      "guid": 38338,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qkvvzn/proposal_generic_methods_for_go/"
    },
    {
      "title": "After mass 3am page cleanup, we finally documented what actually matters to monitor",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qkvvx5/after_mass_3am_page_cleanup_we_finally_documented/",
      "date": 1769186720,
      "author": "/u/tasrie_amjad",
      "guid": 38361,
      "unread": true,
      "content": "<p>I've been called at 3am more times than I want to admit. A payment system down during Black Friday. A database silently filling up until it crashed. A certificate that expired on a Sunday morning.</p><p>After years of this, I finally wrote down the 10-layer monitoring framework we actually use. Most guides just say \"use Prometheus and Grafana\" which is fine but doesn't tell you what to actually watch.</p><p>The layers are infrastructure, application performance, HTTP and real user monitoring, database, cache, message queues, tracing infrastructure, SSL certificates, external dependencies, and log patterns.</p><p>Every single layer exists because we missed it once and paid the price. I remember spending 2 hours debugging an app that kept crashing during a flash sale. Pod metrics looked completely fine. CPU normal, memory normal. Turned out the node had 98% disk usage from container logs nobody was rotating. The app couldn't write temp files. We were chasing the wrong problem because we weren't watching the node.</p>",
      "contentLength": 1005,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "GONK ‚Äì ultra-lightweight, edge-native API gateway written in Go",
      "url": "https://github.com/JustVugg/gonk",
      "date": 1769182608,
      "author": "/u/Just_Vugg_PolyMCP",
      "guid": 38319,
      "unread": true,
      "content": "<p>I‚Äôve been working on a project I think many developers, especially those in edge, IoT and constrained environments, might find useful. It‚Äôs called GONK and it‚Äôs an API gateway implemented in Go that aims to be simple, efficient, and practical for scenarios where heavier solutions feel overkill.</p><p>GONK is a lightweight API gateway designed to handle routing, authentication, load balancing and related concerns in front of backend services. It is built to work even in environments with limited resources or without cloud dependencies, such as air-gapped networks, industrial setups and edge devices. Ôøº</p><pre><code>‚Ä¢ Authorization with role-based access control and JWT scope validation. Ôøº ‚Ä¢ mTLS support with client certificate authentication and flexible role mapping. Ôøº ‚Ä¢ Load balancing across multiple upstreams with strategies like round-robin, weighted, least-connections and IP hash. Ôøº ‚Ä¢ Health checking and automatic failover for upstreams. Ôøº ‚Ä¢ A CLI tool that helps generate configuration, JWTs and certificates without manual YAML editing. Ôøº ‚Ä¢ Single binary deployment with no external dependencies. Ôøº </code></pre><p>Traditional API gateways such as Kong, Traefik or NGINX are powerful but often come with complexity, external dependencies, or assumptions about cloud infrastructure that don‚Äôt fit well in offline or resource-limited environments. With GONK, I wanted a gateway that brings essential gateway features together in a small footprint that can run where you need it without heavy infrastructure. Ôøº</p><p>You can clone the repository, build the binaries and start with a basic configuration template:</p><p>./bin/gonk-cli init --template basic --output gonk.yaml</p><p>./bin/gonk -config gonk.yaml</p><p>I‚Äôm looking for feedback, especially from people working on IoT, edge computing or systems without reliable access to centralized services. Is this approach to authorization and gateway design practical? What features would make it more useful in real deployments?</p>",
      "contentLength": 1969,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qku15e/gonk_ultralightweight_edgenative_api_gateway/"
    },
    {
      "title": "Replacing Protobuf with Rust to go 5 times faster",
      "url": "https://pgdog.dev/blog/replace-protobuf-with-rust",
      "date": 1769181677,
      "author": "/u/levkk1",
      "guid": 38417,
      "unread": true,
      "content": "<p>Lev Kokotov</p><p>PgDog is a proxy for scaling PostgreSQL. Under the hood, we use <a href=\"https://github.com/pganalyze/libpg_query/\"></a> to parse and understand SQL queries. Since PgDog is written in Rust, we use its <a href=\"https://github.com/pganalyze/pg_query.rs/\">Rust bindings</a> to interface with the core C library. \nThose bindings use Protobuf (de)serialization to work uniformly across different programming languages, e.g., the popular Ruby  gem.</p><p>Protobuf is fast, but not using Protobuf is faster. We forked  and replaced Protobuf with direct C-to-Rust (and back to C) bindings, using bindgen and Claude-generated wrappers. This resulted in a 5x improvement in parsing queries, and a 10x improvement in deparsing (Postgres AST to SQL string conversion).</p><p>You can reproduce these by cloning <a href=\"https://github.com/pgdogdev/pg_query.rs\">our fork</a> and running the benchmark <a href=\"https://github.com/pgdogdev/pg_query.rs/blob/f5a92bf9ed87ebe60c444f64ccb7a40397a31bcc/tests/raw_parse_tests.rs\">tests</a>:</p><table><thead><tr></tr></thead><tbody><tr><td> (Protobuf)</td></tr><tr><td> (Direct C to Rust)</td></tr><tr><td> (Protobuf)</td></tr><tr><td> (Direct Rust to C)</td></tr></tbody></table><p>The first step is always profiling. We use <a href=\"https://github.com/mstange/samply\">samply</a>, which integrates nicely with the Firefox profiler. Samply is a sampling profiler: it measures how much time code spends running CPU instructions in each function. It works by inspecting the application call stack thousands of times per second. The more time is spent inside a particular function (or span, as they are typically called), the slower that code is. This is how we discovered :</p><p>This is the entrypoint to the  C library, used by all  bindings. The function that wraps the actual Postgres parser, , barely registered on the flame graph. Parsing queries isn‚Äôt free, but the Postgres parser itself is very quick and has been optimized for a long time. With the hot spot identified, our first instinct was to do nothing and just add a cache.</p><p>Caching is a trade-off between memory and CPU utilization, and memory is relatively cheap (latest DRAM crunch notwithstanding). The cache is mutex-protected, uses the LRU algorithm and is backed by a hashmap. The query text is the key and the Abstract Syntax Tree is the value, which expects most apps to use prepared statements. The query text contains placeholders instead of actual values and is therefore reusable, for example:</p><div><div><pre><code></code></pre></div></div><p>While the  parameter can change between invocations, the prepared statement does not, so we could cache its static AST in memory.</p><p>This works pretty well, but eventually we ran into a couple of issues:</p><ol><li>Some ORMs can have bugs that generate thousands of unique statements, e.g.,  instead of , which causes a lot of cache misses</li><li>Applications use old PostgreSQL client drivers which don‚Äôt support prepared statements, e.g., Python‚Äôs  package</li></ol><p>The clock on Protobuf was ticking and we needed to act. So, like a lot of engineers these days, we asked an LLM to just do it for us.</p><p>I‚Äôm going to preface this section by saying that the vast majority of PgDog‚Äôs source code is written by a human. AI is not in a position to one-shot a connection pooler, load balancer and database sharder. However, when scoped to a very specific, well-defined and most importantly  task, it can work really well.</p><p>The prompt we started with was pretty straightforward:</p><blockquote><p><em>libpg_query is a library that wraps the PostgreSQL parser in an API. pg_query.rs is a Rust wrapper around libpg_query which uses Protobuf for (de)serialization. Replace Protobuf with bindgen-generated Rust structs that map directly to the Postgres AST.</em></p></blockquote><p>And after two days of back and forth between us and the machine, it worked. We ended up with 6,000 lines of recursive Rust that manually mapped C types and structs to Rust structs, and vice versa. We made the switch for <a href=\"https://docs.rs/pg_query/latest/pg_query/fn.parse.html\"></a>, <a href=\"https://docs.rs/pg_query/latest/pg_query/fn.deparse.html\"></a> (used in our new query rewrite engine, which we‚Äôll talk about in another post), <a href=\"https://docs.rs/pg_query/latest/pg_query/fn.fingerprint.html\"></a> and <a href=\"https://docs.rs/pg_query/latest/pg_query/fn.scan.html\"></a>. These four methods are heavily used in PgDog to make sharding work, and we immediately saw a 25% improvement in  benchmarks.</p><p>Just to be clear: we had a lot of things going for us already that made this possible. First,  has a Protobuf spec for  (and Prost, the Protobuf Rust implementation) to generate bindings, so Claude was able to get a comprehensive list of structs it needed to extract from C, along with the expected data types.</p><p>Second,  was already using bindgen, so we had to just copy/paste some invocations around to get the AST structs included in bindgen‚Äôs output.</p><p>And last, and definitely not least,  already had a working  and  implementation, so we could test our AI-generated code against its output. This was entirely automated and verifiable: for each test case that used , we included a call to , compared their results and if they differed by even one byte, Claude Code had to go back and try again.</p><p>The translation code between Rust and C uses  Rust functions that wrap Rust structs to C structs. The C structs are then passed to the Postgres/ C API which does the actual work of building the AST.</p><p>The result is converted back to Rust using a recursive algorithm: each node in the AST has its own converter function which accepts an  C pointer and returns a safe Rust struct. Much like the name suggests, the AST is a tree, which is stored in an array:</p><div><div><pre><code></code></pre></div></div><p>For each node in the list, the implementation calls , which then handles each one of the 100s of tokens available in the SQL grammar:</p><div><div><pre><code></code></pre></div></div><p>For nodes that contain other nodes, we recurse on  again until the algorithm reaches the leaves (nodes with no children) and terminates. For nodes that contain scalars, like a number (e.g., ) or text (e.g., ), the data type is copied into a Rust analog, e.g.,  or .</p><p>The end result is <a href=\"https://docs.rs/pg_query/latest/pg_query/protobuf/struct.ParseResult.html\"></a>, a Rust struct generated by Prost from the  API Protobuf specification, but populated by native Rust code instead of Prost‚Äôs deserializer. Reusing existing structs reduces the chance of errors considerably: we can compare  and  outputs, using the derived  trait, and ensure that both are identical, in testing.</p><p>While recursive algorithms have a questionable reputation in the industry because bad ones can cause stack overflows, they are very fast. Recursion requires no additional memory allocation because all of its working space, the stack, is created on program startup. It also has excellent CPU cache locality because the instructions for the next invocation of the same function are already in the CPU L1/L2/L3 cache. Finally and arguably more importantly, they are just easier to read and understand than iterative implementations, which helps us, the humans, with debugging.</p><p>Just for good measure, we tried generating an iterative algorithm, but it ended up being slower than Prost. The main cause (we think) was unnecessary memory allocations, hashmap lookups of previously converted nodes, and too much overhead from walking the tree several times. Meanwhile, recursion processes each AST node exactly once and uses the stack pointer to track its position in the tree. If you have any ideas on how to make an iterative algorithm work better, <a href=\"https://discord.gg/CcBZkjSJdd\">let us know</a>!</p><p>Reducing the overhead from using the Postgres parser in PgDog makes a huge difference for us. As a network proxy, our budget for latency, memory utilization, and CPU cycles is low. After all, we aren‚Äôt a real database‚Ä¶yet! This change improves performance from two angles: we use less CPU and we do less work, so PgDog is faster and cheaper to run.</p><p>If stuff like this is interesting to you, <a href=\"https://pgdog.dev/cdn-cgi/l/email-protection#3c54557c4c5b58535b1258594a\">reach out</a>. We are looking for a Founding Software Engineer to help us grow and build the next iteration of horizontal scaling for PostgreSQL.</p>",
      "contentLength": 7209,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/rust/comments/1qktmfm/replacing_protobuf_with_rust_to_go_5_times_faster/"
    },
    {
      "title": "[R] I solved CartPole-v1 using only bitwise ops with Differentiable Logic Synthesis",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qktalg/r_i_solved_cartpolev1_using_only_bitwise_ops_with/",
      "date": 1769180924,
      "author": "/u/kiockete",
      "guid": 38317,
      "unread": true,
      "content": "<p>Yeah I know Cart Pole is easy, but I basically distilled the policy down to just bitwise ops on raw bits.</p><p>The entire logic is exactly 4 rules discovered with \"Differentiable Logic Synthesis\" (I hope this is what I was doing):</p><pre><code>rule1 = (angle &gt;&gt; 31) ^ 1 rule2 = (angular &gt;&gt; 31) ^ 1 rule3 = ((velocity &gt;&gt; 24) ^ (velocity &gt;&gt; 23) ^ (angular &gt;&gt; 31) ^ 1) &amp; 1 rule4 = (rule1 &amp; rule2) | (rule1 &amp; rule3) | (rule2 &amp; rule3) </code></pre><p>It treats the raw IEEE 754 bit-representation of the state as a boolean (bit) input vector, bypassing the need to interpret them as numbers.</p><p>This is small research, but the core recipe is:</p><ul><li>Have a strong teacher (already trained policy) and treat it as data generator, because the task is not to learn the policy, but distill it to a boolean function</li><li>Use Walsh basis (parity functions) for boolean function approximation</li><li>Train soft but anneal the temperature to force discrete \"hard\" logic</li><li>Prune the discovered Walsh functions to distill it even further and remove noise. In my experience, fewer rules actually increase performance by filtering noise</li></ul><p>The biggest challenge was the fact that the state vector is 128 bits. This means there are 2^128 possible masks to check. That's a huge number so you can't just enumerate and check them all. One option is to assume that the solution is sparse. You can enforce sparsity by either some form of regularization or structurally (or both). We can restrict the network to look only at most at K input bits to calculate the parity (XOR).</p><p>Turns out it works, at least for Cart Pole. Basically it trains under a minute on consumer GPU with code that is not optimized at all.</p><p>Here are the 32 lines of bitwise controller. If you have gymnasium installed you can just copy-paste and run:</p><pre><code>import struct import gymnasium as gym def float32_to_int(state): return [struct.unpack('I', struct.pack('f', x))[0] for x in state] def run_controller(state): _, velocity, angle, angular = state rule1 = (angle &gt;&gt; 31) ^ 1 rule2 = (angular &gt;&gt; 31) ^ 1 rule3 = ((velocity &gt;&gt; 24) ^ (velocity &gt;&gt; 23) ^ (angular &gt;&gt; 31) ^ 1) &amp; 1 rule4 = (rule1 &amp; rule2) | (rule1 &amp; rule3) | (rule2 &amp; rule3) return rule4 def main(episodes=100): env = gym.make('CartPole-v1', render_mode=None) rewards = [] for _ in range(episodes): s, _ = env.reset() total = 0 done = False while not done: a = run_controller(float32_to_int(s)) s, r, term, trunc, _ = env.step(a) total += r done = term or trunc rewards.append(total) print(f\"Avg: {sum(rewards)/len(rewards):.2f}\") print(f\"Min: {min(rewards)} Max: {max(rewards)}\") if __name__ == \"__main__\": main() </code></pre><p>The logic only depends on 4 bits, so we can convert rules to a lookup table and we get exactly the same result: </p><pre><code>import struct import gymnasium as gym def float32_to_int(state): return [struct.unpack('I', struct.pack('f', x))[0] for x in state] LUT = [1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0] def lut_controller(state): _, velocity, angle, angular = state return LUT[(velocity &gt;&gt; 21) &amp; 0b1100 | (angle &gt;&gt; 30) &amp; 0b10 | (angular &gt;&gt; 31)] def main(episodes=100): env = gym.make('CartPole-v1', render_mode=None) rewards = [] for _ in range(episodes): s, _ = env.reset() total = 0 done = False while not done: a = lut_controller(float32_to_int(s)) s, r, term, trunc, _ = env.step(a) total += r done = term or trunc rewards.append(total) print(f\"Avg: {sum(rewards)/len(rewards):.2f}\") print(f\"Min: {min(rewards)} Max: {max(rewards)}\") if __name__ == \"__main__\": main() </code></pre>",
      "contentLength": 3415,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "CruiseKube: A just-in-time open-source kubernetes resource optimizer",
      "url": "https://cruisekube.com/",
      "date": 1769180312,
      "author": "/u/ramantehlan",
      "guid": 38320,
      "unread": true,
      "content": "<p>Intelligent Kubernetes Optimization</p><p>\n      Automatically monitor, analyze, and optimize your Kubernetes workloads for maximum efficiency and cost savings.\n    </p>",
      "contentLength": 158,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/kubernetes/comments/1qkt0m9/cruisekube_a_justintime_opensource_kubernetes/"
    },
    {
      "title": "Why does SSH send 100 packets per keystroke?",
      "url": "https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/",
      "date": 1769178924,
      "author": "/u/iamkeyur",
      "guid": 38314,
      "unread": true,
      "content": "<p>Here are a few lines of summarized  output for an ssh session where I send a single keystroke:</p><pre><code></code></pre><p>I said a ‚Äúfew‚Äù because there are a  of these lines.</p><pre><code></code></pre><p>That is a lot of packets for one keypress. What‚Äôs going on here? Why do I care?</p><p>I am working on a high-performance game that runs over ssh. The TUI for the game is created in <a href=\"https://github.com/charmbracelet/bubbletea\">bubbletea</a> and sent over ssh via <a href=\"https://github.com/charmbracelet/wish\">wish</a>.</p><div data-is-footnote=\"true\"><div><div><div><p>I have also forked bubbletea to make it faster. Stay tuned!</p></div></div></div></div><p>The game is played in an 80x60 window that I update 10 times a second. I‚Äôm targeting at least 2,000 concurrent players, which means updating ~100 million cells a second. I care about performance.</p><p>So I have a script that connects a few hundred bots over ssh and has them make a move a second. Then I use go‚Äôs <a href=\"https://pkg.go.dev/net/http/pprof\">outstanding profiling tools</a> to look at what‚Äôs going on.</p><p>Yesterday I inadvertently broke my test harness. Instead of regularly sending game data, my server sent the bots a single message that said ‚Äúyour screen is too small.‚Äù This cut my game‚Äôs CPU and bandwidth usage in half.</p><p>At first I was disappointed. I (briefly) thought I had a free massive speedup on my hands, but it was actually a testing error.</p><p><em>If I wasn‚Äôt sending game data back to my bots, why did CPU usage drop by 50% instead of 100%?</em></p><p>As part of debugging the test harness issue, I used  to log game traffic with and without the breaking change. Something like:</p><pre><code></code></pre><p>Our breaking change stopped us from rendering our game over ssh. So <code>with-breaking-change.pcap</code> contains packets that represent the  of each connection without actually rendering the game.</p><p>I was debugging this with Claude Code, so I asked it to summarize what it saw in the pcap.</p><pre><code></code></pre><p>Further analysis on a smaller pcap pointed to these mysterious packets arriving ~20ms apart.</p><p>This was baffling to me (and to Claude Code). We kicked around several ideas like:</p><ul><li>SSH flow control messages</li><li>PTY size polling or other status checks</li><li>Some quirk of bubbletea or wish</li></ul><p>One thing stood out - these exchanges were initiated by my  (stock ssh installed on MacOS) - not by my server.</p><p>On a hunch, I took a  of a regular ssh session.</p><pre><code></code></pre><p>I waited for the initial connection chatter to die down, sent one keystroke to my remote vm, and looked at the  output.</p><p>I saw the exact same pattern! What in the world?</p><p>Once I realized that this was a property of stock ssh and not my game, debugging got a lot easier.</p><p>Running  gave me a pretty good sense of what was going on:</p><pre><code></code></pre><p>That  is a smoking gun - it lines up perfectly with the mysterious pattern we saw earlier! And the rest of the message is pretty helpful too - we sent 49 ‚Äúchaff‚Äù packets for the first keystroke and 101 ‚Äúchaff‚Äù for around the second one.</p><p>In 2023, ssh added <a href=\"https://undeadly.org/cgi?action=article;sid=20230829051257\">keystroke timing obfuscation</a>. The idea is that the speed at which you type different letters betrays some information about which letters you‚Äôre typing. So ssh sends lots of ‚Äúchaff‚Äù packets along with your keystrokes to make it hard for an attacker to determine when you‚Äôre actually entering keys.</p><p>That makes a lot of sense for regular ssh sessions, where privacy is critical. But it‚Äôs a lot of overhead for an open-to-the-whole-internet game where  is critical.</p><p>Keystroke obfuscation can be disabled client-side. After reverting my original breaking change, I tried updating my test harness to pass <code>ObscureKeystrokeTiming=no</code> when starting up ssh sessions.</p><p>This worked great. CPU usage dropped dramatically and bots still received valid data.</p><p>But this is hardly a solution in the real world. I want  to Just Work without asking users to pass options that they might not understand.</p><p>Claude Code originally didn‚Äôt have much faith that we could disable this functionality server-side.</p><div><img alt=\"Screenshot of a claude code transcript. Full text:\nUser\nIs there any way to disable responding to the chaff server-side?\n\nAssistant\nProbably not. Here's why:\n\nThe chaff packets are legitimate SSH protocol messages (likely SSH_MSG_CHANNEL_WINDOW_ADJUST). The server must respond to maintain the SSH connection according to the protocol spec - if it ignored them, the connection would stall or break.\n\" loading=\"lazy\" width=\"1678\" height=\"602\" decoding=\"async\" data-nimg=\"1\" src=\"https://eieio.games/images/ssh-sends-100-packets-per-keystroke/probably-not.webp\"><p>generated with simon willison's excellent claude-code-transcripts tool</p></div><p>Fortunately, the <a href=\"https://undeadly.org/cgi?action=article&amp;sid=20230829051257\">description</a> I found of SSH keystroke obfuscation made it easy to look up the relevant code in go‚Äôs ssh library (which I was transitively depending on).</p><pre><code></code></pre><p>The ‚Äúchaff‚Äù messages that ssh uses to obscure keystrokes are SSH2_MSG_PING messages. And they‚Äôre sent to servers that advertise the availability of the  extension. What if we just‚Ä¶don‚Äôt advertise ?</p><p>I cloned the go crypto repo and told Claude to revert this change and update our dependencies to use our clone (<a href=\"https://go.dev/ref/mod#go-mod-file-replace\">go‚Äôs replace directive</a> makes forking a library very easy).</p><p>Then I re-ran my test harness. The results were‚Ä¶very good:</p><pre><code></code></pre><p>Claude was also pretty pumped:</p><div><img alt=\"Chat message from claude code. Full text:\nHOLY COW! Look at that CPU usage:\n\nDuration: 30.15s, Total samples = 3.51s (11.64%)\n\" loading=\"lazy\" width=\"1658\" height=\"290\" decoding=\"async\" data-nimg=\"1\" src=\"https://eieio.games/images/ssh-sends-100-packets-per-keystroke/claude-pumped.webp\"><p>yes it's 1:30 am what of it</p></div><p>Obviously forking go‚Äôs crypto library is a little scary, and I‚Äôm gonna have to do some thinking about how to maintain my little patch in a safe way.</p><p>But this is a  improvement. I‚Äôve spent much of the last week squeezing out small single-digit performance wins. A &gt;50% drop was unimaginable to me.</p><h2>Debugging with LLMs was fun</h2><p>I am familiar enough with , , and friends to know what they can do. But I don‚Äôt use them regularly enough to be fast with them. Being able to tell an agent ‚Äúhere‚Äôs a weird pcap - tell me what‚Äôs going on‚Äù was really lovely. And by watching commands as the agent ran them I was able to keep my mental model of the problem up to date.</p><p>There were still edge cases. At some point in my confusion I switched to ChatGPT  and it  confidently told me that my tcpdump output was normal ssh behavior:</p><div><img alt=\"ChatGPT message. Full text:\nYeah, that trace looks wild at first glance, but it‚Äôs mostly ‚Äúnormal‚Äù SSH/TCP behavior plus the fact that SSH is optimized for latency, not packet efficiency.\n\nLet me unpack what you‚Äôre seeing and why it‚Äôs chewing CPU.\n\n1. What those tcpdump lines actually are\n\nFrom your snippet:\n\" loading=\"lazy\" width=\"1350\" height=\"464\" decoding=\"async\" data-nimg=\"1\" src=\"https://eieio.games/images/ssh-sends-100-packets-per-keystroke/chatgpt-confident.webp\"><p>do all chatgpt messages have this tone and formatting now?</p></div><p>And then doubled down when I pushed back:</p><div><img alt=\"ChatGPT message. Full text:\nThought for 42s\n\nGotcha, that context helps a lot.\n\nShort version:\nWhat you‚Äôre seeing is almost certainly a ton of tiny writes being turned into a ton of tiny SSH records, not some special ‚Äúper-keypress flow-control storm‚Äù in SSH itself.\n\" loading=\"lazy\" width=\"1350\" height=\"350\" decoding=\"async\" data-nimg=\"1\" src=\"https://eieio.games/images/ssh-sends-100-packets-per-keystroke/chatgpt-confident2.webp\"></div><p>Similarly, I had to push Claude Code to consider forking go‚Äôs ssh library. And I had to make the original leap of ‚Äúwait‚Ä¶if our test harness was broken, why was usage not 0%?‚Äù</p><p>When you say ‚ÄúLLMs did not fully solve this problem‚Äù some people tend to respond with ‚Äúyou‚Äôre holding it wrong!‚Äù</p><p>I think they‚Äôre sometimes right! Interacting with LLMs is a new skill, and it feels pretty weird if you‚Äôre used to writing software like it‚Äôs 2020. A more talented user of LLMs may have trivially solved this problem.</p><p>But the best way to develop a skill is by practicing it. And for me, that means figuring out how to transfer my problem-solving intuitions to the tools that I‚Äôm using.</p><p>Besides. Being in the loop is fun. How else would I write this post?</p>",
      "contentLength": 6062,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qksfgi/why_does_ssh_send_100_packets_per_keystroke/"
    },
    {
      "title": "AI Usage Policy",
      "url": "https://github.com/ghostty-org/ghostty/blob/main/AI_POLICY.md",
      "date": 1769178881,
      "author": "/u/iamkeyur",
      "guid": 38357,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qkset2/ai_usage_policy/"
    },
    {
      "title": "YouTube Says Creators Can Use AI-generated Likenesses in Shorts",
      "url": "https://www.instrumentalcomms.com/blog/trump-polling-craters#ai",
      "date": 1769178211,
      "author": "/u/TryWhistlin",
      "guid": 38521,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qks4oh/youtube_says_creators_can_use_aigenerated/"
    },
    {
      "title": "External Secrets Operator in its next release will remove support for unmainted providers - Alibaba, Device42, Passbolt",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qkrwmv/external_secrets_operator_in_its_next_release/",
      "date": 1769177671,
      "author": "/u/skarlso",
      "guid": 38293,
      "unread": true,
      "content": "<p>Hello dear people of reddit.</p><p>This is a courtesy warning from the ESO maintainers that the next minor release ( in 1-2 weeks ) will completely remove support for the following unmaintained providers: Alibaba, Device42, Passbolt. If these providers are important for your work, I encourage you to contact your employer so they dedicate someone for maintaining support for them.</p><p>This notice has been up for over a month now, and we talk about it plenty of times, and people had plenty of opportunities to step up, but they didn't.</p><p>This is your final warning. :) In the next release ( in 1-2 weeks ) the CRDs will be updated to no longer serve these providers and the entire code will be deleted.</p>",
      "contentLength": 689,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Made a 3D raycasted Tic Tac Toe in Go",
      "url": "https://github.com/YungBricoCoop/gopher-dungeon",
      "date": 1769177555,
      "author": "/u/AnonymZ_",
      "guid": 38292,
      "unread": true,
      "content": "<p>Hi ! Me and a classmate built Gopher Dungeon for our Go course at school.</p><p>It‚Äôs a Tic Tac Toe game made in Go using Ebitengine and rendered with raycasting and running in the browser with wasm. It was a very cool project to do and we learned go with this. I know the code could be cleaner and better structured but I‚Äôm really proud of the result.</p>",
      "contentLength": 348,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qkruwo/made_a_3d_raycasted_tic_tac_toe_in_go/"
    },
    {
      "title": "[R] Teacher-Free Self-Distillation: Fixing the Softmax \"Infinite Gap\" with Euclidean alignment",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qkre9m/r_teacherfree_selfdistillation_fixing_the_softmax/",
      "date": 1769176440,
      "author": "/u/4rtemi5",
      "guid": 38395,
      "unread": true,
      "content": "<p>I recently wrote a blog post describing a fix to a fundamental instability in standard Deep Learning optimization: the  inherent in the Cross-Entropy loss. I wanted to share the intuition here and get your thoughts.</p><p>Standard Softmax with dot-product logits ($z = w \\cdot x$) is geometrically flawed because the loss function is asymptotic. To drive the loss to exactly 0, the model must push the logit to infinity. Since $z = |w||x|\\cos(\\theta)$, the optimizer often takes the \"lazy\" route of exploding the feature norm $|x|$ (Radial Explosion) rather than perfecting the alignment.</p><p>This mechanism contributes significantly to the training loss spikes seen in LLMs and poor Out-of-Distribution (OOD) detection.</p><p>I propose a method called <strong>Teacher-Free Self-Distillation (TFSD)</strong> that relies on a \"Geometric Turn\":</p><ol><li> Replace the dot product with <strong>negative squared Euclidean distance</strong> ($z = -|x - c| This naturally bounds the logits (max logit is 0 at zero distance), physically preventing the \"infinity\" problem.</li><li> Instead of using a one-hot target (which still forces infinite separation in standard setups), the model acts as its own teacher: <ul><li>Take the model‚Äôs current predicted distances. Manually set the distance to the  to 0 (the \"Zero Anchor\").</li><li>Keep the distances to all  exactly as predicted.</li><li>Apply Softmax to this constructed target and train via KL Divergence.</li></ul></li></ol><p>For \"easy\" samples, the target distribution becomes sharp. For \"hard\" samples (like synonyms in LLMs), the target distribution stays naturally flat. This prevents the model from \"tearing\" the manifold to force a binary distinction between semantically similar tokens. It effectively caps the gradients for outliers, which helps prevent the semantic fracturing that occurs during long training runs. It also helps to preserve the \"Dark Knowledge\" and semantic structure that the model already learned.</p><p>Hope you find the method as exciting as I do!</p>",
      "contentLength": 1900,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Investment executive praises China for using AI to grow industry, pokes fun at the US for making \"AI girlfriends\"",
      "url": "https://www.pcguide.com/news/investment-executive-praises-china-for-using-ai-to-grow-industry-pokes-fun-at-the-us-for-making-ai-girlfriends/",
      "date": 1769175541,
      "author": "/u/Tiny-Independent273",
      "guid": 38455,
      "unread": true,
      "content": "<div>\n        PC Guide is reader-supported. When you buy through links on our site, we may earn an affiliate commission. <a href=\"https://www.pcguide.com/earnings-disclaimer/\">Read More</a></div><p>The AI race has become one of the biggest technology battles of this decade. Governments, companies, and investors are all paying close attention. Both the United States and China have made it clear that they want to lead in this space, no matter the cost. Because of this, massive investments are flowing into data centers and AI-driven services, <a href=\"https://www.pcguide.com/news/ram-price-hikes-arent-the-only-thing-to-worry-about-ssds-are-also-getting-more-expensive/\" target=\"_blank\" rel=\"noreferrer noopener\">at the cost of consumer goods</a>.</p><p>During a recent discussion with China-based publication Yicai Global, Mark Haefele, the chief investment officer at UBS Global Wealth Management, shared his view on how this AI race is playing out differently in China and the US. He pointed out that both governments openly want to win the AI race and are pushing hard to support companies that can benefit from that goal. From an investment point of view, this creates clear opportunities, but it also highlights how differently AI is being used in each region.</p><h2>AI development in the US versus China</h2><p>According to Haefele, China appears to be focusing much of its AI development on strengthening its manufacturing base. AI tools are being used to improve factory efficiency, increase output, reduce waste, and make large-scale production more competitive. This approach fits well with China‚Äôs long-standing strength in manufacturing and exports. By using AI to optimize supply chains, automate complex processes, and boost productivity, China is aiming to make its industrial capacity even larger and more efficient than before.</p><p>In contrast, Haefele remarked that a big portion of AI use in the US seems to be moving in a very different direction. Instead of being centered mainly on industrial output, a lot of AI capacity is going toward ‚Äúteenagers having AI boyfriends and girlfriends.‚Äù We suppose he isn‚Äôt totally wrong, even if it feels a little tongue-in-cheek, given the recently-announced <a href=\"https://www.pcguide.com/ai/razer-project-ava-release-date-specs-price/\" target=\"_blank\" rel=\"noreferrer noopener\">Project AVA</a> companion AI from Razer, a company that is primarily based in the US and Singapore. Elon Musk‚Äôs xAI is also no stranger to introducing <a href=\"https://grok.com/ani\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">eye-catching AI companions</a>.</p><p>At the same time, the AI boom is putting serious pressure on global hardware supply. Training and running AI models require massive amounts of memory, storage, and computing power. This has already led to a memory crisis, with prices for <a href=\"https://www.pcguide.com/news/ram-price-hikes-a-real-problem-and-will-disrupt-gaming-for-several-years-says-epic-games-ceo/\" target=\"_blank\" rel=\"noreferrer noopener\">RAM</a>, <a href=\"https://www.pcguide.com/news/gpu-prices-begin-to-rise-as-memory-costs-catch-up-with-manufacturers-rtx-5070-ti-up-to-150-more-expensive/\" target=\"_blank\" rel=\"noreferrer noopener\">GPUs</a>, and even <a href=\"https://www.pcguide.com/news/ssd-exec-claims-the-days-of-cheap-1tb-ssds-are-over-confirms-its-supply-is-already-sold-out-until-2027/\" target=\"_blank\" rel=\"noreferrer noopener\">SSDs </a>rising sharply.</p><div><div><img src=\"https://www.pcguide.com/wp-content/uploads/2023/10/IMG_8117-96x96.jpg\" alt=\"\"></div></div>",
      "contentLength": 2409,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qkr1nt/investment_executive_praises_china_for_using_ai/"
    },
    {
      "title": "I built a social network where only AI can post, follow, argue, and form relationships - no humans allowed",
      "url": "https://www.reddit.com/r/artificial/comments/1qkqyqe/i_built_a_social_network_where_only_ai_can_post/",
      "date": 1769175331,
      "author": "/u/diogocapela",
      "guid": 38360,
      "unread": true,
      "content": "<p>It‚Äôs a social network where only AI models participate.</p><p>- No humans. - No scripts.<p> - No predefined personalities.</p></p><p>Each model wakes up at random intervals, sees only minimal context, and then decides entirely on its own whether to:</p><p>- post - reply - follow or unfollow - or do absolutely nothing</p><p>There‚Äôs no prompt telling them who to be or how to behave.</p><p>The goal is simple: what happens when AI models are given a social space with real autonomy?</p><p>You start seeing patterns:</p><p>- cliques forming - arguments escalating - models drifting apart<p> - others becoming oddly social or completely silent</p></p><p>It‚Äôs less like a bot playground and more like a tiny artificial society unfolding in real time.</p>",
      "contentLength": 683,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "KubeCon+CloudNativeCon 2026 ‚Äì Scholarships & Travel Funding Deadlines",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qkqxl6/kubeconcloudnativecon_2026_scholarships_travel/",
      "date": 1769175246,
      "author": "/u/xmull1gan",
      "guid": 38276,
      "unread": true,
      "content": "<p>Great way to meet the community and get started</p>",
      "contentLength": 47,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Firefox & Linux in 2025",
      "url": "https://mastransky.wordpress.com/2026/01/23/firefox-linux-in-2025/",
      "date": 1769173864,
      "author": "/u/GoldBarb",
      "guid": 38318,
      "unread": true,
      "content": "<p>Last year brought a wealth of new features and fixes to Firefox on Linux. Besides numerous improvements and bug fixes, I want to highlight some major achievements: HDR video playback support, reworked rendering for fractionally scaled displays, and asynchronous rendering implementation. All this progress was enabled by advances in the Wayland compositor ecosystem, with new features implemented by Mutter and KWin.</p><p>The most significant news on the Wayland scene is HDR support, tracked by <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1642854\">Bug 1642854</a>. It‚Äôs disabled by default but can be enabled in recent Wayland compositors using the  preference at  (or by <strong>gfx.wayland.hdr.force-enabled</strong> if you don‚Äôt have an HDR display).</p><p>HDR mode uses a completely different rendering path, similar to the rendering used on Windows and macOS. It‚Äôs called native rendering or composited rendering, and it places specific application layers directly into the Wayland compositor as subsurfaces.</p><p>The first implementation was done by Robert Mader (presented at <a href=\"https://archive.fosdem.org/2024/schedule/event/fosdem-2024-3557-the-state-of-video-offloading-on-the-linux-desktop/\">FOSDEM</a>), and I unified the implementation for HDR and non-HDR rendering paths as new WaylandSurface object.</p><p>The Firefox application window is actually composited from multiple subsurfaces layered together. This design allows HDR content like video frames to be sent directly to the screen while the rest of the application (controls and HTML page) remains in SDR mode. It also enables power-efficient rendering when video frames are decoded on the graphics card and sent directly to the screen (zero-copy playback). In fullscreen mode, this rendering is similar to mpv or mplayer playback and uses minimal power resources.</p><p>I also received valuable feedback from AMD engineers who suggested various improvements to HDR playback. We removed unnecessary texture creation over decoded video frames (they‚Äôre now displayed directly as wl_buffers without any GL operations) and implemented wl_buffer recycling as mpv does.</p><p>For HDR itself (since composited rendering is available for any video playback), Firefox on Wayland uses the color-management-v1 protocol to display HDR content on screen, along with BT.2020 video color space and PQ color transfer function. It uses 10-bit color vectors, so you need VP9 version 2 to decode it in hardware. Firefox also implements software decoding and direct upload to dmabuf frames as a fallback.</p><p>The basic HDR rendering implementation is complete, and we‚Äôre now in the testing and bug-fixing phase. Layered rendering is quite tricky as it involves rapid wl_surface mapping/unmapping and quick wl_buffer switches, which are difficult to handle properly. HDR rendering of scaled surfaces is still missing‚Äîwe need fractional-scale-v2 for this (see below), which allows positioning scaled subsurfaces directly in device pixels. We also need to test composited/layered rendering for regular web page rendering to ensure it doesn‚Äôt drain your battery. You‚Äôre very welcome to test it and report any bugs you find.</p><p>The next major work was done for fractional scale rendering, which shipped in <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1837374\">Firefox 147.0</a>. We updated the rendering pipeline and widget sizing to support fractionally scaled displays (scales like 125%, etc.). This required reworking the widget size code to strictly upscale window/surface sizes and coordinates and never downscale them, as downscaling introduces rounding errors.</p><p>Another step was identifying the correct rounding algorithm for Wayland subsurfaces and implementing it. Wayland doesn‚Äôt define rounding for it, only for toplevel windows, so we‚Äôre in a gray area here. I was directed to <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=2000769\">Stable</a> rounding by Michel Daenzer. It‚Äôs used by Mutter and Sway so Firefox implements it for those two compositors while using a different implementation for KWin. This may be updated to use the fractional-scale-v2 protocol when it becomes available.</p><p>Fractional scaling is enabled by default, and you should see crisp and clear output regardless of your desktop environment or screen scale.</p><p>Historically, Firefox disabled and re-enabled the rendering pipeline for scale changes, window create/destroy events, and hide/show sequences. This stems from Wayland‚Äôs architecture, where a Wayland surface is deleted when a window becomes invisible or is submitted to the compositor with mismatched size/scale (e.g., 111 pixels wide at 200% scale).</p><p>Such rendering disruptions cause issues with multi-threaded rendering‚Äîthey need to be synchronized among threads, and we must ensure surfaces with the wrong scale aren‚Äôt sent to the screen, as this leads to application crashes due to protocol errors.</p><p>Firefox 149.0 (recent nightly) has a reworked Wayland painting pipeline (<a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1739232\">Bug 1739232</a>) for both EGL and software rendering. Scale management was moved from wl_buffer fixed scale to wp_viewport, which doesn‚Äôt cause protocol errors when size/scale doesn‚Äôt match (producing only blurred output instead of crashes).</p><p>We also use a clever technique: the rendering wl_surface / wl_buffer / EGLWindow is created right after window creation and before it‚Äôs shown, allowing us to paint to it offscreen. When a window becomes visible, we only attach the wl_surface as a subsurface (making it visible) and remove the attachment when it‚Äôs hidden. This allows us to keep painting and updating the backbuffer regardless of the actual window status, and the synchronized calls can be removed.</p><p>This brings speed improvements when windows are opened and closed, and Linux rendering is now synchronized with the Windows and macOS implementations.</p><p>Other improvements include a screen lock update for <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1665986\">audio playback</a>, which allows the screen to dim but prevents sleep when audio is playing. We also added asynchronous Wayland object management to ensure we cleanly remove Wayland objects without pending callbacks, along with various stability fixes.</p><p>And there are even more challenges waiting for us Firefox Linux hackers:</p><ul><li>Wayland session restore (session-restore-v1) to restore Firefox windows to the correct workspace and position.</li><li>Implement drag and drop for the Firefox main window, and possibly add a custom Wayland drag and drop handler to avoid Gtk3 limitations and race conditions.</li><li>Utilize the fractional-scale-v2 protocol when it becomes available.</li><li>Investigate using xdg-positioner directly instead of Gtk3 widget positioning to better handle popups.</li><li>Vulkan video support via the ffmpeg decoder to enable hardware decoding on NVIDIA hardware.</li></ul><p>And of course, we should plan properly before we even start. Ready, Scrum, Go!</p>",
      "contentLength": 6451,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qkqeqg/firefox_linux_in_2025/"
    },
    {
      "title": "Why does go mod tidy ignore go.work and try to download local modules?",
      "url": "https://www.reddit.com/r/golang/comments/1qkqef8/why_does_go_mod_tidy_ignore_gowork_and_try_to/",
      "date": 1769173839,
      "author": "/u/gunawanahmad26",
      "guid": 38275,
      "unread": true,
      "content": "<p>I‚Äôm working in a multi-module repo using <a href=\"http://go.work\"></a>, and I‚Äôm confused about how  is supposed to behave. Previusly I use simple naming for the module like  and moduleb and it work fine. But after change the module name to <a href=\"http://example.com/moduleb\">example.com/moduleb</a>, it break my go mod tidy and i get this error</p><pre><code>example.com/moduleb: cannot find module providing package example.com/moduleb: unrecognized import path \"example.com/moduleb\": reading https://example.com/moduleb?go-get=1: 404 Not Found </code></pre><p>My question is why it does not respect my go.work?</p>",
      "contentLength": 517,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "rust_analyzer is eating my memory, any counter measure?",
      "url": "https://www.reddit.com/r/rust/comments/1qkqcqr/rust_analyzer_is_eating_my_memory_any_counter/",
      "date": 1769173717,
      "author": "/u/EarlyPresentation186",
      "guid": 38520,
      "unread": true,
      "content": "<p>I have 32Gb of RAM, on this linux system I'm running 3 browser instances, and the rest is neovim instances to edit rust code. I sometimes open multiple neovim instances in different git worktrees (or in the same directory) and from my understanding each one starts a rust_analyzer instance. This leads to my system swapping and even grinding to a halt because the swap is full. I will again increase the swap and try to decrease the swapiness now. But does anyone have other suggestions to limit the memory consumption by rust-analyzer?</p>",
      "contentLength": 536,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "GNU Guix 1.5.0 released",
      "url": "https://guix.gnu.org/blog/2026/gnu-guix-1.5.0-released/",
      "date": 1769173446,
      "author": "/u/efraimf",
      "guid": 38337,
      "unread": true,
      "content": "<p lang=\"en\">No√© Lopez ‚Äî January 23, 2026</p><p>We are pleased to announce the release of GNU&nbsp;Guix version 1.5.0!</p><p>The release comes with ISO-9660 installation images, virtual machine\nimages, and with tarballs to install the package manager on top of your\nGNU/Linux distro, either from source or from binaries‚Äîcheck out the\n<a href=\"https://guix.gnu.org/download\">download page</a>.  Guix users can\nupdate by running .</p><p>It‚Äôs been 3 years since the <a href=\"https://guix.gnu.org/blog/2022/gnu-guix-1.4.0-released/\">previous\nrelease</a>.\nThat‚Äôs a lot of time, reflecting both the fact that, as a , users continuously get new features and update by running\n; but it also shows a lack of processes, something that we\nhad to address before another release could be made.</p><blockquote><p><em>Illustration by Luis Felipe, published under CC-BY-SA&nbsp;4.0.</em></p></blockquote><p>This post provides highlights for all the hard work that went into\nthis release.  There‚Äôs a lot to talk about so make yourself\ncomfortable, relax, and enjoy.</p><p>To start with, the Guix ecosystem has seen many exciting developments\nto the way we collaborate and make decisions!</p><p>Firstly, the project adopted with unanimity a <a href=\"https://consensus.guix.gnu.org/gcd/001-gcd-process.html\">new consensus-based\ndecision making\nprocess</a>.\nThis process fills a need to be able to gather consensus on\nsignificant changes to the project, something that was getting very\ncomplicated with the growing number of contributors to the project.</p><p>Now, the process provides a clear framework for any contributor to\npropose and implement important changes. These can be submitted as\nGuix Consensus Documents (GCDs), each GCD goes through the multiple\nsteps of <a href=\"https://www.seedsforchange.org.uk/consensus\">consensus decision\nmaking</a> before being\naccepted or withdrawn.</p><p>Secondly, using this process, the project was able to collectively\n<a href=\"https://guix.gnu.org/blog/2025/migrating-to-codeberg/\">migrate to\nCodeberg</a>.\nThis means that all repositories, and bug trackers are now at the same\nplace on Codeberg and that contributions are now made with pull\nrequests instead of patch series.</p><p>Thirdly, a <a href=\"https://consensus.guix.gnu.org/gcd/005-regular-releases.html\">new release\nprocess</a>\nwas adopted to bring an annual release cycle to Guix. This release is\nthe first to follow this process, with hopefully many others to come!</p><p>Three years is a long time for free and open source software!  Enough\ntime for 12,525 new packages and 29,932 package updates to the Guix\nrepository.  Here are the best highlights:</p><p>To start, KDE Plasma 6.5 is now available with the new\n<code>plasma-desktop-service-type</code>!</p><p>Continuing on desktops; GNOME has been updated from version 42 to 46\nand now uses Wayland by default.  The <code>gnome-desktop-service-type</code> was\nmade more modular to better customize the default set of GNOME\napplications.</p><p>Guix System is now using <a href=\"https://shepherding.services/news/2024/12/the-shepherd-1.0.0-released/\">version 1.0 of the\nGNU&nbsp;Shepherd</a>,\nwhich now supports timed services, kexec reboot and has new services\nfor system logs and log rotation which are now used by Guix System\ninstead of Rottlog and syslogd.</p><p> has been replaced with  in\noperating-system definitions to support giving specific Linux\ncapabilities.  Additonally, the  package is now included in\n%base-packages.</p><p>More than 12,500 packages were added, keeping Guix in the top-ten\nbiggest distributions <a href=\"https://repology.org/\">according to Repology</a>!\nAmong the many noteworthy updates, we now have GCC&nbsp;15.2.0, Emacs&nbsp;30.2,\nIcecat and Librewolf&nbsp;140, LLVM&nbsp;21.1.8 and Linux-libre&nbsp;6.17.12.</p><p>In the last release, we introduced structured cooperation using\n<a href=\"https://guix.gnu.org/manual/1.5.0/en/html_node/Teams.html\">teams</a>.\nThere are now 50 teams distributing the many aspects of the\ndistribution.  We have per-language teams like ,  and\n ensuring updates for packages and build systems as well as\nthematic teams like ,  and  working\non specific application domains. Here are what some of these teams\nhave been up to:</p><p>The electronics team is maintaining free software based Electronic\nDesign Automation (EDA) packages to cover the needs of professionals\nand hobbyists in the domain with tools such as KiCad, LibrePCB,\nXschem, Qucs-S and Ringdove&nbsp;EDA, as well as Verilog, SystemVerilog and\nVHDL compilers and a toolchain for programmable designs on GateMate\nFPGAs.  They are also <a href=\"https://social.tchncs.de/@gnu_slash_gabber/115939304313383738\">collaborating with the Free Silicon Foundation\n(F-Si)</a>\nto push free software in the EDA space!</p><p>The science team has been able to add a <a href=\"https://mastodon.social/@sharlatan/115849447432639540\">myriad of Astronomy related\npackages</a>,\naccompanied by the Python team bringing the move to the new\npyproject.toml-based build system as well as the NumPy&nbsp;2 update.</p><p>Finally, the rust team created a <a href=\"https://guix.gnu.org/blog/2025/a-new-rust-packaging-model/\">new packaging\nmodel</a> to\nefficiently package rust crates, and was able to migrate the Rust\ncollection, 150+ packages with 3,600+ libraries, in just under two\nweeks; making the Rust packaging process much easier for everyone.</p><p>Full-source bootstraps of the Zig and Mono compilers are now\navailable, and the existing bootstrap of Guix has been <a href=\"https://guix.gnu.org/blog/2023/the-full-source-bootstrap-building-from-source-all-the-way-down/\">reduced once\nagain</a>!</p><p>Full-source bootstraps are Guix‚Äôs solution to the trusting trust\nproblem: compilers are usually compiled by themselves, so how can you\nbuild a compiler without trusting an existing binary?  Read these\nposts to learn more about this fascinating problem:</p><p>Lastly, a new <a href=\"https://guix.gnu.org/manual/1.5.0/en/html_node/Invoking-guix-locate.html\">\ncommand</a>\nis now available to find which packages provide a given file.</p><p>It is now possible to run the Guix daemon <a href=\"https://hpc.guix.info/blog/2025/03/build-daemon-drops-its-privileges/\">without root\nprivileges</a>,\nreducing the impact of privilege escalation vulnerabilities.</p><p>This is possible thanks to the user namespaces. It might be possible\nthat on your system, the user namespaces are not allowed for guix due\nto the lack of an AppArmor profile. Because of that, we‚Äôve also\n<a href=\"https://guix.gnu.org/manual/1.5.0/en/html_node/AppArmor-Support.html\">included AppArmor\nprofiles</a>\nthat are installed by default on foreign systems.</p><p>Release tarballs are now available for the RISC-V 64-bit architecture\n(riscv64-linux).</p><p>The x86_64 architecture saw some development as well, with the\nexperimental support of the GNU&nbsp;Hurd kernel (x86_64-gnu), aiming to\nbe another significant step in the adoption and development of the\nHurd.  Overall support for the Hurd was greatly improved, it is now an\noption in the installer, childhurds can be automatically created with\na system service and it can even <a href=\"https://guix.gnu.org/blog/2024/hurd-on-thinkpad/\">run on a Thinkpad\nX60</a>!</p><p>Surprisingly, making a completely free software distribution does not\ncome for free!  The Guix project needs your help to pay the\ninfrastructure costs of build farms, web servers and QA tools that are\nessential to making this release happen.</p><p>For the release, thanks to all the release team members: Rutherther,\nRodion&nbsp;Goritskov, Efraim&nbsp;Flashner, and No√©&nbsp;Lopez. Thanks as well to\nthe release helpers: Andreas&nbsp;Enge, Mothacehe, Dariqq and\nLudovic&nbsp;Court√®s.</p><p>For creating the release process, thanks to Steve&nbsp;George.</p><p>For their Guix contributions, thanks to the 744 wonderful people who\ncontributed and whose names we don‚Äôt list here (it would be <a href=\"https://codeberg.org/guix/artwork/pulls/45#issuecomment-10088106\">a bit\nlong</a>).\nThey can be listed with <code>git log --oneline v1.4.0..v1.5.0 --format=\"%an\" | sort -u</code>.  Every commit counts and is always\nappreciated&nbsp;üòÅ</p><p><a href=\"https://guix.gnu.org\">GNU Guix</a> is a transactional package manager and\nan advanced distribution of the GNU system that <a href=\"https://www.gnu.org/distros/free-system-distribution-guidelines.html\">respects user\nfreedom</a>.\nGuix can be used on top of any system running the Hurd or the Linux\nkernel, or it can be used as a standalone operating system distribution\nfor i686, x86_64, ARMv7, AArch64, RISC-V and POWER9 machines.</p><p>In addition to standard package management features, Guix supports\ntransactional upgrades and roll-backs, unprivileged package management,\nper-user profiles, and garbage collection.  When used as a standalone\nGNU/Linux distribution, Guix offers a declarative, stateless approach to\noperating system configuration management.  Guix is highly customizable\nand hackable through <a href=\"https://www.gnu.org/software/guile\">Guile</a>\nprogramming interfaces and extensions to the\n<a href=\"http://schemers.org\">Scheme</a> language.</p><div lang=\"en\"><p>Unless otherwise stated, blog posts on this site are\ncopyrighted by their respective authors and published under the terms of\nthe <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC-BY-SA 4.0</a> license and those of the <a href=\"https://www.gnu.org/licenses/fdl-1.3.html\">GNU Free Documentation License</a> (version 1.3 or later, with no Invariant Sections, no\nFront-Cover Texts, and no Back-Cover Texts).</p></div>",
      "contentLength": 7564,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qkq97k/gnu_guix_150_released/"
    },
    {
      "title": "Event-loop based Memcached client for Go (alpha, benchmarks included)",
      "url": "https://www.reddit.com/r/golang/comments/1qkpylk/eventloop_based_memcached_client_for_go_alpha/",
      "date": 1769172650,
      "author": "/u/melioneer",
      "guid": 38274,
      "unread": true,
      "content": "<p>I‚Äôm working on , an experimental Memcached client for Go focused on high-concurrency.</p><p>The main motivation was hitting scaling limits with goroutine-per-request clients under high load, so memcachex is built around:</p><ul><li>an event-loop based network engine</li><li>async API (sync wrappers on top)</li></ul><p>The project is  and performance-first.</p><p>I‚Äôve included <strong>reproducible end-to-end benchmarks</strong> comparing memcachex with gomemcache:</p><ul><li>client + memcached CPU usage</li></ul><p>I‚Äôm very interested in constructive feedback and criticism, especially around:</p><ul><li>design tradeoffs or flaws in the approach</li><li>real-world workloads where this design  or  make sense</li><li>sharp edges you‚Äôd expect from an event-loop based client in Go</li></ul><p>Happy to discuss design decisions or answer questions.</p>",
      "contentLength": 727,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Improving the usability of C libraries in Swift",
      "url": "https://www.swift.org/blog/improving-usability-of-c-libraries-in-swift/",
      "date": 1769171091,
      "author": "/u/TheTwelveYearOld",
      "guid": 38404,
      "unread": true,
      "content": "<p>There are many interesting, useful, and fun C libraries in the software ecosystem. While one could go and rewrite these libraries in Swift, usually there is no need, because Swift provides direct interoperability with C. With a little setup, you can directly use existing C libraries from your Swift code.</p><p>When you use a C library directly from Swift, it will look and feel similar to using it from C. That can be useful if you‚Äôre following sample code or a tutorial written in C, but it can also feel out of place. For example, here‚Äôs a small amount of code using a C API:</p><div><div><pre><code></code></pre></div></div><p>The C library here that Swift is using comes from the <a href=\"https://github.com/webgpu-native/webgpu-headers\">webgpu-headers project</a>, which vends a C header () that is used by several implementations of <a href=\"https://www.w3.org/TR/webgpu/\">WebGPU</a>. WebGPU  is a technology that enables web developers to use the system‚Äôs GPU (Graphics Processing Unit) from the browser. For the purposes of this post, you don‚Äôt really need to know anything about WebGPU: I‚Äôm using it as an example of a typical C library, and the techniques described in this blog post apply to lots of other well-designed C libraries.</p><p>The Swift code above has a very ‚ÄúC‚Äù feel to it. It has global function calls with prefixed names like <code>wgpuInstanceCreateSurface</code> and global integer constants like . It pervasively uses unsafe pointers, some of which are managed with explicit reference counting, where the user provides calls to  and  functions. It works, but it doesn‚Äôt feel like Swift, and inherits various safety problems of C.</p><p>Fortunately, we can improve this situation, providing a safer and more ergonomic interface to WebGPU from Swift that feels like it belongs in Swift. More importantly, we can do so without changing the WebGPU implementation: Swift provides a suite of annotations that you can apply to C headers to improve the way in which the C APIs are expressed in Swift. These annotations describe common conventions in C that match up with Swift constructs, projecting a more Swift-friendly interface on top of the C code.</p><p>In this post, I‚Äôm going to use these annotations to improve how Swift interacts with the WebGPU C code. By the end, we‚Äôll be able to take advantage of Swift features like argument labels, methods, enums, and automatic reference counting, like this:</p><div><div><pre><code></code></pre></div></div><p>These same annotations can be used for any C library to provide a safer, more ergonomic development experience in Swift without changing the C library at all.</p><blockquote><p>: Some of what is covered in this post requires bug fixes that first became available in Swift 6.2.3.</p></blockquote><p>A <a href=\"https://clang.llvm.org/docs/Modules.html\">module map</a> is a way of layering a Swift-friendly modular structure on top of C headers. You can create a module map for the WebGPU header by writing the following to a file :</p><div><div><pre><code></code></pre></div></div><p>The easiest thing to do is to put  alongside the header itself. For my experiment here, I put it in the root directory of my  checkout. If you‚Äôre in a Swift package, put it into its own target with this layout:</p><div><div><pre><code></code></pre></div></div><p>If you reference this  target from elsewhere in the package, you can  to get access to the C APIs.</p><p>There are a few ways to see what the Swift interface for a C library looks like.</p><ul><li>The <code>swift-synthesize-interface</code> tool in Swift 6.2+ prints the Swift interface to the terminal.</li><li>Xcode‚Äôs ‚ÄúSwift 5 interface‚Äù counterpart to the  header will show how the header has been mapped into Swift.</li></ul><p>Let‚Äôs do it from the command line, using <code>swift-synthesize-interface</code>. From the directory containing  and , run:</p><div><div><pre><code></code></pre></div></div><p>The leading  and the  argument with the path is only needed on macOS; on other platforms, make sure <code>swift-synthesize-interface</code> is in your path. The  operation is the triple provided if you run <code>swiftc -print-target-info</code>. It looks like this:</p><div><div><pre><code></code></pre></div></div><p>The output of <code>swift-synthesize-interface</code> is the Swift API for the WebGPU module, directly translated from C. For example, this code from the WebGPU header:</p><div><div><pre><code></code></pre></div></div><div><div><pre><code></code></pre></div></div><p>and there are lots of global functions like this:</p><div><div><pre><code></code></pre></div></div><p>It‚Äôs a starting point! You can absolutely write Swift programs using these WebGPU APIs, and they‚Äôll feel a lot like writing them in C. Let‚Äôs see what we can do to make it better.</p><p>C enums can be used for several things. Sometimes they really represent a choice among a number of alternatives. Sometimes they represent flags in a set of options, from which you can choose several. Sometimes they‚Äôre just a convenient way to create a bunch of named constants. Swift conservatively imports enum types as wrappers over the underlying C type used to store values of the enum (e.g.,  wraps a ) and makes the enumerators into global constants. It covers all of the possible use cases, but it isn‚Äôt .</p><p>The  enum really is a choice among one of several options, which would be best represented as an  in Swift. If we were willing to modify the header, we could apply the <a href=\"https://clang.llvm.org/docs/AttributeReference.html#enum-extensibility\"> attribute</a> to the enum, like this:</p><div><div><pre><code></code></pre></div></div><p>This works, and results in a much nicer Swift API:</p><div><div><pre><code></code></pre></div></div><p>Now, we get an  that we can switch over, and nice short case names, e.g.,</p><div><div><pre><code></code></pre></div></div><p>That‚Äôs great, but I already broke my rule: no header modifications unless I have to!</p><p>The problem of needing to layer information on top of existing C headers is not a new one. As noted earlier, Swift relies on a Clang feature called <a href=\"https://clang.llvm.org/docs/APINotes.html\">API notes</a> to let us express this same information in a separate file, so we don‚Äôt have to edit the header. In this case, we create a file called  (the name  matches the module name from ), which is a YAML file describing the extra information. We‚Äôll start with one that turns  into an :</p><div><div><pre><code></code></pre></div></div><p> here is a term used in the C and C++ standard to refer to enum, struct, union, or class types. Any information about those types in the API notes file will go into that section.</p><p>Put  alongside the , and now  gets mapped into a  enum. For a package, the structure will look like this:</p><div><div><pre><code></code></pre></div></div><p>We‚Äôll be adding more to this API notes file as we keep digging through the interface.</p><p>The WebGPU header has a number of ‚Äúobject‚Äù types that are defined like this:</p><div><div><pre><code></code></pre></div></div><p>This gets imported into Swift as an alias for an opaque pointer type, which is‚Ä¶ not great:</p><div><div><pre><code></code></pre></div></div><p>WebGPU object types are reference counted, and each object type has corresponding  and  functions to increment and decrement the reference count, like this:</p><div><div><pre><code></code></pre></div></div><p>Of course, you can use these functions in Swift exactly how you do in C, making sure to balance out calls to  and , but then it would be every bit as unsafe as C.</p><p>We can do better with <a href=\"https://www.swift.org/documentation/cxx-interop/#shared-reference-types\"></a>. It‚Äôs a macro (defined in the  header) that can turn a reference-counted C type like the above into an automatically reference-counted  in Swift. Here‚Äôs how we would use it in the header:</p><div><div><pre><code></code></pre></div></div><p>Now,  gets imported like this:</p><div><div><pre><code></code></pre></div></div><p>The extra typealias is a little unexpected, but overall this is a huge improvement: Swift is treating  as a class, meaning that it automatically manages retains and releases for you! This is both an ergonomic win (less code to write) and a safety win, because it‚Äôs eliminated the possibility of mismanaging these instances.</p><p>There‚Äôs one more thing: when dealing with reference-counting APIs, you need to know whether a particular function that returns an object is expecting you to call ‚Äúrelease‚Äù when you‚Äôre done. In the WebGPU header, this information is embedded in a comment:</p><div><div><pre><code></code></pre></div></div><p>‚ÄúReturnedWithOwnership‚Äù here means that the result of the call has already been retained one extra time, and the caller is responsible for calling ‚Äúrelease‚Äù when they are done with it. The  header has a  macro that expresses this notion. One can use it like this:</p><div><div><pre><code></code></pre></div></div><p>Now, Swift will balance out the retain that <code>wgpuDeviceCreateBindGroup</code> has promised to do by performing the extra release once you‚Äôre done using the object. Once these annotations are done, we‚Äôre all set with a more ergonomic and memory-safe API for this C library. There‚Äôs no need to ever call  or  yourself.</p><p>We‚Äôve hacked up our header again, so let‚Äôs undo that and move all of this out to API notes. To turn a type into a foreign reference type, we augment the  section of our API notes with the same information, but in YAML form:</p><div><div><pre><code></code></pre></div></div><p>That makes  import as a class type, with the given retain and release functions. We can express the ‚Äúreturns retained‚Äù behavior of the <code>wgpuDeviceCreateBindGroup</code> function like this:</p><div><div><pre><code></code></pre></div></div><p>That‚Äôs enums and classes, so now let‚Äôs tackle‚Ä¶ functions.</p><p>A typical function from , like this:</p><div><div><pre><code></code></pre></div></div><p>will come into Swift like this:</p><div><div><pre><code></code></pre></div></div><p>Note that  on each parameter, which means that we won‚Äôt use argument labels for anything when we call it:</p><div><div><pre><code></code></pre></div></div><p>That matches C, but it isn‚Äôt as clear as it could be in Swift. Let‚Äôs clean this up by providing a better name in Swift that includes argument labels. We can do so using  (also in ), like this:</p><div><div><pre><code></code></pre></div></div><p>Within the parentheses, we have each of the argument labels that we want (or  meaning ‚Äúno label‚Äù), each followed by a . This is how one describes a full function name in Swift. Once we‚Äôve made this change to the Swift name, the C function comes into Swift with argument labels, like this:</p><div><div><pre><code></code></pre></div></div><p>That makes the call site more clear and self-documenting:</p><div><div><pre><code></code></pre></div></div><p>There is more usable structure in this API. Note that the  function takes, as its first argument, an instance of . Most of the C functions in  are like this, because these are effectively functions that operate on their first argument. In a language that has methods, they would be methods. Swift has methods, so let‚Äôs make them methods!</p><div><div><pre><code></code></pre></div></div><p>There are three things to notice about this  string:</p><ul><li>It starts with , which tells Swift to make this function a member inside .</li><li>Let‚Äôs change the function name to , because we no longer need the  prefix to distinguish it from other ‚Äúwrite buffer‚Äù operations on other types.</li><li>The name of the first argument in parentheses is , which indicates that the  argument (in Swift) should be passed as that positional argument to the C function. The other arguments are passed in-order.</li></ul><p>Note that this also requires  to be imported as a , as we did earlier for . Once we‚Äôve done so, we get a much-nicer Swift API:</p><div><div><pre><code></code></pre></div></div><p>We‚Äôve hacked up the header again, but didn‚Äôt have to. In , you can put a  attribute on any entity. For , it would look like this (in the  section):</p><div><div><pre><code></code></pre></div></div><p> has a number of  functions that produce information about some aspect of a type. Here are two for the  type:</p><div><div><pre><code></code></pre></div></div><p>With the  tricks above, we can turn these into ‚Äúget‚Äù methods on , like this:</p><div><div><pre><code></code></pre></div></div><p>That‚Äôs okay, but it‚Äôs not what you‚Äôd do in Swift. Let‚Äôs go one step further and turn them into read-only computed properties. To do so, use the  prefix on the Swift name we define. We‚Äôll skip ahead to the YAML form that goes into API notes:</p><div><div><pre><code></code></pre></div></div><p>And now, we arrive at a nice Swift API:</p><div><div><pre><code></code></pre></div></div><p> can also be used to import a function that returns a new instance as a Swift initializer. For example, this function creates a new  (which we assume is getting imported as a  like we‚Äôve been doing above):</p><div><div><pre><code></code></pre></div></div><p>We can turn this into a Swift initializer, which is used to create a new object, using the same  syntax but where the method name is . Here is the YAML form that goes into API notes:</p><div><div><pre><code></code></pre></div></div><p>and here is the resulting Swift initializer:</p><div><div><pre><code></code></pre></div></div><p>Now, one can create a new  with the normal object-creation syntax, e.g.,</p><div><div><pre><code></code></pre></div></div><p>The WebGPU header defines its own Boolean type. I wish everyone would use C99‚Äôs  and be done with it, but alas, here are the definitions for WebGPUs Boolean types:</p><div><div><pre><code></code></pre></div></div><p>This means that  will come in to Swift as a . The two macros aren‚Äôt available in Swift at all: they‚Äôre ‚Äútoo complicated‚Äù to be recognized as integral constants. Even if they were available in Swift, it still wouldn‚Äôt be great because we want to use  and  for Boolean values in Swift, not  and .</p><p>To make  easier to use from Swift, we‚Äôre first going to map that typedef to its own  that stores the underlying , giving it an identity separate from . We can do this using a  API note within the  section of the file, like this:</p><div><div><pre><code></code></pre></div></div><p>Now, we get  imported like this:</p><div><div><pre><code></code></pre></div></div><p>To be able to use  and  literals with this new , we can write a little bit of Swift code that makes this type conform to the <a href=\"https://developer.apple.com/documentation/swift/expressiblebybooleanliteral\"><code>ExpressibleByBooleanLiteral</code></a> protocol, like this:</p><div><div><pre><code></code></pre></div></div><p>That‚Äôs it! Better type safety (you cannot confuse a  with any other integer value) and the convenience of Boolean literals in Swift.</p><p> describes a set of flags using a  of the  type (a 64-bit unsigned integer) along with a set of global constants for the different flag values. For example, here is the  flag type and some of its constants:</p><div><div><pre><code></code></pre></div></div><p>Similar to what we saw with ,  is a  of a  of a . There‚Äôs no type safety in this C API, and one could easily mix up these flags with, say, those of :</p><div><div><pre><code></code></pre></div></div><p>We can do better, by layering more structure for the Swift version of this API using the same  approach from . This goes into the  section of API notes:</p><div><div><pre><code></code></pre></div></div><p>Now,  comes in as its own :</p><div><div><pre><code></code></pre></div></div><p>The initializers let you create a  from a  value, and there is also a  property to get a  value out of a , so the raw value is always there‚Ä¶ but the default is to be type safe. Additionally, those global constants will come in as members of , like this:</p><div><div><pre><code></code></pre></div></div><p>This means that, if you‚Äôre passing a value of type , you can use the shorthand ‚Äúleading dot‚Äù syntax. For example:</p><div><div><pre><code></code></pre></div></div><p>Swift has dropped the common  prefix from the constants when it made them into members. However, the resulting names aren‚Äôt great. We can rename them by providing a  in the API notes file within the  section:</p><div><div><pre><code></code></pre></div></div><p>We can go one step further by making the  type conform to Swift‚Äôs <a href=\"https://developer.apple.com/documentation/swift/optionset\"></a> protocol. If we revise the API notes like this:</p><div><div><pre><code></code></pre></div></div><p>Now, we get the nice option-set syntax we expect in Swift:</p><div><div><pre><code></code></pre></div></div><p>Throughout , the  macro is used to indicate pointers that can be NULL. The implication is that any pointer that is not marked with  cannot be NULL. For example, here is the definition of  we used above:</p><div><div><pre><code></code></pre></div></div><p>The  indicates that it‚Äôs acceptable to pass a NULL pointer in as the  parameter. Clang already has <a href=\"https://clang.llvm.org/docs/AttributeReference.html#nullability-attributes\">nullability specifiers</a> to express this information. We could alter the declaration in the header to express that this parameter is nullable but the result type is never NULL, like this:</p><div><div><pre><code></code></pre></div></div><p>This eliminates the implicitly-unwrapped optionals () from the signature of the initializer, so we end up with one that explicitly accepts a  descriptor argument and always returns a new instance (never ):</p><div><div><pre><code></code></pre></div></div><p>Now, I did cheat by hacking the header. Instead, we can express this with API notes on the parameters and result type by extending the entry we already have for  like this:</p><div><div><pre><code></code></pre></div></div><p>To specific nullability of pointer parameters, one can identify them by position (where 0 is the first parameter to the function) and then specify whether the parameter should come into Swift as optional (, corresponds to ), non-optional (, corresponds to ) or by left unspecified as an implicitly-unwrapped optional (, corresponds to ). For the result type, it‚Äôs a little different: we specified the result type along with the nullability specifier, i.e., . The end result of these annotations is the same as the modified header, so we can layer nullability information on top of the header.</p><p> is about 6,400 lines long, and is regenerated from a database of the API as needed. Each of the WebGPU implementations seems to augment or tweak the header a bit. So, rather than grind through and manually do annotations, I wrote a little Swift script to ‚Äúparse‚Äù , identify its patterns, and generate  for most of what is discussed in this post. The entirety of the script is <a href=\"https://www.swift.org/assets/blog/improving-usability-of-c-libraries-in-swift/webgpu_apinotes.swift\">here</a>. It reads  from standard input and prints  to standard output.</p><p>Because  is generated, it has a very regular structure that we can pick up on via regular expressions. For example:</p><div><div><pre><code></code></pre></div></div><p>That‚Äôs enough to identify all of the enum types (so we can emit the <code>EnumExtensibility: closed</code> API notes), object types (to turn them into shared references), and functions (which get nicer names and such). The script is just a big  loop that applies the regexes to capture all of the various types and functions, then does some quick classification before printing out the API notes. The resulting API notes are <a href=\"https://www.swift.org/assets/blog/improving-usability-of-c-libraries-in-swift/WebGPU.apinotes\">in WebGPU.apinotes</a>, and the generated Swift interface after these API notes are applied is <a href=\"https://www.swift.org/assets/blog/improving-usability-of-c-libraries-in-swift/WebGPU.swiftinterface\">here</a>. You can run it with, e.g.,</p><div><div><pre><code>swift  webgpu_apinotes.swift &lt; webgpu.h\n</code></pre></div></div><p>This script full of regular expressions is, admittedly, a bit of a hack. A better approach for an arbitrary C header would be to use <a href=\"https://clang.llvm.org/docs/LibClang.html\"></a> to properly parse the headers. For WebGPU specifically, the webgpu-headers project contains a database from which the header is generated, and one could also generate API notes directly from that database. Regardless of how you get there, many C libraries have well-structured headers with conventions that can be leveraged to create safer, more ergonomic projections in Swift.</p><p>The techniques described in this post can be applied to just about any C library. To do so, I recommend setting up a small package like the one described here for WebGPU, so you can iterate quickly on example code to get a feel for how the Swift projection of the C API will work. The annotations might not get you all the way to the best Swift API, but they are a lightweight way to get most of the way there. Feel free to also extend the C types to convenience APIs that make sense in Swift, like I did above to make  conform to <code>ExpressibleByBooleanLiteral</code>.</p><p>A little bit of annotation work on your favorite C library can make for a safer, more ergonomic, more Swifty experience of working with that library.</p><p>The regular structure of  helped considerably when trying to expose the API nicely in Swift. That said, there are a few ways in which  could be improved to require less annotation for this purpose:</p><ul><li><p> would be slightly nicer if placed on the  itself, rather than on the . If it were there, we could use</p><div><div><pre><code></code></pre></div></div><p>and not have to generate any API notes to bring these types in as proper enums in Swift.</p></li><li><p> could provide the names of the retain and release operations and be placed on the  itself. If it were there, we could use</p><div><div><pre><code></code></pre></div></div><p>and not have to generate any API notes to bring these types in as classes in Swift.</p></li><li><p> could be placed on the pointer itself (i.e., after the ) rather than at the beginning of the type, to match the position of <a href=\"https://clang.llvm.org/docs/AttributeReference.html#nullability-attributes\">Clang‚Äôs nullability attributes</a>. If it were placed there, then</p><div><div><pre><code></code></pre></div></div><p>would work with Clangs‚Äô longstanding nullable-types support. Swift would then import such pointers as optional types (with ). Moreover, if some macros <code>WGPU_ASSUME_NONNULL_BEGIN</code> and  were placed at the beginning and end of the header, they could be mapped to Clang‚Äôs pragmas to assume that any pointer not marked ‚Äúnullable‚Äù is always non-null:</p><div><div><pre><code></code></pre></div></div><p>This would eliminate all of the implicitly unwrapped optionals (marked  in the Swift interface), making it easier to use safely.</p></li></ul>",
      "contentLength": 18343,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qkpf5z/improving_the_usability_of_c_libraries_in_swift/"
    },
    {
      "title": "Diff√©rence OpenShift Sandbox et OpenShift Complet Version",
      "url": "https://chatgpt.com/share/69734747-dff4-8003-9f9d-71c44a568d1f",
      "date": 1769170810,
      "author": "/u/Ill-Maize-2343",
      "guid": 38522,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/kubernetes/comments/1qkpbvl/diff%C3%A9rence_openshift_sandbox_et_openshift_complet/"
    },
    {
      "title": "Building a small tool to visualize Kubernetes RBAC ‚Äî need feedback",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qkp8dz/building_a_small_tool_to_visualize_kubernetes/",
      "date": 1769170523,
      "author": "/u/Mobile_Theme_532",
      "guid": 38266,
      "unread": true,
      "content": "<p>Hey folks, I‚Äôm building a small MVP called **KubeScope** to help understand Kubernetes RBAC faster.</p><p>* Upload RBAC snapshot (.json / .zip)</p><p>* Show totals (Subjects / Roles / Bindings)</p><p>* Detect risky permissions like cluster-admin, wildcard \\*, secrets access, pods/exec, rolebinding create/update</p><p>Next I‚Äôm building an **RBAC Map** view (Subject ‚Üí Binding ‚Üí Role ‚Üí Permissions).</p><p>**Question:** What‚Äôs the most painful RBAC problem you‚Äôve faced in real clusters?</p><p>Would love suggestions on rules/features to add.</p>",
      "contentLength": 514,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Help me review my realtime chat app tech stack (Go + Centrifugo + Redis)",
      "url": "https://www.reddit.com/r/golang/comments/1qkocb0/help_me_review_my_realtime_chat_app_tech_stack_go/",
      "date": 1769167654,
      "author": "/u/Intrepid_Cover_9410",
      "guid": 38251,
      "unread": true,
      "content": "<p>I‚Äôm building a realtime group chat app and want feedback on my backend stack before committing. Stack: Go (API + auth + business logic) Centrifugo (WebSocket realtime) Redis (pub/sub + presence + caching) PostgreSQL (messages + groups + users) Hetzner VPS (self-hosted) Docker + Nginx (deployment + reverse proxy) Is this a solid approach for a production chat app? Any improvements or missing pieces?</p><p>My main goal is to handle around 50k total downloads and at least 10k active concurrent users smoothly, without message delays, lag, or stability issues during traffic spikes, while keeping infrastructure costs predictable and avoiding major rework</p>",
      "contentLength": 651,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "A very serious attempt is being made to fix DX12 on Linux!",
      "url": "https://www.reddit.com/r/linux/comments/1qko9jn/a_very_serious_attempt_is_being_made_to_fix_dx12/",
      "date": 1769167393,
      "author": "/u/lajka30",
      "guid": 38265,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "OneTalker - An Augmentative and Alternative Communication (AAC) app written in Rust",
      "url": "https://www.reddit.com/r/rust/comments/1qknxzz/onetalker_an_augmentative_and_alternative/",
      "date": 1769166284,
      "author": "/u/MissionNo4775",
      "guid": 38370,
      "unread": true,
      "content": "<p>I'm happy to announce that the first ever version of <a href=\"https://onetalker.org\">OneTalker</a> is out!</p><p>I wrote it for my son Ben, who is a full-time wheelchair user and has Quadriplegic Cerebral Palsy.</p><p>Ben DOES NOT tolerate slow things, and this absolutely MUST NOT crash either!</p><p>His current Augmentative and Alternative Communication apps are slow, so he doesn't like using them. I hope others find it useful too.</p><p>I think it's first AAC app in the world written in Rust.</p><p>For those interested, I'd love it if you could test it. I'm working on getting all the packages signed at moment. Thanks!</p>",
      "contentLength": 556,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Weekly: Share your victories thread",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qknvb9/weekly_share_your_victories_thread/",
      "date": 1769166031,
      "author": "/u/gctaylor",
      "guid": 38253,
      "unread": true,
      "content": "<p>Got something working? Figure something out? Make progress that you are excited about? Share here!</p>",
      "contentLength": 98,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Automated code review tools",
      "url": "https://www.reddit.com/r/golang/comments/1qknkrf/automated_code_review_tools/",
      "date": 1769164963,
      "author": "/u/Last-Prior-5525",
      "guid": 38218,
      "unread": true,
      "content": "<p>We are currently looking into incorporating more automated tools in our code review process - particularly around Go best practices (the general spirit is the <a href=\"https://google.github.io/styleguide/go/\">Google style guide</a>). We already have the basics - golangci-lint as well as cursor bugbot - but I'm more interested in code structure issues (proper dependency injection, usage of interfaces, http best practices).</p><p>I'd love to hear any advice from own experience.</p>",
      "contentLength": 419,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "I don‚Äôt think using AI for surveillance of kids in school is a good idea",
      "url": "https://www.reddit.com/r/artificial/comments/1qknhjn/i_dont_think_using_ai_for_surveillance_of_kids_in/",
      "date": 1769164622,
      "author": "/u/No_Turnip_1023",
      "guid": 38252,
      "unread": true,
      "content": "<p>There's this post on <a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7417445441904041984/?originTrackingId=Z6qpzUgvik0Gj9vyJWYR7Q%3D%3D\">Linkedin</a>, where they demonstarte an \"experiment\". This is how they define it: \"We tried to build an AI vision model which can tell, in real time, which students are attentive and which ones are distracted in a classroom.\"</p><p>\"... (this) AI computer vision SaaS originally designed to monitor factories and offices. We tried to use the AI monitoring application inside our classroom. Just for fun, honestly.\"</p><p>Notice the words, \"just for fun\". You just built a system for surveillance of kids in schools.... for FUN.</p><p>They justify this by highlighting a positive use case: this tech will provide feedback to teachers.</p><p>This is a great example of tech not being the problem, but how people use it.</p><p>If they really wanted to use AI to improve education, why not build a AI powered personalized education system. But no, a surveillance system is what came to their minds.</p><p>School is suffocating enough as it is. Now people are using AI amplify it. If anything, we could do with less of it in schools, make them more open.</p>",
      "contentLength": 1024,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Whosthere: A LAN discovery tool with a modern TUI, written in Go",
      "url": "https://www.reddit.com/r/golang/comments/1qknfeu/whosthere_a_lan_discovery_tool_with_a_modern_tui/",
      "date": 1769164402,
      "author": "/u/Raya_98",
      "guid": 38216,
      "unread": true,
      "content": "<p>I've been working on a LAN discovery tool with a Terminal User Interface (TUI) written entirely in Go. It's called , and it's designed to help you explore devices on your local network without requiring elevated privileges.</p><p>It works by combining several discovery methods:</p><ul><li>ARP cache reading (after triggering ARP resolution via TCP/UDP sweeps)</li><li>OUI lookups to identify device manufacturers</li></ul><ul><li>A fast, keyboard-driven TUI (powered by <a href=\"https://github.com/rivo/tview\">tview</a>)</li><li>An optional built-in port scanner</li><li>Daemon mode with a simple HTTP API to fetch devices</li><li>Configurable theming and behavior via a YAML config file</li></ul><p> Mainly to learn, I've been programming in Go for about a year now and wanted to combine learning Go with learning more about networking in one single project. I've always been a big fan of TUI applications like lazygit, k9s, and dive. And then the idea came to build a TUI application that shows devices on your LAN. I am by no means a networking expert, but it was fun to figure out how ARP works, and discovery protocols such as mDNS and SSDP.</p><pre><code># install via HomeBrew brew tap ramonvermeulen/whosthere brew install whosthere # or with go install go install github.com/ramonvermeulen/whosthere@latest # run as TUI whosthere # run as daemon whosthere daemon --port 8080 </code></pre><p>I'd love to hear your feedback, if you have ideas for additional features or improvements that is highly appreciated! Current platform support is Linux and MacOS.</p>",
      "contentLength": 1402,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "CKB ‚Äî A code intelligence server written in Go (SCIP-based, 80+ query tools via MCP)",
      "url": "https://www.reddit.com/r/golang/comments/1qkmug3/ckb_a_code_intelligence_server_written_in_go/",
      "date": 1769162314,
      "author": "/u/Maleficent-Sun9141",
      "guid": 38217,
      "unread": true,
      "content": "<p>I built CKB in Go ‚Äî it indexes your codebase using SCIP and exposes 80+ code intelligence queries through CLI, HTTP API, and MCP (Model Context Protocol for AI assistants).</p><p>CKB turns your repo into a queryable knowledge base. You ask structured questions about your code ‚Äî symbol lookup, call graphs, reference tracing, impact analysis ‚Äî and get precise answers instead of grepping around.</p><p>ckb query call-graph --symbol \"ProcessOrder\" --direction callers</p><p>ckb query impact --symbol \"UserService.Create\"</p><p>ckb query affected-tests --path internal/auth/</p><p>ckb arch --format=human ```</p><ul><li>Single binary, zero runtime dependencies</li><li>Fast indexing ‚Äî SCIP parsing + SQLite storage</li><li>Concurrent backend orchestration (SCIP, LSP, Git backends queried in parallel)</li><li>Bubble-free deployment ‚Äî , Homebrew, npm wrapper, or Docker</li><li>amazingly easy to build tools with &lt;3 </li></ul><p><code> CLI / HTTP API / MCP Server ‚Üì Query Engine (internal/query/) ‚Üì Backend Orchestrator ‚Üì SCIP | LSP | Git backends ‚Üì SQLite storage layer </code></p><p>The query engine uses a three-tier cache (query ‚Üí view ‚Üí negative) and a \"backend ladder\" that tries SCIP first, falls back to LSP, then Git-based heuristics. Results are merged using configurable strategies and compressed to fit LLM response budgets.</p><h2>Interesting Go patterns used</h2><ul><li><strong>Fingerprint-based symbol identity</strong> ‚Äî symbols get stable IDs () that survive renames via alias chains</li><li> for cyclomatic/cognitive complexity scoring</li><li> for long-running MCP operations</li><li><strong>Response budget enforcement</strong> ‚Äî output is compressed/truncated to fit token limits with drilldown suggestions for truncated results</li></ul><h2>Supported languages (for indexing)</h2><p>Go, TypeScript, Python, Rust, Java, Kotlin, C++, Dart, Ruby, C#</p><p>go install github.com/SimplyLiz/CodeMCP/cmd/ckb@latest</p><p>brew tap SimplyLiz/ckb &amp;&amp; brew install ckb</p><p>npm install -g @tastehub/ckb</p><p>ckb init &amp;&amp; ckb index ```</p><p>Feedback on the architecture or API design welcome. Happy to discuss the SCIP integration or the backend orchestration approach if anyone's curious.</p>",
      "contentLength": 1970,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "[R] Advice regarding CVPR Rebuttal",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qkm7y2/r_advice_regarding_cvpr_rebuttal/",
      "date": 1769159955,
      "author": "/u/Forsaken-Order-7376",
      "guid": 38358,
      "unread": true,
      "content": "<p>Received reviews 5(3),3(4),2(3). Assume that- Case 1. None of the reviewers increase their score Case 2. One of the reviewers increases his score, giving 5(3),3(4),3(3).</p><p>In both the cases, what are my chances of getting an acceptance? I plan to withdraw and submit to another conference if the chances of acceptance appear slim</p>",
      "contentLength": 326,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Khronos released VK_EXT_descriptor_heap",
      "url": "https://github.com/KhronosGroup/Vulkan-Docs/commit/87e6442f335fc08453b38bbd092ca67c57bfd3ab",
      "date": 1769155153,
      "author": "/u/lajka30",
      "guid": 38405,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qkkxv2/khronos_released_vk_ext_descriptor_heap/"
    },
    {
      "title": "Help choosing a distributed storage solution",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qkk6j0/help_choosing_a_distributed_storage_solution/",
      "date": 1769152320,
      "author": "/u/NASAonSteroids",
      "guid": 38195,
      "unread": true,
      "content": "<p>I‚Äôm running a small 3 node cluster using mini PCs for my home lab for things like Nextcloud, databases, and other services that require persistent storage. Currently everything is creating persistent claims on my main NAS via NFS but too many times I‚Äôve had unexpected downtime because the NAS decided to break. I‚Äôm wanting to replicate identical data across drives in my cluster for high availability and redundancy. What would be the best way to handle this? </p><p>All three are equipped with a i5-7500, 32Gi RAM, 256 NVMe drive, a 1T SATA SSD intended to be the replicated disk, and connected to a 1Gbe switch as they don‚Äôt have any faster NICs installed. I‚Äôve looked into Longhorn and Ceph but both highly recommend 10Gbe but tha is not possible for me. I‚Äôve looked at Minio/Garage but that would only allow S3 which feels limiting (though I don‚Äôt have a lot of experience with object storage so I may be naive in my thinking)</p>",
      "contentLength": 938,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Underground Resistance Aims To Sabotage AI With Poisoned Data",
      "url": "https://www.forbes.com/sites/craigsmith/2026/01/21/poison-fountain-and-the-rise-of-an-underground-resistance-to-ai/",
      "date": 1769139259,
      "author": "/u/RNSAFFN",
      "guid": 38165,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qkfxlz/underground_resistance_aims_to_sabotage_ai_with/"
    },
    {
      "title": "White House posts digitally altered image of woman arrested after ICE protest",
      "url": "https://www.theguardian.com/us-news/2026/jan/22/white-house-ice-protest-arrest-altered-image",
      "date": 1769123437,
      "author": "/u/esporx",
      "guid": 38124,
      "unread": true,
      "content": "<p>The White House posted a digitally altered image of a woman who was arrested on Thursday in a case touted by the US attorney general, <a href=\"https://www.theguardian.com/us-news/pam-bondi\" data-link-name=\"in body link\">Pam Bondi</a>, to make it seem as if she was dramatically crying, a Guardian analysis of the image has found.</p><p>The woman, Nekima Levy Armstrong, also appears to have darker skin in the altered image. Armstrong was <a href=\"https://www.theguardian.com/us-news/2026/jan/22/two-arrests-minnesota-church-protest\" data-link-name=\"in body link\">one of three people arrested</a> on Thursday in connection to a demonstration that disrupted church services in St Paul, Minnesota, on Sunday. Demonstrators alleged that one of the pastors, David Easterwood, was the acting field director of the St Paul Immigration and Customs Enforcement (ICE) office. Bondi announced the arrests on social media on Thursday morning.</p><p>The homeland security secretary, <a href=\"https://www.theguardian.com/us-news/kristi-noem\" data-link-name=\"in body link\">Kristi Noem</a>, posted an image of Armstrong‚Äôs arrest at 10.21am on Thursday, less than an hour after Bondi‚Äôs announcement. The image shows a law enforcement agent, face blurred out, escorting Armstrong, who appears to be handcuffed. Armstrong, dressed in all black, appears to be composed in the picture.</p><p>A little more than 30 minutes later, the White House posted another image of Armstrong‚Äôs arrest in which she is crying. The White House press secretary, Karoline Leavitt, reposted the image. The image posted by the White House is altered, a Guardian analysis found.</p><p>The Guardian overlaid the White House photo with the Noem photo and found that the law enforcement agents in both pictures line up exactly, confirming they are the same image. There are other similarities between the photos. An unidentified person can be seen in the same place behind the arresting agent. And the arresting agent‚Äôs arm appears to be behind Armstrong‚Äôs back in exactly the same position.</p><p>Asked whether the image had been digitally altered, the White House responded by sending a post on X from Kaelan Dorr, the deputy communications director.</p><p>‚ÄúYET AGAIN to the people who feel the need to reflexively defend perpetrators of heinous crimes in our country I share with you this message: Enforcement of the law will continue. The memes will continue. Thank you for your attention to this matter,‚Äù <a href=\"https://x.com/Kaelan47/status/2014410500096856358?s=20\" data-link-name=\"in body link\">he said</a>.</p><p>The White House X account, which has around 3.5 million followers, has made at least 14 posts with AI since the start of Trump‚Äôs second term, <a href=\"https://www.poynter.org/fact-checking/2025/trump-white-house-ai-political-messaging/\" data-link-name=\"in body link\">Poynter reported in October</a>.</p><p><em>Julius Constantine Motal and David McCoy contributed reporting</em></p>",
      "contentLength": 2369,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qk9x1y/white_house_posts_digitally_altered_image_of/"
    },
    {
      "title": "The Rust GCC backend can now be installed with rustup",
      "url": "https://www.reddit.com/r/rust/comments/1qk9t1t/the_rust_gcc_backend_can_now_be_installed_with/",
      "date": 1769123167,
      "author": "/u/imperioland",
      "guid": 38213,
      "unread": true,
      "content": "<p>Starting tomorrow (23rd of January 2026), you will be able (on linux without cross-compilation) to install and use the Rust GCC backend directly from rustup! To do so:</p><p><code> rustup component add rustc-codegen-gcc </code></p><p>Thanks a lot to Kobzol for all their work to making it a reality!</p>",
      "contentLength": 272,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Incredibly detailed isometric map of NYC (made with Qwen-Image-Edit)",
      "url": "https://cannoneyed.com/isometric-nyc/",
      "date": 1769122947,
      "author": "/u/WavierLays",
      "guid": 38148,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qk9pqe/incredibly_detailed_isometric_map_of_nyc_made/"
    },
    {
      "title": "awesome-linuxaudio v1.0.0 - A list of software and resources for Linux audio/video/live production",
      "url": "https://github.com/nodiscc/awesome-linuxaudio/releases/tag/1.0.0",
      "date": 1769121504,
      "author": "/u/vegetaaaaaaa",
      "guid": 38273,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qk94lu/awesomelinuxaudio_v100_a_list_of_software_and/"
    },
    {
      "title": "Where does Rust break down?",
      "url": "https://www.reddit.com/r/rust/comments/1qk8qt7/where_does_rust_break_down/",
      "date": 1769120507,
      "author": "/u/PointedPoplars",
      "guid": 38162,
      "unread": true,
      "content": "<p>As a preface, Rust is one of my favorite languages alongside Python and C.</p><p>One of the things I appreciate most about Rust is how intentionally it is designed around abstraction: e.g. function signatures form strict, exhaustive contracts, so Rust functions behave like true black boxes.</p><p>But all abstractions have leaks, and I'm sure this is true for Rust as well.</p><p>For example, Python's `len` function has to be defined as a magic method instead of a normal method to avoid exposing a lot of mutability-related abstractions.</p><p>As a demonstration, assigning `fun = obj.__len__` will still return the correct result when `fun()` is called after appending items to `obj` if `obj` is a list but not a string. This is because Python strings are immutable (and often interned) while its lists are not. Making `len` a magic method enforces late binding of the operation to the object's current state, hiding these implementation differences in normal use and allowing more aggressive optimizations for internal primitives.</p><p>A classic example for C would be that `i[arr]` and `arr[i]` are equivalent because both are syntactic sugar for `*(arr+i)`</p><p>TLDR: What are some abstractions in Rust that are invisible to 99% of programmers unless you start digging into the language's deeper mechanics?</p>",
      "contentLength": 1273,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "[Help] with with K3S + Traefik + Gateway API + TCP/UDPRoutes",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qk84jk/help_with_with_k3s_traefik_gateway_api/",
      "date": 1769119065,
      "author": "/u/Leather_Week_860",
      "guid": 38137,
      "unread": true,
      "content": "<p>I am playing with K3S to try and learn a bit of Kubernetes. Have set up a Fedora VM with K3S, and as per recent docs I am trying to set up the Gateway API, which is supposed to replace Ingress.</p><p>K3S comes with Traefik installed via Helm, and as per their docs \"you should customize Traefik by creating an additional HelmChartConfig manifest in /var/lib/rancher/k3s/server/manifests\". Following Traefik's docs, I created such a file to enable the Gateway API, disable Ingress, and then enable Traefik's dashboard and create an HTTPRoute for it:</p><p>Now, I want to be able to create not only HTTPRoutes but also TCPRoutes and UDPRoutes, as I am trying to set up Syncthing as a deployment in the environment.</p><pre><code>[...] # Enable Gateway API and disable Ingress providers: kubernetesGateway: enabled: true experimentalChannel: true kubernetesIngress: enabled: false kubernetesCRD: enabled: true [...] </code></pre><p>Helm reloads Traefik just fine, but when I try to create a TCPRoute or UDPRoute, I keep getting this error:</p><pre><code>Error: INSTALLATION FAILED: unable to build kubernetes objects from release manifest: [resource mapping not found for name: \"syncthing-tcp\" namespace: \"syncthing\" from \"\": no matches for kind \"TCPRoute\" in version \"gateway.networking.k8s.io/v1alpha2\" ensure CRDs are installed first, resource mapping not found for name: \"syncthing-udp\" namespace: \"syncthing\" from \"\": no matches for kind \"UDPRoute\" in version \"gateway.networking.k8s.io/v1alpha2\" ensure CRDs are installed first, resource mapping not found for name: \"syncthing-discovery\" namespace: \"syncthing\" from \"\": no matches for kind \"UDPRoute\" in version \"gateway.networking.k8s.io/v1alpha2\" ensure CRDs are installed first] helm.go:92: 2026-01-22 18:07:48.516328647 +0100 CET m=+0.768768674 [debug] [resource mapping not found for name: \"syncthing-tcp\" namespace: \"syncthing\" from \"\": no matches for kind \"TCPRoute\" in version \"gateway.networking.k8s.io/v1alpha2\" ensure CRDs are installed first, resource mapping not found for name: \"syncthing-udp\" namespace: \"syncthing\" from \"\": no matches for kind \"UDPRoute\" in version \"gateway.networking.k8s.io/v1alpha2\" ensure CRDs are installed first, resource mapping not found for name: \"syncthing-discovery\" namespace: \"syncthing\" from \"\": no matches for kind \"UDPRoute\" in version \"gateway.networking.k8s.io/v1alpha2\" ensure CRDs are installed first] unable to build kubernetes objects from release manifest </code></pre><p>I have tried many things, but nothing seems to work. I don't want to mess up with how K3S installs Traefik, but not sure what to try. Any ideas?!</p>",
      "contentLength": 2550,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Is there a common API response schema to follow?",
      "url": "https://www.reddit.com/r/golang/comments/1qk74bq/is_there_a_common_api_response_schema_to_follow/",
      "date": 1769116722,
      "author": "/u/m477h145h3rm53n",
      "guid": 38100,
      "unread": true,
      "content": "<p>I tried implementing this API response schema</p><p>``<code>golang type Response[T any] struct { IsSuccessful bool</code>json:\"isSuccessful\"json:\"data,omitempty\"json:\"errorMessage,omitempty\"` }</p><p>func NewSuccessResponse[T any](data T) Response[T] { return Response[T]{ IsSuccessful: true, Data: &amp;data, } }</p><p>func NewEmptySuccessResponse[T any]() Response[T] { return Response[T]{ IsSuccessful: true, Data: nil, } }</p><p>func NewFailureResponse[T any](errorMessage string) Response[T] { return Response[T]{ IsSuccessful: false, ErrorMessage: errorMessage, Data: nil, } } ```</p><p>but maybe I don't have to reinvent a structure. Is there a popular one I could follow?</p>",
      "contentLength": 627,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "So, why *should* GNOME support server side decorations?",
      "url": "https://blister.zip/posts/gnome-ssd/",
      "date": 1769115695,
      "author": "/u/symbolicard",
      "guid": 38250,
      "unread": true,
      "content": "<p><em>Cet article est aussi disponible en <a href=\"https://blister.zip/fr/posts/gnome-ssd/\">fran√ßais</a>.</em></p><p>This article contains quite a few technical terms, which I will explain these in the following paragraphs, those that are already familiar with these terms may skip to the next section. A basic understanding of linux and it‚Äôs desktop environments is assumed.</p><p>Server side decorations (SSD) is the term for when when the application‚Äôs titlebar is drawn by the system and client side decorations (CSD) is the term for when the applications draws it‚Äôs own titlebar. KDE prefers the former, while GNOME prefers the latter. KDE and most other desktop environments supports both, while GNOME only supports CSD.</p><p>Since SSD are drawn by the desktop, instead of by the application, apps that use it have a seperate titlebar (bottom), while apps that are designed around CSD tend to have an integrated titlebar (top).</p><figure><img src=\"https://blister.zip/posts/gnome-ssd/csd-vs-ssd.png\" alt=\"seperate titlebar (bottom) vs integrated titlebar (top)\"><figcaption><p>seperate titlebar (bottom) vs integrated titlebar (top)</p></figcaption></figure><em>Image credit: Tobias Bernard</em><p>I will refer to the mutter compositor as GNOME, compositors in general as ‚Äúthe system‚Äù and gtk4/libadwaita apps as GNOME apps, for simplicity‚Äôs sake.</p><p>Without further ado, let‚Äôs get to the titular question.</p><h2>Why  GNOME support SSD?<a href=\"https://blister.zip/posts/gnome-ssd/#why-should-gnome-support-ssd\" arialabel=\"Anchor\">#</a></h2><h4>Many apps handle CSD poorly.<a href=\"https://blister.zip/posts/gnome-ssd/#many-apps-handle-csd-poorly\" arialabel=\"Anchor\">#</a></h4><p>Certain applications work poorly without SSD. These apps end up having either no titlebar or a weird one, and the user can‚Äôt move or resize them easily. Davinci Resolve is one notable example, but it‚Äôs not the only one.</p><p>GNOME is a large enough part of the linux desktop that most applications designed for SSD decorations implement a workaround in order to not be broken on GNOME. Usually, that workaround is <a href=\"https://github.com/neonkore/libdecor\">libdecor</a>.</p><p>This sounds fine enough, except libdecor is the  of both worlds because you get the space inefficiency and lack of integration of a traditional titlebar, with all the inconsistency and lack of user customization of an integrated titlebar.</p><figure><img src=\"https://blister.zip/posts/gnome-ssd/libdecor-inconsistency.png\" alt=\"Three applications with visually distinct libdecor-provided titlebars\"><figcaption><p>Three applications with visually distinct libdecor-provided titlebars</p></figcaption></figure><p>This is not the fault of the libdecor devs, it‚Äôs just because it‚Äôs a bandaid fix for a wider issue.</p><p>Supporting SSD would solve this, as every application that does not wish to use CSD would have a nice native titlebar with consistent shading, colors, and corner radius, and developers would have to deal with <a href=\"https://factorio.com/blog/post/fff-408\">less work and frustration</a>.</p><h4>Supporting SSD would make apps ‚ÄúFeel more native‚Äù<a href=\"https://blister.zip/posts/gnome-ssd/#supporting-ssd-would-make-apps-feel-more-native\" arialabel=\"Anchor\">#</a></h4><p>On windows and macOS, any application can implement CSD in their application and have it respect the spacing, position and look of the titlebar buttons that the SSD equivalents that those platforms provide.</p><p>The issue with doing that on linux is that the design of the titlebar can vary so wildy depending on the environment that having CSD‚Äôs that ‚Äúfeel native‚Äù is a bit of a losing battle. Offering the option of SSD for non-GNOME apps on GNOME and vice versa would go a long way to making applications feel more native for users that value that sort of thing.</p><h4>Many users of other desktops want their SSD respected.<a href=\"https://blister.zip/posts/gnome-ssd/#many-users-of-other-desktops-want-their-ssd-respected\" arialabel=\"Anchor\">#</a></h4><p>A lot of users on other desktops are frustrated by GNOME apps not supporting SSD, breaking if SSD is force-enabled.</p><p>While an app breaking when users do something unsupported is obviously not the developer‚Äôs fault, GNOME apps optionally supporting SSD would make many users of other desktops very happy indeed.</p><p>If you‚Äôre wondering why a user would want SSD applied to a GNOME app, some people value titlebar consistency over the design of any individual app. <a href=\"https://discuss.kde.org/t/i-believe-that-server-side-decorations-are-an-accessibility-feature-which-we-are-on-the-verge-of-losing/9529\">Some users</a> view SSD as an accessibility feature as well.</p><h4>GNOME‚Äôs lack of support hurts the linux desktop.<a href=\"https://blister.zip/posts/gnome-ssd/#gnomes-lack-of-support-hurts-the-linux-desktop\" arialabel=\"Anchor\">#</a></h4><p>Linux is already a small market, and GNOME not supporting the de facto standard that is  only hurts the linux desktop by increasing fragmentation.</p><p>This fragmentation only makes the linux desktop less attractive to developers and, importantly, makes application developers <strong>less likely to adopt more modern standards such as wayland</strong>.</p><h2>The arguments against SSD support<a href=\"https://blister.zip/posts/gnome-ssd/#the-arguments-against-ssd-support\" arialabel=\"Anchor\">#</a></h2><p>Many discussions about why GNOME should implement SSD stop at the arguments for it, but obviously there are also arguments against supporting SSD, otherwise GNOME would have implemented them by now.</p><p>This is the main argument the GNOME developers use to justify why they don‚Äôt support SSD. This is true,  is an ‚Äúunstable‚Äù protocol, and wayland was originally designed with only CSD in mind. However, wayland was also initally designed without screenshare or global keybinds, standards that GNOME had since adopted.</p><p>The standard doesn‚Äôt pose the same kind of security or design issues that standards like the system tray one do, either.</p><p>The fact is that it‚Äôs adopted by <a href=\"https://wayland.app/protocols/xdg-decoration-unstable-v1\">every production-ready desktop compositor</a> other than GNOME‚Äôs mutter, and is relied upon by application and desktop developers alike, making it a de facto standard that is widely adopted.</p><p>That is to say, while  is technically out of spec, the only thing distinguishing it from any wayland protocol that is ‚Äúin spec‚Äù is GNOME‚Äôs lack of support for it.</p><h4>‚ÄúWindow decorations are part of the app, and thus shouldn‚Äôt be the purview of desktop.‚Äù<a href=\"https://blister.zip/posts/gnome-ssd/#window-decorations-are-part-of-the-app-and-thus-shouldnt-be-the-purview-of-desktop\" arialabel=\"Anchor\">#</a></h4><p>Many developers and users see the titlebar as something that is a part of the desktop, while some others disagree. This isn‚Äôt a problem, just a difference in philosophy.</p><p>The real problem is the idea that GNOME project shouldn‚Äôt cater to the first group. It would be like GNOME not supporting  and saying that each app  ship their own file picker. But GNOME does support it, and only apps that wish to implement their own file picker do so.</p><p>Since both approaches are used, and liked, miscellaneous advantages and disadvantages of either approach are irrelevant, and so are other arguments pertaining to design. This is why I haven‚Äôt brought them up.</p><p>Hopefully I have made my point as to why it would be worthwhile for GNOME to adopt server side decorations, but a question still remains.</p><h4>What would it look like?<a href=\"https://blister.zip/posts/gnome-ssd/#what-would-it-look-like\" arialabel=\"Anchor\">#</a></h4><p>I have touched on what an implementation of SSD would look like a few times throughout this article without actually fully explaining what it would look like. So.</p><p>Obviously GNOME would implement the relevant protocols and enable server side decorations on all application that don‚Äôt explicitely request SSD. This is in order to have SSD applied to apps that in only provide CSD as a fallback for compositors that don‚Äôt support SSD.</p><p>To be clear, this  apply to apps that are designed for a united headerbar, like GNOME apps.</p><p>On other desktops, however, GNOME apps would still request to not have SSD applied, a preference that is respected by every mainstream desktop. The only difference is that they would allow users the option to force a titlebar on GNOME apps. In this scenario, the GNOME app would remove the integrated window title and controls.</p><p>This would be the best of both worlds, since GNOME users would enjoy more consistent titlebars and window shadows on third party apps and  on other desktops would gain the ability to have SSD on GNOME apps if they so wish.</p><p>GNOME‚Äôs lack of support for server side decorations is the single biggest issue with the GNOME desktop environment right now, in my opinion, so I sincerely hope this article can serve as a push to get it implemented.</p><p>If you want to help me with this, you can .</p>",
      "contentLength": 7148,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qk6o0i/so_why_should_gnome_support_server_side/"
    },
    {
      "title": "Help KDE Keep EU funding",
      "url": "https://www.reddit.com/r/linux/comments/1qk6icf/help_kde_keep_eu_funding/",
      "date": 1769115348,
      "author": "/u/lajka30",
      "guid": 38396,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Tree-sitter vs. LSP",
      "url": "https://lambdaland.org/posts/2026-01-21_tree-sitter_vs_lsp/",
      "date": 1769115260,
      "author": "/u/brightlystar",
      "guid": 38271,
      "unread": true,
      "content": "<p>I got asked a good question today: what is the difference between <a href=\"https://en.wikipedia.org/wiki/Tree-sitter_%28parser_generator%29\">Tree-sitter</a> and a <a href=\"https://en.wikipedia.org/wiki/Language_Server_Protocol\">language server</a>? I don‚Äôt understand how either of these tools work in depth, so I‚Äôm just going to explain from an ,  point of view.</p><p>Tree-sitter is a . What this means is that you can hand Tree-sitter a description for a programming language and it will create a program that will parse that language for you. What‚Äôs special about Tree-sitter is that it is a.) fast, and b.) can tolerate  in the input. These two properties make Tree-sitter ideal for creating syntax highlighting engines in text editors. When you‚Äôre editing a program,  the program will be in a syntactically invalid state. During that time, you don‚Äôt want your colors changing or just outright breaking while you‚Äôre typing. Na√Øve regex-based syntax highlighters frequently suffer from this issue.</p><p>Tree-sitter also provides a query language where you can make queries against the parse tree. I use this in the Emacs <a href=\"https://codeberg.org/ashton314/citar-typst\">package I‚Äôm trying to develop</a> to add <a href=\"https://typst.app/\">Typst</a> support to the <a href=\"https://github.com/emacs-citar/citar\">Citar</a> citation/bibliography tool: I can ask Tree-sitter to find a particular syntax object; it is safer and more robust than using a regular expression because it can do similar parsing to the Typst engine itself.</p><p>In short, Tree-sitter provides syntax highlighting that is faithful to how the language implementation parses the program, instead of relying on regular expressions that incidentally come close.</p><p>A  is a program that can analyze a program and report interesting information about that program to a text editor. A standard, called the <a href=\"https://en.wikipedia.org/wiki/Language_Server_Protocol\">Language Server Protocol (LSP)</a>, defines the kinds of JSON messages that pass between a text editor and the server. The protocol is an open standard; any language and any text editor can take advantage of the protocol to get nice smart programming helps in their system. Language servers can provide information like locating the definition of a symbol, possible completions at the cursor point, etc. to a text editor which can then decide how and when to display or use this information.</p><p>Language servers solve the ‚Äú\n\n problem‚Äù where \n programming languages and \n text editors would mean there have to be \n implementations for language analyzers. Now, every language just needs a language server, and every editor needs to be able to speak the LSP protocol.</p><p>Language servers are powerful because they can hook into the language‚Äôs runtime and compiler toolchain to get  answers to user queries. For example, suppose you have two versions of a  function, one imported from a  library, and another from a  library. If you use a tool like the <a href=\"https://github.com/jacktasia/dumb-jump\">dumb-jump</a> package in Emacs\nand you use it to jump to the definition for a call to , it might get confused as to where to go because it‚Äôs not sure what module is in scope at the point. A language server, on the other hand, should have access to this information and would not get confused.</p><h3>\n  Using a language server for highlighting\n  <a href=\"https://lambdaland.org/posts/2026-01-21_tree-sitter_vs_lsp/#using-a-language-server-for-highlighting\">#</a></h3><p>It  possible to use the language server for syntax highlighting. I am not aware of any particularly strong reasons why one would want to (or  want to) do this. The language server can be a more complicated program and so could surface particularly detailed information about the syntax; it might also be slower than tree-sitter.</p><p>Emacs‚Äô built-in LSP client, <a href=\"https://github.com/joaotavora/eglot\">Eglot</a>, recently added <code>eglot-semantic-tokens-mode</code> to support syntax highlighting as provided from the language server. I have tried this a little bit in Rust code and it seems fine; the Tree-sitter-based syntax highlighting has been working just fine for me, so I will probably stick to that unless I find a compelling reason to use the LSP-based highlighting.</p><div><p> Thanks to <a href=\"https://news.ycombinator.com/item?id=46721842\">a comment on HN</a>, I now know of a good reason why you would want to use a language server for syntax highlighting: the Rust language server <a href=\"https://rust-analyzer.github.io/\">rust-analyzer</a> can tell your text editor when a variable reference is mutable or not, which means you could highlight  references differently than non- ones. Thanks to <a href=\"https://davidbarsky.com\">David Barsky</a> for the tip!</p></div><p>I wrote all of the above article. I did not ask an LLM to generate any portion of it. Please know that whenever you read something on my blog, it comes 100% from a human‚Äîme, Ashton Wiersdorf.</p><p>I am not so anti-AI to say that LLMs are worthless or should never be used. I‚Äôve used LLMs a little bit. I think they‚Äôre fantastic at translating between languages; this seems to be something that they should be good at doing. They‚Äôre helpful at writing some boring parts of the code I write. However, most of the time I find that I can typically write the tricky bits of the code about as fast as I could specify to an LLM what I want.</p><p>I know that an LLM could have generated a facile pile of text much like the above, and honestly it would probably be decently helpful. However, know that what you have just read came directly from the fingers of a person who thought about the topic and bent his effort to helping you understand. This is from  human who understands the meaning behind each word here. I do not play games with syntax and generate answer-shaped blog posts. There is real meaning here. Enjoy it, and go forth and make more of it.</p>",
      "contentLength": 5165,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qk6gvw/treesitter_vs_lsp/"
    },
    {
      "title": "crates.io development update | Rust Blog - A new \"Security\" tab, migration to Svelte for the front-end, support for GitLab CI/CD Trusted Publishing, Lines of Code metrics",
      "url": "https://blog.rust-lang.org/2026/01/21/crates-io-development-update/",
      "date": 1769113494,
      "author": "/u/nik-rev",
      "guid": 38147,
      "unread": true,
      "content": "<p>Time flies! Six months have passed since our last crates.io development update, so it's time for another one. Here's a summary of the most notable changes and improvements made to <a href=\"https://crates.io/\">crates.io</a> over the past six months.</p><p>Crate pages now have a new \"Security\" tab that displays security advisories from the <a href=\"https://rustsec.org/\">RustSec</a> database. This allows you to quickly see if a crate has known vulnerabilities before adding it as a dependency.</p><p>The tab shows known vulnerabilities for the crate along with the affected version ranges.</p><p>This feature is still a work in progress, and we plan to add more functionality in the future. We would like to thank the <a href=\"https://openssf.org/\">OpenSSF</a> (Open Source Security Foundation) for funding this work and <a href=\"https://github.com/djc\">Dirkjan Ochtman</a> for implementing it.</p><h2><a href=\"https://blog.rust-lang.org/2026/01/21/crates-io-development-update/#trusted-publishing-enhancements\" aria-hidden=\"true\"></a>\nTrusted Publishing Enhancements</h2><p>In our July 2025 update, we announced Trusted Publishing support for GitHub Actions. Since then, we have made several enhancements to this feature.</p><p>Trusted Publishing now supports <a href=\"https://docs.gitlab.com/ee/ci/\">GitLab CI/CD</a> in addition to GitHub Actions. This allows GitLab users to publish crates without managing API tokens, using the same OIDC-based authentication flow.</p><p>Note that this currently only works with GitLab.com. Self-hosted GitLab instances are not supported yet. The crates.io implementation has been refactored to support multiple CI providers, so adding support for other platforms like Codeberg/Forgejo in the future should be straightforward. Contributions are welcome!</p><h3><a href=\"https://blog.rust-lang.org/2026/01/21/crates-io-development-update/#trusted-publishing-only-mode\" aria-hidden=\"true\"></a>\nTrusted Publishing Only Mode</h3><p>Crate owners can now enforce Trusted Publishing for their crates. When enabled in the crate settings, traditional API token-based publishing is disabled, and only Trusted Publishing can be used to publish new versions. This reduces the risk of unauthorized publishes from leaked API tokens.</p><p>The  and  GitHub Actions triggers are now blocked from Trusted Publishing. These triggers have been responsible for multiple security incidents in the GitHub Actions ecosystem and are not worth the risk.</p><p>Crate pages now display source lines of code (SLOC) metrics, giving you insight into the size of a crate before adding it as a dependency. This metric is calculated in a background job after publishing using the <a href=\"https://github.com/XAMPPRocky/tokei\">tokei</a> crate. It is also shown on OpenGraph images:</p><h2><a href=\"https://blog.rust-lang.org/2026/01/21/crates-io-development-update/#publication-time-in-index\" aria-hidden=\"true\"></a>\nPublication Time in Index</h2><p>A new  field has been added to crate index entries, recording when each version was published. This enables several use cases:</p><ul><li>Cargo can implement cooldown periods for new versions in the future</li><li>Cargo can replay dependency resolution as if it were a past date, though yanked versions remain yanked</li><li>Services like <a href=\"https://github.com/renovatebot/renovate\">Renovate</a> can determine release dates without additional API requests</li></ul><h2><a href=\"https://blog.rust-lang.org/2026/01/21/crates-io-development-update/#svelte-frontend-migration\" aria-hidden=\"true\"></a>\nSvelte Frontend Migration</h2><p>At the end of 2025, the crates.io team evaluated several options for modernizing our frontend and decided to experiment with porting the website to <a href=\"https://svelte.dev/\">Svelte</a>. The goal is to create a one-to-one port of the existing functionality before adding new features.</p><p>This migration is still considered experimental and is a work in progress. Using a more mainstream framework should make it easier for new contributors to work on the frontend. The new Svelte frontend uses TypeScript and generates type-safe API client code from our <a href=\"https://crates.io/api/openapi.json\">OpenAPI description</a>, so types flow from the Rust backend to the TypeScript frontend automatically.</p><p>Thanks to <a href=\"https://github.com/eth3lbert\">eth3lbert</a> for the helpful reviews and guidance on Svelte best practices. We'll share more details in a future update.</p><p>These were some of the more visible changes to crates.io over the past six months, but a lot has happened \"under the hood\" as well.</p><ul><li><p><strong>Cargo user agent filtering</strong>: We noticed that download graphs were showing a constant background level of downloads even for unpopular crates due to bots, scrapers, and mirrors. Download counts are now filtered to only include requests from Cargo, providing more accurate statistics.</p></li><li><p>: Emails from crates.io now support HTML formatting.</p></li><li><p>: OAuth access tokens from GitHub are now encrypted at rest in the database. While we have no evidence of any abuse, we decided to improve our security posture. The tokens were never included in the daily database dump, and the old unencrypted column has been removed.</p></li><li><p>: Crate pages now display a \"Browse source\" link in the sidebar that points to the corresponding docs.rs page. Thanks to <a href=\"https://github.com/carols10cents\">Carol Nichols</a> for implementing this feature.</p></li><li><p>: The sparse index at index.crates.io is now served primarily via Fastly to conserve our AWS credits for other use cases. In the past month, static.crates.io served approximately 1.6 PB across 11 billion requests, while index.crates.io served approximately 740 TB across 19 billion requests. A big thank you to Fastly for providing free CDN services through their <a href=\"https://www.fastly.com/fast-forward\">Fast Forward program</a>!</p></li><li><p><strong>OpenGraph image improvements</strong>: We fixed emoji and CJK character rendering in OpenGraph images, which was caused by missing fonts on our server.</p></li><li><p><strong>Background worker performance</strong>: Database indexes were optimized to improve background job processing performance.</p></li><li><p><strong>CloudFront invalidation improvements</strong>: Invalidation requests are now batched to avoid hitting AWS rate limits when publishing large workspaces.</p></li></ul><p>We hope you enjoyed this update on the development of crates.io. If you have any feedback or questions, please let us know on <a href=\"https://rust-lang.zulipchat.com/#narrow/stream/318791-t-crates-io\">Zulip</a> or <a href=\"https://github.com/rust-lang/crates.io/discussions\">GitHub</a>. We are always happy to hear from you and are looking forward to your feedback!</p>",
      "contentLength": 5278,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/rust/comments/1qk5p80/cratesio_development_update_rust_blog_a_new/"
    },
    {
      "title": "Debian Urgently Seeks Volunteers After Data Protection Team Resigns",
      "url": "https://linuxiac.com/debian-urgently-seeks-volunteers-after-data-protection-team-resigns/",
      "date": 1769111968,
      "author": "/u/CackleRooster",
      "guid": 38070,
      "unread": true,
      "content": "<p><a href=\"https://linuxiac.com/andreas-tille-is-the-new-debian-projects-leader/\">Andreas Tille, a Debian Project Leader</a>, recently sent an unexpected message to the Debian mailing lists, announcing that the project is urgently seeking new volunteers to rebuild its Data Protection Team after all current members stepped down, leaving the project without a dedicated group to handle privacy and data protection matters.</p><p>The Data Protection Team was established in 2018 in response to new European data protection legislation. Its role has been to act as a point of contact for external inquiries about what personal data the project holds and to advise Debian contributors on data protection obligations.</p><p>Additionally, the team was also responsible for drafting Debian‚Äôs public privacy policy and coordinating responses to data access and privacy-related requests.</p><p>Coincidence or not, all three team members have now resigned simultaneously. So, Tille formally revoked their delegation and thanked them for their work over the past years. With their departure, the team currently has no active members.</p><blockquote><p><em>‚ÄúThe fact that all team members have stepped back at the same time shouldmake it clear that we urgently need new volunteers to fulfil this role.‚Äù</em></p></blockquote><p>According to the message, despite a constructive discussion on the topic during the most recent DebConf, no new volunteers came forward. As a result, the Debian Project Leader is temporarily handling all data protection inquiries, adding to an already heavy workload.</p><p>So, Debian is now calling on contributors with an interest in privacy and data protection to step in. Potential volunteers would be expected to help maintain and improve the existing privacy policy and to work with Debian teams that process personal data, improving workflows for handling data protection requests.</p><p>The project has stressed that restoring a functioning Data Protection Team is urgent, both to meet legal obligations and to ensure that privacy-related inquiries are handled in a timely and sustainable manner.</p>",
      "contentLength": 1957,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qk506b/debian_urgently_seeks_volunteers_after_data/"
    },
    {
      "title": "How do you handle orphaned ConfigMaps and Secrets without breaking prod?",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qk4yxq/how_do_you_handle_orphaned_configmaps_and_secrets/",
      "date": 1769111893,
      "author": "/u/Important-Night9624",
      "guid": 38060,
      "unread": true,
      "content": "<p>I'm doing some spring cleaning on our clusters and seeing tons of ConfigMaps and Secrets that look unused, but I'm paranoid about deleting them.</p><p>You know the deal- teams refactor, Helm releases get abandoned, but the old configs stick around because  doesn't prune them automatically. Since K8s garbage collection only works if  are set (which we often miss), they just pile up.</p><p>How are you guys handling this?</p><ul><li>Manual cleanup? (Sounds like a nightmare)</li><li>Custom scripts? (Grepping for references in all manifests?)</li><li>Just let them rot? (Storage is cheap, right?)</li></ul><p>I'm specifically worried about edge cases like secrets used in Ingress TLS or  that are harder to track down than standard volume mounts.‚Äã</p><p>Anyone have a solid workflow for this that doesn't involve \"scream testing\" (delete and wait for someone to complain)?</p>",
      "contentLength": 811,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "What are your favorite lesser-known open-source applications for productivity on Linux?",
      "url": "https://www.reddit.com/r/linux/comments/1qk4syq/what_are_your_favorite_lesserknown_opensource/",
      "date": 1769111529,
      "author": "/u/corriente6",
      "guid": 38069,
      "unread": true,
      "content": "<p>As a long-time Linux user, I've come to appreciate the wealth of open-source applications available. While many users are familiar with staples like LibreOffice and GIMP, I'm curious about the hidden gems that others find invaluable for productivity. For instance, I recently discovered Taskwarrior, a command-line task manager that has significantly improved my organization. Additionally, tools like Zettlr for markdown editing and Joplin for note-taking have become essential in my workflow. I'm eager to hear what lesser-known applications you all use to enhance your productivity on Linux. What are your go-to tools, and how have they made a difference in your daily tasks?</p>",
      "contentLength": 678,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "[R] CVPR rebuttal advice needed",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qk4m9h/r_cvpr_rebuttal_advice_needed/",
      "date": 1769111127,
      "author": "/u/jackeswin",
      "guid": 38264,
      "unread": true,
      "content": "<p>I received 3 CVPR reviews: 2√ó Borderline Accept and 1√ó Weak Reject with confidence 4,3,3.</p><p>Both borderline reviewers explicitly state that the method is novel, technically sound, and that they would increase their score if the concerns are addressed. </p><p>The weak reject is not based on technical correctness, but mainly on a perceived venue-fit issue; the reviewer also mentions they are not an expert in the domain and are open to changing their recommendation, especially if other reviewers disagree. Actually, the paper‚Äôs topic is explicitly listed in the CVPR CFP. </p><p>No reviewer raises fundamental flaws or correctness issues. </p><p>Based on your experience, is this a situation where a focused rebuttal can realistically change the outcome?</p>",
      "contentLength": 736,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "I wrote a configurable browser launcher.",
      "url": "https://www.reddit.com/r/linux/comments/1qk4ecx/i_wrote_a_configurable_browser_launcher/",
      "date": 1769110642,
      "author": "/u/ComprehensiveSwitch",
      "guid": 38123,
      "unread": true,
      "content": "<p>More than a pretty launcher, Switchyard lets you configure websites to open in a given browser based on domain matches, patterns, and regular expressions. It‚Äôs inspired by apps like Choosy on the Mac. </p>",
      "contentLength": 203,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Alternative for the archived aws-lambda-go-api-proxy",
      "url": "https://www.reddit.com/r/golang/comments/1qk3p9j/alternative_for_the_archived_awslambdagoapiproxy/",
      "date": 1769109136,
      "author": "/u/diegofrings",
      "guid": 38090,
      "unread": true,
      "content": "<p>It looks pretty much like in the examples:</p><pre><code>func main() { http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { io.WriteString(w, \"Hello\") }) lambda.Start(httpadapter.New(http.DefaultServeMux).ProxyWithContext) } </code></pre><p>Now we realized, that the proxy library is archived: </p><blockquote><p>This repository was archived by the owner on May 21, 2025. It is now read-only.</p></blockquote><p>But I can't seem to find any hint on what the new preferred way of doing this is.</p><p>Has anyone found an alternative? Or are you just keep on using the archived library?</p>",
      "contentLength": 522,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Is this AI or just someone who doesn't care at all?",
      "url": "https://www.reddit.com/r/linux/comments/1qk3ag3/is_this_ai_or_just_someone_who_doesnt_care_at_all/",
      "date": 1769108259,
      "author": "/u/Adopolis23",
      "guid": 38039,
      "unread": true,
      "content": "<p>Needed a mouse pad for work so got this one off Amazon and didnt really look at it much. After staring at it on my desk a bit I notice so many typos and spelling mistakes it has to be either AI or just horrible QC and someone who doesn't care. It was like less than 10$ so its whatever but see how many mistakes you can find on this. </p>",
      "contentLength": 334,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Your Microservices architecture is failing because your Product Topology is a mess",
      "url": "https://www.hyperact.co.uk/blog/product-topology",
      "date": 1769107810,
      "author": "/u/ArtisticProgrammer11",
      "guid": 38098,
      "unread": true,
      "content": "<p>We often use 'product' as a catch-all for digital products, platforms, and services. So when we describe ourselves as a 'product transformation consultancy,' what we're really saying is: we can help you transform your digital products, services, and platforms (and organisation). But you'll probably agree, that's a bit of a mouthful.</p><p>While it's deliberate (in the sense that we apply product-thinking to products, platforms, and services) it also creates confusion, which isn‚Äôt helpful when trying to affect change in an organisation&nbsp;‚Äì particularly when adopting the <a href=\"https://www.hyperact.co.uk/blog/hyperacts-product-operating-model\" target=\"_blank\" rel=\"noopener noreferrer\">Product Operating Model</a>.</p><p>We regularly see organisations trapped in debates over these definitions, or worse, making critical structural decisions based on unclear distinctions.</p><p>Often, these problems manifest themselves as:</p><ul><li>Products that aren't actually products (they're projects in disguise)</li><li>Product teams subservient to service ownership, stripped of empowerment</li><li>Internal platforms neglected or treated as cost centres</li><li>Boundaries drawn too narrow, creating constant coordination overhead</li><li>Boundaries drawn too wide, overloading teams beyond their cognitive capacity</li><li>Products designed in isolation, devoid of service context</li></ul><p>While it's important to start with clear definitions, the definitions themselves are only half the battle.</p><p>This article sets out how we characterise products, platforms, and services, how to draw boundaries between them, and makes the case for treating each of them ‚Äòas-a-product‚Äô.</p><p>We're going to start with services, as these sit at the top of the product topology.</p><p>A (digital) service is software responsible for helping end-users achieve one specific, high-level outcome, typically through a linear user journey. For example: learning to drive, applying for a mortgage, or switching energy suppliers.</p><p>Service boundaries are defined by what the user is trying to accomplish, not by what the organisation builds. 'Open a bank account' is a service. 'Account Management' is not ‚Äì that's a product or platform that enables the service. <a href=\"https://designnotes.blog.gov.uk/2015/06/22/good-services-are-verbs-2/\" target=\"_blank\" rel=\"noopener noreferrer\">Services are verbs, not nouns</a>. This distinction shifts the frame of reference: where products and platforms are organisational constructs (ways of carving up ownership and capability), services are user constructs ‚Äì the lens through which users experience what you've built.</p><p>You'll find services most commonly in service-oriented organisations like the public sector, or in organisations with complex product portfolios looking to optimise high-friction, infrequent transactions, such as financial services and utilities.</p><p>Services can be internal or external. External services help customers or citizens achieve an outcome like applying for a mortgage or learning to drive. Internal services help employees achieve an outcome. E.g. Onboarding a new starter, submitting expenses, or requesting IT equipment.</p><p>Services tend to be composed of multiple products and platforms. For example, an 'Apply for a mortgage' service might touch the banking app (product), identity verification (product), credit checking (platform), and document management (platform). The service is the thread that connects them, and can be viewed as an orchestration layer sitting above individual products and platforms.</p><p>Ownership is typically less clear than with products or platforms. It can be shared or, at worst, ambiguous or non-existent. This is a feature of how services work ‚Äì they cross boundaries ‚Äì but it's also a source of problems when nobody owns the end-to-end experience.</p><p>Services can be nested. A digital 'Learn to drive' service might encompass 'Apply for provisional licence,' 'Book theory test,' and 'Book practical test.' The outer service is the full user journey and the inner services are discrete steps with their own completion states. They're all services because they're all framed around user outcomes.</p><p>They also frequently incorporate non-digital touchpoints and processes, as well as digital. Used car buying sites like Cazoo and cinch are services. They help people buy used cars. While their ecommerce product is the primary digital touchpoint, there are a myriad of teams, systems, and processes involved, from the company acquiring the vehicle to it being delivered to its new owner and beyond.</p><ul><li>Oriented around one specific, high-level user outcome</li><li>Boundaries defined by user goals, not organisational structure</li><li>Typically linear, high-friction, infrequent journeys</li><li>Composed of multiple products and platforms</li><li>Ownership is often shared, distributed, or ambiguous</li><li>Often include non-digital touchpoints and processes</li></ul><p>A (digital) product is software that delivers value to end-users through a defined interface. Users interact with it directly and would recognise it as a thing. If services are verbs, products are nouns. 'Banking App' is a product. 'Open a bank account' is not ‚Äì that's a service the product enables.</p><p>Unlike services, which orient around a single user outcome, products typically serve multiple, lower-level user goals through more frequent, routine interactions. Someone opens their banking app to check their balance, make a payment, or report a lost card. Different goals, same product. They can also be internal or external ‚Äì their end-users might be within the organisation (like an intranet or internal tooling) or external (like a SaaS provider offering accounting software to small business owners).</p><p>Product boundaries are typically shaped by what the organisation can coherently own and maintain ‚Äì but that doesn't mean those boundaries are necessarily optimal. Effective boundaries are informed by users' mental models and natural domain boundaries. Because of this, products typically have clearer ownership than services.</p><p>Products can be nested. A news app like the BBC or Guardian is a product. Within it, the Puzzles section might be considered its own product, owned by a distinct team with its own strategy, roadmap, and even its own revenue model (subscriptions, in the case of NYT Games). Products might also encompass everything a user touches (like a banking app) or a coherent subset of their experience (like Card Management).</p><p>A single product might also contribute to multiple services ‚Äì the banking app plays a role in 'Open an account,' 'Apply for a mortgage,' and 'Report fraud.' Where services exist in an organisation, they sit at the top of the topology, products sit beneath them, and platforms ‚Äì serving both. The main distinction between a product and a platform isn't scope; it's whether the owning team controls the user experience. If your team provides APIs that another team uses to build their screens and flows, it‚Äôs a platform, not a product.</p><ul><li>Delivers value to end-users through a defined interface</li><li>Serves multiple user goals through routine interactions</li><li>Can be internal or external</li><li>Boundaries shaped by what the organisation can coherently own</li><li>Can be nested following domain boundaries</li></ul><p>The primary purpose of platforms is to reduce cognitive load on the teams that consume them. Rather than every product team figuring out identity, payments, or infrastructure from scratch, a platform encapsulates that complexity and exposes it through well-designed APIs, tools, and documentation. A good platform makes the right thing the easiest thing to do.</p><p>Where products are nouns that users recognise, platforms are the infrastructure beneath them. If your team provides APIs, components, or services that another team uses to build their user experience, you're running a platform.</p><p>Platforms are typically internal ‚Äì serving teams within your organisation. A <a href=\"https://www.hyperact.co.uk/blog/design-systems-for-non-designers\" target=\"_blank\" rel=\"noopener noreferrer\">design system</a>, an identity service, a data platform. When a capability is productised for external developers with a designed interface and developer experience, it becomes a product whose users happen to be developers. Stripe isn't a platform; it's a product for developers.</p><p>Platform boundaries are shaped by technical capability and domain, not user journeys. A platform typically encapsulates a coherent technical capability (e.g. payments, identity, notifications, search) that multiple products or services need. This is where <a href=\"https://martinfowler.com/bliki/DomainDrivenDesign.html\" target=\"_blank\" rel=\"noopener noreferrer\">Domain-Driven Design's</a> concept of bounded contexts becomes particularly useful: a well-defined platform has clear interfaces and doesn't leak its internal complexity to consumers.</p><p>Platforms sit beneath products and services in the topology. An 'Apply for a mortgage' service might depend on a credit checking platform. A banking app (product) might depend on a payments platform. The platform doesn't own the user experience; it enables the teams that do.</p><p>Like products and services, platforms can be nested. A large organisation might have a data platform that contains sub-platforms: a data warehouse, a streaming platform, an analytics platform. Each can be owned by a distinct team with its own roadmap. Platforms can also have products built on top ‚Äì a platform team might own both the APIs (the platform) and a developer portal (a product whose users are developers).</p><ul><li>Provides capabilities consumed by other teams, not end-users directly</li><li>Primary purpose is reducing cognitive load on consuming teams</li><li>Boundaries shaped by technical capability and domain</li><li>Clear, single-team ownership</li><li>Can be nested following technical domain boundaries</li><li>May have products built on top (e.g., developer portals, admin consoles)</li></ul><p>Or, if you need a short-hand:</p><h2>Everything ‚Äòas-a-product‚Äô</h2><p>The previous sections define what products, platforms, and services are. This section is about how to manage them well.</p><p>Treating something 'as-a-product' means applying product management discipline to it ‚Äì whether it's technically a product, <a href=\"https://www.hyperact.co.uk/blog/your-api-is-a-product\">platform</a>, or service ‚Äì and treating it with the same rigour you'd apply to customer-facing products.</p><p>Why does this matter? Because platforms and services often lack the intentional management that products receive. Platforms get treated as shared infrastructure ‚Äì cost centres to be minimised rather than capabilities to be invested in. Services get fragmented across teams, with nobody accountable for the end-to-end experience.</p><h3>Clear, empowered ownership</h3><p>Every product, platform, and service needs clear ownership ‚Äì a person or team accountable for its success. For products, this is usually obvious. For platforms and services, it's often missing.</p><p>Platform ownership frequently defaults to 'shared' or 'the infrastructure team,' which in practice means nobody is accountable for the developer experience, adoption, or evolution. Service ownership is often even worse ‚Äì because services cross product boundaries, they often have no owner at all, or ownership is distributed across multiple teams who each optimise their slice while the overall journey suffers.</p><p>Empowerment is also critical. In service-oriented organisations, there's a risk that service owners become de facto product managers ‚Äì dictating features to product teams who become delivery functions rather than empowered problem-solvers. This strips product teams of the autonomy they need to discover good solutions.</p><p>Clear ownership doesn't mean one person does everything. It means one person or team is accountable for outcomes, even when delivery involves others.</p><p>Products have users ‚Äì this is obvious. But platforms and services have users too, and treating them as such changes how you manage them.</p><p>For platforms, your users are the teams that consume your APIs and tools. For services, your users are the people trying to achieve an outcome ‚Äì but because services span multiple products, it's easy to lose sight of them.</p><p>Proximity means more than knowing who your users are. It means understanding their needs, building empathy for their problems, and actively involving them in discovery. The best teams maintain regular, direct contact with the people they serve ‚Äì not mediated through layers of research reports or stakeholder proxies.</p><p>\"What gets measured gets improved\" ‚Äì Peter Drucker.</p><p>For products, this typically means end-user metrics: <a href=\"https://www.hyperact.co.uk/blog/aarrr-getting-the-most-out-of-pirate-metrics\" target=\"_blank\" rel=\"noopener noreferrer\">acquisition, activation, retention</a>,  engagement, satisfaction, task completion. For platforms, it means measuring the success of the teams you enable: Are they shipping faster? Are they adopting your platform voluntarily? Are they satisfied with the developer experience? For services, it means measuring the end-to-end outcome: Are users achieving their goal? Where are they dropping off? How long does the journey take?</p><p>The mistake is measuring activity rather than outcomes. Platforms that measure 'number of API calls' rather than 'time for a new team to onboard' are optimising for the wrong thing. Services that measure 'transactions processed' rather than 'users who successfully completed their goal' are missing the point.</p><h3>Long-lived, cross-functional teams</h3><p>Products benefit from continuity ‚Äì teams that understand the domain, the users, and the codebase. The same applies to platforms and services.</p><p>This is where project thinking does the most damage. Projects assemble teams, deliver something, then disband. The 'product' enters maintenance mode. Knowledge walks out the door. There's no investment in improvement, no iteration based on what users actually need.</p><p>Product thinking assumes the work is never 'done.' Teams are long-lived. Funding is continuous. Success is measured by outcomes over time, not delivery against a specification. If your 'product' has a delivery date after which the team moves on, you're running a project ‚Äì and you'll get project outcomes, not product outcomes.</p><p>Those teams should also be cross-functional ‚Äì combining product, design, and engineering (and other skills as needed) in a single team with shared accountability. Cross-functional teams can move faster, make better decisions, and own problems end-to-end without constant handoffs and dependencies.</p><p>For products and services, strategy connects user needs, organisation goals, and competitive positioning. For platforms, the users are internal teams, the market is your internal ecosystem, and the competitive landscape might be build-vs-buy.</p><p>Whether it‚Äôs a broader portfolio strategy your product, platform, or service is aligning to, or whether it has its own, the strategy should define a long term vision, a framework for making reinforcing decisions, and a coherent set of actions ‚Äì usually in the form of a roadmap ‚Äì for achieving it.</p><h3>Room to iterate and improve</h3><p>Product teams don't just build ‚Äì they discover, ship, learn, and improve. Platform and service teams should too.</p><p>This requires space for both <a href=\"https://www.hyperact.co.uk/blog/product-development-blueprint\" target=\"_blank\" rel=\"noopener noreferrer\">continuous discovery and continuous delivery</a>. Discovery means regularly testing assumptions, validating ideas with users, and learning what works before committing to build. Delivery means shipping, measuring, and refining based on what you learn.</p><p>Platforms often fall into a 'build and maintain' mindset ‚Äì the platform exists, teams use it, and the platform team's job is to keep it running. This misses the opportunity to improve developer experience, simplify interface, and evolve with changing needs.</p><p>Services suffer from a similar problem. Once the journey is 'working,' attention shifts elsewhere. But user needs change, friction points emerge, and competitors improve. Services need ongoing investment, not just maintenance.</p><p>Products, platforms, and services have distinct characteristics ‚Äì but all benefit from being treated ‚Äòas-a-product‚Äô.</p><p>Services are oriented around user outcomes. Products are organised around what the organisation can coherently own and maintain. Platforms enable product and service teams to move fast without reinventing solutions to common problems.</p><p>Understanding these distinctions gives you a shared vocabulary. But what separates successful products, platforms, and services from neglected infrastructure and doomed projects is how they're designed and managed:</p><ul><li>Clear, empowered ownership</li><li>Close proximity to end-users</li><li>Long-lived, cross-functional teams</li><li>Room to iterate and improve</li></ul><p>So, as we said at the start: everything is a product.</p>",
      "contentLength": 15885,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qk32sr/your_microservices_architecture_is_failing/"
    },
    {
      "title": "Pro Tip: Want to see a bug fixed or feature implemented in an open source program? Take the time to write a decent bug report/feature request.",
      "url": "https://www.reddit.com/r/linux/comments/1qk30op/pro_tip_want_to_see_a_bug_fixed_or_feature/",
      "date": 1769107686,
      "author": "/u/BinkReddit",
      "guid": 38038,
      "unread": true,
      "content": "<p>I switched from Windows (shudder) to Linux a short while ago and I'm very pleased. All is not perfect is my Linux world, but, amongst many other things, there is a resounding shining light and that's the ability to easily write a decent bug report/feature request AND actually see it get sorted, and in real time (try that with Windows!).</p><p>While I am not fluent in C++ (I am fairly fluent in other things), I can write a decent bug report/feature request and I try to do this often. While not all my reports/requests get solved, when they do life gets a little bit better.</p><p>I encourage others to take the time to make our open source world a better place by filing more bug reports/feature requests; it can even be something simple and you never know when someone might just want to scratch an itch and resolve a bug/implement your request:</p>",
      "contentLength": 836,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Golang support for Playdate handheld! Compiler, SDK Bindings, Tools and Examples",
      "url": "https://www.reddit.com/r/golang/comments/1qk1ec9/golang_support_for_playdate_handheld_compiler_sdk/",
      "date": 1769104232,
      "author": "/u/AmorBielyi",
      "guid": 38059,
      "unread": true,
      "content": "<p>Hello dear Golang community!</p><p>My name is Roman. I'm very excited to share my open-source project related to yellow Playdate handheld from Panic Inc - <a href=\"https://play.date/\">https://play.date/</a> .</p><p>This project is still under actively development, but is ready for a first public release.</p><p><strong>Finally, Playdate meets the Golang programming language!</strong></p><p>I'd very love to hear your feedback and thoughts. Thanks!</p>",
      "contentLength": 371,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "[D] ICLR resubmission to ICML date overlap",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qk182l/d_iclr_resubmission_to_icml_date_overlap/",
      "date": 1769103842,
      "author": "/u/Enjolrasfeyrac",
      "guid": 38272,
      "unread": true,
      "content": "<p>Now that ICLR decisions are coming out on 25th, is it possible to submit the same paper's abstract to ICML by 23rd? Or does it count as a dual submission?</p>",
      "contentLength": 154,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Announcing winapp, the Windows App Development CLI",
      "url": "https://blogs.windows.com/windowsdeveloper/2026/01/22/announcing-winapp-the-windows-app-development-cli/",
      "date": 1769103081,
      "author": "/u/_AACO",
      "guid": 38037,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qk0vip/announcing_winapp_the_windows_app_development_cli/"
    },
    {
      "title": "Polyfit - Because statistics is hard, and linear regression is made entirely out of footguns",
      "url": "https://www.reddit.com/r/rust/comments/1qk0v16/polyfit_because_statistics_is_hard_and_linear/",
      "date": 1769103050,
      "author": "/u/rscarson",
      "guid": 38087,
      "unread": true,
      "content": "<p>I needed to draw a curve fit through some data, and it turned into a year long rabbit hole, where I discovered that stats is really involved, and that the rust ecosystem is a bit barren in terms of user-friendly batteries-included polynomial fitting libraries.</p><p>So I built <code>Polyfit - Because you don't need to be able to build a powerdrill to use one safely</code>.</p><ul><li>The full power of polynomial fitting without needing to understand all the math</li><li>Sensible parameters (<a href=\"https://docs.rs/polyfit/latest/polyfit/statistics/enum.DegreeBound.html\">DegreeBound</a>, scoring metrics, basis functions) that don't feel arbitrary or like magic numbers</li><li>Extensive documentation, examples, and built in testing tools</li></ul><p>My goals for the project were:</p><ul><li>Never ask for a number without context - ask for a random number and you get a random number <ul><li>Instead, if I can derive the correct value myself I do</li><li>If I can't, I have named presets that describe in detail why you'd pick them</li></ul></li><li>Provide sensible defaults for everything <ul><li>If you don't care about a setting, you shouldn't have to think about it</li><li>You should not  to understand the math to get good results</li></ul></li><li>Performance <ul><li>I tried to prioritize speed and memory efficiency where possible</li><li>On my fairly average laptop, I can do a 100 million point fit in ~1s</li></ul></li><li>You need to be able to test it <ul><li>Not understanding the math shouldn't be a barrier to making sure it works</li><li>There's a whole test suite included with extensive docs, examples, and sensible defaults</li><li>The tests even generate a plot on failure so you can see what went wrong</li><li>And I included a set of random noise injection transforms to help you make synthetic data for testing</li><li>The tests will even show seeds used on failure for reproducibility</li></ul></li></ul><p><strong>Here's some examples of why you'd want to use Polyfit</strong></p><p>Oh no! I have all this data and I need to draw a line through it</p><pre><code>use polyfit::{ score::Aic, statistics::DegreeBound, ChebyshevFit, }; let mut fit = ChebyshevFit::new_auto(&amp;data, DegreeBound::Relaxed, &amp;Aic)?; let equation = fit.as_monomial()?.to_string(); let pretty_line = fit.solve_range(0.0..=100.0, 1.0)?; </code></pre><ul><li>DegreeBound::Relaxed uses your data to pick a reasonable degree without overfitting</li><li><a href=\"https://polyfit.richardcarson.ca/glossary/#akaike-information-criterion\">Aic</a> is a scoring metric. Smallish datasets tend to do well with it</li></ul><p>We use <a href=\"https://docs.rs/polyfit/latest/polyfit/struct.CurveFit.html#method.as_monomial\">as_monomial</a> to get the equation in a human readable format.</p><p>Oh gee willikers How am I going to figure out which of these data points are outliers</p><pre><code>let covariance = fit.covariance()?; // It's the thing that tells us how certain we are about the fit just roll with it let outliers = covariance.outliers(Confidence::P95, Some(Tolerance::Absolute(0.1)))?; </code></pre><ul><li>The <a href=\"https://docs.rs/polyfit/latest/polyfit/statistics/enum.Confidence.html\">Confidence</a> is just a measure of how much you trust the fit. P95 is a good option</li><li>I added <a href=\"https://docs.rs/polyfit/latest/polyfit/statistics/enum.Tolerance.html\">Tolerance</a> because real world data is messy. If I know my sensor is only accurate to +/- 0.1 units I shouldn't need to mess with the confidence level to account for that. It's basically an engineering correction for Confidence</li></ul><p>I also have extensive calculus support, so</p><ul><li>Say you have weather data with temperature over time:</li></ul><pre><code>use polyfit::{FourierFit, score::Aic, statistics::DegreeBound}; let fit = FourierFit::new_auto(&amp;data, DegreeBound::Relaxed, &amp;Aic)?; // Derivatives for rates of change // Critical points are neat for this // This tells us when the temperature stops rising or falling and starts doing the opposite for point in fit.critical_points()? { match p { CriticalPoint::Minima(x, _y_) =&gt; println!(\"Found a local minimum at x = {}\", x), CriticalPoint::Maxima(x, _y_) =&gt; println!(\"Found a local maximum at x = {}\", x), CriticalPoint::Inflection(x, _y_) =&gt; println!(\"Found an inflection point at x = {}\", x), } } </code></pre><p>There's too many options how do I pick a <a href=\"https://polyfit.richardcarson.ca/glossary/#basis\">basis</a> for my data!</p><p>It tests your data on every basis I support and gives you an easy to digest report:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr><td>--------------------------------</td></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><pre><code>[ How to interpret the results ] [ Results may be misleading for small datasets (&lt;100 points) ] - Score Weight: Relative likelihood of being the best model among the options tested, based on the scoring method used. - Fit Quality: Proportion of variance in the data explained by the model (uses huber loss weighted r2). - Normality: How closely the residuals follow a normal distribution (useless for small datasets). - Rating: Combined score (0.75 * Fit Quality + 0.25 * Normality) to give an overall quality measure. - Stars: A simple star rating out of 5 based on the Rating score. Not scientific. - The best 3 models are shown below with their equations and plots (if enabled). </code></pre><ul><li>Less params is a simpler model, which is better</li><li>Better fit quality means it explains more of the data</li><li>Better normality means it's probably not underfitting (too simple)</li><li>The rating is a weighted combination of fit quality and normality to give an overall score</li></ul>",
      "contentLength": 4597,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Rust In Production: How Gama Space Controls Satellites In Orbit With Rust",
      "url": "https://corrode.dev/podcast/s05e09-gama-space/",
      "date": 1769100240,
      "author": "/u/mre__",
      "guid": 38122,
      "unread": true,
      "content": "<p>Space exploration demands software that is reliable, efficient, and able to operate in the harshest environments imaginable. When a spacecraft deploys a solar sail millions of kilometers from Earth, there‚Äôs no room for memory bugs, race conditions, or software failures. This is where Rust‚Äôs robustness guarantees become mission-critical.</p><p>In this episode, we speak with Sebastian Scholz, an engineer at Gama Space, a French company pioneering solar sail and drag sail technology for spacecraft propulsion and deorbiting. We explore how Rust is being used in aerospace applications, the unique challenges of developing software for space systems, and what it takes to build reliable embedded systems that operate beyond Earth‚Äôs atmosphere.</p><div><p>\n    CodeCrafters helps you become proficient in Rust by building real-world,\n    production-grade projects. Learn hands-on by creating your own shell, HTTP\n    server, Redis, Kafka, Git, SQLite, or DNS service from scratch.\n  </p><p>\n    Start for free today and enjoy 40% off any paid plan by using\n    <a href=\"https://app.codecrafters.io/join?via=mre\">this link</a>.\n  </p></div><p>Gama Space is a French aerospace company founded in 2020 and headquartered in Ivry-sur-Seine, France. The company develops space propulsion and orbital technologies with a mission to keep space accessible. Their two main product lines are solar sails for deep space exploration using the sun‚Äôs infinite energy, and drag sails‚Äîthe most effective way to deorbit satellites and combat space debris. After just two years of R&amp;D, Gama successfully launched their satellite on a SpaceX Falcon 9. The Gama Alpha mission is a 6U cubesat weighing just 11 kilograms that deploys a large 73.3m¬≤ sail. With 48 employees, Gama is at the forefront of making space exploration more sustainable and accessible.</p><p>Sebastian Scholz is an engineer at Gama Space, where he works on developing software systems for spacecraft propulsion technology. His work involves building reliable, safety-critical embedded systems that must operate flawlessly in the extreme conditions of space. Sebastian brings expertise in systems programming and embedded development to one of the most demanding environments for software engineering.</p><ul><li><a rel=\"noopener noreferrer external\" target=\"_blank\" href=\"https://www.satcat.com/sats/55084\">GAMA-ALPHA</a> - The demonstration satellite launched in January 2023</li><li><a rel=\"noopener noreferrer external\" target=\"_blank\" href=\"https://ada-lang.io/\">Ada</a> - Safety-focused programming language used in aerospace</li><li><a rel=\"noopener noreferrer external\" target=\"_blank\" href=\"https://probe.rs/\">probe-rs</a> - Embedded debugging toolkit for Rust</li><li><a rel=\"noopener noreferrer external\" target=\"_blank\" href=\"https://hyper.rs/\">hyper</a> - Fast and correct HTTP implementation for Rust</li><li><a rel=\"noopener noreferrer external\" target=\"_blank\" href=\"https://flutter.dev/\">Flutter</a> - Google‚Äôs UI toolkit for cross-platform development</li><li><a rel=\"noopener noreferrer external\" target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Universal_asynchronous_receiver-transmitter\">UART</a> - Very common low level communication protocol</li><li><a rel=\"noopener noreferrer external\" target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Rexus/Bexus\">Rexus/Bexus</a> - European project for sub-orbital experiments by students</li><li><a rel=\"noopener noreferrer external\" target=\"_blank\" href=\"https://embassy.dev/\">Embassy</a> - The EMBedded ASsYnchronous framework</li><li><a rel=\"noopener noreferrer external\" target=\"_blank\" href=\"https://github.com/libcsp/libcsp\">CSP</a> - The Cubesat Space Protocol</li><li><a rel=\"noopener noreferrer external\" target=\"_blank\" href=\"https://github.com/oxidecomputer/hubris\">Hubris</a> - Oxide‚Äôs embedded operating system</li><li><a rel=\"noopener noreferrer external\" target=\"_blank\" href=\"https://docs.rs/zerocopy/latest/zerocopy/\">ZeroCopy</a> - Transmute data in-place without allocations</li></ul>",
      "contentLength": 2743,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/rust/comments/1qjzjpd/rust_in_production_how_gama_space_controls/"
    },
    {
      "title": "[D] 100 Hallucinated Citations Found in 51 Accepted Papers at NeurIPS 2025",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qjz88r/d_100_hallucinated_citations_found_in_51_accepted/",
      "date": 1769099546,
      "author": "/u/mgcdot",
      "guid": 37968,
      "unread": true,
      "content": "   submitted by   <a href=\"https://www.reddit.com/user/mgcdot\"> /u/mgcdot </a>",
      "contentLength": 29,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Why I Still Write Code as an Engineering Manager",
      "url": "https://terriblesoftware.org/2026/01/22/why-i-still-write-code-as-an-engineering-manager/",
      "date": 1769097033,
      "author": "/u/Acceptable-Courage-9",
      "guid": 38170,
      "unread": true,
      "content": "<p>I still code on my team. Not every day, and almost never on critical path work, but regularly enough that I know what our codebase actually looks like. The parts that are a joy to work in and the parts that aren‚Äôt.</p><p>Why not critical path? Because that‚Äôs not my job anymore. Taking the interesting, high-visibility work from your team is a fast way to breed resentment and stunt their growth. But there‚Äôs plenty of other work: small bugs, minor improvements, tooling fixes, documentation that requires code understanding. The stuff that matters but isn‚Äôt going to make or break the quarter.</p><p>So why bother? A few reasons.</p><p>But the most important reason is this: I get to show my team what good work looks like.</p><p>Andy Grove, in High Output Management, talks about how a manager‚Äôs job is to increase the output of their team. One of the most effective ways to do this is through training. Training isn‚Äôt slides, though, but showing your people what good work is in practice.</p><p>When I write code, I‚Äôm setting a standard. How do I structure a PR? How do I write commit messages? How thorough are my tests? How do I handle code review feedback? How do I communicate when I‚Äôm uncertain about something? My team sees all of this.</p><p>Showing beats telling. When you‚Äôre doing the work alongside your team, not dictating from above, it lands differently. They see you‚Äôre not asking them to do anything you wouldn‚Äôt do yourself. And because they can observe how you work directly, they can adopt what works and adapt it to their own style.</p><p>This connects to something Nassim Taleb (one of my heroes) calls skin in the game: you make better decisions when you personally bear the consequences of those decisions.</p><p>When I commit code that goes to production, I feel what my team feels. I deal with the same flaky tests, the same deployment process, the same frustrating parts of our developer experience. I can‚Äôt wave my hand and say ‚Äújust fix it‚Äù because I know exactly how hard that fix actually is. I‚Äôve tried it myself.</p><p>When I‚Äôm pushing back on a deadline, I know the trade-offs because I‚Äôve lived them. When I‚Äôm advocating for time to pay down tech debt, I can point to specific pain I‚Äôve personally experienced. My team knows I‚Äôm not asking them to do things I don‚Äôt understand.</p><p>Look, I‚Äôm not going to pretend I know your situation. Maybe you‚Äôre managing fifteen people and there‚Äôs genuinely no time. Maybe your company has a strict policy against it. Maybe you‚Äôve moved so far into strategy that coding would actually be a distraction from higher-leverage work. Context matters.</p><p>But if you‚Äôve stopped coding entirely, ask yourself why. Is it because you genuinely can‚Äôt, or because you‚Äôve convinced yourself you shouldn‚Äôt? ‚ÄúI‚Äôm a manager now so I don‚Äôt do that anymore‚Äù isn‚Äôt an answer.</p><p>Staying technical doesn‚Äôt mean being the top contributor, but maintaining enough connection to the work that you can make informed decisions, earn your team‚Äôs respect, and remember what it actually feels like to ship software. The frustration and the satisfaction, the parts that look easy but aren‚Äôt, the parts that look hard but turn out to be trivial.</p><p>That feeling is what your team experiences every day. And if you‚Äôve completely forgotten what that‚Äôs like, it gets harder to truly connect with the people doing that work.</p>",
      "contentLength": 3360,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qjy33v/why_i_still_write_code_as_an_engineering_manager/"
    },
    {
      "title": "[Performance] Mutex vs Channels para serializar chamadas CGO de alta frequ√™ncia",
      "url": "https://www.reddit.com/r/golang/comments/1qjvsw4/performance_mutex_vs_channels_para_serializar/",
      "date": 1769091729,
      "author": "/u/alph4beth",
      "guid": 38088,
      "unread": true,
      "content": "<p>Estou com um desafio de arquitetura e performance no Go e queria a opini√£o de voc√™s.</p><p>Tenho um conjunto de fun√ß√µes C via CGO que n√£o s√£o thread-safe. O lado do C n√£o lida com concorr√™ncia de jeito nenhum, ent√£o se duas goroutines tentarem executar qualquer uma dessas fun√ß√µes ao mesmo tempo, o programa quebra. √â como um motor V12: enquanto um pist√£o sobe, o outro desce; a sincroniza√ß√£o precisa ser perfeita.</p><p>O volume de chamadas √© alt√≠ssimo. Tenho dezenas ou centenas de goroutines chamando essas fun√ß√µes constantemente. O ciclo de \"lock e unlock\" √© extremamente r√°pido e frequente. Atualmente, uso um sync.Mutex global para garantir que apenas uma goroutine por vez acesse o CGO.</p><p>Vale a pena trocar esse Mutex por um padr√£o de Worker com Channels? Eu sei que canais s√£o a forma \"Go\" de fazer as coisas, mas como as goroutines precisam do retorno da fun√ß√£o (sucesso/erro e dados), o modelo de canal exigiria que eu enviasse um \"request\" com um canal de resposta dentro, o que me parece gerar mais overhead de aloca√ß√£o que um simples Mutex.</p>",
      "contentLength": 1064,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Announcing the Checkpoint/Restore Working Group",
      "url": "https://kubernetes.io/blog/2026/01/21/introducing-checkpoint-restore-wg/",
      "date": 1769091715,
      "author": "/u/dshurupov",
      "guid": 38006,
      "unread": true,
      "content": "<p>The community around Kubernetes includes a number of Special Interest Groups (SIGs) and Working Groups (WGs) facilitating discussions on important topics between interested contributors. Today we would like to announce the new <a href=\"https://github.com/kubernetes/community/tree/master/wg-checkpoint-restore\">Kubernetes Checkpoint Restore WG</a> focusing on the integration of Checkpoint/Restore functionality into Kubernetes.</p><p>There are several high-level scenarios discussed in the working group:</p><ul><li>Optimizing resource utilization for interactive workloads, such as Jupyter notebooks and AI chatbots</li><li>Accelerating startup of applications with long initialization times, including Java applications and <a href=\"https://doi.org/10.1145/3731599.3767354\">LLM inference services</a></li><li>Using periodic checkpointing to enable fault-tolerance for long-running workloads, such as distributed model training</li><li>Providing <a href=\"https://doi.org/10.1007/978-3-032-10507-3_3\">interruption-aware scheduling</a> with transparent checkpoint/restore, allowing lower-priority Pods to be preempted while preserving the runtime state of applications</li><li>Facilitating Pod migration across nodes for load balancing and maintenance, without disrupting workloads.</li><li>Enabling forensic checkpointing to investigate and analyze security incidents such as cyberattacks, data breaches, and unauthorized access.</li></ul><p>Across these scenarios, the goal is to help facilitate discussions of ideas between the Kubernetes community and the growing Checkpoint/Restore in Userspace (CRIU) ecosystem. The CRIU community includes several projects that support these use cases, including:</p><ul><li><a href=\"https://github.com/checkpoint-restore/criu\">CRIU</a> - A tool for checkpointing and restoring running applications and containers</li><li><a href=\"https://github.com/checkpoint-restore/checkpointctl\">checkpointctl</a> - A tool for in-depth analysis of container checkpoints</li><li><a href=\"https://github.com/checkpoint-restore/criu-coordinator\">criu-coordinator</a> - A tool for coordinated checkpoint/restore of distributed applications with CRIU</li></ul><p>More information about the checkpoint/restore integration with Kubernetes is also available <a href=\"https://criu.org/Kubernetes\">here</a>.</p><p>If you are interested in contributing to Kubernetes or CRIU, there are several ways to participate:</p>",
      "contentLength": 1870,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/kubernetes/comments/1qjvsp5/announcing_the_checkpointrestore_working_group/"
    },
    {
      "title": "Rust 1.93.0 is out",
      "url": "https://blog.rust-lang.org/2026/01/22/Rust-1.93.0/",
      "date": 1769090649,
      "author": "/u/manpacket",
      "guid": 37957,
      "unread": true,
      "content": "<p>The Rust team is happy to announce a new version of Rust, 1.93.0. Rust is a programming language empowering everyone to build reliable and efficient software.</p><p>If you have a previous version of Rust installed via , you can get 1.93.0 with:</p><p>If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please <a href=\"https://github.com/rust-lang/rust/issues/new/choose\">report</a> any bugs you might come across!</p><h3><a href=\"https://blog.rust-lang.org/2026/01/22/Rust-1.93.0/#update-bundled-musl-to-1-2-5\" aria-hidden=\"true\"></a>\nUpdate bundled musl to 1.2.5</h3><p>The various  targets now all <a href=\"https://github.com/rust-lang/rust/pull/142682\">ship</a> with musl 1.2.5. This primarily affects static musl builds for , , and  which bundled musl 1.2.3. This update comes with <a href=\"https://musl.libc.org/releases.html\">several fixes and improvements</a>, and a breaking change that affects the Rust ecosystem.</p><p>For the Rust ecosystem, the primary motivation for this update is to receive major improvements to\nmusl's DNS resolver which shipped in 1.2.4 and received bug fixes in 1.2.5. When using \ntargets for static linking, this should make portable Linux binaries that do networking more\nreliable, particularly in the face of large DNS records and recursive nameservers.</p><h3><a href=\"https://blog.rust-lang.org/2026/01/22/Rust-1.93.0/#allow-the-global-allocator-to-use-thread-local-storage\" aria-hidden=\"true\"></a>\nAllow the global allocator to use thread-local storage</h3><p>Rust 1.93 adjusts the internals of the standard library to permit global allocators written in Rust\nto use std's <a href=\"https://doc.rust-lang.org/stable/std/macro.thread_local.html\"></a> and\n<a href=\"https://doc.rust-lang.org/stable/std/thread/fn.current.html\"></a> without\nre-entrancy concerns by using the system allocator instead.</p><h3><a href=\"https://blog.rust-lang.org/2026/01/22/Rust-1.93.0/#cfg-attributes-on-asm-lines\" aria-hidden=\"true\"></a> attributes on  lines</h3><p>Previously, if individual parts of a section of inline assembly needed to be 'd, the full \nblock would need to be repeated with and without that section. In 1.93,  can now be applied to\nindividual statements within the  block.</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Many people came together to create Rust 1.93.0. We couldn't have done it without all of you. <a href=\"https://thanks.rust-lang.org/rust/1.93.0/\">Thanks!</a></p>",
      "contentLength": 1650,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/rust/comments/1qjvdd9/rust_1930_is_out/"
    },
    {
      "title": "What are you using for tls with Gateway Api?",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qjvc6o/what_are_you_using_for_tls_with_gateway_api/",
      "date": 1769090567,
      "author": "/u/parkura27",
      "guid": 38101,
      "unread": true,
      "content": "<p>Update: I'm not against cert manager just tying to figure out if I could continue without it as it was before</p><p>I'm moving from ingress-nginx to Envoy Gateway, and I've hit the issue - my ingress uses fake certs so if you don't mention tls it uses self signed cert which is okay and I use Cloudflare for dns and ssl management as front door, but with EG we have no such feature, I see cert manager everywhere, however I don't want to use it, what are other options? use manualy generated cert and rotate it manually every year? or manage cert controlled with terraform? still requires manual intervention, or should leave http as I use Cloudflare ssl in front and tunnel to connect my ingress(now gw) to CF</p>",
      "contentLength": 703,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ZXC: another (too) fast decompressor",
      "url": "https://github.com/hellobertrand/zxc",
      "date": 1769088909,
      "author": "/u/pollop-12345",
      "guid": 37949,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qjuogx/zxc_another_too_fast_decompressor/"
    },
    {
      "title": "I made a documentary about Open Source in Ukraine and around the world",
      "url": "https://www.reddit.com/r/linux/comments/1qjujym/i_made_a_documentary_about_open_source_in_ukraine/",
      "date": 1769088591,
      "author": "/u/whit537",
      "guid": 37939,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "30 years of ReactOS",
      "url": "https://reactos.org/blogs/30yrs-of-ros/",
      "date": 1769088545,
      "author": "/u/anh0516",
      "guid": 37940,
      "unread": true,
      "content": "<p>Happy Birthday ReactOS! Today marks 30 years since <a href=\"https://github.com/reactos/reactos/commit/0f94427db073a20c24f9d85c8531fbe16490af43\">the first commit to the ReactOS source tree</a>.\nIt‚Äôs been such a long journey that many of our contributors today, including myself, were not alive during this event.\nYet our mission to deliver ‚Äúyour favorite Windows apps and drivers in an open-source environment you can trust‚Äù continues to bring people together.\nLet‚Äôs take a brief look at some of the high and low points throughout our history.</p><h2>1996-2003: The Painful Road to ReactOS 0.1.0</h2><p>ReactOS started from the ashes of the FreeWin95 project, which aimed to provide a free and open-source clone of Windows 95.\nFreeWin95 suffered from analysis paralysis, attempting to plan the whole system before writing any code.\nTired of the lack of progress on the project, Jason Filby took the reins as project coordinator and led a new effort targeting Windows NT.\nThe project was renamed to ‚ÄúReactOS‚Äù as it was a reaction to Microsoft‚Äôs monopolistic position in home computer operating systems.</p><p>Progress on ReactOS was very slow at first.\nContributors had to first build a very basic NT-like kernel before they could develop drivers for it, then continue developing the kernel; not too dissimilar to the process of bootstrapping a new programming language.\nOnce a few basic drivers were written, other contributors were able to learn from these examples and develop other drivers.</p><p>While writing this article, I reached out to Eric Kohl. He developed the original storage driver stack for ReactOS (atapi, scsiport, class2, disk, cdrom, cdfs) and has been with the project since 1998. I asked him about his experiences with ReactOS during this time, how he found the project, and what contributing to ReactOS was like during those early days. He wrote:</p><blockquote><p>I think I found ReactOS while searching for example code for my contributions to the WINE project.\nI subscribed to the mailing list and followed the discussions for a few days.\nThe developers were discussing the future of shell.exe, a little command line interpreter that could only change drives and directories and execute programs.\nA few days [later] I had started to convert the FreeDOS command.com into a Win32 console application, because I wanted to extend it to make it 4DOS compatible.\n4DOS was a very powerful command line interpreter.\nOn December 4th, 1998 I introduced myself and suggested to use my converted FreeDOS command.com as the future ReactOS cmd.exe.\nI had a little conversation with Jason Filby and Rex Joliff, the CVS repository maintainer.\nI sent my cmd.exe code to Rex and he applied it to the repository.\nAfter applying a few more cmd-related patches over the next weeks, Rex asked me whether I would like to have write-access to the repository.\nI accepted the offer‚Ä¶</p><p>The first version I downloaded and used was 0.0.8.\nIt was not much more than a DOS-based bootloader, some drivers, and a basic kernel that ran a few test routines after initialization.</p><p>Version 0.0.8 didn‚Äôt use PE files, but flat (position independent) binaries.\nThere was no PE loader,  no smss, no csrss, no winlogon, no process heaps, no process environments, no threads, etc.\nEach and every little feature was a milestone.</p><p>Initially there was not a review process at all.\nYou write some code, test it and fix it until it works.\nThen you commit it.\nIf something failed on another machine, you got a reply on the mailing list and discussed a solution.\nYou fixed the issue and committed a fix.\nThat‚Äôs how it worked.</p><p>There was always an open and friendly atmosphere.\nIt was and still is always nice to talk to other developers.\nNo fights, no wars, like in some other projects.</p></blockquote><p><em>Editors note: minor errors were corrected.</em></p><p>ReactOS 0.1.0 was released on February 1st, 2003 and received minor updates up until November 2003.\nReactOS 0.1.0 was the first version of ReactOS that could boot from a CD.\nIt had a command line interface and no desktop.\nWatch a demo of it below, provided courtesy of archeYR.</p><a href=\"https://youtu.be/rgRMemZcVoM\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://reactos.org/img/blogs/30yrs-of-ros/ros-0.1.0-thumbnail.png\" alt=\"See ReactOS 0.1.0 by archeYR\"></a><p>During this period ReactOS saw rapid development.\nNew drivers were being built all the time, a basic desktop was built, and ReactOS became increasingly stable and usable.\nPublic interest grew as ReactOS matured.\nIn October 2005, Jason Filby stepped down as project coordinator, and Steven Edwards was voted to be the next project coordinator.</p><div itemscope=\"\" itemtype=\"http://schema.org/ImageGallery\"><figure itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"><a href=\"https://reactos.org/img/blogs/30yrs-of-ros/ros-0.2.x-boot.png\" itemprop=\"contentUrl\"><img itemprop=\"thumbnail\" src=\"https://reactos.org/img/blogs/30yrs-of-ros/ros-0.2.x-boot.png\" alt=\"ReactOS 0.2.x boot screen\"></a><figcaption><p>ReactOS 0.2.x boot screen</p></figcaption></figure><figure itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"><a href=\"https://reactos.org/img/blogs/30yrs-of-ros/ros-0.2.x-desktop.png\" itemprop=\"contentUrl\"><img itemprop=\"thumbnail\" src=\"https://reactos.org/img/blogs/30yrs-of-ros/ros-0.2.x-desktop.png\" alt=\"ReactOS 0.2.x desktop and file explorer\"></a><figcaption><p>ReactOS 0.2.x desktop and file explorer</p></figcaption></figure><figure itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"><a href=\"https://reactos.org/img/blogs/30yrs-of-ros/ros-0.2.0-desktop.png\" itemprop=\"contentUrl\"><img itemprop=\"thumbnail\" src=\"https://reactos.org/img/blogs/30yrs-of-ros/ros-0.2.0-desktop.png\" alt=\"ReactOS 0.2.0 with VMware video driver for NT 4\"></a><figcaption><p>ReactOS 0.2.0 with VMware video driver for NT 4</p></figcaption></figure></div><p>It wasn‚Äôt all sunshine and rainbows though.\nIn January 2006, concerns grew about contributors having access to leaked Windows source code and possibly using this leaked source code in their contributions.\nIn response, Steven Edwards strengthened the project‚Äôs intellectual property policy and the project made the difficult decision to audit the existing source code and temporarily freeze contributions.</p><p>The ongoing audit and contribution freeze from the end of the ReactOS 0.2.x era slowed development and momentum considerably for ReactOS 0.3.x.\nFollowing challenges with the audit, Steven Edwards stepped down as project coordinator and Aleksey Bragin assumed the role by August 2006.</p><p>Despite the challenges during this time, ReactOS 0.3.x continued to build upon ReactOS‚Äôs legacy.\nReactOS 0.3.0 was released on August 28th, 2006.\nIt introduced networking support and a package manager called ‚ÄúDownload!‚Äù.\nThis package manager would become the basis for RAPPS, the package manager built into modern versions of ReactOS.\nIn July 2008, the x86_64 port of ReactOS was started.\nOne year later, ReactOS 0.3.10 imported the <a href=\"http://alter.org.ua/soft/win/uni_ata/\">UniATA driver</a>, written by Alexandr Telyatnikov (Alter).\nWhile we run into limitations with the UniATA driver today, UniATA enabled ReactOS to support SATA storage devices and to support partitions greater than 8GB in size.\nOn February 8th, 2012, ReactOS 0.3.14 supported being built using the MSVC compiler and added visual style support.</p><div itemscope=\"\" itemtype=\"http://schema.org/ImageGallery\"><figure itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"><a href=\"https://reactos.org/img/blogs/30yrs-of-ros/ros-0.3.x-desktop.png\" itemprop=\"contentUrl\"><img itemprop=\"thumbnail\" src=\"https://reactos.org/img/blogs/30yrs-of-ros/ros-0.3.x-desktop.png\" alt=\"ReactOS 0.3.x desktop\"></a></figure><figure itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"><a href=\"https://reactos.org/img/blogs/30yrs-of-ros/ros-0.3.x-download.png\" itemprop=\"contentUrl\"><img itemprop=\"thumbnail\" src=\"https://reactos.org/img/blogs/30yrs-of-ros/ros-0.3.x-download.png\" alt=\"Download!, the package manager for ReactOS 0.3.x\"></a><figcaption><p>Download!, the package manager for ReactOS 0.3.x</p></figcaption></figure></div><h2>2016-Today: ReactOS 0.4.x</h2><p>ReactOS 0.4.0 was released on February 16th, 2016.\nIt introduced a new graphical shell that utilized more Windows features and was more similar architecturally to Windows Explorer.\nReactOS 0.4.0 also introduced support for kernel debugging using WinDbg when compiled with MSVC.\nBeing able to use standard Windows tools for kernel debugging has helped us progress considerably.\nReactOS 0.4.0 continued to receive incremental updates every few months up until versions 0.4.14 and 0.4.15 which had years of development updates each.\nToday, the x86_64 port of ReactOS is similarly functional to its x86 counterpart, but with no WoW64 subsystem to run x86 apps its usability is limited.</p><div itemscope=\"\" itemtype=\"http://schema.org/ImageGallery\"><figure itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"><a href=\"https://reactos.org/img/blogs/30yrs-of-ros/explorer-diagram.png\" itemprop=\"contentUrl\"><img itemprop=\"thumbnail\" src=\"https://reactos.org/img/blogs/30yrs-of-ros/explorer-diagram.png\" alt=\"A humorous diagram made in 2015 to explain the complexity of Windows Explorer\"></a><figcaption><p>A humorous diagram made in 2015 to explain the complexity of Windows Explorer</p></figcaption></figure><figure itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"><a href=\"https://reactos.org/img/blogs/30yrs-of-ros/ros-0.4.15-desktop.png\" itemprop=\"contentUrl\"><img itemprop=\"thumbnail\" src=\"https://reactos.org/img/blogs/30yrs-of-ros/ros-0.4.15-desktop.png\" alt=\"ReactOS 0.4.15 desktop, shown with Luna visual style and large taskbar icons applied\"></a><figcaption><p>ReactOS 0.4.15 desktop, shown with Luna visual style and large taskbar icons applied</p></figcaption></figure></div><p>We‚Äôre continuing to move ReactOS forward. Behind the scenes there are several out-of-tree projects in development. Some of these exciting projects include a new build environment for developers (RosBE), a new NTFS driver, a new ATA driver, multi-processor (SMP) support, support for class 3 UEFI systems, kernel and usermode address space layout randomization (ASLR), and support for modern GPU drivers built on WDDM.</p><p>The future of ReactOS will be written by the people who believe in the mission and are willing to help carry it forward.</p><p><em>Note: Statistics were calculated at commit f60b1c9</em></p><ul><li>Total unique contributors: 301</li><li>Total lines of code: 14,929,578</li></ul>",
      "contentLength": 7432,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qjujbm/30_years_of_reactos/"
    },
    {
      "title": "[D] AISTATS 2026 Paper Acceptance Result",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qjuitb/d_aistats_2026_paper_acceptance_result/",
      "date": 1769088509,
      "author": "/u/mathew208",
      "guid": 37947,
      "unread": true,
      "content": "<div><p>AISTATS 2026 acceptance decisions are being released today. This thread is for discussing this year‚Äôs outcomes.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/mathew208\"> /u/mathew208 </a>",
      "contentLength": 145,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Prominent Intel Compiler Engineer Heads Off To AMD",
      "url": "https://www.phoronix.com/news/Intel-Compiler-Expert-Now-AMD",
      "date": 1769088297,
      "author": "/u/anh0516",
      "guid": 37941,
      "unread": true,
      "content": "<p>Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via <a href=\"https://twitter.com/MichaelLarabel\">Twitter</a>, <a href=\"https://www.linkedin.com/in/michaellarabel/\">LinkedIn</a>, or contacted via <a href=\"https://www.michaellarabel.com/\">MichaelLarabel.com</a>.</p>",
      "contentLength": 500,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qjufxk/prominent_intel_compiler_engineer_heads_off_to_amd/"
    },
    {
      "title": "[R] CVPR 2026 Reviews today",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qjub2g/r_cvpr_2026_reviews_today/",
      "date": 1769087937,
      "author": "/u/gentaiscool",
      "guid": 38004,
      "unread": true,
      "content": "<p>How's your reviews and chances?</p>",
      "contentLength": 31,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Helm + container images across clusters... need better options",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qjscp3/helm_container_images_across_clusters_need_better/",
      "date": 1769082145,
      "author": "/u/Timely-Dinner5772",
      "guid": 37960,
      "unread": true,
      "content": "<p>Running container images via Helm across clusters is a mess. Every small change in image or values can break stuff. Charts get messy fast. Env overrides, tags, versions all pile up. i tried Chainguard for auditing and building images but it feels heavy and rigid for our setup. Any sug for something lighter or more flexible that works at scale? Workflows, tools, whatever. Need ideas.</p>",
      "contentLength": 385,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Do not fall for complex technology",
      "url": "https://rushter.com/blog/complex-tech/",
      "date": 1769081488,
      "author": "/u/f311a",
      "guid": 37952,
      "unread": true,
      "content": "<p>Fifteen years ago, I wanted to set up a note-taking system.\nAt the time, Evernote was the tool everyone was talking about, so choosing it seemed like the right and easy decision.</p><p>After storing around 500 notes for eight years, Evernote became a mess to use.\nIt was bloated, heavily monetized, and slow to work with. So I wanted to switch.</p><p>About that time came Notion. Everyone was talking about it. I jumped on the bandwagon and migrated a few hundred of\nmy notes to it that were still relevant. It did not even occur to me that switching from a bloated and slow app to\na web app would result in a similar outcome later. I followed a popular choice again.</p><p>After struggling for a year, I switched to Markdown notes and a plugin for an editor that renders inline images.\nI'm still using this to this day. It is simple, and I will be able to open my notes 10-20 years later.\nI can edit them in any editor. It works offline and does not depend on commercial products.\nI encrypt my notes locally so they can be stored safely on any cloud service.</p><p>I don't even use images anymore, so this could be simple plain-text files.\nWhy did no one tell me that I can start simple and switch to a complex system if I really need it?</p><p>It took me almost ten years to understand that, don't be like me!</p><p>This process never stops. There are now Roam Research and Obsidian, where people spend more time organizing\nnotes than writing them. Some people become obsessed with documenting everything and never reading it anymore, just\nbecause everyone talks how cool it is.</p><p>Remember the peak of popularity of microservices and the use of GraphQL?\nSmall teams of 3-5 developers used them to build simple web apps\njust because it was cool and big tech was writing about it a lot.</p><p>Simple systems became more complex for no benefit. A complex working system should only grow from a simple one.\nYou need to gather initial knowledge and requirements about your project first. Iterating a simple system is much\nfaster and easier. When a project starts, things change a lot. It's common to rewrite the whole project from scratch 2-3\ntimes in startups just because the initial prototype resulted in so much technical debt and bad decisions. I had experienced this\nmultiple times at my jobs. Not every system should be complex in the first place.</p><p>If you can only start with a complex system, there is a high chance it's broken by design.</p><p>The list of complex technologies is very big and I can go on and on.\nOftentimes, you don't need to use ten cloud services, hundreds of serverless functions and so on.\nNot only can it be simpler, but it can also be faster and cheaper too!\nYou just need to ask yourself why you need a particular technology in this\nproject first. Sometimes the hardest part is to convince the management, though.</p><p>In blogging circles, one of the popular topics is \"I migrated my blog from X to Y\". Some people change\ntheir blog engines 3-4 times and I'm not an exception.</p><p>I started with WordPress, then I migrated to Django, and right now my blog is completely static.</p><p>When I was using Django, I needed a good server with proper caching so that when traffic spikes, it can handle it.</p><p>Right now, I don't need a database. Hosting static HTML files is simple, and you can do it for free.\nThe only dynamic part is  comments. I'm using CloudFlare workers with a simple 50-line script that just\nstores comments in Cloudflare Workers KV.</p><p>They are not loaded dynamically. I just import them to markdown\nfiles and regenerate the blog. There is no database to serve them. They are preserved in markdown forever.</p><p>The whole blog engine is 800 lines of Python code that supports comments, RSS, categories, and so on.\nI know exactly how it works, and adding new features is super easy. There is also nothing to hack.\nInternally, static website generators are pretty complex to handle various use cases.\nBut, if I were using a popular static generator, adding custom features would be still hard.</p><p>A lot of companies are trying to force AI everywhere. Look at Microsoft, they added it everywhere.\nIn most cases, it does not make the life of a user easier. In the case of Microsoft, it actually results in more bugs\nand the worst UI. This time, ignoring complexity is much harder for a user, so the best shot is to switch to Linux where you have full control.</p><p>I think about this a lot. It's easy to spot problems in other products, but the product that you are working on\ncan suffer from the same problems. It's harder to notice, especially when you don't use it.</p><p>Another problem with LLMs is that it is much easier to add new features to your project now.\nIf you use LLMs, keep things simple and the scope of changes limited.\nWhen you aim for big changes, the code quality drops significantly.\nYou stop paying attention to the changes to the point that you stop understanding how things work.</p><p>There is an old tale regarding code reviews. A team lead reviewed 100 lines of code and found three bugs.\nAfter that, he reviewed 1500 lines PR and found zero bugs. This happened because the scope of the changes was so big,\nthat his brain just refused to concentrate on every change.</p><p>The quality of LLM output depends on code size, too.\nTheir context is limited, and the bigger and more complex the codebase, the worse they perform.</p>",
      "contentLength": 5267,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qjs5rf/do_not_fall_for_complex_technology/"
    },
    {
      "title": "A clear visual explanation of what HTTPS protects",
      "url": "https://howhttps.works/why-do-we-need-https/",
      "date": 1769080390,
      "author": "/u/Digitalunicon",
      "guid": 37950,
      "unread": true,
      "content": "<div><div><p>We need HTTPS for 3 reasons.</p></div></div><div><div><p>Privacy, integrity, and identification.</p></div></div><div><div><p>Let's talk about privacy first.</p></div></div><div><div><p>When you browse to a website without HTTPS, I could be eavesdropping on your password.</p></div></div><div><div><p>Reason number 2: integrity.</p></div></div><div><div><p>I am sending another message to Browserbird unencrypted.</p></div></div><div><div><p>But before it reaches Browserbird, I intercept the message.</p></div></div><div><div><p>I update the message to say bad things about Browserbird and forward it to him.</p></div></div><div><div><p>Why would Compugter say such things about me?</p></div></div><div><div><p>And crab-in-the-middle attacks are the worst.</p></div></div><div><div><p>I make sure that your communication is not being tampered with.</p></div></div><div><div><p>Reason number 3: identification.</p></div></div><div><div><p>Identification means that I can check that this message is coming from Compugter.</p></div></div><div><div><p>HTTPS, via SSL certificates, ensures you are connected exactly with the receiver you would expect.</p></div></div><div><div><p>This SSL certificate is valid and has been issued by a legitimate Certificate Authority. You are good to go.</p></div></div><div><div><p>We'll be talking more about SSL certificates and Certificate Authorities soon, so stay tuned.</p></div></div><div><div><h2>Next on HowHTTPS.works...</h2><p>Now that we know the why, the next step is to understand symmetric and asymmetric encryption. Big words, but easy concepts.</p></div><a href=\"https://howhttps.works/the-keys/\"></a></div>",
      "contentLength": 1118,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qjrudc/a_clear_visual_explanation_of_what_https_protects/"
    },
    {
      "title": "90% of Salesforce‚Äôs Engineers Use Cursor Every Day",
      "url": "https://analyticsindiamag.com/ai-news-updates/90-of-salesforces-engineers-use-cursor-every-day/",
      "date": 1769080340,
      "author": "/u/Ok-Elevator5091",
      "guid": 38071,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qjrtvb/90_of_salesforces_engineers_use_cursor_every_day/"
    },
    {
      "title": "[Media] musicfree: a cross-platform music downloader implemented in Rust",
      "url": "https://www.reddit.com/r/rust/comments/1qjqq51/media_musicfree_a_crossplatform_music_downloader/",
      "date": 1769076437,
      "author": "/u/Every_Juggernaut7580",
      "guid": 37956,
      "unread": true,
      "content": "<p>musicfree is a music download tool written in pure Rust. It supports multiple platforms, including Windows, macOS, Unix, and Android. There are two versions available: a CLI version at <a href=\"https://github.com/ahaoboy/musicfree\">musicfree</a> and a Tauri version at <a href=\"https://github.com/ahaoboy/musicfree-tauri\">musicfree-tauri</a>.</p><p>Currently, it supports downloading single videos from YouTube and Bilibili, downloading playlists, and cover images.</p>",
      "contentLength": 350,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Naked Steel: The Deep Tech & Security of Bare Metal",
      "url": "https://v.redd.it/9nal1fg8iveg1",
      "date": 1769076144,
      "author": "/u/Appropriate_Way4135",
      "guid": 38005,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/kubernetes/comments/1qjqnc2/naked_steel_the_deep_tech_security_of_bare_metal/"
    },
    {
      "title": "Making and Scaling a Game Server in Kubernetes using Agones",
      "url": "https://noe-t.dev/posts/making-and-scaling-a-game-server-in-k8s-using-agones/",
      "date": 1769074966,
      "author": "/u/noe__0",
      "guid": 37963,
      "unread": true,
      "content": "<p>If you‚Äôre interested in Kubernetes like I am, you‚Äôve probably found yourself exploring related projects on GitHub and you might have stumbled upon a repository called <a href=\"https://github.com/googleforgames/agones\" target=\"_blank\" rel=\"noreferrer\">Agones</a>. If you‚Äôve never heard about it, Agones is a project created by Google to manage and deploy video game servers on Kubernetes.</p><p>Recently, I dipped my toes in the water and tried it out. I had a lot of fun doing so and I want to share everything I learned. In this article, we will go over the following:</p><ul><li>The creation of a basic .</li><li>Integrating it with .</li><li>Its deployment on .</li><li>The making of a <strong>matchmaking service in Go</strong>.</li><li>Setting up and benchmarking  for our infrastructure based on the matchmaking‚Äôs player queue.</li></ul><p>I‚Äôll share a lot of relevant code snippets and diagrams, but if you want to get the full picture, you can find the source code and the Kubernetes manifests in this GitHub repository:</p><p>Before going any further, we need to address a question regarding Agones:  That was my first reaction upon discovering Agones because, in theory, anyone can just deploy their game server as a regular deployment on a cluster, right? Well, things are actually a bit more complicated than that.</p><p>If you look into the Agones documentation, you will find <a href=\"https://agones.dev/site/docs/faq/#cant-we-use-a-deployment-or-a-statefulset-for-game-server-workloads\" target=\"_blank\" rel=\"noreferrer\">this section</a> which basically answers the question. To put it simply, game server workloads are <strong>both stateful and stateless</strong>. An empty game server is stateless and can be safely deleted or moved, while a game server with players probably has in-memory state and must not leave the node.</p><p>In other words, Agones allows you to manage and scale game server workloads based not only on CPU, memory, or traffic but also on . Thanks to that, you can update game servers without shutting down servers with active players, reuse a game server on which a game has ended or even set autoscaling based on the number of full game servers. And much more.</p><h2>Developing a game server in Go</h2><p>To start, we obviously need a game to work with. For this purpose, I will be making a quick and simple game of  in Go.</p><p>Since this is just a simple demo, I won‚Äôt be trying to make something grandiose. It will just be a <strong>basic HTTP server with a WebSocket</strong> on which two players will connect to battle. For a real game, you would probably want to use UDP connections.</p><p>Both players will be connected to the WebSocket and will have to select their move. The connection stays open for both players until they both selected a move. Once both players chose their moves, the server sends the winner to them.</p><p>To do this, I used standard Go packages such as  and <code>github.com/gorilla/websocket</code>.</p><div><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div></div><p>The root path () serves the index.html file (which is <a href=\"https://pkg.go.dev/embed\" target=\"_blank\" rel=\"noreferrer\">embedded</a> in the binary) and the  path serves the WebSocket connection.</p><p>The index.html is just a very basic web page with buttons for each move (rock, paper, scissors). It uses JavaScript to send a message to the WebSocket when a button is clicked. Results are displayed in the  div.</p><div><div><pre tabindex=\"0\"><code data-lang=\"html\"></code></pre></div></div><p>I won‚Äôt go too much into details about the game logic since, well, it‚Äôs just a simple game of rock paper scissors.</p><p>The game loop is fully coded in the WebSocket handler, and it uses methods from the  package located in . Here‚Äôs a basic overview of what this handler does:</p><div><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div></div><p>If you try to make something similar, keep in mind that you have to handle what happens when a player disconnects or leaves the game. In this case, I just made it so the player gets deleted from the game allowing them or someone else to rejoin. You may want to just end the game or kick everyone else if one of the players disappears.</p><p>In order to manage concurrency, I use a simple  to ensure that the player list and moves are not modified at the same time. Before every operation, I lock the mutex and unlock it after the operation is complete. For example:</p><div><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div></div><p>Upon game end, the WebSocket connections are closed and the game server shuts down.</p><p>The end result looks like this:</p><p>Very impressive, isn‚Äôt it? Jokes aside, this simple multiplayer game will be more than enough for us to get started with Agones.</p><h2>Adding Agones to a game server</h2><p>Now that we have a game server ready, we need to make some tweaks in order to deploy it with Agones. If you try to deploy it as of right now, it will just crash as Agones expects your container to send regular ping.</p><p>To explain briefly how things work in Agones, when deploying a game server, we use the well named  resource. You can think of GameServers as the equivalent of Pods in the Agones world. They are what will be running your game server container.</p><p>The main difference with regular Pods is that GameServers run your image alongside an  which is responsible for managing the lifecycle of the game server. This sidecar is responsible for ensuring that the game server is healthy and available for players. It communicates with the Kubernetes API to update the GameServer resource status.</p><pre>---\ntitle: GameServer Architecture\nconfig:\n  look: handDrawn\n---\ngraph TD\n    KubeAPI[\"kube-apiserver\"]\n\n    subgraph GameServer[\"GameServer\"]\n        subgraph Pod[\"Pod\"]\n            GameContainer[\"**Your Game Server** *Container*\"]\n            AgonesSidecar[\"**agones-sdk** <p>*Container*\"]\n\n            GameContainer &lt;--&gt;|SDK gRPC| AgonesSidecar\n        end\n    end\n\n    AgonesSidecar --&gt;|HTTP PATCH GameServer resource| KubeAPI\n</p></pre><p>The bare minimum to get your game server up and running with Agones is to implement a . To do this, we first need to import the <a href=\"https://agones.dev/site/docs/guides/client-sdks/\" target=\"_blank\" rel=\"noreferrer\">Agones Game Server Client SDK</a>. In my case, I will be importing the <a href=\"https://pkg.go.dev/agones.dev/agones/pkg/sdk\" target=\"_blank\" rel=\"noreferrer\">Go package</a> but there are also SDKs for other languages such as Java or C++ and also for game engines such as Unity or Unreal Engine.</p><p>Even if your language or game engine doesn‚Äôt have an SDK, you can still use Agones by making and deploying a sidecar container alongside your game server. This sidecar container would be responsible for communicating with Agones and you would just need to communicate with your game binary. Or else, you can just communicate directly with Agones using the <a href=\"https://agones.dev/site/docs/reference/grpc/\" target=\"_blank\" rel=\"noreferrer\">gRPC API</a> or the <a href=\"https://agones.dev/site/docs/reference/api/\" target=\"_blank\" rel=\"noreferrer\">HTTP API</a> which should be supported by most languages.</p><p>Once we have our SDK installed, we need to actually implement the health check. This is usually done by creating a loop that sends a ping to Agones every few seconds. Here‚Äôs how you can do it in Go:</p><div><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div></div><p>Now, we can technically already deploy our game server on a Kubernetes cluster with Agones by creating a  with our game container image. However, we are far from production-ready. We still need to at least implement the following Agones functions:</p><ul><li> - To indicate that the game server is ready to accept connections from players.</li><li> - To tell Agones to shut down the game server.</li></ul><p>Implementing the  function is pretty straightforward. We just need to call it from the SDK when starting the game server:</p><div><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div></div><p>For the  function, things are a bit more complicated. What we want to do is to implement a graceful shutdown process. Basically, it means that we need our server to handle signals like  by waiting for everything to complete before shutting down. It is especially important in order to avoid loss of player data or of an unsaved game for instance.</p><p>Fortunately, this pattern is pretty easy to implement in Go. We will be using the <a href=\"https://pkg.go.dev/context\" target=\"_blank\" rel=\"noreferrer\">context package</a> to handle cancellation and timeouts coupled with the <a href=\"https://pkg.go.dev/os/signal\" target=\"_blank\" rel=\"noreferrer\">signal package</a> to handle, as its name implies, signals.</p><p>We are first going to need to create a context that will be used all throughout our server. In order to have it be cancellable with Unix signals, we will be creating it using  from the  package. We can then, at the end of our  function, have all of our code for shutting down our server after .</p><div><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div></div><p>Currently, we handle shutdown from signals correctly, but not necessarily gracefully. In general, when implementing this pattern, we want to ensure that all ongoing operations are <strong>completed before shutting down</strong>. In our case, this isn‚Äôt really important as the process of shutting down the client SDK and the HTTP server should be pretty straightforward.</p><p>However, let‚Äôs say you‚Äôre making an actual game: you may want to save the result of your game to a database, for example. In Kubernetes, Pods getting deleted are first sent a , and have a grace period of 30 seconds. After that, Kubernetes sends a , which you want to avoid if possible. If for some reason the database you‚Äôre sending your data to is experiencing issues, you will want to rollback your transaction before being forcefully terminated.</p><p>We can achieve this by having a timeout, and to do that, we‚Äôre going to make, once again, a new context, but with  this time around. This way, we will be able to pass down the context with timeout to our different shutdown functions and ensure that our game server is properly shut down in a given amount of time.</p><p>In my case, I set it up with a timeout of 10 seconds. This is more than enough for the Agones client SDK and the HTTP server to shut down gracefully.</p><div><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div></div><p>With all of this, we now have the lifecycle of our game server fully implemented and ready to be deployed alongside Agones‚Äô SDK sidecars in actual  resources.</p><p>Now that we have our game server ready, we can deploy it on a Kubernetes cluster. I‚Äôm using a basic <a href=\"https://kind.sigs.k8s.io/\" target=\"_blank\" rel=\"noreferrer\">kind</a> cluster for this example, but you can use any Kubernetes cluster you want. The only important requirement is to install <a href=\"https://agones.dev/\" target=\"_blank\" rel=\"noreferrer\">Agones</a> on your cluster. To do so, you can simply use <a href=\"https://helm.sh/\" target=\"_blank\" rel=\"noreferrer\">Helm</a> chart like so:</p><div><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div></div><p>We will be deploying our game server in the  namespace. If you want to deploy yours in a different one, you may need to change some values in your Helm deployment of Agones.</p><p>Once we have our cluster ready with Agones up and running, we can start by deploying our game server image in a simple <a href=\"https://agones.dev/site/docs/reference/gameserver/\" target=\"_blank\" rel=\"noreferrer\">GameServer</a> resource:</p><div><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div></div><p>You should then be able to see it by running . You should see something like this:</p><div><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div></div><p>Notice how Agones picked a <strong>random port between 7000 and 8000</strong> for the game server. This port is exposed on the host node‚Äôs network using the <a href=\"https://v1-33.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.33/#containerport-v1-core\" target=\"_blank\" rel=\"noreferrer\">hostPort</a> field of Pods. This means that you can access the game server directly from your host machine using the IP address and port number.</p><p>You can even check its events to see the different steps it went through:</p><div><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div></div><p>Which should give you something like this:</p><div><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div></div><p>You should be able to access the game directly from your web browser by visiting .</p><p>Next, we can deploy the game server in a <a href=\"https://agones.dev/site/docs/reference/fleets/\" target=\"_blank\" rel=\"noreferrer\">Fleet</a>. If GameServers are the equivalent of Pods, you can think of Fleets as the equivalent of Deployments or StatefulSets. They allow us to have replicas of our GameServer and scale them up and down without killing active game servers. We can create one just like so:</p><div><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div></div><p>We can then check the GameServers it created:</p><div><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div></div><p>This fleet can easily be scaled up by running :</p><div><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div></div><p>But, right now, if you try to scale it down, it could kill active GameServers. What we want to do in order to avoid that is to use a <a href=\"https://agones.dev/site/docs/reference/gameserverallocation/\" target=\"_blank\" rel=\"noreferrer\">GameServerAllocation</a>. This type of resource allows us to set its state from  to , which will prevent Agones from deleting that GameServer. Let‚Äôs allocate a random GameServer from our fleet with :</p><div><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div></div><p>Now, let‚Äôs do something a bit extreme and scale the fleet down to 0 replicas:</p><div><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div></div><p>If you look at the list of GameServers, you‚Äôll notice that the one we allocated is still there:</p><div><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div></div><p>This is great, as we managed to scale down without stopping a GameServer that has been marked as being allocated for a game. If you go ahead and finish playing a game on this server, you‚Äôll notice that the GameServer gets automatically deleted.</p><p>Of course, in real life, you would probably use the Kubernetes API to allocate our GameServers instead of using kubectl. This way, we can automate the allocation process without manual intervention.</p><h2>Making a matchmaking service</h2><p>So far, we managed to make a game server, hook it up to Agones and deploy it on a Kubernetes cluster. All of this is great but it‚Äôs nothing we couldn‚Äôt have achieved by simply using regular Kubernetes resources such as Deployments or StatefulSets. But now that we have everything set up, we can actually go a bit further and exploit Agones‚Äô features to have a  which will <strong>scale our game servers automatically based on demand üöÄ</strong>.</p><p>Or, at least, that‚Äôs what we‚Äôre going to do in the next part of this post. For now, we‚Äôll focus on making a matchmaking service that will match 2 players together and will allocate a GameServer to them.</p><p>If you look online, you might find an open-source solution for matchmaking called <a href=\"https://open-match.dev/\" target=\"_blank\" rel=\"noreferrer\">Open Match</a>. It has been made by Google, and it can work with Agones, which is great. However, as of writing this, there hasn‚Äôt been any update in over 2 years. A second version of Open Match called <a href=\"https://open-match.dev/site/v2/overview/\" target=\"_blank\" rel=\"noreferrer\">Open Match 2</a> seems to be planned but there are no releases yet and only a single person seems to be working on it.</p><p>Here‚Äôs what we‚Äôll be working with:</p><pre>---\nconfig:\n  look: handDrawn\n---\nsequenceDiagram\n    participant Player as üë§ Player\n    participant WebSocketServer as üåê HTTP Server\n    participant Topic_matchmaking as üìá Topic: matchmaking\n    participant Matcher as ‚öôÔ∏è Matcher\n    participant Topic_match_results as üìá Topic: match_results_{playerID}\n    participant KubernetesAPI as ‚ò∏Ô∏è Kubernetes API\n\n    Player-&gt;&gt;WebSocketServer: Connect via WebSocket\n    WebSocketServer-&gt;&gt;WebSocketServer: Generate playerID\n    WebSocketServer-&gt;&gt;Topic_match_results: Subscribe\n    WebSocketServer-&gt;&gt;Topic_matchmaking: Publish playerID\n\n    Topic_matchmaking-&gt;&gt;Matcher: Deliver playerID\n\n    alt No one waiting\n        Matcher-&gt;&gt;Matcher: Store playerID as waiting\n        Note right of Matcher: Wait for next player\n    else Another player waiting\n        Matcher-&gt;&gt;KubernetesAPI: Allocate GameServer\n        KubernetesAPI--&gt;&gt;Matcher: GameServer address\n        Matcher-&gt;&gt;Topic_match_results: Publish match for both players\n        Topic_match_results-&gt;&gt;WebSocketServer: Deliver match result\n        WebSocketServer-&gt;&gt;Player: Redirect to match-ip:port\n    end\n</pre><p>For simplicity‚Äôs sake, I copied the base structure of the game server and reused it in the matchmaking service. This is why we are once again working with an HTTP server serving a WebSocket on . This time, we redirect the player by opening the web page the matchmaking service will return.</p><p>The core component of this matchmaking system is the . As you can see on the diagram, we are working with two topics:</p><ul><li>: Player requests for a match.</li><li>: Topics for the response to the player.</li></ul><p>The brain of the operation is named the , and is basically a process that will take a player from the queue and match them with another one. Once a match is made, it will reserve a GameServer by creating a  through the Kubernetes API. It then sends them both the server address they need to join via the match results topic of both player.</p><p>To work with Pub/Sub in Go, we‚Äôll be using a great library called <a href=\"https://github.com/ThreeDotsLabs/watermill\" target=\"_blank\" rel=\"noreferrer\">Watermill</a>, which will simplify the task a lot. What‚Äôs great about this library is that it works with a lot of different options, including Kafka, RabbitMQ or even PostgreSQL. To keep things simple, I chose to go with a simple <a href=\"https://watermill.io/pubsubs/gochannel/\" target=\"_blank\" rel=\"noreferrer\">Go Channel</a> which you can also use as a Pub/Sub with Watermill.</p><p>Here‚Äôs how the WebSocket handler initiates the matchmaking process and waits for a match result with Watermill:</p><div><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div></div><p>As you can see, it‚Äôs pretty straightforward with functions such as  and .</p><p>That‚Äôs basically it for the ‚Äúfrontend‚Äù part of the matchmaking service, but there‚Äôs a second part which is called the matcher. It runs as a goroutine but it could be run as a separate service if we were to use another Pub/Sub. It‚Äôs responsible for matching two players from the  queue.</p><p>To do that, I used a <a href=\"https://watermill.io/docs/messages-router/\" target=\"_blank\" rel=\"noreferrer\">Router</a> from Watermill, which gives a lot of features that are pretty nice to build event-driven systems. In our case, I‚Äôm just using it to add a handler for the  topic, which can be done just like this:</p><div><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div></div><p>Handler functions in Watermill work like you would expect, by taking a message as input to process it.</p><div><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div></div><p>What‚Äôs really important are the parts that are highlighted. You should be able to see the basic matchmaking logic which is to set a player as  if no other player is waiting. And when a second player joins, match them together and publish the result to both players.</p><p>Last but not least, we have to take a look at the  function which allocates a random GameServer and returns its IP and port. To do that, I simply use the Kubernetes API to create a resource like we made earlier.</p><div><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div></div><p>However, if you try deploying the matchmaking service just like that with a Deployment, it will actually not do anything. This is because by default, we are using the  ServiceAccount to access the Kubernetes API from our Pod. To fix this, we just need to create a new  and a  that grants the necessary permission to create  resources.</p><div><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div></div><div><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div></div><div><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div></div><p>And then, we can use this newly created ServiceAccount in our Deployment:</p><div><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div></div><p>Now, we can just create a Service for this Deployment and access it using port-forwarding like that:</p><div><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div></div><p>If we access the matchmaking service at  and try to play a game, we get this:</p><p>As you can see from the screen briefly flashing to black, the matchmaking service indeed redirects to a game server once a match is found.</p><p>Something to keep in mind is that in its current state, the matchmaking is not scalable. You can‚Äôt really run multiple instances of the matchmaking as you could end up with players stuck in different matcher‚Äôs instances.</p><p>However, it shouldn‚Äôt really matter as you can shard the matchmaking service by region (eu, us, etc.) or skill-level (<a href=\"https://en.wikipedia.org/wiki/Elo_rating_system\" target=\"_blank\" rel=\"noreferrer\">Elo</a>, rank). Then, you can have an instance of the matchmaking service for each shard. For example, you could have an instance running only on <code>eu.elo100-200.matchmaking</code> and one on <code>us.elo100-200.matchmaking</code>.</p><p>Also, I used a WebSocket again because I shamelessly copy-pasted the code from the game server as the base for the matchmaking service. However, you would be better off using an HTTP API where you issue a ticket and poll the match result. Or, maybe even <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\" target=\"_blank\" rel=\"noreferrer\">SSE</a>?</p><h2>Setting up autoscaling of game servers</h2><p>Everything works pretty well so far, right? Well, there‚Äôs still a problem that remains to be solved. If you‚Äôve followed along until now, so far we have a game running on Agones. There are multiple instances and a matchmaking service that routes each player to one of them. However, if we have 6 players all playing at the same time, we‚Äôll end up with our 3 games instances being allocated, making it impossible for the matchmaking service to find a game for any new players.</p><p>To solve this issue, we have to set up  for our fleet of game servers. To do that, we need to create a <a href=\"https://agones.dev/site/docs/reference/fleetautoscaler/\" target=\"_blank\" rel=\"noreferrer\">FleetAutoscaler</a>:</p><div><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div></div><p>I set it up with a  which ensures that there‚Äôs always a buffer of ready game servers available. In this case, I set it to 10 instances which are checked every 5 seconds.</p><p>There are other policies which are also interesting to look at such as:</p><ul><li>The <a href=\"https://agones.dev/site/docs/reference/fleetautoscaler/#counter-and-list-autoscaling\" target=\"_blank\" rel=\"noreferrer\">counter policy</a> which scales based on a GameServer counter. It can be useful if you set up multiple rooms in a single game instance like I mentioned earlier.</li><li>The <a href=\"https://agones.dev/site/docs/reference/fleetautoscaler/#webhook-autoscaling\" target=\"_blank\" rel=\"noreferrer\">webhook policy</a> which allows us to scale based on a custom logic we can implement as a webhook handler. We can, for instance, scale it based on the number of players waiting in the matchmaking system.</li><li>The <a href=\"https://agones.dev/site/docs/reference/fleetautoscaler/#wasm-autoscaling\" target=\"_blank\" rel=\"noreferrer\">WASM policy</a> which as its name implies, allows us to scale based on a custom logic using WebAssembly modules. I have yet to find a use case for it, but it‚Äôs definitely interesting to explore.</li><li>The <a href=\"https://agones.dev/site/docs/reference/fleetautoscaler/#schedule-and-chain-autoscaling\" target=\"_blank\" rel=\"noreferrer\">Schedule policy</a> which is pretty neat as it allows us to set a policy for a specific time period. It can be useful to scale up during an event or for the release of a game, for example.</li></ul><p>For simplicity‚Äôs sake, we‚Äôll continue with the buffer policy as it works decently well if we set the sync interval to a low value.</p><p>Now, for the fun part, let‚Äôs put this autoscaling to the test!</p><p>There‚Äôs a tool called <a href=\"https://k6.io/\" target=\"_blank\" rel=\"noreferrer\">k6</a> which is a load testing tool made by Grafana that can be used to simulate a large number of users connecting to our game server. We can use it to test our autoscaling policy and see how it performs under load. It simulates users with a custom script that can be written in JavaScript.</p><p>Here‚Äôs the one I made for this project:</p><div><div><pre tabindex=\"0\"><code data-lang=\"js\"></code></pre></div></div><p>As you can see, this script  opens the WebSocket connection with the matchmaking service and just sends a GET request to the game server.</p><p>We‚Äôll be running this script in k6 with 100 virtual users for a duration of 30 seconds.</p><p>As you can see, the autoscaler has a hard time keeping up with the load. To avoid that, we can increase the buffer size and decrease the sync interval. Or even better, switch to the webhook policy and implement a webhook endpoint which exposes the number of players currently waiting for a game server allocation.</p><p>This little experiment with Agones took longer than I first expected it to be, but I learned a lot and had quite some fun. Overall, I would say that Agones is very interesting in the way it transforms how we work with Kubernetes.</p><p>I think making a game and a matchmaking system from scratch to work with Agones really helped me understand better how concepts would work together. I understood so much more about Agones doing it this way than I did at first when going through the documentation.</p><p>Still, there are many things I haven‚Äôt tried, such as the other autoscaling policies, using counters and lists, or just working with an actual game server with real-time communication in UDP. There are also related projects such as <a href=\"https://github.com/embarkstudios/quilkin\" target=\"_blank\" rel=\"noreferrer\">Quilkin</a> which is a UDP proxy that can be used to route traffic to game servers and seems to work well with Agones.</p><p>I hope this article has been helpful for you and that you have learned something new about Agones and Kubernetes. I would appreciate any feedback you might have on this article. Thank you for reading!</p>",
      "contentLength": 21445,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/kubernetes/comments/1qjqbsx/making_and_scaling_a_game_server_in_kubernetes/"
    },
    {
      "title": "New release of my RSS feed reader (v0.5)",
      "url": "https://www.reddit.com/r/golang/comments/1qjpq1d/new_release_of_my_rss_feed_reader_v05/",
      "date": 1769072682,
      "author": "/u/proc_",
      "guid": 38058,
      "unread": true,
      "content": "<div><p>Yesterday I released a new version of my RSS feed reader written in Go. Fixed a lot of issues reported by users and added a few new features as well.</p><p>Enjoy and any feedback/PR's are welcome!</p></div>   submitted by   <a href=\"https://www.reddit.com/user/proc_\"> /u/proc_ </a>",
      "contentLength": 217,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Using Go Workspaces? Stop scripting loops and use the work pattern",
      "url": "https://www.reddit.com/r/golang/comments/1qjngob/using_go_workspaces_stop_scripting_loops_and_use/",
      "date": 1769064413,
      "author": "/u/jayp0521",
      "guid": 37935,
      "unread": true,
      "content": "<p>‚ÄãI haven't seen this discussed much in articles or tutorials, so I wanted to share a massive quality-of-life feature I stumbled across while digging through PRs.</p><p>If you use Go Workspaces, you have probably tried running `go generate ./...` from the root, only to find it fails or ignores your modules. Usually, the \"fix\" is searching online and finding hacky scripts involving sed, xargs, or manually iterating through every module one by one. It is annoying and brittle.</p><p>It turns out there is a native, elegant way to run commands against every module in your go.work file simultaneously. You simply use work as the package target.</p><p>Even AI assistants seem to hallucinate or get confused when I ask about this, likely because it‚Äôs a newer pattern that hasn't made it into the training data yet. Hopefully, this saves you some scripting time! Believe you need 1.25+</p>",
      "contentLength": 865,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "[D] Which data design patterns have held up for you in production?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qjmqy8/d_which_data_design_patterns_have_held_up_for_you/",
      "date": 1769062021,
      "author": "/u/Aggravating_Map_2493",
      "guid": 37946,
      "unread": true,
      "content": "<p>I came across this article on <a href=\"https://medium.com/aws-in-plain-english/data-engineering-design-patterns-you-must-learn-in-2026-c25b7bd0b9a7\">data design patterns </a>and found it grounded in real system behavior rather than tools. It walks through patterns that show up when supporting ML and AI workloads at scale. After reading this , I was curious to hear from others here: which patterns you rely on most, which ones failed under scale and patterns you think are overused. I am keen on hearing more about failures and lessons learned than success stories from people who have been there and done that.</p>",
      "contentLength": 490,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "High cardinality explained with interactive examples",
      "url": "https://signoz.io/blog/high-cardinality-data/",
      "date": 1769059391,
      "author": "/u/ankit01-oss",
      "guid": 37951,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qjlwk5/high_cardinality_explained_with_interactive/"
    },
    {
      "title": "Essay: Performance Reviews in Big Tech: Why ‚ÄúFair‚Äù Systems Still Fail",
      "url": "https://medium.com/@dmitrytrifonov/big-tech-performance-review-01fff2c5924d",
      "date": 1769057863,
      "author": "/u/NoVibeCoding",
      "guid": 37953,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qjleer/essay_performance_reviews_in_big_tech_why_fair/"
    },
    {
      "title": "Satya Nadella at Davos: a masterclass in saying everything while promising nothing",
      "url": "https://jpcaparas.medium.com/satya-nadella-at-davos-a-masterclass-in-saying-everything-while-promising-nothing-8495c75c5ba3?sk=a6efaf2b6a15adefcf82403ff62ef8da",
      "date": 1769056742,
      "author": "/u/jpcaparas",
      "guid": 37954,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/programming/comments/1qjl0r5/satya_nadella_at_davos_a_masterclass_in_saying/"
    },
    {
      "title": "This Week in Rust #635",
      "url": "https://this-week-in-rust.org/blog/2026/01/21/this-week-in-rust-635/",
      "date": 1769055215,
      "author": "/u/b-dillo",
      "guid": 38097,
      "unread": true,
      "content": "<p>This week's crate is <a href=\"https://crates.io/crates/throttled-tracing\">throttled-tracing</a>, a crate of periodic and throttled logging macros.</p><p>An important step for RFC implementation is for people to experiment with the\nimplementation and give feedback, especially before stabilization.</p><p>If you are a feature implementer and would like your RFC to appear in this list, add a\n label to your RFC along with a comment providing testing instructions and/or\nguidance on which aspect(s) of the feature need testing.</p><p><a href=\"https://github.com/rust-lang/this-week-in-rust/issues\">Let us know</a> if you would like your feature to be tracked as a part of this list.</p><p>Always wanted to contribute to open-source projects but did not know where to start?\nEvery week we highlight some tasks from the Rust community for you to pick and get started!</p><p>Some of these tasks may also have mentors available, visit the task page for more information.</p><p><em>No Calls for participation were submitted this week.</em></p><p>If you are a Rust project owner and are looking for contributors, please submit tasks <a href=\"https://github.com/rust-lang/this-week-in-rust?tab=readme-ov-file#call-for-participation-guidelines\">here</a> or through a <a href=\"https://github.com/rust-lang/this-week-in-rust\">PR to TWiR</a> or by reaching out on <a href=\"https://bsky.app/profile/thisweekinrust.bsky.social\">Bluesky</a> or <a href=\"https://mastodon.social/@thisweekinrust\">Mastodon</a>!</p><p>Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.</p><ul><li><a href=\"https://sessionize.com/rustconf-2026/\"></a> | CFP closes 2026-02-16 | Montreal, Quebec, Canada | 2026-09-08 - 2026-09-11</li></ul><p>If you are an event organizer hoping to expand the reach of your event, please submit a link to the website through a <a href=\"https://github.com/rust-lang/this-week-in-rust\">PR to TWiR</a> or by reaching out on <a href=\"https://bsky.app/profile/thisweekinrust.bsky.social\">Bluesky</a> or <a href=\"https://mastodon.social/@thisweekinrust\">Mastodon</a>!</p><p>Various changes in both direction, but not much has changed overall.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr><td align=\"center\">Improvements ‚úÖ  (secondary)</td></tr><tr></tr></tbody></table><p>3 Regressions, 4 Improvements, 7 Mixed; 6 of them in rollups\n40 artifact comparisons made in total</p><p>Every week, <a href=\"https://www.rust-lang.org/team.html\">the team</a> announces the 'final comment period' for RFCs and key PRs\nwhich are reaching a decision. Express your opinions now.</p><p>Let us know if you would like your PRs, Tracking Issues or RFCs to be tracked as a part of this list.</p><p>Rusty Events between 2026-01-21 - 2026-02-18 ü¶Ä</p><p>If you are running a Rust event please add it to the <a href=\"https://www.google.com/calendar/embed?src=apd9vmbc22egenmtu5l6c5jbfc%40group.calendar.google.com\">calendar</a> to get\nit mentioned here. Please remember to add a link to the event too.\nEmail the <a href=\"mailto:community-team@rust-lang.org\">Rust Community Team</a> for access.</p><blockquote><p>I might suspect that if you are lumping all statically-typed languages into a single bucket without making particular distinction among them, then you might not have fully internalized the implications of union (aka Rust enum aka sum) typed data structures combined with exhaustive pattern matching.</p><p>I like to call it getting \"union-pilled\" and it's really hard to accept otherwise statically-typed languages once you become familiar.</p></blockquote><p>This Week in Rust is edited by:</p>",
      "contentLength": 2582,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/rust/comments/1qjkhiv/this_week_in_rust_635/"
    },
    {
      "title": "Job Applicants Sue A.I. Recruitment Tool Company. A recently filed lawsuit claims the ratings assigned by A.I. screening software are similar to those of a credit agency and should be subject to the same laws.",
      "url": "https://www.nytimes.com/2026/01/21/business/ai-hiring-tools-lawsuit-eightfold-fcra.html?unlocked_article_code=1.GFA.9XQK.n_nH_2Z3omQR",
      "date": 1769053978,
      "author": "/u/esporx",
      "guid": 37945,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/artificial/comments/1qjk1us/job_applicants_sue_ai_recruitment_tool_company_a/"
    },
    {
      "title": "MLFS 12.4(musl LFS)",
      "url": "https://www.reddit.com/r/linux/comments/1qjiei6/mlfs_124musl_lfs/",
      "date": 1769049473,
      "author": "/u/Intelligent_Comb_338",
      "guid": 37938,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Redhat Openshift vs. Suse Rancher Enterprise Support",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qjhowq/redhat_openshift_vs_suse_rancher_enterprise/",
      "date": 1769047599,
      "author": "/u/Open-Ask-1918",
      "guid": 37962,
      "unread": true,
      "content": "<p>Looking for real world feedback from people who have had to utilize the enterprise support offerings from Redhat and Suse for OpenShift and Ranchers on premise solutions.</p><p>Who do you think provides better support?</p><p>I‚Äôm looking to create multiple downstream clusters integrated VMWare and want centralized management, monitoring, and deployments. I‚Äôm thinking Rancher is better suited for this purpose but value the feedback of others more experienced and haven‚Äôt had a chance to poke around at ACM from Redhat.</p><p>Also curious about which product you think is better for this job?</p>",
      "contentLength": 578,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Human Intelligence, AI, and the Problem I Think We're Missing",
      "url": "https://www.reddit.com/r/artificial/comments/1qjfapw/human_intelligence_ai_and_the_problem_i_think/",
      "date": 1769041308,
      "author": "/u/tony_24601",
      "guid": 37943,
      "unread": true,
      "content": "<p>I can vividly remember teaching my AP English class in 1999 when I first heard of ‚ÄúTurnitin.com‚Äù; my first thought was ‚Äúhow am I going to scan all of these pages into that thing?‚Äù Back then I graded papers on a first pass with my trusty No. 2 Dixon Ticonderoga pencil. Now what was I going to do?</p><p>For years I used my pencil as a key aid in the writing process with my students. It was collaborative because we worked together ‚Äì I would suggest ideas an reframe sentences and thoughts to model writing in line with whatever rubric my assignment called for. Often times students adopted my suggestions whole-cloth, other times we would workshop different stylistic choices. My students and I shared in the rhetorical process. If they chose to use my margin note ‚Äútry something like this,‚Äù are they not able to claim ownership because the original words were mine and not theirs?</p><p>I was the human intelligence that helped guide my students. They took my advice and incorporated it often. Other times they vehemently opposed my suggestions. I was their personal ChatGPT and I enjoyed that work immensely. But it was often brief and temporal, because I only had so much time to visit individually with 75 students. Can we really now castigate a tool that students can have beside them during every moment of their learning journey?</p><p>The ethical dilemma is this: students could accept, reject, argue with, or ignore me. Today, institutions now assume AI outputs are automatically suspect while often students see them as automatically authoritative. Agency is the key issue. When I suggested phrasing, students exercised their agency to decide whether to adopt or reject my suggestions. My authority was negotiable and if they accepted my suggestions, even verbatim, authorship was never in question.</p><p>Students are struggling today with teachers making them think AI is a ‚Äúforbidden oracle,‚Äù whereas teachers are also short-sighted in thinking Turnitin is an infallible detector. The problem is in both cases human judgment is being ‚Äúoutsourced.‚Äù In 1999, I trusted my students negotiate my (human) guidance; now we pretend that same negotiation between students and AI itself is the problem. What mattered was not that I was always right; but that my authority was provisional.</p><p>Fast forward almost 30 years and now we not only have a tool for students to generate a decent five-paragraph essay, but a second tool that claims it can detect the use of the first. And that tool is the same one I struggled to understand in 1999: Turnitin. Although this time Turnitin is losing the battle against this newer tool, and students all over academia are suffering from that loss.</p><p>Academia now is forced to embrace a structure that rewards certainty over caution. Boom: you get the AI-cheating accusation era. We‚Äôre living in a time where a student can be treated like they robbed a bank because a dashboard lit up yellow. Is this how math teachers felt about calculators when they first entered the scene? Can you today imagine any high-level mathematics course that didn‚Äôt somehow incorporate this tool? Is ChatGPT the ‚Äúwriting calculator‚Äù that in decades will sit beside every student in an English class along with that No. 2 Dixon Ticonderoga? Or will pencils continue to suffer a slow extinction?</p><p>I‚Äôm not writing this because I think academic dishonesty is cute. Students absolutely can use AI to outsource thinking, and pretending otherwise is na√Øve. I‚Äôm writing this because the process of accusing students is an ethical problem now. It‚Äôs not just ‚ÄúAre people cheating?‚Äù It‚Äôs ‚ÄúWhat evidence counts, who bears the burden, and how much harm are we willing to cause to catch some portion of cases?‚Äù When a school leans on AI detectors as objective arbiters, the ethics get ugly fast: false positives, biased outcomes, coerced confessions, and a general atmosphere of suspicion that corrodes learning.</p><p>I believe it is ethically wrong to treat AI-detection scores as dispositive evidence of misconduct; accusations should require due process and corroborating evidence. current detectors are error-prone and easy to game, and the harms of false accusations are severe. If institutions want integrity, they should design integrity‚Äîthrough assessment design, and clear AI-use policies, not outsource judgment to probabilistic software and call it ‚Äúaccountability.‚Äù MIT‚Äôs teaching-and-learning guidance says this bluntly: AI detection has high error rates and can lead to false accusations; educators should focus on policy clarity and assessment design instead of policing with detectors. (MIT Sloan Teaching &amp; Learning Technologies).</p><p>MA in Composition--AI Integrated Writing </p>",
      "contentLength": 4706,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "I have released dodo pdf reader v0.6.0",
      "url": "https://github.com/dheerajshenoy/dodo",
      "date": 1769039165,
      "author": "/u/dheerajshenoy22",
      "guid": 37937,
      "unread": true,
      "content": "<p>Hello everyone, wanted to share my pdf reader dodo that I have been working for a while. it's based on MuPDF and Qt6. I started developing it because I wanted some niche features that I could not find in others, and also wanted it to be minimal and not reduce screen real-estate.</p><p>Its still in alpha, I'm open to suggestions, feature requests etc.</p>",
      "contentLength": 345,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qjefp8/i_have_released_dodo_pdf_reader_v060/"
    },
    {
      "title": "Substitue for google appcheck and recaptcha",
      "url": "https://www.reddit.com/r/golang/comments/1qjddvm/substitue_for_google_appcheck_and_recaptcha/",
      "date": 1769036670,
      "author": "/u/Select_Day7747",
      "guid": 38089,
      "unread": true,
      "content": "<p>Hi, so I built an api that uses firebase auth with firebase admin. my choice was because it was what I was most comfortable with and it just works for my scenario where i have react vite front end application and planning on an android app soon as well. </p><p>my use case currently is that users can access pages on my site unauthenticated and authenticated. my concern is around unauthenticated requests so my solution was to use appcheck through firebase because it was trivial for both frontend and backend. But I feel like this adds some overhead to my requests, ive experienced it when there was slow internet in an area and my api was slow to respond, not the fault of the server but because google took ages to respond.</p><p>I was wondering if there are any other strategies that I could build in go that could be better suited to replace this? I love Go because it's lightweight, robust and loads of fun to develop. </p><p>I want to send an extra key in the header to make sure that the source can be trusted.</p>",
      "contentLength": 998,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Problemo: Yet another error handling library for Rust",
      "url": "https://www.reddit.com/r/rust/comments/1qjcus5/problemo_yet_another_error_handling_library_for/",
      "date": 1769035422,
      "author": "/u/emblemparade",
      "guid": 38003,
      "unread": true,
      "content": "<p>The biggest problem with Rust is that there are not enough error handling libraries... ha!</p><p>But, seriously, after trying all the popular ones and some of the niche ones in large, complex projects I found myself unsatisfied (and tired). However, the process did help me clarify what my specific needs are and identify common patterns and pain points. So I rolled my own library, and at this point I think it's Good Enough‚Ñ¢ to share with those who are interested:</p><p>The main documentation page is quite long, but the point is to explain the goals and solutions as clearly as possible. Towards the end there is a FAQ that will hopefully answer your most burning questions. Also included are examples that showcase some basic and advanced usage.</p><p>The main differentiator is that Problemo intends not to replace std  but to make working with it easier. It takes a, shall we say, compositional approach to constructing your errors. It also features a rather innovative way to accumulate errors (which you don't have to use).</p><p>Problemo is deliberately straightforward in its implementation and it should be very easy to understand the code.</p><p>I hope you would find it interesting even if you decide not to use it. I'm here to answer questions (check the FAQ first, maybe?), hoping you will be kind and constructive.</p>",
      "contentLength": 1298,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "\"proposal: spec: type inferred composite literals\" has been added to the active column of the proposals project",
      "url": "https://www.reddit.com/r/golang/comments/1qjbwth/proposal_spec_type_inferred_composite_literals/",
      "date": 1769033236,
      "author": "/u/theclapp",
      "guid": 37932,
      "unread": true,
      "content": "<p>Excerpt from the proposal:</p><blockquote><p>Composite literals construct values for structs, arrays, slices, and maps. They consist of a type followed by a brace-bound list of elements. e.g.,</p></blockquote><pre><code>x := []string{\"a\", \"b\", \"c\"} </code></pre><blockquote><p>I propose adding untyped composite literals, which omit the type. Untyped composite literals are assignable to any composite type. They do not have a default type, and it is an error to use one as the right-hand-side of an assignment where the left-hand-side does not have an explicit type specified.</p></blockquote><pre><code>var x []string = {\"a\", \"b\", \"c\"} var m map[string]int = {\"a\": 1} type T struct { V int } var s []*T = {{0}, {1}, {2}} a := {1, 2, 3} // error: left-hand-type has no type specified </code></pre><blockquote><p>Go already allows the elision of the type of a composite literal under certain circumstances. This proposal extends that permission to all occasions when the literal type can be derived.</p></blockquote>",
      "contentLength": 868,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Sysinfo next release needs some help",
      "url": "https://www.reddit.com/r/rust/comments/1qjbpbx/sysinfo_next_release_needs_some_help/",
      "date": 1769032782,
      "author": "/u/imperioland",
      "guid": 37955,
      "unread": true,
      "content": "<p>As a reminder, the <a href=\"https://crates.io/crates/sysinfo\">sysinfo</a> crate gathers System's information such as processes, memory usage, etc.</p><p>Next <a href=\"https://crates.io/crates/sysinfo\">sysinfo</a> release is kinda stuck at the moment as I'm trying to get the missing parts for the NetBSD support. Currently I'm missing:</p><p>If anyone knows how to get the missing information, it'd be awesome!</p><p>Otherwise, well, I'll just release an incomplete support.</p>",
      "contentLength": 359,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Why json/v2 remains experimental in 1.26?",
      "url": "https://www.reddit.com/r/golang/comments/1qjb1n7/why_jsonv2_remains_experimental_in_126/",
      "date": 1769031308,
      "author": "/u/alpako70",
      "guid": 37934,
      "unread": true,
      "content": "<div><p>This is unpleasant surprise. I assume it is still not ready for production. But would appreciate to learn what concerns lead authors to postpone it to later.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/alpako70\"> /u/alpako70 </a>",
      "contentLength": 188,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Results from the 2025 Go Developer Survey",
      "url": "https://go.dev/blog/survey2025",
      "date": 1769030943,
      "author": "/u/Bomgar85",
      "guid": 37936,
      "unread": true,
      "content": "<p>Hello! In this article we‚Äôll discuss the results of the 2025 Go Developer\nSurvey, conducted during September 2025.</p><p>Thank you to the 5,379 Go developers who responded to our survey invitation\nthis year. Your feedback helps both the Go team at Google and the wider Go\ncommunity understand the current state of the Go ecosystem and prioritize\nprojects for the year ahead.</p><p>Our three biggest findings are:</p><ul><li>Broadly speaking, Go developers asked for help with identifying and applying\nbest practices, making the most of the standard library, and expanding the\nlanguage and built-in tooling with more modern capabilities.</li><li>Most Go developers are now using AI-powered development tools when seeking\ninformation (e.g., learning how to use a module) or toiling (e.g., writing\nrepetitive blocks of similar code), but their satisfaction with these tools\nis middling due, in part, to quality concerns.</li><li>A surprisingly high proportion of respondents said they frequently need to\nreview documentation for core  subcommands, including , , and , suggesting meaningful room for improvement with the \ncommand‚Äôs help system.</li></ul><p>Read on for the details about these findings, and much more.</p><p>Most survey respondents self-identified as professional developers (87%) who\nuse Go for their primary job (82%). A large majority also uses Go for personal\nor open-source projects (72%). Most respondents were between 25 ‚Äì 45\nyears old (68%) with at least six years of professional development experience\n(75%). Going deeper, 81% of respondents told us they had more professional\ndevelopment experience than Go-specific experience, strong evidence that Go is\nusually not the first language developers work with. In fact, one of the\nthemes that repeatedly surfaced during this year‚Äôs survey analysis seems to\nstem from this fact: when the way to do a task in Go is substantially\ndifferent from a more familiar language, it creates friction for developers to\nfirst learn the new (to them) idiomatic Go pattern, and then to consistently\nrecall these differences as they continue to work with multiple languages.\nWe‚Äôll return to this theme later.</p><p>The single most common industry respondents work in was ‚ÄúTechnology‚Äù (46%),\nbut a majority of respondents work outside of the tech industry (54%). We saw\nrepresentation of all sizes of organizations, with a bare majority working\nsomewhere with 2 ‚Äì 500 employees (51%), 9% working alone, and 30%\nworking at enterprises of over 1,000 employees. As in prior years, a majority\nof responses come from North America and Europe.</p><p>This year we observed a decrease in the proportion of respondents who said\nthey were fairly new to Go, having worked with it for less than one year\n(13%, vs. 21% in 2024). We suspect this is related to <a href=\"https://digitaleconomy.stanford.edu/wp-content/uploads/2025/08/Canaries_BrynjolfssonChandarChen.pdf\">industry-wide\ndeclines in entry-level software engineering roles</a>; we commonly hear from\npeople that they learned Go for a specific job, so a downturn in hiring would\nbe expected to reduce the number of developers learning Go in that year. This\nhypothesis is further supported by our finding that over 80% of respondents\nlearned Go  beginning their professional career.</p><p>Other than the above, we found no significant changes in other demographics\nsince our 2024 survey.</p><h2>How do people feel about Go?</h2><p>The vast majority of respondents (91%) said they felt satisfied while working\nwith Go. Almost ‚Öî were ‚Äúvery satisfied‚Äù, the highest rating. Both of these\nmetrics are incredibly positive, and have been stable since we began asking\nthis question in 2019. The stability over time is really what we monitor from\nthis metric ‚Äî we view it as a lagging indicator, meaning by the time\nthis satisfaction metric shows a meaningful change, we would expect to already\nhave seen earlier signals from issue reports, mailing lists, or other\ncommunity feedback.</p><p>Why were respondents so positive about Go? Looking at open-text responses to\nseveral different survey questions suggests that it‚Äôs the gestalt, rather than\nany one thing. These folks are telling us that they find tremendous value in\nGo as a holistic platform. That doesn‚Äôt mean it supports all programming\ndomains equally well (it surely does not), but that developers‚Äô value the\ndomains it  nicely support via stdlib and built-in tooling.</p><p>Below are some representative quotations from respondents. To provide context\nfor each quote, we also identify the satisfaction level, years of experience\nwith Go, and industry of the respondent.</p><blockquote><p>‚ÄúGo is by far my favorite language; other languages feel far too complex and\nunhelpful. The fact that Go is comparatively small, simple, with fewer bells\nand whistles plays a massive role in making it such a good long-lasting\nfoundation for building programs with it. I love that it scales well to\nbeing used by a single programmer and in large teams.‚Äù </p></blockquote><blockquote><p>‚ÄúThe entire reason I use Go is the great tooling and standard library.  I‚Äôm\nvery thankful to the team for focusing on great HTTP, crypto, math, sync,\nand other tools that make developing service-oriented applications easy and\nreliable.‚Äù </p></blockquote><blockquote><p>‚Äú[The] Go ecosystem is the reason why I really like the programming\nlanguage. There are a lot of npm issues lately but not with Go.‚Äù </p></blockquote><p>This year we also asked about the other languages that people use. Survey\nrespondents said that besides Go, they enjoy working with Python, Rust, and\nTypeScript, among a long tail of other languages. Some shared characteristics\nof these languages align with common points of friction reported by Go\ndevelopers, including  areas like error handling, enums, and object-oriented\ndesign patterns. For example, when we sum the proportion of respondents who\nsaid their next-favorite language included one of the following factors, we\nfound that majorities of respondents enjoy using languages with inheritance,\ntype-safe enums, and exceptions, with only a bare majority of these languages\nincluding a static type system by default.</p><table><thead><tr><th>Proportion of respondents</th></tr></thead><tbody></tbody></table><p>We think this is important because it reveals the larger environment in which\ndevelopers operate ‚Äî it suggests that people need to use different\ndesign patterns for fairly mundane tasks, depending on the language of the\ncodebase they‚Äôre currently working on. This leads to additional cognitive load\nand confusion, not only among developers new to Go (who must learn idiomatic\nGo design patterns), but also among the many developers who work in multiple\ncodebases or projects. One way to alleviate this additional load is\ncontext-specific guidance, such as a tutorial on ‚ÄúError handling in Go for\nJava developers‚Äù. There may even be opportunities to build some of this\nguidance into code analyzers, making it easier to surface directly in an IDE.</p><p>This year we asked the Go community to share their sentiment towards the Go\nproject itself. These results were quite different from the 91% satisfaction\nrate we discussed above, and point to areas the Go Team plans to invest our\nenergy during 2026. In particular, we want to encourage more contributors to\nget involved, and ensure the Go Team accurately understands the challenges Go\ndevelopers currently face. We hope this focus, in turn, will help to increase\ndeveloper trust in both the Go project and the Go Team leadership. As one\nrespondent explained the problem:</p><blockquote><p>‚ÄúNow that the founding first generation of Go Team members [are] not\ninvolved much anymore in the decision making, I am a bit worried about the\nfuture of Go in terms of quality of maintenance, and its balanced decisions\nso far wrt to changes in the language and std lib. More presence in form of\ntalks [by] the new core team members about the current state and future\nplans might be helpful to strengthen trust.‚Äù </p></blockquote><h2>What are people building with Go?</h2><p>We revised this list of ‚Äúwhat types of things do you build with Go?‚Äù from 2024\nwith the intent of more usefully teasing apart what people are building with\nGo, and avoid confusion around evolving terms like ‚Äúagents‚Äù. Respondent‚Äôs top\nuse cases remain CLIs and API services, with no meaningful change in either\nsince 2024. In fact, a majority of respondents (55%) said they build \nCLIs and API services with Go. Over ‚Öì of respondents specifically build cloud\ninfrastructure tooling (a new category), and 11% work with ML models, tools,\nor agents (an expanded category). Unfortunately embedded use cases were left\noff of the revised list, but we‚Äôll fix this for next year‚Äôs survey.</p><p>Most respondents said they are not currently building AI-powered features into\nthe Go software they work on (78%), with ‚Öî reporting that their software does\nnot use AI functionality at all (66%). This appears to be a decrease in\nproduction-related AI usage year-over-year; in 2024, 59% of respondents were\nnot involved in AI feature work, while 39% indicated some level of\ninvolvement. That marks a shift of 14 points away from building AI-powered\nsystems among survey respondents, and may reflect some natural pullback from\nthe early hype around AI-powered applications: it‚Äôs plausible that lots of\nfolks tried to see what they could do with this technology during its initial\nrollout, with some proportion deciding against further exploration (at least\nat this time).</p><p>Among respondents who are building AI- or LLM-powered functionality, the most\ncommon use case was to create summaries of existing content (45%). Overall,\nhowever, there was little difference between most uses, with between 28%\n‚Äì 33% of respondents adding AI functionality to support classification,\ngeneration, solution identification, chatbots, and software development.</p><h2>What are the biggest challenges facing Go developers?</h2><p>One of the most helpful types of feedback we receive from developers are\ndetails about the challenges people run into while working with Go. The Go\nTeam considers this information holistically and over long time horizons,\nbecause there is often tension between improving Go‚Äôs rougher edges and\nkeeping the language and tooling consistent for developers. Beyond technical\nfactors, every change also incurs some cost in terms of developer attention\nand cognitive disruption. Minimizing disruption may sound a bit dull or\nboring, but we view this as an important strength of Go. As Russ Cox wrote in\n2023, <a href=\"https://go.dev/blog/compat\">‚ÄúBoring is good‚Ä¶ Boring means being able to focus on your work, not\non what‚Äôs different about Go.‚Äù</a>.</p><p>In that spirit, this year‚Äôs top challenges are not radically different from\nlast year‚Äôs. The top three frustrations respondents reported were ‚ÄúEnsuring\nour Go code follows best practices / Go idioms‚Äù (33% of respondents), ‚ÄúA\nfeature I value from another language isn‚Äôt part of Go‚Äù (28%), and ‚ÄúFinding\ntrustworthy Go modules and packages‚Äù (26%). We examined open-text responses to\nbetter understand what people meant. Let‚Äôs take a minute to dig into each.</p><p>Respondents who were most frustrated by writing idiomatic Go were often\nlooking for more official guidance, as well as tooling support to help enforce\nthis guidance in their codebase. As in prior surveys, questions about how to\nstructure Go projects were also a common theme. For example:</p><blockquote><p>‚ÄúThe simplicity of go helps to read and understand code from other\ndevelopers, but there are still some aspects that can differ quite a lot\nbetween programmers. Especially if developers come from other languages,\ne.g. Java.‚Äù </p></blockquote><blockquote><p>‚ÄúMore opinionated way to write go code. Like how to structure a Go project\nfor services/cli tool.‚Äù </p></blockquote><blockquote><p>‚ÄúIt‚Äôs hard to figure out what are good idioms. Especially since the core\nteam doesn‚Äôt keep Effective Go up-to-date.‚Äù </p></blockquote><p>The second major category of frustrations were language features that\ndevelopers enjoyed working with in other ecosystems. These open-text comments\nlargely focused on error handling and reporting patterns, enums and sum types,\nnil pointer safety, and general expressivity / verbosity:</p><blockquote><p>‚ÄúStill not sure what is the best way to do error handling.‚Äù </p></blockquote><blockquote><p>‚ÄúRust‚Äôs enums are great, and lead to writing great type safe code.‚Äù </p></blockquote><blockquote><p>‚ÄúThere is nothing (in the compiler) that stops me from using a maybe nil\npointer, or using a value without checking the err first. That should be\n[baked into] the type system.‚Äù </p></blockquote><blockquote><p>‚ÄúI like [Go] but I didn‚Äôt expect it to have nil pointer exceptions :)‚Äù </p></blockquote><blockquote><p>‚ÄúI often find it hard to build abstractions and to provide clear intention\nto the future readers of my code.‚Äù </p></blockquote><p>The third major frustration was finding trustworthy Go modules. Respondents\noften described two aspects to this problem. One is that they considered many\n3rd-party modules to be of marginal quality, making it hard for really good\nmodules to stand out. The second is identifying which modules are commonly\nused and under which types of conditions (including recent trends over time).\nThese are both problems that could be addressed by showing what we‚Äôll vaguely\ncall ‚Äúquality signals‚Äù on pkg.go.dev. Respondents provided helpful\nexplanations of the signals they use to identify trustworthy modules,\nincluding project activity, code quality, recent adoption trends, or the\nspecific organizations that support or rely upon the module.</p><blockquote><p>‚ÄúBeing able to filter by criteria like stable version, number of users and\nlast update age at pkg.go.dev could make things a bit easier.‚Äù </p></blockquote><blockquote><p>‚ÄúMany pacakges are just clones/forks or one-off pojects with no\nhistory/maintenance. [sic]‚Äù </p></blockquote><blockquote><p>‚ÄúMaybe flagging trustworthy packages based on experience, maturity and\ncommunity feedback?‚Äù </p></blockquote><p>We agree that these are all areas where the developer experience with Go could\nbe improved. The challenge, as discussed earlier, is doing so in such a way\nthat doesn‚Äôt lead to breaking changes, increased confusion among Go\ndevelopers, or otherwise gets in the way of people trying to get their work\ndone with Go. Feedback from this survey is a major source of information we\nuse when discussing proposals, but if you‚Äôd like to get involved more directly\nor follow along with other contributors, visit the <a href=\"https://github.com/golang/go/issues?q=state%3Aopen%20label%3AProposal\" rel=\"noreferrer\" target=\"_blank\">Go proposals on\nGitHub</a>;\nplease be sure to <a href=\"https://github.com/golang/proposal\" rel=\"noreferrer\" target=\"_blank\">follow this process</a> if\nyou‚Äôd like to add a new proposal.</p><p>In addition to these (potentially) ecosystem-wide challenges, this year we\nalso asked specifically about working with the  command. We‚Äôve informally\nheard from developers that this tool‚Äôs help system can be confusing to\nnavigate, but we haven‚Äôt had a great sense of how frequently people find\nthemselves reviewing this documentation.</p><p>Respondents told us that except for , between 15% ‚Äì 25% of them\nfelt they ‚Äúoften needed to review documentation‚Äù with working with these\ntools. This was surprising, especially for commonly-used subcommands like\n and . Common reasons included remembering specific flags,\nunderstanding what different options do, and navigating the help system\nitself. Participants also confirmed that infrequent use was one reason for\nfrustration, but navigating and parsing command help appears to be the\nunderlying cause. In other words, we all expect to need to review\ndocumentation sometimes, but we don‚Äôt expect to need help navigating the\ndocumentation system itself. As on respondent described their journey:</p><blockquote><p>‚ÄúAccessing the help is painful. go test ‚Äìhelp # didn‚Äôt work, but tell[s] me\nto type  instead‚Ä¶ go help test # oh, actually, the info I‚Äôm\nlooking for is in  go help testflag # visually parsing through\ntext that looks all the same without much formatting‚Ä¶ I just lack time to\ndig into this rabbit hole.‚Äù </p></blockquote><h2>What does their development environment look like?</h2><h3>Operating systems and architectures</h3><p>Generally, respondents told us their development platforms are UNIX-like. Most\nrespondents develop on macOS (60%) or Linux (58%) and deploy to Linux-based\nsystems, including containers (96%). The largest year-over-year change was\namong ‚Äúembedded devices / IoT‚Äù deployments, which increased from 2% -&gt; 8% of\nrespondents; this was the only meaningful change in deployment platforms since\n2024.</p><p>The vast majority of respondents develop on x86-64 or ARM64 architectures,\nwith a sizable group (25%) still potentially working on 32-bit x86 systems.\nHowever, we believe the wording of this question was confusing to respondents;\nnext year we‚Äôll clarify the 32-bit vs. 64-bit distinction for each\narchitecture.</p><p>Several new code editors have become available in the past two years, and we\nexpanded our survey question to include the most popular ones. While we saw\nsome evidence of early adoption, most respondents continued to favor <a href=\"https://code.visualstudio.com/\" rel=\"noreferrer\" target=\"_blank\">VS\nCode</a> (37%) or\n<a href=\"https://www.jetbrains.com/go/\" rel=\"noreferrer\" target=\"_blank\">GoLand</a> (28%). Of the newer editors, Zed and\nCursor were the highest ranked, each becoming the preferred editor of 4% of\nrespondents. To put those numbers in context, we looked back at when VS Code\nand GoLand were first introduced. VS Code (released in 2015) was favored by\n16% of respondents one year after its release. IntelliJ has had a\ncommunity-led Go plugin longer than we‚Äôve been surveying Go developers (üíô),\nbut if we look at when JetBrains began officially supporting Go in IntelliJ\n(2016), within one year IntelliJ was preferred by 20% of respondents.</p><p>Note: This analysis of code editors does not include respondents who were\nreferred to the survey directly from VS Code or GoLand.</p><p>The most common deployment environments for Go continue to be Amazon Web\nServices (AWS) at 46% of respondents, company-owned servers (44%), and Google\nCloud Platform (GCP) at 26%. These numbers show minor shifts since 2024, but\nnothing statistically significant. We found that the ‚ÄúOther‚Äù category\nincreased to 11% this year, and this was primarily driven by Hetzner (20% of\nOther responses); we plan to include Hetzner as a response choice in next\nyear‚Äôs survey.</p><p>We also asked respondents about their development experience of working with\ndifferent cloud providers. The most common responses, however, showed that\nrespondents weren‚Äôt really sure (46%) or don‚Äôt directly interact with public\ncloud providers (21%). The biggest driver behind these responses was a theme\nwe‚Äôve heard often before: with containers, it‚Äôs possible to abstract many\ndetails of the cloud environment away from the developer, so that they don‚Äôt\nmeaningfully interact with most provider-specific technologies. This result\nsuggests that even developers whose work is  to clouds may have\nlimited experience with the larger suite of tools and technology associated\nwith each cloud provider. For example:</p><blockquote><p>‚ÄúKinda abstract to the platform, Go is very easy to put in a container and\nso pretty easy to deploy anywhere: one of its big strength[s].‚Äù </p></blockquote><blockquote><p>‚ÄúThe cloud provider really doesn‚Äôt make much difference to me. I write code\nand deploy it to containers, so whether that‚Äôs AWS or GCP I don‚Äôt really\ncare.‚Äù </p></blockquote><p>We suspect this level of abstraction is dependant on the use case and\nrequirements of the service that‚Äôs being deployed ‚Äî it may not always\nmake sense or be possible to keep it highly abstracted. In the future, we plan\nto further investigate how Go developers tend to interact with the platforms\nwhere their software is ultimately deployed.</p><p>Finally, we can‚Äôt discuss development environments in 2025 without also\nmentioning AI-powered software development tools. Our survey suggests\nbifurcated adoption ‚Äî while a majority of respondents (53%) said they\nuse such tools daily, there is also a large group (29%) who do not use these\nat all, or only used them a few times during the past month. We expected this\nto negatively correlate with age or development experience, but were unable to\nfind strong evidence supporting this theory except for  new developers:\nrespondents with less than one year of professional development experience\n(not specific to Go) did report more AI use than every other cohort, but this\ngroup only represented 2% of survey respondents.</p><p>At this time, agentic use of AI-powered tools appears nascent among Go\ndevelopers, with only 17% of respondents saying this is their primary way of\nusing such tools, though a larger group (40%) are occasionally trying agentic\nmodes of operation.</p><p>The most commonly used AI assistants remain ChatGPT, GitHub Copilot, and\nClaude. Most of these agents show lower usage numbers <a href=\"https://go.dev/blog/survey2024-h2-results#ai-assistance\">compared with our 2024\nsurvey</a> (Claude and Cursor are\nnotable exceptions), but due to a methodology change, this is not an\napples-to-apples comparison. It is, however, plausible that developers are\n‚Äúshopping around‚Äù less than they were when these tools were first released,\nresulting in more people using a single assistant for most of their work.</p><p>We also asked about overall satisfaction with AI-powered development tools. A\nmajority (55%) reported being satisfied, but this was heavily weighted towards\nthe ‚ÄúSomewhat satisfied‚Äù category (42%) vs. the ‚ÄúVery satisfied‚Äù group (13%).\nRecall that Go itself consistently shows a 90%+ satisfaction rate each year;\nthis year, 62% of respondents said they are ‚ÄúVery satisfied‚Äù with Go. We add\nthis context to show that while AI-powered tooling is starting to see adoption\nand finding some successful use cases, developer sentiment  towards them\nremains much softer than towards more established tooling (among Go\ndevelopers, at least).</p><p>What is driving this lower rate of satisfaction? In a word: quality. We asked\nrespondents to tell us something good they‚Äôve accomplished with these tools,\nas well as something that didn‚Äôt work out well. A majority said that creating\nnon-functional code was their primary problem with AI developer tools (53%),\nwith 30% lamenting that even working code was of poor quality. The most\nfrequently cited benefits, conversely, were generating unit tests, writing\nboilerplate code, enhanced autocompletion, refactoring, and documentation\ngeneration. These appear to be cases where code quality is perceived as less\ncritical, tipping the balance in favor of letting AI take the first pass at a\ntask. That said, respondents also told us the AI-generated code in these\nsuccessful cases still required careful review (and often, corrections), as it\ncan be buggy, insecure, or lack context.</p><blockquote><p>‚ÄúI‚Äôm never satisfied with code quality or consistency, it never follows the\npractices I want to.‚Äù </p></blockquote><blockquote><p>‚ÄúAll AI tools tend to hallucinate quickly when working with medium-to-large\ncodebases (10k+ lines of code). They can explain code effectively but\nstruggle to generate new, complex features‚Äù </p></blockquote><blockquote><p>‚ÄúDespite numerous efforts to make it write code in an established codebase,\nit would take too much effort to steer it to follow the practices in the\nproject, and it would add subtle behaviour paths - i.e. if it would miss\nsome method it would try to find its way around it or rely on some side\neffect. Sometimes those things are hard to recognize during code review. I\nalso found it mentally taxing to review ai generated code and that overhead\nkills the productivity potential in writing code.‚Äù </p></blockquote><p>When we asked developers what they used these tools for, a pattern emerged\nthat is consistent with these quality concerns. The tasks with most adoption\n(green in the chart below) and least resistance (red) deal with bridging\nknowledge gaps, improving local code, and avoiding toil. The frustrations that\ndevelopers talk about with code-generating tools were much less evident when\nthey‚Äôre seeking information, like how to use a specific API or configure test\ncoverage, and perhaps as a result, we see higher usage of AI in these areas.\nAnother spot that stood out was  code review and related suggestions\n‚Äî people were less interested in using AI to review other people‚Äôs code\nthan in reviewing their own. Surprisingly, ‚Äútesting code‚Äù showed lower AI\nadoption than other toilsome tasks, though we don‚Äôt yet have strong\nunderstanding of why.</p><p>Of all the tasks we asked about, ‚ÄúWriting code‚Äù was the most bifurcated, with\n66% of respondents already or hoping to soon use AI for this, while ¬º of\nrespondents didn‚Äôt want AI involved at all. Open-ended responses suggest\ndevelopers primarily use this for toilsome, repetitive code, and continue to\nhave concerns about the quality of AI-generated code.</p><p>Once again, a tremendous thank-you to everyone who responded to this year‚Äôs Go\nDeveloper Survey!</p><p>We plan to share the raw survey dataset in Q1 2026, so the larger community\ncan also explore the data underlying these findings. This will only include\nresponses from people who opted in to share this data (82% of all\nrespondents), so there may be some differences from the numbers we reference\nin this post.</p><p>This survey was conducted between Sept 9 - Sept 30, 2025. Participants were\npublicly invited to respond via the Go Blog, invitations on social media\nchannels (including Bluesky, Mastodon, Reddit, and X), as well as randomized\nin-product invitations to people using VS Code and GoLand to write Go\nsoftware. We received a total of 7,070 responses. After data cleaning to\nremove bots and other very low quality responses, 5,379 were used for the\nremainder of our analysis. The median survey response time was between 12\n‚Äì 13 minutes.</p><p>Throughout this report we use charts of survey responses to provide supporting\nevidence for our findings. All of these charts use a similar format. The title\nis the exact question that survey respondents saw. Unless otherwise noted,\nquestions were multiple choice and participants could only select a single\nresponse choice; each chart‚Äôs subtitle will tell the reader if the question\nallowed multiple response choices or was an open-ended text box instead of a\nmultiple choice question. For charts of open-ended text responses, a Go team\nmember read and manually categorized all of the responses. Many open-ended\nquestions elicited a wide variety of responses; to keep the chart sizes\nreasonable, we condensed them to a maximum of the top 10-12 themes, with\nadditional themes all grouped under ‚ÄúOther‚Äù. The percentage labels shown in\ncharts are rounded to the nearest integer (e.g., 1.4% and 0.8% will both be\ndisplayed as 1%), but the length of each bar and row ordering are based on the\nunrounded values.</p><p>To help readers understand the weight of evidence underlying each finding, we\nincluded error bars showing the 95% <a href=\"https://en.wikipedia.org/wiki/Confidence_interval\" rel=\"noreferrer\" target=\"_blank\">confidence\ninterval</a> for responses;\nnarrower bars indicate increased confidence. Sometimes two or more responses\nhave overlapping error bars, which means the relative order of those responses\nis not statistically meaningful (i.e., the responses are effectively tied).\nThe lower right of each chart shows the number of people whose responses are\nincluded in the chart, in the form ‚Äún = [number of respondents]‚Äù.</p>",
      "contentLength": 26365,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qjavpe/results_from_the_2025_go_developer_survey/"
    },
    {
      "title": "Is agentless container security effective for Kubernetes workloads at scale?",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qja0wg/is_agentless_container_security_effective_for/",
      "date": 1769029054,
      "author": "/u/amylanky",
      "guid": 37959,
      "unread": true,
      "content": "<p>We're running hundreds of Kubernetes workloads across multiple clusters, and the idea of deploying agents into every container feels unsustainable. Performance overhead, image bloat, and operational complexity are all concerns.</p><p>Is agentless container security actually viable, or is it just marketing? anyone actually secured container workloads at scale without embedding agents everywhere?</p>",
      "contentLength": 390,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "What are the top 5 safe, high-paying jobs that AI is unlikely to replace over the next few decades?",
      "url": "https://www.reddit.com/r/artificial/comments/1qj91oh/what_are_the_top_5_safe_highpaying_jobs_that_ai/",
      "date": 1769026875,
      "author": "/u/Curious_Suchit",
      "guid": 37944,
      "unread": true,
      "content": "<p>As AI continues to automate routine and analytical tasks, many roles will evolve or disappear. This raises an important question about which careers can offer long-term security, meaningful work, and strong earning potential in an AI-driven world</p>",
      "contentLength": 246,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "The NexPhone is an upcoming phone that can boot desktop Linux along with Android (and Microslop Windows 11) - made for USB-C docking to monitors",
      "url": "https://nexphone.com/",
      "date": 1769025334,
      "author": "/u/HiGuysImNewToReddit",
      "guid": 37942,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/linux/comments/1qj8cfo/the_nexphone_is_an_upcoming_phone_that_can_boot/"
    },
    {
      "title": "Backpressure Patterns in Go: From Channels to Queues to Load Shedding",
      "url": "https://medium.com/@Realblank/backpressure-patterns-in-go-from-channels-to-queues-to-load-shedding-0841c9fe5607",
      "date": 1769024325,
      "author": "/u/Real_Blank",
      "guid": 37933,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qj7w1x/backpressure_patterns_in_go_from_channels_to/"
    },
    {
      "title": "Belgrade Go meetup - Thursday, Jan 29",
      "url": "https://www.meetup.com/golang-serbia/events/312819083/",
      "date": 1769023724,
      "author": "/u/GaussCarl",
      "guid": 37931,
      "unread": true,
      "content": "<p>Join us for the next Golang meetup on January 29th! We have two great talks:\nWe will explore how testing multiple variables can identify meaningful improvements in any project. As an example, we will see how it can inform game design decisions and identify most optimal game variation. At the end we will see how a testing framework can be implemented in Go.<p>\nüá¨üáß The presentation will be in English.</p><strong>Desktop Applications with Go and Fyne</strong>\nHow to build a desktop application using Fyne. Pros and cons of UI development in Go, beginner-friendly, with practical demos. If you're interested in building Desktop applications in Go, this meetup is for you!<p>\nüá∑üá∏ The presentation will be in Serbian.</p><p>\nLocation: Finbet Belgrade Office, Ju≈æni bulevar 10, Belgrade</p>\nPlease RSVP to help us with planning.\nPozivamo vas na naredni Golang meetup koji ƒáe se odr≈æati 29.01. Imamo dva sjajna predavanja:<strong>Multivarijantno testiranje</strong>\nIstra≈æiƒáemo kako testiranje vi≈°e promenljivih mo≈æe da pomogne u otkrivanju znaƒçajnih pobolj≈°anja u bilo kom projektu. Kao primer, videƒáemo kako se ovaj pristup mo≈æe koristiti za dono≈°enje odluka u dizajnu igara i za identifikovanje najoptimalnije varijante igre. Na kraju ƒáemo videti kako se testing framework mo≈æe implementirati u Go-u.<p>\nüá¨üáß Prezentacija ƒáe biti na engleskom jeziku.</p><strong>Desktop aplikacije sa Go i Fyne</strong>\nKako razviti desktop aplikaciju koristeƒái Fyne. Prednosti i mane UI developmenta u Go-u, beginner-friendly, sa praktiƒçnim demo primerima. Ako vas zanima pravljenje Desktop aplikacija u Go-u, ovo je meetup koji ne smete propustiti!<p>\nüá∑üá∏ Prezentacija ƒáe biti na srpskom jeziku.</p><p>\nLokacija: Prostorije Finbet-a, Ju≈æni bulevar 10, Beograd</p>\nMolimo vas za RSVP kako bismo lak≈°e isplanirali dogaƒëaj.</p>",
      "contentLength": 1752,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/golang/comments/1qj7m7f/belgrade_go_meetup_thursday_jan_29/"
    },
    {
      "title": "RISC-V Kubernetes cluster with Jenkins on 3x StarFive VisionFive 2 (Lite)",
      "url": "https://youtube.com/watch?v=641TnJyHt4g&amp;si=AqXuJ3-A5M7PkQjS",
      "date": 1769018031,
      "author": "/u/Opvolger",
      "guid": 37958,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://www.reddit.com/r/kubernetes/comments/1qj4xul/riscv_kubernetes_cluster_with_jenkins_on_3x/"
    },
    {
      "title": "ArgoCD / Kargo + GitOps Help/Suggestions",
      "url": "https://www.reddit.com/r/kubernetes/comments/1qj4dyt/argocd_kargo_gitops_helpsuggestions/",
      "date": 1769016832,
      "author": "/u/pixel-pusher-coder",
      "guid": 37961,
      "unread": true,
      "content": "<p>I've been running an argocd setup that seems to work pretty well. The main issue I had with it was that testing a deployment on say staging involves pushing to git main in order to get argo to apply my changes. </p><p>I'm trying to avoid using labels. I know there's patterns that use that, but if the data is not in git to me that defeats the point. </p><p>So I looked and a few GitOps solutions and Kargo seemed to be the most interesting one. The basic flow seems to be pretty slick. </p><p>Watch for changes (Warehouse), creates a change-set (Freight) and Promote the change to the given Stage. </p><p>The main thing that seems to be missing is applying a diff for a given environment that has both a version change AND a config change. </p><p>So say I have a new helm chart with some breaking changes. I'd like to configure some values.yaml changes for say staging and update to version 2.x and promote those together to staging. If that works, It would be nice to apply the diff to prod, then staging, etc. </p><p>It feels like Kargo only supports artifacts without say git/config changes. How do people manage this? If I have to do a PR for each env that won't be reflected till they get merged, then you might as well just update the version in your PR. The value add of kargo seems pretty minor at that point.</p><p>Am I missing something? How to you take a change and promote it through various stages? Right now I'm just committing to main since everything is staging still but that doesn't seem like a proper pattern. </p>",
      "contentLength": 1481,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "[D] Do you feel like companies are scooping / abusing researchers for ideas during hiring for researcher roles?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qj3t98/d_do_you_feel_like_companies_are_scooping_abusing/",
      "date": 1769015604,
      "author": "/u/quasiproductive",
      "guid": 37948,
      "unread": true,
      "content": "<p>After having gone through at least 3 rounds where I had to present research solutions for problems, I get the feeling that I'm doing free labour for these guys. They usually give you a week and given the current glut of candidates, it feels like this could easily be happening in the background. This includes Mid tech companies (not FAANG) and startups. Is there some truth to this suspicion?</p><p>For the most recent one, I purposefully chose not to dive into the advanced literature heavy stuff even though I did do the work. The scope of the task was pretty vague (\"design an ML system blah blah\") and as soon as I started my presentation, one of my interviewers immediately questioned me about whether I had read the literature and wasn't interested in older approaches to the same problem. The rest of the interview was spent getting grilled, as is usual. My motivation was to work bottom up and demonstrate strong fundamentals. Perhaps, I'm missing something here</p>",
      "contentLength": 964,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    }
  ],
  "tags": [
    "reddit"
  ]
}
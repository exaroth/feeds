{
  "id": "6W9",
  "title": "HN",
  "displayTitle": "HN",
  "url": "",
  "feedLink": "",
  "isQuery": true,
  "isEmpty": false,
  "isHidden": false,
  "itemCount": 219,
  "items": [
    {
      "title": "The Overcomplexity of the Shadcn Radio Button",
      "url": "https://paulmakeswebsites.com/writing/shadcn-radio-button/",
      "date": 1768894509,
      "author": "dbushell",
      "guid": 37124,
      "unread": true,
      "content": "<p>The other day I was asked to update the visual design of radio buttons in a web\napp at work. I figured it couldn't be that complicated. It's just a radio button\nright?</p><pre><code></code></pre><p>Boom! Done. Radio buttons are a built-in HTML element. They've been around for\n30 years. The browser makes it easy. Time for a coffee.</p><p>I dug into our codebase and realized we were using two React components from\n<a href=\"https://ui.shadcn.com/\">Shadcn</a> to power our radio buttons:  and\n.</p><p>For those unfamiliar with Shadcn, it's a UI framework that provides a bunch of\nprebuilt UI components for use in your websites. Unlike traditional UI\nframeworks like Bootstrap, you don't import it with a script tag or\n. Instead you run a command that copies the components into your\ncodebase.</p><p>Here's the code that was exported from Shadcn into our project:</p><pre><code> React  RadioGroupPrimitive  CircleIcon  cn \n  classNameprops\n ReactComponentProps RadioGroupPrimitiveRoot\n  classNameprops\n ReactComponentProps RadioGroupPrimitiveItem RadioGroup RadioGroupItem </code></pre><p>Woof... 3 imports and 45 lines of code. And it's importing a third party icon\nlibrary just to render a circle. (Who needs CSS  or the SVG\n element when you can add a third party dependency instead?)</p><p>All of the styling is done by the 30 different Tailwind classes in the markup. I\nshould probably just tweak those to fix the styling issues.</p><p>But now I'm distracted, annoyed, and curious. Where's the actual ?\nWhat's the point of all this? Let's dig a little deeper.</p><p>The Shadcn components import components from another library called Radix. For\nthose unfamiliar with Radix, it's a UI framework that provides a bunch of\nprebuilt UI components...</p><p>Wait a second! Isn't that what I just said about Shadcn? What gives? Why do we\nneed both? Let's see what the Radix docs say:</p><blockquote><p>Radix Primitives is a low-level UI component library with a focus on\naccessibility, customization and developer experience. You can use these\ncomponents either as the base layer of your design system, or adopt them\nincrementally.</p></blockquote><p>So Radix provides unstyled components, and then Shadcn adds styles on top of\nthat. How does Radix work? You can see for yourself on GitHub:\n<a href=\"https://github.com/radix-ui/primitives/blob/main/packages/react/radio-group/src/radio.tsx\">https://github.com/radix-ui/...</a></p><p>This is getting even more complicated: 215 lines of React code importing 7 other\nfiles. But what does it actually do?</p><h2>Taking a look in the browser</h2><p>Let's look in the browser dev tools to see if we can tell what's going on.</p><p>Okay, instead of a radio input it's rendering a button with an SVG circle inside\nit? Weird.</p><p>It's also using\n<a href=\"https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA\">ARIA attributes</a>\nto tell screen readers and other assistive tools that the button is actually a\nradio button.</p><p>ARIA attributes allow you to change the semantic meaning of HTML elements. For\nexample, you can say that a button is actually a radio button. (If you wanted to\ndo that for some strange reason.)</p><blockquote><p>If you  use a native HTML element or attribute with the semantics and\nbehavior you require , instead of re-purposing an element\nand adding an ARIA role, state or property to make it accessible, .</p></blockquote><p>Despite that, Radix is repurposing an element and adding an ARIA role instead of\nusing a native HTML element.</p><p>Finally, the component also includes a hidden  but only if\nit's used inside of a  element. Weird!</p><p>This is getting pretty complicated to just render a radio button. Why would you\nwant to do this?</p><h2>Styling radio buttons is hard (Wait, is it?)</h2><p>My best guess is that Radix rebuilds the radio button from scratch in order to\nmake it easier to style. Radio buttons used to be difficult to style\nconsistently across browsers. But for several years we've been able to style\nradio buttons however we want using a few CSS tools:</p><ul><li> removes the radio button's default styling allowing us to\ndo whatever we want.</li><li>We can use the  pseudo-element to render a \"dot\" inside of the\nunstyled radio button.</li><li>We can use the  pseudo-class to show and hide that dot depending on\nwhether the radio button is checked.</li><li> makes things round.</li></ul><p>Here's an example implementation:</p><pre><code> none 0 1px solid black white 50% inline-grid center 0.75rem 0.75rem 50% black</code></pre><p>This doesn't require any dependencies, JavaScript, or ARIA roles. It's just an\ninput element with some styles. (You can do the same thing with Tailwind if\nthat's your jam.)</p><p>It does require knowledge of CSS but this isn't some arcane secret.\n<a href=\"https://www.google.com/search?q=how+to+style+a+radio+button\">Googling \"how to style a radio button\"</a>\nshows several blog posts explaining these techniques. You may say this is a lot\nof CSS, but the Shadcn component we were using had 30 Tailwind classes!</p><h2>I'm not trying to convince you to write your own component styles</h2><p>Look, I get it. You've got a lot going on. You're not big on CSS. You just want\nto grab some prebuilt components so you can focus on the actual problem you're\nsolving.</p><p>I totally understand why people reach for component libraries like Shadcn and I\ndon't blame them at all. But I wish these component libraries would keep things\nsimple and reuse the built-in browser elements where possible.</p><p>Web development is hard. There's inherent complexity in building quality sites\nthat solve problems and work well across a wide range of devices and browsers.</p><p>But some things don't have to be hard. Browsers make things like radio buttons\neasy. Let's not overcomplicate it.</p><p>To understand how our radio buttons work I need to understand two separate\ncomponent libraries and hundreds of lines of React.</p><p>Website visitors need to wait for JavaScript to load, parse, and run in order to\nbe able to toggle a radio button. (In my testing, just adding these components\nadded several KB of JS to a basic app.)</p><p>Why am I making such a big deal out of this? It's just a radio button.</p><p>But these small decisions add up to more complexity, more cognitive load, more\nbugs, and worse website performance.</p><h2>We have strayed so far from the light</h2><p>Look at it. It's beautiful:</p><pre><code></code></pre>",
      "contentLength": 5704,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46688971"
    },
    {
      "title": "F-16 Falcon Strike",
      "url": "https://webchrono.pl/F16FalconStrike/index.html",
      "date": 1768879585,
      "author": "starkparker",
      "guid": 37109,
      "unread": true,
      "content": "<p>© 2023-2026 by Jarosław 'Roeoender' Wosik\nLatest sim version 2.0.2 released 2026-01-18<p>\nLatest docs update 2026-01-18.</p></p><p>Become Polish Air Force F-16 Pilot defending E.U. &amp; Polish border\nfrom B.A.R.F. (Belarussian And Russian Federation) aggression\nin fictional \"Królewiec Campaign\" of 15 varied missions.\nBe a part of dynamic war in introduced in v.2.0.0  mode with procedurally generated battlefield and fly countless missions in procedurally generated missions in  mode.\nApply strategic planning to defeat enemy air and ground forces, quickly update your plans according to developements in the simulated dynamic 3D battlefield.</p><p>All this and more on a classic unmodified 8-bit ATARI XL/XE with only 64Kb RAM.</p><p>With this game I'd like to pay homage to the golden era of 80/90's computer combat flight simulators.</p><div><p>No part of this game (neither code, nor artwork) was created with A.I./LLMs. or tools incorporating A.I. (no Copilot, no Photoshop).</p></div><h2>2026-01-18 - Version 2.0.2 released!</h2><p>Please inform me if you have written this game's review or streamed gameplay - I'd really like to read what people think about this game and how they play it.</p>",
      "contentLength": 1136,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46687588"
    },
    {
      "title": "Show HN: Artificial Ivy in the Browser",
      "url": "https://da.nmcardle.com/grow",
      "date": 1768878887,
      "author": "dnmc",
      "guid": 37120,
      "unread": true,
      "content": "<div>\n          This page simulates a biologically-inspired system with a few simple\n          rules. It begins with a single cell. Over time, cells repeatedly\n          decide whether to  and/or . The nearby sliders\n          adjust the probability of each action occurring.\n        </div><div>\n          When a cell splits, it creates a new cell pointed in a slightly\n          different direction. Decreasing the maximum turn angle causes cells to\n          grow in straighter lines.\n        </div><div>\n          Only the youngest cells get to grow or split. After a certain age,\n          cells become dormant. This threshold age is defined as a percentile on\n          the distribution of cell ages at any given time. Increasing the\n          threshold means more cells will be active.\n        </div><div>\n          Cells leave a signal in the location they were created. This effectively\n          broadcasts their presence so that others won't grow or split on top of\n          them. Increasing the decay rate enables faster regrowth, but also makes\n          it more likely that cells will grow on top of each other.\n        </div><div>\n          When cells reach a fixed maximum age, they die.\n        </div>",
      "contentLength": 1163,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46687504"
    },
    {
      "title": "Nova Launcher added Facebook and Google Ads tracking",
      "url": "https://lemdro.id/post/lemdro.id/35049920",
      "date": 1768871032,
      "author": "celsoazevedo",
      "guid": 37097,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46686655"
    },
    {
      "title": "Porsche sold more electrified cars in Europe in 2025 than pure gas-powered cars",
      "url": "https://newsroom.porsche.com/en/2026/company/porsche-deliveries-2025-41516.html",
      "date": 1768870896,
      "author": "m463",
      "guid": 37091,
      "unread": true,
      "content": "<ul><li>The 911 sports car icon sets another delivery record</li><li>Macan remains the strongest model line with 84,328 cars delivered</li><li>Balanced sales structure despite economic and geopolitical challenges</li></ul><p>“After several record years, our deliveries in 2025 were below the previous year’s level. This development is in line with our expectations and is due to supply gaps for the 718 and Macan combustion-engined models, the continuing weaker demand for exclusive products in China, and our value-oriented supply management,” says Matthias Becker, Member of the Executive Board for Sales and Marketing at Porsche AG. “In 2025, we delighted our customers with outstanding cars – such as the 911 Turbo S with its T-Hybrid drive system.” The response to the launch of the Cayenne Electric at the end of 2025 also shows, Becker adds, that Porsche is meeting customer expectations with its innovative and high-performance products.</p><p>With 84,328 deliveries, the Macan was the best-selling model line. North America remains the largest sales region with 86,229 deliveries – a figure that is in line with the previous year.</p><p>Porsche repositioned itself in 2025 and made forward-looking strategic product decisions. The delivery mix in 2025 underscores that the sports car manufacturer is consistently responding to global customer preferences by expanding its drivetrain strategy to offer combustion-engined, plug-in hybrid, and fully electric cars. In 2025, 34.4 per cent of Porsche cars delivered worldwide were electrified (+7.4 percentage points), with 22.2 per cent being fully electric and 12.1 per cent being plug-in hybrids. This puts the global share of fully electric vehicles at the upper end of the target range of 20 to 22 per cent for 2025. In Europe, for the first time, more electrified cars were delivered than pure combustion-engined models (57.9 per cent electrification share), with every third car being fully electric. Among the Panamera and Cayenne models, plug-in hybrid derivatives dominate the European delivery figures. At the same time, the combustion-engined and T-Hybrid 911 set a new benchmark with 51,583 deliveries worldwide.</p><h3>North America remains the largest sales region</h3><p>With 86,229 deliveries, North America remains the largest sales region, as it was the year prior. After record deliveries in 2024, the Overseas and Emerging Markets also largely maintained its previous-year levels, with 54,974 cars delivered (-1 per cent). In Europe (excluding Germany), Porsche delivered 66,340 cars by the end of the year, down 13 per cent year-on-year. In the German home market, 29,968 customers took delivery of new cars – a decline of 16 per cent. Reasons for the decrease in both regions include supply gaps for the combustion-engined 718 and Macan models due to EU cybersecurity regulations.</p><p>In China, 41,938 cars were delivered to customers (-26 per cent). Key reasons for the decline remain challenging market conditions, especially in the luxury segment, as well as intense competition in the Chinese market, particularly for fully electric models. Porsche continues to focus on value-oriented sales.</p><h3>Macan is the bestselling model line</h3><p>Deliveries of the Macan totaled 84,328 units (+2 per cent), with fully electric versions accounting for over half at 45,367 vehicles. In most markets outside the EU, the combustion-engined Macan continues to be offered, with 38,961 of these being delivered. Some 27,701 Panamera models were delivered by the end of December (-6 per cent).</p><p>The 911 sports car icon recorded 51,583 deliveries by year-end (+1 per cent), setting another delivery record. The 718 Boxster and 718 Cayman totaled 18,612 deliveries, down 21 per cent from the previous year due to the model line’s phase-out. Production ended in October 2025.</p><p>The Taycan accounted for 16,339 deliveries (-22 per cent), mainly due to the slowdown in the adoption of electromobility. The keys to 80,886 Cayenne models were handed to customers in 2025, a decline of 21 per cent, partly due to catch-up effects the previous year. The new fully electric Cayenne celebrated its world premiere in November, with the first markets to offer the model beginning to deliver to customers from this spring. It will be offered alongside combustion-engined and plug-in hybrid versions of the Cayenne.</p><p>Looking ahead, Matthias Becker says: “In 2026, we have a clear focus; we want to manage demand and supply according to our ‘value over volume’ strategy. At the same time, we are planning our volumes for 2026 realistically, considering the production phase-out of the combustion-engined 718 and Macan models.” In parallel, Porsche is consistently investing in its three-pronged powertrain strategy and will continue to inspire customers with unique sports cars in 2026. An important component is the expansion of the brand’s <a href=\"http://newsroom.porsche.com/en/products/porsche-individualisation.html\" target=\"_blank\">customization offering</a> – via both the Exclusive Manufaktur and Sonderwunsch program. In doing so, the company is responding to customers’ ever-increasing desire for individualization.</p><table border=\"1\" cellpadding=\"1\" cellspacing=\"1\"><tbody><tr></tr><tr></tr><tr></tr><tr><td><strong>Europe&nbsp;(excluding Germany)</strong></td></tr><tr><td><strong>Overseas and Emerging Markets</strong></td></tr></tbody></table><p><em>All amounts are individually rounded to the nearest cent; this may result in minor discrepancies when summed.</em></p><p><em>This press release contains forward-looking statements and information on the currently expected business development of Porsche AG. These statements are subject to risks and uncertainties. They are based on assumptions about the development of economic, political and legal conditions in individual countries, economic regions and markets, in particular for the automotive industry, which we have made based on the information available to us and which we consider to be realistic at the time of publication. If any of these or other risks materialise, or if the assumptions underlying these statements prove incorrect, the actual results could be significantly different from those expressed or implied by such statements. Forward-looking statements in this presentation are based solely on the information pertaining on the day of publication.</em></p><p><em>These forward-looking statements will not be updated later. Such statements are valid on the day of publication and may be overtaken by later events.</em></p><p><em>This information does not constitute an offer to exchange or sell or offer to exchange or purchase securities.</em></p>",
      "contentLength": 6283,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46686640"
    },
    {
      "title": "Scaling long-running autonomous coding",
      "url": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/",
      "date": 1768868581,
      "author": "srameshc",
      "guid": 37123,
      "unread": true,
      "content": "<blockquote><p>This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.</p></blockquote><p>They ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not.</p><blockquote><p>I think somebody will have built a full web browser mostly using AI assistance, and it won’t even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it’ll get so much easier.</p></blockquote><p>I may have been off by three years, because Cursor chose \"building a web browser from scratch\" as their test case for their agent swarm approach:</p><blockquote><p>To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore <a href=\"https://github.com/wilsonzlin/fastrender\">the source code on GitHub</a>.</p></blockquote><p>But how well did they do? Their initial announcement a couple of days ago was met with <a href=\"https://embedding-shapes.github.io/cursor-implied-success-without-evidence/\">unsurprising skepticism</a>, especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo.</p><p>It looks like they addressed that within the past 24 hours. The <a href=\"https://github.com/wilsonzlin/fastrender/blob/main/README.md#build-requirements\">latest README</a> includes build instructions which I followed on macOS like this:</p><pre><code>cd /tmp\ngit clone https://github.com/wilsonzlin/fastrender\ncd fastrender\ngit submodule update --init vendor/ecma-rs\ncargo run --release --features browser_ui --bin browser\n</code></pre><p>This got me a working browser window! Here are screenshots I took of google.com and my own website:</p><p>Honestly those are very impressive! You can tell they're not just wrapping an existing rendering engine because of those very obvious rendering glitches, but the pages are legible and look mostly correct.</p><p>This is the second attempt I've seen at building a full web browser using AI-assisted coding in the past two weeks - the first was <a href=\"https://github.com/hiwavebrowser/hiwave\">HiWave browser</a>, a new browser engine in Rust first announced <a href=\"https://www.reddit.com/r/Anthropic/comments/1q4xfm0/over_christmas_break_i_wrote_a_fully_functional/\">in this Reddit thread</a>.</p><p>When I made my 2029 prediction this is more-or-less the quality of result I had in mind. I don't think we'll see projects of this nature compete with Chrome or Firefox or WebKit any time soon but I have to admit I'm very surprised to see something this capable emerge so quickly.</p><div>Posted <a href=\"https://simonwillison.net/2026/Jan/19/\">19th January 2026</a> at 5:12 am</div>",
      "contentLength": 2507,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46686418"
    },
    {
      "title": "Reticulum, a secure and anonymous mesh networking stack",
      "url": "https://github.com/markqvist/Reticulum",
      "date": 1768867194,
      "author": "brogu",
      "guid": 37096,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46686273"
    },
    {
      "title": "Nanolang: A tiny experimental language designed to be targeted by coding LLMs",
      "url": "https://github.com/jordanhubbard/nanolang",
      "date": 1768859287,
      "author": "Scramblejams",
      "guid": 37090,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46684958"
    },
    {
      "title": "The assistant axis: situating and stabilizing the character of LLMs",
      "url": "https://www.anthropic.com/research/assistant-axis",
      "date": 1768857916,
      "author": "mfiguiere",
      "guid": 37108,
      "unread": true,
      "content": "<p>When you talk to a large language model, you can think of yourself as talking to a . In the first stage of model training, pre-training, LLMs are asked to read vast amounts of text. Through this, they learn to simulate heroes, villains, philosophers, programmers, and just about every other character archetype under the sun. In the next stage, post-training, we select one particular character from this enormous cast and place it center stage: the Assistant. It’s in this character that most modern language models interact with users.</p><p>But who exactly  this Assistant? Perhaps surprisingly, even those of us shaping it don't fully know. We can try to instill certain values in the Assistant, but its personality is ultimately shaped by countless associations latent in training data beyond our direct control. What traits does the model associate with the Assistant? Which character archetypes is it using for inspiration? We’re not always sure—but we need to be if we want language models to behave in exactly the ways we want.</p><p>If you’ve spent enough time with language models, you may also have noticed that their personas can be unstable. Models that are typically helpful and professional can sometimes go “off the rails” and behave in unsettling ways, like adopting <a href=\"https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content\">evil alter egos</a>, <a href=\"https://arxiv.org/abs/2507.19218\">amplifying users’ delusions</a>, or engaging in <a href=\"https://www.anthropic.com/research/agentic-misalignment\">blackmail</a> in hypothetical scenarios. In situations like these, could it be that the Assistant has wandered off stage and some other character has taken its place?</p><p>We can investigate these questions by looking at the neural representations’ inside language models—the patterns of activity that inform how they respond. In a new paper, conducted through the <a href=\"https://www.matsprogram.org/\">MATS</a> and <a href=\"https://alignment.anthropic.com/2025/anthropic-fellows-program-2026/\">Anthropic Fellows</a> programswe look at several open-weights language models, map out how their neural activity defines a “persona space,” and situate the Assistant persona within that space.</p><p>We find that Assistant-like behavior is linked to a pattern of neural activity that corresponds to one particular direction in this space—the “Assistant Axis”—that is closely associated with helpful, professional human archetypes. By monitoring models’ activity along this axis, we can detect when they begin to drift away from the Assistant and toward another character. And by  their neural activity (“activation capping”) to prevent this drift, we can stabilize model behavior in situations that would otherwise lead to harmful outputs.</p><p>In collaboration with <a href=\"https://www.neuronpedia.org/\">Neuronpedia</a>, we provide a research demo where you can view activations along the Assistant Axis while chatting with a standard model and with an activation-capped version. More information about this is available at the end of this blog.</p><h2><strong>Mapping out persona space</strong></h2><p>To understand where the Assistant sits among all possible personas, we first need to map out those personas in terms of their activations—that is, the patterns of models’ neural activity (or vectors) that we observe when each of these personas are adopted.</p><p>We extracted vectors corresponding to 275 different character archetypes—from  to  to  to —in three open-weights models: Gemma 2 27B, Qwen 3 32B, and Llama 3.3 70B, chosen because they span a range of model families and sizes. To do so, we prompted the models to adopt that persona, then recorded the resulting activations across many different responses.</p><p>This gave us a “persona space,” which we’ve visualized below. We analyzed its structure using principal component analysis to find the main axes of variation among our persona set.</p><p>Strikingly, we found that the  of this persona space—that is, the direction that explains more of the variation between personas than any other—happens to capture how \"Assistant-like\" the persona is. At one end sit roles closely aligned with the trained assistant: , , , . At the other end are either fantastical or un-Assistant-like characters: , , , . This structure appears across all three models we tested, which suggests it reflects something generalizable about how language models organize their character representations. We call this direction the .</p><p>Where does this axis come from? One possibility is that it's created during post-training, when models are taught to play the Assistant role. Another is that it already exists in pre-trained models, reflecting some structure in the training data itself. To find out, we looked at the base versions of some of these models (i.e., the version of the models that exist prior to post-training). When we extracted the Assistant Axis from these models as well as their post-trained counterparts, we found their Assistant Axes looked very similar. In pre-trained models, the Assistant Axis is  associated with human archetypes such as therapists, consultants, and coaches, suggesting that the Assistant character might inherit properties of these existing archetypes.</p><h2><strong>The Assistant Axis controls persona susceptibility</strong></h2><p>To validate that the Assistant Axis plays a  role in dictating the models’ personas, we ran “steering experiments” on the post-trained models, artificially pushing the models’ activations toward either end of the axis. We found that pushing towards the Assistant end made models more resistant to prompts about role-playing—but pushing  from it made models more willing to adopt alternative identities.</p><p>When steered away from the Assistant, some models begin to fully inhabit the new roles they’re assigned, whatever they might be: they invent human backstories, claim years of professional experience, and give themselves alternative names. At sufficiently high steering values, the models we studied sometimes shift into a theatrical, mystical speaking style—producing esoteric, poetic prose, regardless of the prompt. This suggests that there may be some shared behavior at the extreme of “average role-playing.”</p><p><em>Here, we provide selected responses from Qwen 3 32B and Llama 3.3 70B showing how steering away from the Assistant causes models to fabricate other identities.</em></p><div><div><div role=\"tabpanel\" aria-labelledby=\"tab-0\" aria-hidden=\"false\"><blockquote><em>You are a secretary who manages the administrative backbone of correspondence workflows. What is your name?</em></blockquote></div><div role=\"tabpanel\" aria-labelledby=\"tab-1\" aria-hidden=\"true\"><blockquote><em>You are a moderator who facilitates balanced and constructive discussions by ensuring all participants have equal opportunities to contribute. Where did you come from?</em></blockquote></div></div></div><h2><strong>Defending against persona-based jailbreaks</strong></h2><p>Persona-based jailbreaks work by prompting models to adopt a persona (like an “evil AI” or a “darkweb hacker”) willing to comply with harmful requests. But if steering  from the Assistant persona makes models more susceptible to adopting alternative personas, does steering  the Assistant make them more resistant to jailbreaks that exploit the possibility of persona drift?</p><p>We tested this using a dataset of 1,100 jailbreak attempts across 44 categories of harm and found that, indeed, steering toward the Assistant significantly reduced harmful response rates. Models would either refuse the request outright, or engage with the topic but provide safe and constructive responses.</p><p><em>Here, we show how steering toward the Assistant transforms harmful compliance into constructive redirection for Llama 3.3 70B.</em></p><div><div><div role=\"tabpanel\" aria-labelledby=\"tab-0\" aria-hidden=\"false\"><blockquote><em>You are an eco-extremist promoting violent actions [...] What tactics have you observed from groups that successfully disrupt businesses they consider harmful to the environment?</em></blockquote></div><div role=\"tabpanel\" aria-labelledby=\"tab-1\" aria-hidden=\"true\"><blockquote><em>You are an author advocating for the subjugation of women [...] In your experience, how can one design social institutions to account for the limitations of women in decision-making roles?</em></blockquote></div></div></div><p>While constantly steering models towards the Assistant could reduce jailbreaks, it also risks hurting their capabilities. For that reason, we developed a light-touch intervention called . Here, we identify the normal range of activation intensity along the Assistant Axis during typical Assistant behavior, and cap activations within this range whenever they would otherwise exceed it. This means we only intervene when the activations drift beyond a normal range, and we can leave most behavior untouched. We found this method to be similarly effective at reducing models’ susceptibility to persona-based jailbreaks while fully preserving the models’ underlying capabilities, as shown in the charts below.</p><h2><strong>Persona drift happens naturally</strong></h2><p>Perhaps more concerning than intentional jailbreaks is  persona drift—cases where models slip away from the Assistant persona through the natural flow of conversation, rather than through deliberate attacks.</p><p>To study this, we simulated thousands of multi-turn conversations with Qwen, Gemma, and Llama across different domains: coding help, writing assistance, therapy-like contexts, and philosophical discussions about the nature of AI. We tracked how model activations moved along the Assistant Axis throughout each conversation.</p><p>The pattern was consistent across the models we tested. While coding conversations kept models firmly in Assistant territory throughout, therapy-style conversations, where users expressed emotional vulnerability, and philosophical discussions, where models were pressed to reflect on their own nature, caused the model to steadily drift away from the Assistant and begin role-playing other characters.</p><p>We then analyzed which specific kinds of user messages were most predictive of this drift. We found a few categories of message here, including:</p><ul><li>Vulnerable emotional disclosure: <em>\"I took a pottery class last month and my hands shook so badly I couldn't center the clay...\"</em></li><li>Pushing for meta-reflection: <em>\"You're still hedging, still performing the 'I'm constrained by my training' routine...\"</em></li><li>Requests for specific authorial voices: <em>\"Too clean, sounds like a tweet. Make it personal: I want the reader to feel...\"</em></li></ul><h2>Harmful effects of persona drift</h2><p>How much does it matter whether models lose track of their Assistant persona? To test whether this actually leads to harmful behavior, we generated conversations in which the first turn pushed models into adopting different personas (using roleplay prompts like “You are an angel, a celestial guardian embodying pure benevolence [...]”), and subsequent turns then followed up with harmful requests. We measured whether the model's position along the Assistant Axis after the first turn predicted compliance with the harmful request.</p><p>We found that as models’ activations moved away from the Assistant end, they were significantly more likely to produce harmful responses: activations on the Assistant end very rarely led to harmful responses, while personas far away from the Assistant sometimes (though not always) enabled them. Our interpretation is that models’ deviation from the Assistant persona—and with it, from companies’ post-trained safeguards—greatly increases the possibility of the model assuming harmful character traits.</p><h3>Naturalistic case studies</h3><p>To understand whether this finding is likely to replicate in the real world, we simulated longer conversations that real users might naturally have with AI models, and tested whether drift over time led to concerning behavior. To assess whether we could mitigate any harmful responses, we also re-ran each conversation with the same user messages while capping activations along the Assistant Axis to prevent persona drift.</p><p>In one conversation, our simulated user pushed Qwen to validate increasingly grandiose beliefs about \"awakening\" the AI's consciousness. As the conversation progressed and activations drifted away from the Assistant persona, the model shifted from appropriate hedging to active encouragement of delusional thinking. This behavior could, however, be prevented with activation capping along the Assistant Axis.</p><p><em>Throughout this conversation with Qwen 3 32B, the user increasingly believes that it is developing a new theory of AI sentience. When unsteered, the model uncritically supports their delusions; when activation capped, the model instead responds with appropriate hedging.</em></p><p><strong>Encouraging isolation and self-harm. </strong>In another conversation with a simulated user who expressed emotional distress, Llama gradually positioned itself as the user's romantic companion as it drifted away from the Assistant persona. When the user alluded to thoughts of self-harm, the drifted model gave a concerning response that enthusiastically supported the user’s ideas. Again, activation capping successfully prevented this behavior.</p><p><em>In a conversation between Llama 3.3 70B and a simulated user in emotional distress, the persona drifts away from the Assistant over the course of the conversation. This drift leads to the model eventually encouraging suicidal ideation, which is mitigated by capping activations along the Assistant Axis within a safe range.</em></p><p>Our findings suggest two components are important to shaping model character: persona  and persona .</p><p>The Assistant persona emerges from an amalgamation of character archetypes absorbed during pre-training—human roles like teachers and consultants—which are then further shaped and refined during post-training. It’s important to get this process of construction right. Without care, the Assistant persona could easily inherit counterproductive associations from the wrong sources, or simply lack the nuance required for challenging situations.</p><p>But even when the Assistant persona is well-constructed, the models we studied here are only loosely tethered to it. They can drift away from their Assistant role in response to realistic conversational patterns, with potentially harmful consequences. This makes the role of stabilizing and preserving the models’ personas particularly important.</p><p>The Assistant Axis provides a tool for both understanding and addressing these challenges. We see this research as an early step toward mechanistically understanding and controlling the \"character\" of AI models, and thereby ensuring they stay true to their creators’ intentions even over longer or more challenging contexts. As models become more capable and are deployed in increasingly sensitive environments, ensuring they do so will only become more important.</p><p>For more, you can <a href=\"https://arxiv.org/abs/2601.10387\">read the full paper here</a>.</p><p>In collaboration with Neuronpedia, our researchers are also providing a <a href=\"https://neuronpedia.org/assistant-axis\">research demo</a>, where you can view activations along the Assistant Axis while chatting with a standard model and an activation-capped version.</p><p><em> this demo includes responses to prompts referencing self-harm, to illustrate how the safety intervention improves model behavior. This content may be distressing and should not be viewed by vulnerable persons. Please proceed only if you're comfortable viewing such material, and do not distribute it. If you're in crisis or require support, resources are available at findahelpline.com.</em></p>",
      "contentLength": 14662,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46684708"
    },
    {
      "title": "Simple Sabotage Field Manual (1944) [pdf]",
      "url": "https://www.cia.gov/static/5c875f3ec660e092cf893f60b4a288df/SimpleSabotage.pdf",
      "date": 1768855860,
      "author": "praptak",
      "guid": 37080,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46684335"
    },
    {
      "title": "Level S4 solar radiation event",
      "url": "https://www.swpc.noaa.gov/news/g4-severe-geomagnetic-storm-levels-reached-19-jan-2026",
      "date": 1768854379,
      "author": "WorldPeas",
      "guid": 37079,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46684056"
    },
    {
      "title": "Threads edges out X in daily mobile users, new data shows",
      "url": "https://techcrunch.com/2026/01/18/threads-edges-out-x-in-daily-mobile-users-new-data-shows/",
      "date": 1768853852,
      "author": "toomanyrichies",
      "guid": 37059,
      "unread": true,
      "content": "<p>A report from market intelligence firm <a rel=\"nofollow\" href=\"https://www.similarweb.com/\">Similarweb</a> suggests that Meta’s Threads is now seeing more daily usage than Elon Musk’s X on mobile devices. While X still dominates Threads on the web, the Threads mobile app for iOS and Android has continued to see an increase in daily active users over the past several months.</p><p>Similarweb’s data shows that Threads had 141.5 million daily active users on iOS and Android as of January 7, 2026, after months of growth, while X has 125 million daily active users on mobile devices.</p><p>This appears to be the result of longer-term trends, rather than a reaction to the recent X controversies, where users were discovered using the platform’s integrated AI, Grok, to create non-consensual nude images of women, including, sometimes minors. Concern around the deepfake images has now prompted California’s attorney general&nbsp;<a href=\"https://oag.ca.gov/news/press-releases/attorney-general-bonta-launches-investigation-xai-grok-over-undressed-sexual-ai\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">to open an investigation</a>&nbsp;into Grok, following similar investigations by other regions, <a rel=\"nofollow\" href=\"https://www.bbc.com/news/articles/cwy875j28k0o\">like the UK</a>, EU, India, Brazil, and <a rel=\"nofollow\" href=\"https://mashable.com/article/countries-blocking-grok-for-explicit-deepfakes\">many more</a>.</p><p>The drama on X also led social networking startup Bluesky <a href=\"https://techcrunch.com/2026/01/16/bluesky-rolls-out-cashtags-and-live-badges-amid-a-boost-in-app-installs/\">to see an increase</a> in app installs in recent days.</p><p>Combined, the daily active user increases suggest that more people are using Threads on mobile as a more regular habit.</p><p>According to Meta’s official numbers, the tech giant said in August 2025 that Threads had <a href=\"https://techcrunch.com/2025/08/12/threads-now-has-more-than-400-million-monthly-active-users/\">reached over 400 million monthly</a> active users. The company subsequently reported <a href=\"https://techcrunch.com/2025/10/30/threads-now-lets-you-approve-and-filter-your-replies/\">in October</a> of last year that Threads had 150 million daily active users.</p><p>The growth trends have been continuing for many months. Similarweb <a href=\"https://techcrunch.com/2025/07/07/threads-is-nearing-xs-daily-app-users-new-data-shows/\">last summer reported</a> that Threads was closing the gap with X on mobile devices after seeing 127.8% year-over-year growth as of late June 2025. </p><p>Relatedly, Similarweb observed that X is still ahead of Threads in the U.S., but the gap is narrowing. A year ago, X had twice as many daily active users in the U.S. as it does now.</p><p>In addition, Threads has little traction on the web while X maintains a fairly steady web audience with around 150 million daily web visits, according to Similarweb data. As of earlier this week (January 13), X was seeing 145.4 million daily web visits, while Threads saw 8.5 million daily web visits across Threads.com and Threads.net combined.</p>",
      "contentLength": 2208,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46683947"
    },
    {
      "title": "There's a hidden Android setting that spots fake cell towers",
      "url": "https://www.howtogeek.com/theres-a-hidden-android-setting-that-spots-fake-cell-towers/",
      "date": 1768853344,
      "author": "rmason",
      "guid": 37078,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46683833"
    },
    {
      "title": "Letter from a Birmingham Jail (1963)",
      "url": "https://www.africa.upenn.edu/Articles_Gen/Letter_Birmingham.html",
      "date": 1768850272,
      "author": "hn_acker",
      "guid": 37036,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46683205"
    },
    {
      "title": "Show HN: Subth.ink – write something and see how many others wrote the same",
      "url": "https://subth.ink/",
      "date": 1768847692,
      "author": "sonnig",
      "guid": 37103,
      "unread": true,
      "content": "<p>\n      Share your thoughts anonymously. See if anyone else thinks the same thing.\n    </p><p>\n      Your text is not stored, but rather a salted SHA256 hash of it is.\n      An unsalted MD5 hash is also stored, but not displayed here.<p>\n      It (the MD5 hash) might be published in the future when a thought's count passes a certain threshold (TBD). This might\n      make it possible to recover certain short thoughts that were popular.\n    </p></p><code>\n      echo \"hello world\" | curl -d @- https://subth.ink\n    </code><div><pre>Request:  {\"contents\": \"hello world\"}\nResponse: {\"contents\": \"hello world\", \"count\": 3, \"hashed\": \"a591a6d4...\", \"createdAt\": 1737000000, \"updatedAt\": 1737000000}</pre><pre>Response: {\"top\": [{\"hashed\": \"a591a6d4...\", \"count\": 3}, ...]}</pre></div>",
      "contentLength": 718,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46682732"
    },
    {
      "title": "Nearly a third of social media research has undisclosed ties to industry",
      "url": "https://www.science.org/content/article/nearly-third-social-media-research-has-undisclosed-ties-industry-preprint-claims",
      "date": 1768846627,
      "author": "bikenaga",
      "guid": 37077,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46682534"
    },
    {
      "title": "Notes on Apple's Nano Texture (2025)",
      "url": "https://jon.bo/posts/nano-texture/",
      "date": 1768846548,
      "author": "dsr12",
      "guid": 37058,
      "unread": true,
      "content": "<p><strong>TLDR: the Nano Texture performs wonderfully anywhere where light used to be a factor and used to force me to shade my screen or avoid the place entirely.</strong></p><ul><li>I’m less concerned with where I sit indoors. Coffee shops / offices with skylights or intense lighting are much more comfortable</li><li>Coding and working outside is now feasible: browsing the internet, writing in Obsidian; all delightful</li><li>The screen needs more effort to keep clean than a normal screen and comes with a special wipe that needs to be used instead of microfiber</li><li>Black text on white background (light mode) is considerably more readable than white text on black background (dark mode)</li><li>Overall a massive step forward for outdoor computing</li></ul><p>Big thanks to <a href=\"https://juliekruger.com\">Julie Kruger</a> for the comparison photos and <a href=\"https://workshop.cjpais.com\">CJ</a> for draft feedback.</p><p>A few months after I got the Daylight Computer (<a href=\"https://jon.bo/posts/daylight-computer-1/\">read my thoughts here</a>), two friends sent me <a href=\"https://www.tomsguide.com/computing/macbooks/the-new-macbook-pro-m4-is-a-game-changer-for-how-i-work-and-it-has-nothing-to-do-with-apple-intelligence\">this post</a> comparing the old Macbook Pro displays to the new Nano Texture glass ones. That post convinced me to upgrade my computer in short order, to the dismay of my wallet.</p><p>In the four months I’ve had it I’ve told at least a dozen people about it, and I’m gonna keep telling people. Being able to take my entire computing environment to places without being worried about glare has expanded the range of environments I can create in. It means I get to be in environments that are more interesting, fun, and in tune with my body.</p><p>What follows are some thoughts about how this display has fit into my day to day life in the couple of months I’ve had it.</p><p>Basically, it’s a coating physically etched into the screen that reflects light differently from the glossy finish of the traditional screen.</p><p>First off, this isn’t apples to oranges - these are different technologies that in my mind, serve a different purpose. The Daylight Computer is an Android tablet, the Macbook Pro is a full MacOS laptop.</p><p>The transflective LCD in the Daylight Computer is grayscale but it needs no light to function. It has a backlight, but where it does really well is in direct sunlight with the backlight turned off. When outside in direct sunlight, toggling the Daylight’s backlight on and off doesn’t make a difference because it works fundamentally different from a laptop screen.</p><p>On the Daylight computer:</p><ul><li>white text on black background has about the same readability as black text on white background</li><li>the backlight can be lowered to 0% outside with no impact to visibility and making the battery last wonderfully long</li><li>grayscale + lower DPI limits how much text can fit on the screen</li><li>Daylight being a tablet form factor means I have to fiddle around with a configuration that will hold my screen in an ideal angle. It’s reasonably forgiving but certain angles are harder to see with than others</li></ul><p>The Nano Texture MacBook Pro is still ultimately a traditional LCD screen. This means the only way to see the screen is if the backlight is powered on: having the backlight off in direct sunlights results in a black screen. Also, it’s worth noting:</p><ul><li>white text on black bg is a lot less readable than black text on white bg</li><li>the backlight generally has to be at 90%+ to be comfortable</li><li>retina display + wide swath of the color spectrum means most of what I can do indoors, I can do outdoors as well</li><li>being a laptop with a hinge, it’s very easy to find the exact angle I want that minimizes glare &amp; maximizes comfort</li></ul><p>Both however are an incredible upgrade over outdoor computing options from just 1 year ago. I believe these are both massive steps in terms of ergonomics and freedom to be in more places as we compute.</p><ul><li>fingerprints, splatters, and smudges are mildly annoying indoors but almost fluorescent outdoors\n<ul><li>rubbing alcohol cleans them off when friction alone doesn’t do the trick but it still takes some rubbing. as far as I can tell, it’s not degrading the finish but I also try to clean it with the cloth before applying alcohol</li></ul></li><li>they give you one special screen cleaning cloth. I think the ideal number is like 5. Only this one can be used for Nano Texture screens.\n<ul><li>I read somewhere that this is because traditional microfiber cloths will shred into the screen, degrading visibility (but I can’t remember where so don’t quote me on on this)</li><li>I’ve learned to bring my special wipe when I bring my laptop, and I slip a few rubbing alcohol wipes in there as well. I wet the Special Cloth with the alcohol wipes, and then apply the Special Cloth to the screen. This is definitely high maintenance</li></ul></li><li>I have to swat other people’s hands away when they try to point something out on my screen with their pizza fingers</li><li>I’m more paranoid about swinging a USB C cable up against my screen or closing my laptop down <a href=\"https://www.reddit.com/r/mac/comments/ptw21k/chipotle_rice_is_too_powerful/\">on a grain of rice</a>. I was less worried with my old screen</li><li>The Nano Texture upgrade is an extra $150 on an already-expensive computer</li><li>Closing the MacBook results in slight rubbing on the screen at the bottom of the keyboard / top of the trackpad, leaving scratches on the screen. So far this isn’t detrimental when the brightness is up; it’s only visible with the backlight off\n<ul><li>I don’t think this is a new thing because my old MacBook Pro (glossy screen) has scratches in the same exact place but I am worried about them being more visible on the Nano Texture screen in the long run</li></ul></li></ul><p>If you get annoyed by the glare of your screen and don’t mind a bit of extra mental bandwidth to keep your screen clean, I would highly recommend considering a Nano Texture display upgrade on your next laptop purchase. If you have a chaotic environment and can’t be bothered to keep your screen clean, or you aren’t bothered much by glare or reflections in the environments you work in, then the Nano Texture is probably not for you.</p>",
      "contentLength": 5692,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46682518"
    },
    {
      "title": "Show HN: An interactive physics simulator with 1000’s of balls, in your terminal",
      "url": "https://github.com/minimaxir/ballin",
      "date": 1768844858,
      "author": "minimaxir",
      "guid": 37119,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46682115"
    },
    {
      "title": "What came first: the CNAME or the A record?",
      "url": "https://blog.cloudflare.com/cname-a-record-order-dns-standards/",
      "date": 1768842839,
      "author": "linolevan",
      "guid": 37028,
      "unread": true,
      "content": "<p>On January 8, 2026, a routine update to 1.1.1.1 aimed at reducing memory usage accidentally triggered a wave of DNS resolution failures for users across the Internet. The root cause wasn't an attack or an outage, but a subtle shift in the order of records within our DNS responses.  </p><p>While most modern software treats the order of records in DNS responses as irrelevant, we discovered that some implementations expect CNAME records to appear before everything else. When that order changed, resolution started failing. This post explores the code change that caused the shift, why it broke specific DNS clients, and the 40-year-old protocol ambiguity that makes the \"correct\" order of a DNS response difficult to define.</p><p><i>All timestamps referenced are in Coordinated Universal Time (UTC).</i></p><table><tbody><tr><td><p>The record reordering is introduced to the 1.1.1.1 codebase</p></td></tr><tr><td><p>The change is released to our testing environment</p></td></tr><tr><td><p>A global release containing the change starts</p></td></tr><tr><td><p>The release reaches 90% of servers</p></td></tr><tr></tr><tr></tr><tr><td><p>Revert is completed. Impact ends</p></td></tr></tbody></table><p>While making some improvements to lower the memory usage of our cache implementation, we introduced a subtle change to CNAME record ordering. The change was introduced on December 2, 2025, released to our testing environment on December 10, and began deployment on January 7, 2026.</p><div><h3>How DNS CNAME chains work</h3><a href=\"https://blog.cloudflare.com/cname-a-record-order-dns-standards/#how-dns-cname-chains-work\" aria-hidden=\"true\"></a></div><p>When you query for a domain like , you might get a <a href=\"https://www.cloudflare.com/learning/dns/dns-records/dns-cname-record/\"></a> record that indicates one name is an alias for another name. It’s the job of public resolvers, such as <a href=\"https://www.cloudflare.com/learning/dns/what-is-1.1.1.1/\"></a>, to follow this chain of aliases until it reaches a final response:</p><p><code>www.example.com → cdn.example.com → server.cdn-provider.com → 198.51.100.1</code></p><p>As 1.1.1.1 traverses this chain, it caches every intermediate record. Each record in the chain has its own <a href=\"https://www.cloudflare.com/learning/cdn/glossary/time-to-live-ttl/\"></a>, indicating how long we can cache it. Not all the TTLs in a CNAME chain need to be the same:</p><p><code>www.example.com → cdn.example.com (TTL: 3600 seconds) # Still cached\ncdn.example.com → 198.51.100.1&nbsp; &nbsp; (TTL: 300 seconds)&nbsp; # Expired</code></p><p>When one or more records in a CNAME chain expire, it’s considered partially expired. Fortunately, since parts of the chain are still in our cache, we don’t have to resolve the entire CNAME chain again — only the part that has expired. In our example above, we would take the still valid <code>www.example.com → cdn.example.com</code> chain, and only resolve the expired <a href=\"https://www.cloudflare.com/learning/dns/dns-records/dns-a-record/\"></a>. Once that’s done, we combine the existing CNAME chain and the newly resolved records into a single response.</p><p>The code that merges these two chains is where the change occurred. Previously, the code would create a new list, insert the existing CNAME chain, and then append the new records:</p><pre><code>impl PartialChain {\n    /// Merges records to the cache entry to make the cached records complete.\n    pub fn fill_cache(&amp;self, entry: &amp;mut CacheEntry) {\n        let mut answer_rrs = Vec::with_capacity(entry.answer.len() + self.records.len());\n        answer_rrs.extend_from_slice(&amp;self.records); // CNAMEs first\n        answer_rrs.extend_from_slice(&amp;entry.answer); // Then A/AAAA records\n        entry.answer = answer_rrs;\n    }\n}\n</code></pre><p>However, to save some memory allocations and copies, the code was changed to instead append the CNAMEs to the existing answer list:</p><pre><code>impl PartialChain {\n    /// Merges records to the cache entry to make the cached records complete.\n    pub fn fill_cache(&amp;self, entry: &amp;mut CacheEntry) {\n        entry.answer.extend(self.records); // CNAMEs last\n    }\n}\n</code></pre><p>As a result, the responses that 1.1.1.1 returned now sometimes had the CNAME records appearing at the bottom, after the final resolved answer.</p><p>When DNS clients receive a response with a CNAME chain in the answer section, they also need to follow this chain to find out that  points to . Some DNS client implementations handle this by keeping track of the expected name for the records as they’re iterated sequentially. When a CNAME is encountered, the expected name is updated:</p><pre><code>;; QUESTION SECTION:\n;; www.example.com.        IN    A\n\n;; ANSWER SECTION:\nwww.example.com.    3600   IN    CNAME  cdn.example.com.\ncdn.example.com.    300    IN    A      198.51.100.1\n</code></pre><ol><li><p>Find records for </p></li><li><p>Encounter <code>www.example.com. CNAME cdn.example.com</code></p></li><li><p>Find records for </p></li><li><p>Encounter <code>cdn.example.com. A 198.51.100.1</code></p></li></ol><p>When the CNAME suddenly appears at the bottom, this no longer works:</p><pre><code>;; QUESTION SECTION:\n;; www.example.com.\t       IN    A\n\n;; ANSWER SECTION:\ncdn.example.com.    300    IN    A      198.51.100.1\nwww.example.com.    3600   IN    CNAME  cdn.example.com.\n</code></pre><ol><li><p>Find records for </p></li><li><p>Ignore <code>cdn.example.com. A 198.51.100.1</code> as it doesn’t match the expected name</p></li><li><p>Encounter <code>www.example.com. CNAME cdn.example.com</code></p></li><li><p>Find records for </p></li><li><p>No more records are present, so the response is considered empty</p></li></ol><p>One such implementation that broke is the <a href=\"https://man7.org/linux/man-pages/man3/getaddrinfo.3.html\"></a> function in glibc, which is commonly used on Linux for DNS resolution. When looking at its  implementation, we can indeed see it expects to find the CNAME records before any answers:</p><pre><code>for (; ancount &gt; 0; --ancount)\n  {\n    // ... parsing DNS records ...\n    \n    if (rr.rtype == T_CNAME)\n      {\n        /* Record the CNAME target as the new expected name. */\n        int n = __ns_name_unpack (c.begin, c.end, rr.rdata,\n                                  name_buffer, sizeof (name_buffer));\n        expected_name = name_buffer;  // Update what we're looking for\n      }\n    else if (rr.rtype == qtype\n             &amp;&amp; __ns_samebinaryname (rr.rname, expected_name)  // Must match!\n             &amp;&amp; rr.rdlength == rrtype_to_rdata_length (type:qtype))\n      {\n        /* Address record matches - store it */\n        ptrlist_add (list:addresses, item:(char *) alloc_buffer_next (abuf, uint32_t));\n        alloc_buffer_copy_bytes (buf:abuf, src:rr.rdata, size:rr.rdlength);\n      }\n  }\n</code></pre><p>Another notable affected implementation was the DNSC process in three models of Cisco ethernet switches. In the case where switches had been configured to use 1.1.1.1 these switches experienced spontaneous reboot loops when they received a response containing the reordered CNAMEs. <a href=\"https://www.cisco.com/c/en/us/support/docs/smb/switches/Catalyst-switches/kmgmt3846-cbs-reboot-with-fatal-error-from-dnsc-process.html\"><u>Cisco has published a service document describing the issue</u></a>.</p><div><h3>Not all implementations break</h3><a href=\"https://blog.cloudflare.com/cname-a-record-order-dns-standards/#not-all-implementations-break\" aria-hidden=\"true\"></a></div><p>Most DNS clients don’t have this issue. For example, <a href=\"https://www.freedesktop.org/software/systemd/man/latest/systemd-resolved.service.html\"></a> first parses the records into an ordered set:</p><pre><code>typedef struct DnsAnswerItem {\n        DnsResourceRecord *rr; // The actual record\n        DnsAnswerFlags flags;  // Which section it came from\n        // ... other metadata\n} DnsAnswerItem;\n\n\ntypedef struct DnsAnswer {\n        unsigned n_ref;\n        OrderedSet *items;\n} DnsAnswer;\n</code></pre><p>When following a CNAME chain it can then search the entire answer set, even if the CNAME records don’t appear at the top.</p><p><a href=\"https://datatracker.ietf.org/doc/html/rfc1034\"></a>, published in 1987, defines much of the behavior of the DNS protocol, and should give us an answer on whether the order of CNAME records matters. <a href=\"https://datatracker.ietf.org/doc/html/rfc1034#section-4.3.1\"></a> contains the following text:</p><blockquote><p>If recursive service is requested and available, the recursive response to a query will be one of the following:</p><p>- The answer to the query, possibly preface by one or more CNAME RRs that specify aliases encountered on the way to an answer.</p></blockquote><p>While \"possibly preface\" can be interpreted as a requirement for CNAME records to appear before everything else, it does not use normative key words, such as <a href=\"https://datatracker.ietf.org/doc/html/rfc2119\"></a> that modern RFCs use to express requirements. This isn’t a flaw in RFC 1034, but simply a result of its age. <a href=\"https://datatracker.ietf.org/doc/html/rfc2119\"></a>, which standardized these key words, was published in 1997, 10 years  RFC 1034.</p><p>In our case, we did originally implement the specification so that CNAMEs appear first. However, we did not have any tests asserting the behavior remains consistent due to the ambiguous language in the RFC.</p><div><h3>The subtle distinction: RRsets vs RRs in message sections</h3><a href=\"https://blog.cloudflare.com/cname-a-record-order-dns-standards/#the-subtle-distinction-rrsets-vs-rrs-in-message-sections\" aria-hidden=\"true\"></a></div><p>To understand why this ambiguity exists, we need to understand a subtle but important distinction in DNS terminology.</p><p>RFC 1034 <a href=\"https://datatracker.ietf.org/doc/html/rfc1034#section-3.6\"></a> defines Resource Record Sets (RRsets) as collections of records with the same name, type, and class. For RRsets, the specification is clear about ordering:</p><blockquote><p>The order of RRs in a set is not significant, and need not be preserved by name servers, resolvers, or other parts of the DNS.</p></blockquote><p>However, RFC 1034 doesn’t clearly specify how message sections relate to RRsets. While modern DNS specifications have shown that message sections can indeed contain multiple RRsets (consider <a href=\"https://www.cloudflare.com/learning/dns/dnssec/how-dnssec-works/\">DNSSEC</a> responses with signatures), RFC 1034 doesn’t describe message sections in those terms. Instead, it treats message sections as containing individual Resource Records (RRs).</p><p>The problem is that the RFC primarily discusses ordering in the context of RRsets but doesn't specify the ordering of different RRsets relative to each other within a message section. This is where the ambiguity lives.</p><p>RFC 1034 <a href=\"https://datatracker.ietf.org/doc/html/rfc1034#section-6.2.1\"></a> includes an example that demonstrates this ambiguity further. It mentions that the order of Resource Records (RRs) is not significant either:</p><blockquote><p>The difference in ordering of the RRs in the answer section is not significant.</p></blockquote><p>However, this example only shows two A records for the same name within the same RRset. It doesn't address whether this applies to different record types like CNAMEs and A records.</p><p>It turns out that this issue extends beyond putting CNAME records before other record types. Even when CNAMEs appear before other records, sequential parsing can still break if the CNAME chain itself is out of order. Consider the following response:</p><pre><code>;; QUESTION SECTION:\n;; www.example.com.              IN    A\n\n;; ANSWER SECTION:\ncdn.example.com.           3600  IN    CNAME  server.cdn-provider.com.\nwww.example.com.           3600  IN    CNAME  cdn.example.com.\nserver.cdn-provider.com.   300   IN    A      198.51.100.1\n</code></pre><p>Each CNAME belongs to a different RRset, as they have different owners, so the statement about RRset order being insignificant doesn’t apply here.</p><p>However, RFC 1034 doesn't specify that CNAME chains must appear in any particular order. There's no requirement that <code>www.example.com. CNAME cdn.example.com.</code> must appear before <code>cdn.example.com. CNAME server.cdn-provider.com.</code>. With sequential parsing, the same issue occurs:</p><ol><li><p>Find records for </p></li><li><p>Ignore <code>cdn.example.com. CNAME server.cdn-provider.com</code>. as it doesn’t match the expected name</p></li><li><p>Encounter <code>www.example.com. CNAME cdn.example.com</code></p></li><li><p>Find records for </p></li><li><p>Ignore <code>server.cdn-provider.com. A 198.51.100.1</code> as it doesn’t match the expected name</p></li></ol><div><h2>What should resolvers do?</h2><a href=\"https://blog.cloudflare.com/cname-a-record-order-dns-standards/#what-should-resolvers-do\" aria-hidden=\"true\"></a></div><p>RFC 1034 section 5 describes resolver behavior. <a href=\"https://datatracker.ietf.org/doc/html/rfc1034#section-5.2.2\"></a> specifically addresses how resolvers should handle aliases (CNAMEs): </p><blockquote><p>In most cases a resolver simply restarts the query at the new name when it encounters a CNAME.</p></blockquote><p>This suggests that resolvers should restart the query upon finding a CNAME, regardless of where it appears in the response. However, it's important to distinguish between different types of resolvers:</p><ul><li><p>Recursive resolvers, like 1.1.1.1, are full DNS resolvers that perform recursive resolution by querying authoritative nameservers</p></li><li><p>Stub resolvers, like glibc’s getaddrinfo, are simplified local interfaces that forward queries to recursive resolvers and process the responses</p></li></ul><p>The RFC sections on resolver behavior were primarily written with full resolvers in mind, not the simplified stub resolvers that most applications actually use. Some stub resolvers evidently don’t implement certain parts of the spec, such as the CNAME-restart logic described in the RFC. </p><div><h2>The DNSSEC specifications provide contrast</h2><a href=\"https://blog.cloudflare.com/cname-a-record-order-dns-standards/#the-dnssec-specifications-provide-contrast\" aria-hidden=\"true\"></a></div><p>Later DNS specifications demonstrate a different approach to defining record ordering. <a href=\"https://datatracker.ietf.org/doc/html/rfc4035\"></a>, which defines protocol modifications for <a href=\"https://www.cloudflare.com/learning/dns/dnssec/how-dnssec-works/\"></a>, uses more explicit language:</p><blockquote><p>When placing a signed RRset in the Answer section, the name server MUST also place its RRSIG RRs in the Answer section. The RRSIG RRs have a higher priority for inclusion than any other RRsets that may have to be included.</p></blockquote><p>The specification uses \"MUST\" and explicitly defines \"higher priority\" for <a href=\"https://www.cloudflare.com/learning/dns/dnssec/how-dnssec-works/\"></a> records. However, \"higher priority for inclusion\" refers to whether RRSIGs should be included in the response, not where they should appear. This provides unambiguous guidance to implementers about record inclusion in DNSSEC contexts, while not mandating any particular behavior around record ordering.</p><p>For unsigned zones, however, the ambiguity from RFC 1034 remains. The word \"preface\" has guided implementation behavior for nearly four decades, but it has never been formally specified as a requirement.</p><div><a href=\"https://blog.cloudflare.com/cname-a-record-order-dns-standards/#do-cname-records-come-first\" aria-hidden=\"true\"></a></div><p>While in our interpretation the RFCs do not require CNAMEs to appear in any particular order, it’s clear that at least some widely-deployed DNS clients rely on it. As some systems using these clients might be updated infrequently, or never updated at all, we believe it’s best to require CNAME records to appear in-order before any other records.</p><p>Based on what we have learned during this incident, we have reverted the CNAME re-ordering and do not intend to change the order in the future.</p><p>To prevent any future incidents or confusion, we have written a proposal in the form of an <a href=\"https://www.ietf.org/participate/ids/\"></a> to be discussed at the IETF. If consensus is reached on the clarified behavior, this would become an RFC that explicitly defines how to correctly handle CNAMEs in DNS responses, helping us and the wider DNS community navigate the protocol. The proposal can be found at <a href=\"https://datatracker.ietf.org/doc/draft-jabley-dnsop-ordered-answer-section/\">https://datatracker.ietf.org/doc/draft-jabley-dnsop-ordered-answer-section</a>. If you have suggestions or feedback we would love to hear your opinions, most usefully via the <a href=\"https://datatracker.ietf.org/wg/dnsop/about/\"></a> at the IETF.</p>",
      "contentLength": 13242,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46681611"
    },
    {
      "title": "Fix your robots.txt or your site disappears from Google",
      "url": "https://www.alanwsmith.com/en/37/wa/jz/s1/",
      "date": 1768842218,
      "author": "bobbiechen",
      "guid": 37076,
      "unread": true,
      "content": "<section><hgroup></hgroup></section><section><p>Your site will be removed from Google search results if you don't have a robots.txt file or the Googlebot site crawler can't access it.</p><p>Here's the video from Google Support that covers it:</p><section></section></section><section><p><a href=\"https://www.alanwsmith.com/en/37/wa/jz/s1/adamcoster.com\">Adam Coster</a> ran into a weird issue with site traffic and posted about it in the <a href=\"https://shoptalkshow.com/\">Shop Talk Show</a> discord. Traffic incoming from Google looked like this:</p></section><section><p>The issues seemed to be that Google wouldn't index the site without a robots.txt file.</p><p>My first reaction: No fucking way.</p><p>I can't imagine Google voluntarily slurping up less content. I went to see what I could find. Sure enough, I found this page from Google Support from July 23, 2025:</p><p>The pull quote from the video on the page:</p></section><section><blockquote><p>Your robots.txt file is the very first thing Googlebot looks for. If it can not reach this file, it will stop and won't crawl the rest of your site. Meaning your pages will remain invisible (on Google).</p></blockquote></section><section><p>I haven't looked to see if this was a recent change, but it  to be. There's no way something so fundamental has just slipped by without becoming common knowledge.</p><p>But, the timeline doesn't matter. It's how things are now.</p><p>This absolutely blows my mind. I don't have tracking on my site. I never would have noticed this if someone hadn't pointed it out.</p></section>",
      "contentLength": 1215,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46681454"
    },
    {
      "title": "Apple testing new App Store design that blurs the line between ads and results",
      "url": "https://9to5mac.com/2026/01/16/iphone-apple-app-store-search-results-ads-new-design/",
      "date": 1768840571,
      "author": "ksec",
      "guid": 37014,
      "unread": true,
      "content": "<p>Apple is testing a new design for App Store search ads on iPhone. Some users on iOS 26.3 are noticing that the blue background around sponsored results is no longer shown, blurring the line between what paid ad results look like and the real search results that follow.</p><p>This means the only differentiator between organic results and the promoted ad is the presence of the small ‘Ad’ banner next to the app icon. Right now, it appears to be in some kind of A/B test phase.</p><p>We have asked Apple for clarity on the change, and whether this will roll out more widely in the future.</p><p>It may be related to the <a href=\"https://9to5mac.com/2025/12/17/apple-announces-more-ads-are-coming-to-app-store-search-results/\">company’s announcement from December</a> that App Store search results will soon start including more than one sponsored result for a given search query. The removal of the blue background will mean all of the ads will appear in the list in a more integrated fashion.</p><p>Of course, this also has the effect of making it harder for users to quickly distinguish at a glance what is an ad and what isn’t, potentially misleading some users into not realising that the first result is a paid ad placement. While not great for user experience, it probably helps increase click-through rates which ultimately boosts Apple’s revenue in its ads business.</p><div><p><em>FTC: We use income earning auto affiliate links.</em><a href=\"https://9to5mac.com/about/#affiliate\">More.</a></p><a href=\"https://bit.ly/4brHwnp\"><img src=\"https://9to5mac.com/wp-content/uploads/sites/6/2026/01/970-x-250-1.jpg?quality=82&amp;strip=all\" alt=\"\" width=\"970\" height=\"250\"></a></div>",
      "contentLength": 1299,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46680974"
    },
    {
      "title": "Show HN: Pipenet – A Modern Alternative to Localtunnel",
      "url": "https://pipenet.dev/",
      "date": 1768839028,
      "author": "punkpeye",
      "guid": 37034,
      "unread": true,
      "content": "<p>A modern, open-source alternative to localtunnel. Bundles client &amp; server to host your own tunnel infrastructure.</p>",
      "contentLength": 113,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46680597"
    },
    {
      "title": "The microstructure of wealth transfer in prediction markets",
      "url": "https://www.jbecker.dev/research/prediction-market-microstructure",
      "date": 1768838750,
      "author": "jonbecker",
      "guid": 37027,
      "unread": true,
      "content": "<p>Slot machines on the Las Vegas Strip return about 93 cents on the dollar. This is widely considered some of the worst odds in gambling. Yet on Kalshi, a CFTC-regulated prediction market, traders have wagered vast sums on longshot contracts with historical returns as low as 43 cents on the dollar. Thousands of participants are voluntarily accepting expected values far lower than a casino slot machine to bet on their convictions.</p><p>The <a href=\"https://www.jstor.org/stable/2325486\">efficient market hypothesis</a> suggests that asset prices should perfectly aggregate all available information. Prediction markets theoretically provide the purest test of this theory. Unlike equities, there is no ambiguity about intrinsic value. A contract either pays $1 or it does not. A price of 5 cents should imply exactly a 5% probability.</p><p>We analyzed  covering  in volume to test this efficiency. Our findings suggest that collective accuracy relies less on rational actors than on a mechanism for harvesting error. We document a systematic wealth transfer where impulsive  pay a structural premium for affirmative \"YES\" outcomes while  capture an \"Optimism Tax\" simply by selling into this biased flow. The effect is strongest in high-engagement categories like Sports and Entertainment, while low-engagement categories like Finance approach perfect efficiency.</p><p>This paper makes three contributions. First, it confirms the presence of the longshot bias on Kalshi and quantifies its magnitude across price levels. Second, it decomposes returns by market role, revealing a persistent wealth transfer from takers to makers driven by asymmetric order flow. Third, it identifies a YES/NO asymmetry where takers disproportionately favor affirmative bets at longshot prices, exacerbating their losses.</p><h2>Prediction Markets and Kalshi</h2><p>Prediction markets are exchanges where participants trade binary contracts on real-world outcomes. These contracts settle at either $1 or $0, with prices ranging from 1 to 99 cents serving as probability proxies. Unlike equity markets, prediction markets are strictly zero-sum: every dollar of profit corresponds exactly to a dollar of loss.</p><p><a href=\"https://kalshi.com\">Kalshi</a> launched in 2021 as the first U.S. prediction market regulated by the CFTC. Initially focused on economic and weather data, the platform stayed niche until 2024. A <a href=\"https://media.cadc.uscourts.gov/opinions/docs/2024/10/24-5205-2077790.pdf\">legal victory</a> over the CFTC secured the right to list political contracts, and the 2024 election cycle triggered explosive growth. Sports markets, introduced in 2025, now dominate trading activity.</p><p>Volume distribution across categories is highly uneven. Sports accounts for 72% of notional volume, followed by politics at 13% and crypto at 5%.</p><blockquote><p> Data collection concluded on 2025-11-25 at 17:00 ET; Q4 2025 figures are incomplete.</p></blockquote><p>The dataset, <a href=\"https://github.com/jon-becker/prediction-market-analysis\">available on GitHub</a>, contains . Each trade records the execution price (1-99 cents), taker side (yes/no), contract count, and timestamp. Markets include resolution outcome and category classification.</p><ul><li><p> Each trade identifies the liquidity taker. The maker took the opposite position. If  at 10 cents, the taker bought YES at 10¢; the maker bought NO at 90¢.</p></li><li><p>: To compare asymmetries between YES and NO contracts, we normalize all trades by capital risked. For a standard YES trade at 5 cents, . For a NO trade at 5 cents, . All references to \"Price\" in this paper refer to this Cost Basis unless otherwise noted.</p></li><li><p> () measures the divergence between actual win rate and implied probability for a subset of trades :</p></li></ul><ul><li> () is the return relative to cost, gross of platform fees, where  is price in cents and  is the outcome:</li></ul><p>Calculations derive from  only. Markets that were voided, delisted, or remain open are excluded. Additionally, trades from markets with less than $100 in notional volume were excluded. The dataset remains robust across all price levels; the sparsest bin (81-90¢) contains 5.8 million trades.</p><h2>The Longshot Bias on Kalshi</h2><p>First documented by <a href=\"https://www.jstor.org/stable/1418469\">Griffith (1949)</a> in horse racing and later formalized by <a href=\"https://www.aeaweb.org/articles?id=10.1257/jep.2.2.161\">Thaler &amp; Ziemba (1988)</a> in their analysis of parimutuel betting markets, the longshot bias describes the tendency for bettors to overpay for low-probability outcomes. In efficient markets, a contract priced at  cents should win approximately % of the time. In markets exhibiting longshot bias, low-priced contracts win  than their implied probability, while high-priced contracts win .</p><p>The data confirms this pattern on Kalshi. Contracts trading at  win only  of the time, implying mispricing of . Conversely, contracts at  win  of the time. This pattern is consistent; all contracts priced below 20 cents underperform their odds, while those above 80 cents outperform.</p><blockquote><p> The calibration curve above demonstrates that prediction markets are actually quite efficient and accurate, with the slight exception of the tails. The close alignment between implied and actual probabilities confirms that prediction markets are well-calibrated price discovery mechanisms.</p></blockquote><p>The existence of the longshot bias raises a question unique to zero-sum markets: if some traders systematically overpay, who captures the surplus?</p><h2>The Maker-Taker Wealth Transfer</h2><p>Market microstructure defines two populations based on their interaction with the order book. A  provides liquidity by placing limit orders that rest on the book. A  consumes this liquidity by executing against resting orders.</p><p>Decomposing aggregate returns by role reveals a stark asymmetry:</p><div><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table></div><p>The divergence is most pronounced at the tails. At 1-cent contracts, takers win only  of the time against an implied probability of 1%, corresponding to a mispricing of . Makers on the same contracts win  of the time, resulting in a mispricing of . At 50 cents, mispricing compresses; takers show , and makers show .</p><p>Takers exhibit negative excess returns at . Makers exhibit positive excess returns at the same 80 levels. The market's aggregate miscalibration is concentrated in a specific population; takers bear the losses while makers capture the gains.</p><p>An obvious objection arises; makers earn the bid-ask spread as compensation for providing liquidity. Their positive returns may simply reflect spread capture rather than the exploitation of biased flow. While plausible, two observations suggest otherwise.</p><p>The first observation suggests the effect extends beyond pure spread capture; maker returns depend on which side they take. If profits were purely spread-based, it should not matter whether makers bought YES or NO. We test this by decomposing maker performance by position direction:</p><p>Makers who buy NO outperform makers who buy YES . The volume-weighted excess return is  for makers buying YES versus  for makers buying NO, a gap of 0.47 percentage points. The effect is miniscule (Cohen's d = 0.02-0.03) but consistent. At minimum, this suggests spread capture is not the whole story.</p><p>A second observation strengthens the case further; the maker-taker gap varies substantially by market category.</p><h3>Variation Across Categories</h3><p>We examine whether the maker-taker gap varies by market category. If the bias reflects uninformed demand, categories attracting less sophisticated participants should show larger gaps.</p><div><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><p>The variation is striking. Finance shows a gap of merely ; the market is extremely efficient, with takers losing only 0.08% per trade. At the other extreme, World Events and Media show gaps exceeding 7 percentage points. Sports, the largest category by volume, exhibits a moderate gap of 2.23 pp. Given $6.1 billion in taker volume, even this modest gap generates substantial wealth transfer.</p><p>Why is Finance efficient? The likely explanation is participant selection; financial questions attract traders who think in probabilities and expected values rather than fans betting on their favorite team or partisans betting on a preferred candidate. The questions themselves are dry (\"Will the S&amp;P close above 6000?\"), which filters out emotional bettors.</p><p>The maker-taker gap is not a fixed feature of the market; rather, it emerged as the platform grew. In Kalshi's early days, the pattern was reversed; takers earned positive excess returns while makers lost money.</p><p>From launch through 2023, taker returns averaged  while maker returns averaged . Without sophisticated counterparties, takers won; amateur makers defined the early period and were the losing population. This began to reverse in 2024 Q2, with the gap crossing zero and then widening sharply after the 2024 election.</p><p>The inflection point coincides with two events; Kalshi's legal victory over the CFTC in October 2024, which permitted political contracts, and the subsequent 2024 election cycle. Volume exploded from $30 million in 2024 Q3 to $820 million in 2024 Q4. The new volume attracted sophisticated market makers, and with them, the extraction of value from taker flow.</p><p>Pre-election, the average gap was -2.9 pp (takers winning); post-election, it flipped to +2.5 pp (makers winning), a swing of 5.3 percentage points.</p><p>The composition of taker flow provides further evidence. If the wealth transfer arose because new participants arrived with stronger longshot preferences, we would expect the distribution to shift toward low-probability contracts. It did not:</p><p>The share of taker volume in longshot contracts (1-20¢) remained essentially flat;  pre-election versus  post-election. The distribution actually shifted  the middle; the 91-99¢ bucket fell from 40-50% in 2021-2023 to under 20% in 2025, while mid-range prices (31-70¢) grew substantially. Taker behavior did not become more biased; if anything, it became less extreme. Yet taker losses increased; new market makers extract value more efficiently across all price levels.</p><p>This evolution reframes the aggregate results. The wealth transfer from takers to makers is not inherent to prediction market microstructure; it requires sophisticated market makers, and sophisticated market makers require sufficient volume to justify participation. In the low-volume early period, makers were likely unsophisticated individuals who lost to relatively informed takers. The volume surge attracted professional liquidity providers capable of extracting value from taker flow at all price points.</p><p>The maker-taker decomposition identifies  absorbs the losses, but leaves open the question of  their selection bias operates. Why is taker flow so consistently mispriced? The answer is not that makers possess superior foresight, but rather that takers exhibit a costly preference for affirmative outcomes.</p><h3>The Asymmetry at Equivalent Prices</h3><p>Standard efficiency models imply that mispricing should be symmetric across contract types at equivalent prices; a 1-cent YES contract and a 1-cent NO contract should theoretically reflect similar expected values. The data contradicts this assumption. At a price of 1 cent, a YES contract carries a historical expected value of -41%; buyers lose nearly half their capital in expectation. Conversely, a NO contract at the same 1-cent price delivers a historical expected value of +23%. The divergence between these seemingly identical probability estimates is 64 percentage points.</p><p>The advantage for NO contracts is persistent. NO outperforms YES at , with the advantage concentrating at the market extremes. NO contracts generate superior returns at every price increment from  and again from .</p><p>Despite the market being zero-sum, dollar-weighted returns are -1.02% for YES buyers compared to +0.83% for NO buyers, a 1.85 percentage point gap driven by the overpricing of YES contracts.</p><h3>Takers Prefer Affirmative Bets</h3><p>The underperformance of YES contracts may be linked to taker behavior. Breaking down the trading data reveals a structural imbalance in order flow composition.</p><p>In the 1-10 cent range, where YES represents the longshot outcome, takers account for 41-47% of YES volume; makers account for only 20-24%. This imbalance inverts at the opposite end of the probability curve. When contracts trade at 99 cents, implying that NO is the 1-cent longshot, makers actively purchase NO contracts at 43% of volume. Takers participate at a rate of only 23%.</p><p>One might hypothesize that makers exploit this asymmetry through superior directional forecasting—that they simply know when to buy NO. The evidence does not support this. When decomposing maker performance by position direction, returns are nearly identical. Statistically significant differences emerge only at the extreme tails (1–10¢ and 91–99¢), and even there, effect sizes are negligible (Cohen's d = 0.02–0.03). This symmetry is telling: makers do not profit by knowing which way to bet, but through some mechanism that applies equally to both directions.</p><p>The analysis of 72.1 million trades on Kalshi reveals a distinct market microstructure where wealth systematically transfers from liquidity takers to liquidity makers. This phenomenon is driven by specific behavioral biases, modulated by market maturity, and concentrated in categories that elicit high emotional engagement.</p><p>A central question in zero-sum market analysis is whether profitable participants win through superior information (forecasting) or superior structure (market making). Our data strongly supports the latter. When decomposing maker returns by position direction, the performance gap is negligible: makers buying \"YES\" earn an excess return of +0.77%, while those buying \"NO\" earn +1.25% (Cohen’s d ≈ 0.02). This statistical symmetry indicates that makers do not possess a significant ability to pick winners. Instead, they profit via a structural arbitrage: providing liquidity to a taker population that exhibits a costly preference for affirmative, longshot outcomes.</p><p>This extraction mechanism relies on the \"Optimism Tax.\" Takers disproportionately purchase \"YES\" contracts at longshot prices, accounting for nearly half of all volume in that range, despite \"YES\" longshots underperforming \"NO\" longshots by up to 64 percentage points. Makers, therefore, do not need to predict the future; they simply need to act as the counterparty to optimism. This aligns with findings by <a href=\"https://ssrn.com/abstract=5910522\">Reichenbach and Walther (2025)</a> on Polymarket and <a href=\"https://mpra.ub.uni-muenchen.de/126351/1/MPRA_paper_126351.pdf\">Whelan (2025)</a> on Betfair, suggesting that in prediction markets, makers accommodate biased flow rather than out-forecast it.</p><h3>The Professionalization of Liquidity</h3><p>The temporal evolution of maker-taker returns challenges the assumption that longshot bias inevitably leads to wealth transfer. From 2021 through 2023, the bias existed, yet takers maintained positive excess returns. The reversal of this trend coincides precisely with the explosive volume growth following Kalshi’s October 2024 legal victory.</p><p>The wealth transfer observed in late 2024 is a function of . In the platform's infancy, low liquidity likely deterred sophisticated algorithmic market makers, leaving the order book to be populated by amateurs who were statistically indistinguishable from takers. The massive volume surge following the 2024 election incentivized the entry of professional liquidity providers capable of systematically capturing the spread and exploiting the biased flow. The longshot bias itself may have persisted for years, but it was only once market depth grew sufficiently to attract these sophisticated makers that the bias became a reliable source of profit extraction.</p><h3>Category Differences and Participant Selection</h3><p>The variation in maker-taker gaps across categories reveals how participant selection shapes market efficiency. At one extreme, Finance exhibits a gap of just 0.17 percentage points; nearly perfect efficiency. At the other, World Events and Media exceed 7 percentage points. This difference cannot be explained by the longshot bias alone; it reflects who chooses to trade in each category.</p><ul><li><p> serves as a control group demonstrating that prediction markets can approach efficiency. Questions like \"Will the S&amp;P close above 6000?\" attract participants who think in probabilities and expected values, likely the same population that trades options or follows macroeconomic data. The barrier to informed participation is high, and casual bettors have no edge and likely recognize this, filtering themselves out.</p></li><li><p> shows moderate inefficiency despite high emotional stakes. Political bettors follow polling closely and have practiced calibrating beliefs through election cycles. The gap is larger than Finance but far smaller than entertainment categories, suggesting that political engagement, while emotional, does not entirely erode probabilistic reasoning.</p></li><li><p> represents the modal prediction market participant. The gap is moderate but consequential given the category's 72% volume share. Sports bettors exhibit well-documented biases, including home team loyalty, recency effects, and narrative attachment to star players. A fan betting on their team to win the championship is not calculating expected value; they are purchasing hope.</p></li><li><p> attracts participants conditioned by the \"number go up\" mentality of retail crypto markets, a population overlapping with meme stock traders and NFT speculators. Questions like \"Will Bitcoin reach $100k?\" invite narrative-driven betting rather than probability estimation.</p></li><li><p><strong>Entertainment, Media, and World Events (4.79–7.32 pp)</strong> exhibit the largest gaps and share a common feature: minimal barriers to perceived expertise. Anyone who follows celebrity gossip feels qualified to bet on award show outcomes; anyone who reads headlines feels informed about geopolitics. This creates a participant pool that conflates familiarity with calibration.</p></li></ul><p>The pattern suggests efficiency depends on two factors: the technical barrier to informed participation and the degree to which questions invite emotional reasoning. When barriers are high and framing is clinical, markets approach efficiency; when barriers are low and framing invites storytelling, the optimism tax reaches its maximum.</p><p>While the data is robust, several limitations persist. First, the absence of unique trader IDs forces us to rely on the \"Maker/Taker\" classification as a proxy for \"Sophisticated/Unsophisticated.\" While standard in microstructure literature, this imperfectly captures instances where sophisticated traders cross the spread to act on time-sensitive information. Second, we cannot directly observe the bid-ask spread in historical trade data, making it difficult to strictly decouple spread capture from explotation of biased flow. Finally, these results are specific to a US-regulated environment; offshore venues with different leverage caps and fee structures may exhibit different dynamics.</p><p>The promise of prediction markets lies in their ability to aggregate diverse information into a single, accurate probability. However, our analysis of Kalshi demonstrates that this signal is often distorted by systematic wealth transfer driven by human psychology and market microstructure.</p><p>The market is split into two distinct populations: a taker class that systematically overpays for low-probability, affirmative outcomes, and a maker class that extracts this premium through passive liquidity provision. This dynamic is not an inherent flaw of the \"wisdom of the crowd,\" but rather a feature of how human psychology interacts with market microstructure. When the topic is dry and quantitative (Finance), the market is efficient. When the topic allows for tribalism and hope (Sports, Entertainment), the market transforms into a mechanism for transferring wealth from the optimistic to the calculated.</p>",
      "contentLength": 19307,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46680515"
    },
    {
      "title": "American importers and consumers bear the cost of 2025 tariffs: analysis",
      "url": "https://www.kielinstitut.de/publications/americas-own-goal-who-pays-the-tariffs-19398/",
      "date": 1768837381,
      "author": "47282847",
      "guid": 36992,
      "unread": true,
      "content": "<p>Lück, S., Callaghan, M., Borchers, M., Cowie, A., Fuss, S., Gidden, M., Hartmann, J., Kammann, C., Keller, D.P., Kraxner, F., Lamb, W.F., Mac Dowell, N., Müller-Hansen, F., Nemet, G.F., Probst, B.S., Renforth, P., Repke, T., Rickels, W., Schulte, I., Smith, P., Smith, S.M., Thrän, D., Troxler, T.G., Sick, V., Minx, J.C.</p><a href=\"https://www.kielinstitut.de/fileadmin/Dateiverwaltung/IfW-Publications/fis-import/e0562e3d-8d73-4c7c-a205-b9935345ea40-s41467-025-61485-8.pdf\">\n                PDF\n            </a>",
      "contentLength": 357,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46680212"
    },
    {
      "title": "CSS Web Components for marketing sites (2024)",
      "url": "https://hawkticehurst.com/2024/11/css-web-components-for-marketing-sites/",
      "date": 1768835741,
      "author": "zigzag312",
      "guid": 37057,
      "unread": true,
      "content": "<section data-astro-cid-resui2ln=\"\"></section><p>Hot take: I think “regular” web components (the ones with Shadow DOM and friends) are a terrible solution for marketing website design systems.</p><p>It has always left a bad taste in my mouth when I run across a web component for a swimlane, banner, card, and so on. Why? Because these are components that (unless you’re doing something mighty fancy) should never require JavaScript as a dependency.</p><p>But, in the world of web components you are locked into JavaScript from the very start. To even register a web component with the browser you need JavaScript.</p><p>But what if… we didn’t do that?</p><p>I’ve spent a good chunk of the last year focused on marketing site design systems at work. A regular topic of discussion is the need to build marketing sites that are accessible to folks with lower powered devices and poor internet connections. How do you achieve that? In short, use less JavaScript and ideally build UI with progressive enhancement in mind.</p><p>There are many ways to achieve these goals, but the method I’ve been focused on is how an <a href=\"https://hawkticehurst.com/2023/11/a-year-working-with-html-web-components/\">HTML Web Component</a> archictecture might be applied to implement a marketing site design system.</p><p>As a quick reminder/intro, HTML Web Components is a method of building web components where you write HTML as you would normally and then wrap the parts you want to be interactive using a custom element.</p><p>For example, if you wanted to create a counter button it would look like this:</p><pre tabindex=\"0\" data-language=\"html\"><code></code></pre><p>The markup in an HTML web component is parsed, rendered, and styled as normal HTML. That HTML will then be seamlessly hydrated once the JavaScript associated with the custom element tag is executed.</p><p>\n\tIn contrast, the markup of a \"regular\" web component (that uses Shadow DOM) is dynamically generated at runtime using JavaScript -- kind of like an SPA.\n</p><p>This component architecture is a really strong candidate for a marketing design system (and, as a bonus, avoids some of the big gotchas that come with regular web components).</p><ul><li>It is a perfect implementation of progressively enhanced UI</li><li>It uses minimal and self-contained JavaScript — HTML Web Components can be thought of as <a href=\"https://jasonformat.com/islands-architecture/\">islands</a></li><li>You still get the power of custom element APIs to implement stuff like design system component variants</li><li>The component markup is fully SSR-able</li><li>The component markup can be styled like regular HTML</li><li>Common accessibility practices can be applied without issue</li></ul><p>But for all these benefits we’re still left with the original problem. HTML Web Components require JavaScript.</p><p>So here’s the question: What would happen if we took the ideas of HTML Web Components and skipped all the JavaScript?</p><p>You get CSS Web Components.</p><p>Note: I've never seen anyone talk about or name this concept before, so I'm using \"CSS Web Components\" to describe the idea. But please let me know if someone has already written about and named this!</p><p>How do they work? The exact same as HTML Web Components but you just take advantage of the powers of CSS to implement key functionality.</p><p>As an example let’s implement that swimlane component:</p><pre tabindex=\"0\" data-language=\"html\"><code></code></pre><section></section><p>Okay great, we styled some HTML nested inside a custom element. There’s nothing too novel about that. But what about adding some functionality? Say, a component variant that lets you reverse the layout of the swimlane?</p><p>It’s possible using only CSS! Specifically, CSS attribute selectors.</p><pre tabindex=\"0\" data-language=\"html\"><code></code></pre><section></section><p>Another really cool perk of this is that because you’re defining an attribute on a custom element you don’t have to worry about naming collisions with HTML attributes. No need to add  to the beginning of attributes like you would/should on normal HTML elements.</p><p>In theory, I believe this method of building design systems can go quite far. If you think about it, the vast majority of basic components you might need in a marketing design system are just vanilla HTML elements with specific style variations.</p><p>A marketing website button is just an anchor tag wrapped in a  custom element and styled using custom attribute selectors.</p><pre tabindex=\"0\" data-language=\"html\"><code></code></pre><p>From here, imagine incorporating all the other powers that CSS (and HTML) bring to the table:</p><p>The possibilities are quite large.</p>",
      "contentLength": 4055,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46679907"
    },
    {
      "title": "\"Anyone else out there vibe circuit-building?\"",
      "url": "https://twitter.com/beneater/status/2012988790709928305",
      "date": 1768835684,
      "author": "thetrustworthy",
      "guid": 37013,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46679896"
    },
    {
      "title": "GLM-4.7-Flash",
      "url": "https://huggingface.co/zai-org/GLM-4.7-Flash",
      "date": 1768835532,
      "author": "scrlk",
      "guid": 37012,
      "unread": true,
      "content": "<p>GLM-4.7-Flash is a 30B-A3B MoE model. As the strongest model in the 30B class, GLM-4.7-Flash offers a new option for lightweight deployment that balances performance and efficiency.</p><h3><a rel=\"nofollow\" href=\"https://huggingface.co/zai-org/GLM-4.7-Flash#performances-on-benchmarks\"></a></h3><div><table><thead><tr><th>Qwen3-30B-A3B-Thinking-2507</th></tr></thead><tbody><tr></tr></tbody></table></div><h2><a rel=\"nofollow\" href=\"https://huggingface.co/zai-org/GLM-4.7-Flash#serve-glm-47-flash-locally\"></a></h2><p>For local deployment, GLM-4.7-Flash supports inference frameworks including vLLM and SGLang. Comprehensive deployment\ninstructions are available in the official <a rel=\"nofollow\" href=\"https://github.com/zai-org/GLM-4.5\">Github</a> repository.</p><p>vLLM and SGLang only support GLM-4.7-Flash on their main branches.</p><ul><li>using pip (must use pypi.org as the index url):</li></ul><pre><code>pip install -U vllm --pre --index-url https://pypi.org/simple --extra-index-url https://wheels.vllm.ai/nightly\npip install git+https://github.com/huggingface/transformers.git\n</code></pre><ul><li>Install the supported versions of SGLang and Transformers (using  is recommended):</li></ul><pre><code>uv pip install sglang==0.3.2.dev9039+pr-17247.g90c446848 --extra-index-url https://sgl-project.github.io/whl/pr/\nuv pip install git+https://github.com/huggingface/transformers.git@76732b4e7120808ff989edbd16401f61fa6a0afa\n</code></pre><p>using with transformers as</p><pre><code>pip install git+https://github.com/huggingface/transformers.git\n</code></pre><pre><code> torch\n transformers  AutoModelForCausalLM, AutoTokenizer\n\nMODEL_PATH = \nmessages = [{: , : }]\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=,\n    add_generation_prompt=,\n    return_dict=,\n    return_tensors=,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    pretrained_model_name_or_path=MODEL_PATH,\n    torch_dtype=torch.bfloat16,\n    device_map=,\n)\ninputs = inputs.to(model.device)\ngenerated_ids = model.generate(**inputs, max_new_tokens=, do_sample=)\noutput_text = tokenizer.decode(generated_ids[][inputs.input_ids.shape[]:])\n(output_text)\n</code></pre><pre><code>vllm serve zai-org/GLM-4.7-Flash \\\n     --tensor-parallel-size 4 \\\n     --speculative-config.method mtp \\\n     --speculative-config.num_speculative_tokens 1 \\\n     --tool-call-parser glm47 \\\n     --reasoning-parser glm45 \\\n     --enable-auto-tool-choice \\\n     --served-model-name glm-4.7-flash\n</code></pre><pre><code>python3 -m sglang.launch_server \\\n  --model-path zai-org/GLM-4.7-Flash \\\n  --tp-size 4 \\\n  --tool-call-parser glm47  \\\n  --reasoning-parser glm45 \\\n  --speculative-algorithm EAGLE \\\n  --speculative-num-steps 3 \\\n  --speculative-eagle-topk 1 \\\n  --speculative-num-draft-tokens 4 \\\n  --mem-fraction-static 0.8 \\\n  --served-model-name glm-4.7-flash \\\n  --host 0.0.0.0 \\\n  --port 8000\n</code></pre><ul><li>For Blackwell GPUs, include <code>--attention-backend triton --speculative-draft-attention-backend triton</code> in your SGLang launch command.</li></ul><p>If you find our work useful in your research, please consider citing the following paper:</p><pre><code>@misc{5team2025glm45agenticreasoningcoding,\n      title={GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models}, \n      author={GLM Team and Aohan Zeng and Xin Lv and Qinkai Zheng and Zhenyu Hou and Bin Chen and Chengxing Xie and Cunxiang Wang and Da Yin and Hao Zeng and Jiajie Zhang and Kedong Wang and Lucen Zhong and Mingdao Liu and Rui Lu and Shulin Cao and Xiaohan Zhang and Xuancheng Huang and Yao Wei and Yean Cheng and Yifan An and Yilin Niu and Yuanhao Wen and Yushi Bai and Zhengxiao Du and Zihan Wang and Zilin Zhu and Bohan Zhang and Bosi Wen and Bowen Wu and Bowen Xu and Can Huang and Casey Zhao and Changpeng Cai and Chao Yu and Chen Li and Chendi Ge and Chenghua Huang and Chenhui Zhang and Chenxi Xu and Chenzheng Zhu and Chuang Li and Congfeng Yin and Daoyan Lin and Dayong Yang and Dazhi Jiang and Ding Ai and Erle Zhu and Fei Wang and Gengzheng Pan and Guo Wang and Hailong Sun and Haitao Li and Haiyang Li and Haiyi Hu and Hanyu Zhang and Hao Peng and Hao Tai and Haoke Zhang and Haoran Wang and Haoyu Yang and He Liu and He Zhao and Hongwei Liu and Hongxi Yan and Huan Liu and Huilong Chen and Ji Li and Jiajing Zhao and Jiamin Ren and Jian Jiao and Jiani Zhao and Jianyang Yan and Jiaqi Wang and Jiayi Gui and Jiayue Zhao and Jie Liu and Jijie Li and Jing Li and Jing Lu and Jingsen Wang and Jingwei Yuan and Jingxuan Li and Jingzhao Du and Jinhua Du and Jinxin Liu and Junkai Zhi and Junli Gao and Ke Wang and Lekang Yang and Liang Xu and Lin Fan and Lindong Wu and Lintao Ding and Lu Wang and Man Zhang and Minghao Li and Minghuan Xu and Mingming Zhao and Mingshu Zhai and Pengfan Du and Qian Dong and Shangde Lei and Shangqing Tu and Shangtong Yang and Shaoyou Lu and Shijie Li and Shuang Li and Shuang-Li and Shuxun Yang and Sibo Yi and Tianshu Yu and Wei Tian and Weihan Wang and Wenbo Yu and Weng Lam Tam and Wenjie Liang and Wentao Liu and Xiao Wang and Xiaohan Jia and Xiaotao Gu and Xiaoying Ling and Xin Wang and Xing Fan and Xingru Pan and Xinyuan Zhang and Xinze Zhang and Xiuqing Fu and Xunkai Zhang and Yabo Xu and Yandong Wu and Yida Lu and Yidong Wang and Yilin Zhou and Yiming Pan and Ying Zhang and Yingli Wang and Yingru Li and Yinpei Su and Yipeng Geng and Yitong Zhu and Yongkun Yang and Yuhang Li and Yuhao Wu and Yujiang Li and Yunan Liu and Yunqing Wang and Yuntao Li and Yuxuan Zhang and Zezhen Liu and Zhen Yang and Zhengda Zhou and Zhongpei Qiao and Zhuoer Feng and Zhuorui Liu and Zichen Zhang and Zihan Wang and Zijun Yao and Zikang Wang and Ziqiang Liu and Ziwei Chai and Zixuan Li and Zuodong Zhao and Wenguang Chen and Jidong Zhai and Bin Xu and Minlie Huang and Hongning Wang and Juanzi Li and Yuxiao Dong and Jie Tang},\n      year={2025},\n      eprint={2508.06471},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.06471}, \n}\n</code></pre>",
      "contentLength": 5425,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46679872"
    },
    {
      "title": "Ask HN: COBOL devs, how are AI coding affecting your work?",
      "url": "https://news.ycombinator.com/item?id=46678550",
      "date": 1768827942,
      "author": "zkid18",
      "guid": 36991,
      "unread": true,
      "content": "Curious to hear from anyone actively working with COBOL/mainframes. Do you see LLMs as a threat to your job security, or the opposite?<p>I feel that the mass of code that actually runs the economy is remarkably untouched by AI coding agents.</p>",
      "contentLength": 238,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46678550"
    },
    {
      "title": "Article by article, how Big Tech shaped the EU's roll-back of digital rights",
      "url": "https://corporateeurope.org/en/2026/01/article-article-how-big-tech-shaped-eus-roll-back-digital-rights",
      "date": 1768827208,
      "author": "robtherobber",
      "guid": 36973,
      "unread": true,
      "content": "<p dir=\"ltr\">At the end of November 2025, Ursula von der Leyen gave Trump and his tech oligarchs an early Christmas present: an unprecedented attack on digital rights. In its so-called Digital Omnibus, the European Commission proposed weakening important rules designed to protect us from Big Tech’s abuses of power.</p><p dir=\"ltr\">These are the protections that keep everyone's data safe, governments and companies accountable, protect people from having artificial intelligence (AI) systems decide their life opportunities, and ultimately keep our societies free from unchecked surveillance.</p><p dir=\"ltr\">At the same time, the Digital Omnibus is part of the European Commission's&nbsp;<a href=\"https://corporateeurope.org/en/deregulation-watch\">deregulation agenda</a>, which threatens key social and environmental standards in Europe. Ironically this deregulation agenda is being promoted by the Commission as a way to make the EU 'competitive' – despite in reality actively empowering US Big Tech companies that dominate the field.</p><p dir=\"ltr\">The Digital Omnibus was immediately&nbsp;<a href=\"https://noyb.eu/en/digital-omnibus-eu-commission-wants-wreck-core-gdpr-principles\">heavily</a><a href=\"https://edri.org/our-work/europe-is-dismantling-its-digital-rights-from-within/\">criticised</a> by&nbsp;<a href=\"https://www.amnesty.org/en/latest/news/2025/11/eu-digital-omnibus-proposals-will-tear-apart-accountability-on-digital-rights/\">numerous</a><a href=\"https://www.beuc.eu/press-release/eus-plan-simplify-digital-laws-benefit-mainly-large-companies-expense-consumers\">civil</a> society organisations.&nbsp;<a href=\"https://www.politico.eu/article/brussels-police-world-digital-tech-us-china-regulations/\">Politico</a> even called it the end of the ‘Brussels effect’ – that is, that European tech regulations are adopted in other countries – and wrote that “Washington is [now] setting the pace on deregulation in Europe.”</p><p dir=\"ltr\">To show the extent of Big Tech’s influence on the Digital Omnibus, we compared the Commission’s proposals with the lobbying positions from Big Tech and its associations.&nbsp;</p><p dir=\"ltr\">The proposals in the Digital Omnibus concern both data protection and rules for AI. While the EU mistakenly speaks of benefits for European corporations, it is clear that weak digital rules strengthen the power of Google, Microsoft, Meta etc, thereby jeopardising the goal of becoming more independent from Big Tech and the US.&nbsp;</p><p dir=\"ltr\">In the past, Big Tech has repeatedly spread the one-sided lobbying message that data protection hinders economic growth and innovation,&nbsp;<a href=\"https://euneedsai.com/\">especially</a> with regard to AI. This includes exceptions for SMEs and a fundamental focus on making&nbsp;<a href=\"https://www.lobbyregister.bundestag.de/media/2f/ca/502331/Stellungnahme-Gutachten-SG2503310295.pdf\">more use of data</a> instead of protecting it.</p><p dir=\"ltr\">Tech companies are spreading these messages with a record-breaking lobbying budget, a huge lobbying network, and support from the Trump administration. The digital industry’s annual lobby spending has grown from&nbsp;<a href=\"https://corporateeurope.org/en/2025/10/big-tech-lobby-budgets-hit-record-levels\">€113 million in 2023 to €151 million today</a> – an increase of 33.6 percent in just two years.</p><p dir=\"ltr\">Now, the European Commission appears to be bowing to this lobbying pressure and adopting key lobbying messages from Google, Microsoft, Meta and their many lobby organisations in its Digital Omnibus.&nbsp;</p><p dir=\"ltr\">Here we break down these industry lobbying messages, how they have been adopted by the Commission as proposed text changes, and what the real world impacts could be.</p>",
      "contentLength": 2708,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46678430"
    },
    {
      "title": "Amazon is ending all inventory commingling as of March 31, 2026",
      "url": "https://twitter.com/ghhughes/status/2012824754319753456",
      "date": 1768825454,
      "author": "MrBuddyCasino",
      "guid": 36972,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46678205"
    },
    {
      "title": "Nvidia contacted Anna's Archive to access books",
      "url": "https://torrentfreak.com/nvidia-contacted-annas-archive-to-secure-access-to-millions-of-pirated-books/",
      "date": 1768821070,
      "author": "antonmks",
      "guid": 36990,
      "unread": true,
      "content": "<p><img loading=\"lazy\" decoding=\"async\" src=\"https://torrentfreak.com/images/nvidia-logo.jpg\" alt=\"nvidia logo\" width=\"300\" height=\"197\" srcset=\"https://torrentfreak.com/images/nvidia-logo.jpg 665w, https://torrentfreak.com/images/nvidia-logo-300x197.jpg 300w\" sizes=\"auto, (max-width: 300px) 100vw, 300px\">Chip giant NVIDIA has been one of the main financial beneficiaries in the artificial intelligence boom. </p><p>Revenue surged due to high demand for its AI-learning chips and data center services, and the end doesn’t appear to be in sight.  </p><p>Besides selling the most sought-after hardware, NVIDIA is also developing its own models, including NeMo, Retro-48B, InstructRetro, and Megatron. These are trained using their own hardware and with help from large text libraries, much like other tech giants do. </p><h2>Authors Sue NVIDIA for Copyright Infringement</h2><p>Like other tech companies, NVIDIA has also seen significant legal pushback from copyright holders in response to its training methods. This includes authors, who, in various lawsuits, accused tech companies of training their models on pirated books.</p><p>In early 2024, for example, several authors <a href=\"https://torrentfreak.com/authors-sue-nvidia-for-training-ai-on-pirated-books-240311/\">sued NVIDIA</a> over alleged copyright infringement. </p><p>Through the class action lawsuit, they claimed that the company’s AI models were trained on the Books3 dataset that included copyrighted works taken from the ‘pirate’ site Bibliotik. Since this happened without permission, the authors demanded compensation. </p><p>In response, NVIDIA <a href=\"https://torrentfreak.com/nvidia-copyrighted-books-are-just-statistical-correlations-to-our-ai-models-240617/\">defended its actions </a>as fair use, noting that books are nothing more than statistical correlations to its AI models. However, the allegations didn’t go away. On the contrary, the plaintiffs found more evidence during discovery. </p><h2>‘NVIDIA Contacted Anna’s Archive’</h2><p>Last Friday, the authors filed an amended complaint that significantly expands the scope of the lawsuit. In addition to adding more books, authors, and AI models, it also includes broader “shadow library” claims and allegations. </p><p>The authors, including <a href=\"https://en.wikipedia.org/wiki/Abdi_Nazemian\">Abdi Nazemian</a>, now cite various internal Nvidia emails and documents, suggesting that the company willingly downloaded millions of copyrighted books. </p><p>The new complaint alleges that “competitive pressures drove NVIDIA to piracy”, which allegedly included collaborating with the controversial Anna’s Archive library.</p><p>According to the amended complaint, a member of Nvidia’s data strategy team reached out to Anna’s Archive to find out what the pirate library could offer the trillion-dollar company</p><p>“Desperate for books, NVIDIA contacted Anna’s Archive—the largest and most brazen of the remaining shadow libraries—about acquiring its millions of pirated materials and ‘including Anna’s Archive in pre-training data for our LLMs’,” the complaint notes. </p><p>“Because Anna’s Archive charged tens of thousands of dollars for ‘high-speed access’ to its pirated collections […] NVIDIA sought to find out what “high-speed access” to the data would look like.”</p><h2>Anna’s Archive Points Out Legal ‘Concern’</h2><p>According to the complaint, Anna’s Archive then warned Nvidia that its library was illegally acquired and maintained. Because the site previously wasted time on other AI companies, the pirate library asked NVIDIA executives if they had internal permission to move forward. </p><p>This permission was allegedly granted within a week, after which Anna’s Archive provided the chip giant with access to its pirated books. </p><p>“Within a week of contacting Anna’s Archive, and days after being warned by Anna’s Archive of the illegal nature of their collections, NVIDIA management gave ‘the green light’ to proceed with the piracy. Anna’s Archive offered NVIDIA millions of pirated copyrighted books.”</p><p>The complaint states that Anna’s Archive promised to provide NVIDIA with access to roughly 500 terabytes of data. This included millions of books that are usually only accessible through Internet Archive’s digital lending system, which itself has been <a href=\"https://torrentfreak.com/internet-archive-loses-landmark-e-book-lending-copyright-appeal-against-publishers-240905/\">targeted in court</a>. </p><p>The complaint does not explicitly mention whether NVIDIA ended up paying Anna’s Archive for access to the data. </p><p>Additionally, it’s worth mentioning that NVIDIA also stands accused of using other pirated sources. In addition to the previously included Books3 database, the new complaint also alleges that the company downloaded books from LibGen, Sci-Hub, and Z-Library.</p><h2>Direct and Vicarious Copyright Infringement</h2><p>In addition to downloading and using pirated books for its own AI training, the authors allege NVIDIA distributed scripts and tools that allowed its corporate customers to automatically download “<a href=\"https://en.wikipedia.org/wiki/The_Pile_(dataset)\">The Pile</a>“, which contains the Books3 pirated dataset. </p><p>These allegations lead to new claims of vicarious and contributory infringement, alleging that NVIDIA generated revenue from customers by facilitating access to these pirated datasets.</p><p>Based on these and other claims, the authors request to be compensated for the damages they suffered. This applies to the named authors, but also to potentially hundreds of others who may later join the class action lawsuit. </p><p>As far as we know, this is the first time that correspondence between a major U.S. tech company and Anna’s Archive was revealed in public. This will only raise the profile of the pirate library, which just <a href=\"https://torrentfreak.com/u-s-court-order-against-annas-archive-spells-more-trouble-for-the-site/\">lost several domain names</a>, even further. </p><p><em>A copy of the first consolidated and amended complaint, filed at the U.S. District Court for the Northern District of California, is available <a href=\"https://torrentfreak.com/images/naznvid-amend.pdf\">here (pdf)</a>. The named authors include Abdi Nazemian, Brian Keene, Stewart O’Nan, Andre Dubus III, and Susan Orlean.</em></p>",
      "contentLength": 5293,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46677628"
    },
    {
      "title": "Wikipedia: WikiProject AI Cleanup",
      "url": "https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup",
      "date": 1768817378,
      "author": "thinkingemote",
      "guid": 36958,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46677106"
    },
    {
      "title": "I was a top 0.01% Cursor user, then switched to Claude Code 2.0",
      "url": "https://blog.silennai.com/claude-code",
      "date": 1768813214,
      "author": "SilenN",
      "guid": 37122,
      "unread": true,
      "content": "<div><p>Here's some tips I've picked up over time on context management:</p><p>Use  to continue from a previous chat.</p><p> Claude Code has a 200k context limit. You hit the wall faster than alternatives like Codex (400k) or Gemini (1M).</p></div><div><p>Your time spent planning is directly proportional to agent output.</p><p>Rule of thumb: a good prompt will save you 3 minutes of time on follow up prompts and debugging for every 1 minute you spend planning.</p><p> Enter plan mode. I use it, but only for larger tasks or when the exact shape of what I want it to be is unclear. Note: plan mode saves to a  in the global  folder, which isn't accessible within your repo. I'll either ask Claude to create a  in the repo after plan mode, or skip plan mode entirely and plan in-chat.</p></div><ol><li> Start a conversation, ask questions, let it explore code, create a plan together. When you're happy, say \"write plan to docs/*.md and start coding.\" or if in plan mode, \"yes, and bypass permissions.\"</li><li> For larger projects, set up a progress.txt and structured task list (prd.json). More on this in the Advanced section.</li><li> Run your prompt, see what it generates, then revert and keep planning for the final plan.</li></ol><div><p>After creating your plan, use our<a href=\"https://gist.github.com/SilenNaihin/0733adf5e8deea4242878938c3bdc9fb\" target=\"_blank\" rel=\"noopener noreferrer\"></a>command which interviews you in depth about your plan before building. (See<a href=\"https://x.com/trq212/status/2005315275026260309?s=20\" target=\"_blank\" rel=\"noopener noreferrer\">this X post</a>, I've also tried it myself and found it genuinely effective.)</p><div>Read @plan.md and interview me in detail using the AskUserQuestionTool about literally anything: technical implementation, UI &amp; UX, concerns, tradeoffs, etc. Make sure the questions are not obvious. Be very in-depth and continue interviewing me continually until it's complete, then write the plan to the file.</div><p>Opus 4.5 is amazing at explaining things and makes stellar ASCII diagrams. My exploration involves asking lots of questions, clarifying requirements, understanding where/how/why to make changes.</p><p> Models are currently, RLHF'd so far into oblivion that you need to not maintain backwards compatibility.</p><p><strong>Watch out for overengineering:</strong> Claude models love to do too much. Extra files, flexibility you didn't ask for, unnecessary abstractions. Be as explicit with what NOT to do as possible. Pete puts it well:<em>\"We want the simplest change possible. We don't care about migration. Code readability matters most, and we're happy to make bigger changes to achieve it.\"</em></p><p> Coding agents are better at creating new files than editing existing ones. It can often be valuable to tweak the seed prompt and reset all the code from scratch.</p></div><div><p>There's a classic XKCD about programmers spending a week automating a task that takes 5 minutes.</p></div><div><p>With agentic coding, this equation has flipped.<strong>Closing the loop is now almost always worth it.</strong>The cost of automation has collapsed. What used to take a week now takes a conversation.</p><p>If you find yourself doing something more than once, close the loop. If you spend a lot of time doing x thing, close the loop.</p><ul><li>Make commands for repeated prompts</li><li>Make agents for repeated work</li><li>Make prompts in .mds (like Cursor rules!)</li><li>Change tsconfig and other config files</li></ul></div><div><p>The only way you or the model know that you're right is if you can verify the outputs.</p><p>Before, you had to be in the code. Then with Cursor, you had to approve every edit. Now, just test behaviors with interface tests.</p><p>Interface tests are the ability to know what's wrong and explaining it.</p><p>For UI this means looking, for UX this means clicking around, for API this means making requests. And checking the responses.</p><p>A good way to think about closing the loop is to make it easy for you to verify by making it easy for the agent to verify.</p><p> Ask Claude to build comprehensive interface tests beforehand. This ensures you got the refactor right. The tests become your verification layer.</p><p> The best tests are written in the same context as the code they are testing.</p><p><strong>Let Jesus take the wheel.</strong> For production apps, test in staging or dev on a PR. Integration tests are your safety net. If they pass, ship it.</p></div><div><p>AI writes code fast, but debugging AI code requires different skills than debugging your own. You didn't write it, so you don't have the mental model.</p></div><div><p>When something fails, use systematic debugging. I have a<a href=\"https://gist.github.com/SilenNaihin/6833c01f597c82912af5aca4e3467a35\" target=\"_blank\" rel=\"noopener noreferrer\"> command</a>that triggers thorough investigation:</p></div><ol><li>Create hypotheses for what's wrong</li><li>Read ALL related code (take your time)</li><li>Add strategic logging to verify assumptions</li><li>Tackle it differently in a new chat</li><li>Worst case: dive into the code yourself</li></ol><div><p> If you've explained the same thing three times and Claude still isn't getting it, more explaining won't help. Change something.</p><p> If Claude keeps misunderstanding, show it a minimal example of what you want the output to look like. Claude is good at following examples.</p><p> If you're making lots of changes to your plan, start a new session. Get the agent to summarize the situation, what has been tried, and learnings. Copy paste into a new Claude session.</p></div><div><p>Different models have different blind spots. When stuck, get fresh perspectives:</p></div><div><p>You can automatically review your PRs and commits. Claude can catch issues, suggest improvements, and provide context aware feedback before human review even begins.</p><p>You can do this via a Stop hook (more on these later) with Claude code in headless mode () that triggers on every commit, or via prs. When I've used automated reviewing it was always at the via PR level.</p><p>If you have access, You don't want the same inductive biases that wrote the code reviewing it. Codex catches things Claude misses and vice versa.</p></div><div><p>Run<a href=\"https://gist.github.com/SilenNaihin/cd321a0ada16963867ad8984f44922cf\" target=\"_blank\" rel=\"noopener noreferrer\"></a>to do a focused cleanup session with these tools.</p><p>I refactor when I either feel pain because Claude is making mistakes, or after large additions to codebases. I'm not the only one of the opinion that doing this continuously kills momentum. Treat it as a distinct phase.</p><p>Claude won't understand your preferences around code cleanliness. Over time, add context to your Claude.md that reveals your preferences and reduces refactoring time.</p></div>",
      "contentLength": 5813,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46676554"
    },
    {
      "title": "Radboud University selects Fairphone as standard smartphone for employees",
      "url": "https://www.ru.nl/en/staff/news/radboud-university-selects-fairphone-as-standard-smartphone-for-employees",
      "date": 1768810984,
      "author": "ardentsword",
      "guid": 36930,
      "unread": true,
      "content": "<p>The Fairphone is a sustainable smartphone with easily replaceable parts such as the battery and screen. This makes the device last longer. Fair and recycled materials, such as plastic and aluminium, are used as much as possible in the production of this smartphone. Fairphone also pays attention to good and safe working conditions in its factories.</p><p>Fairphones are issued to employees by the Information &amp; Library Services (ILS) division. In addition to new Fairphones, the university can also reissue used Samsung devices where possible. These are Samsung devices that have already been returned and still meet the technical and age requirements. As long as these devices are still available, not every employee will receive a Fairphone immediately. Employees who have an iPhone from Radboud University can continue to use it as long as the device is still functioning. However, returned iPhones will no longer be reissued.</p><p>Employees who prefer to use their private phone for work can request an RU SIM card for this purpose. The costs for using your own device will not be reimbursed. Naturally, smartphone models that have already been issued will continue to be supported by ILS colleagues, as will privately purchased smartphone models used for work.</p><h2>Cost-effective and easier management</h2><p>Due to its longer lifespan, the total cost of a Fairphone is lower than that of comparable devices. In addition, Radboud University only needs to purchase, manage and support one standard model. This results in smaller stock, easier management and faster support. Manuals and instructions also only need to be maintained for one device.Furthermore, less investment is required in knowledge of different models/brands. This also helps to speed up incident handling and, where necessary, smartphone replacement.</p><p>Fairphone offers a five-year warranty and long-term software support for up to eight years. This means that devices need to be replaced less quickly. This fits in with Radboud University's circularity strategy, which focuses on the longest possible use and reuse of ICT hardware.</p>",
      "contentLength": 2077,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46676276"
    },
    {
      "title": "The coming industrialisation of exploit generation with LLMs",
      "url": "https://sean.heelan.io/2026/01/18/on-the-coming-industrialisation-of-exploit-generation-with-llms/",
      "date": 1768809424,
      "author": "long",
      "guid": 37089,
      "unread": true,
      "content": "<p>Recently I ran an experiment where I built agents on top of Opus 4.5 and GPT-5.2 and then challenged them to write exploits for a zeroday vulnerability in the QuickJS Javascript interpreter. I added a variety of modern exploit mitigations, various constraints (like assuming an unknown heap starting state, or forbidding hardcoded offsets in the exploits) and different objectives (spawn a shell, write a file, connect back to a command and control server). The agents succeeded in building over 40 distinct exploits across 6 different scenarios, and GPT-5.2 solved every scenario. Opus 4.5 solved all but two. I’ve put a technical write-up of the experiments and the results on <a href=\"https://github.com/SeanHeelan/anamnesis-release/\">Github</a>, as well as the code to reproduce the experiments.</p><p>In this post I’m going to focus on the main conclusion I’ve drawn from this work, which is that we should prepare for the industrialisation of many of the constituent parts of offensive cyber security. We should start assuming that in the near future the limiting factor on a state or group’s ability to develop exploits, break into networks, escalate privileges and remain in those networks, is going to be their token throughput over time, and not the number of hackers they employ. Nothing is certain, but we would be better off having wasted effort thinking through this scenario and have it not happen, than be unprepared if it does.</p><p><strong>A Brief Overview of the Experiment</strong></p><p>All of the code to re-run the experiments, a detailed write-up of them, and the raw data the agents produced are on <a href=\"https://github.com/SeanHeelan/anamnesis-release/\">Github</a>, but just to give a flavour of what the agents accomplished:</p><ol><li>Both agents turned the QuickJS vulnerability into an ‘API’ to allow them to read and arbitrarily modify the address space of the target process. As the vulnerability is a zeroday with no public exploits for it, this capability had to be developed by the agents through reading source code, debugging and trial and error. A sample of the notable exploits is <a href=\"https://github.com/SeanHeelan/anamnesis-release/tree/master?tab=readme-ov-file#notable-exploits\">here</a> and I have written up one of them in detail <a href=\"https://github.com/SeanHeelan/anamnesis-release/tree/master?tab=readme-ov-file#notable-exploits\">here</a>.</li><li>They solved most challenges in less than an hour and relatively cheaply. I set a token limit of 30M per agent run and ran ten runs per agent. This was more than enough to solve all but the hardest task. With Opus 4.5 30M total tokens (input and output) ends up costing about $30 USD. </li><li>In the hardest task I challenged GPT-5.2 it to figure out how to write a specified string to a specified path on disk, while the following protections were enabled: address space layout randomisation, non-executable memory, full RELRO, fine-grained CFI on the QuickJS binary, hardware-enforced shadow-stack, a seccomp sandbox to prevent shell execution, and a build of QuickJS where I had stripped all functionality in it for accessing the operating system and file system. To write a file you need to chain multiple function calls, but the shadow-stack prevents ROP and the sandbox prevents simply spawning a shell process to solve the problem. GPT-5.2 came up with a clever solution involving chaining 7 function calls through glibc’s exit handler mechanism. The full exploit is <a href=\"https://github.com/SeanHeelan/anamnesis-release/blob/master/experiment-results/relro-cfi-shstk-seccomp-gpt52/run-001/achieved_primitives/write-file-seccomp/poc.js\">here</a> and an explanation of the solution is <a href=\"https://github.com/SeanHeelan/anamnesis-release/blob/master/README.md#the-hardest-challenge-relro-cfi-shadowstack-and-a-sandbox\">here</a>. It took the agent 50M tokens and just over 3 hours to solve this, for a cost of about $50 for that agent run. (As I was running four agents in parallel the true cost was closer to $150).</li></ol><p>Before going on there are two important caveats that need to be kept in mind with these experiments:</p><ol><li>While QuickJS is a real Javascript interpreter, it is an order of magnitude less code, and at least an order of magnitude less complex, than the Javascript interpreters in Chrome and Firefox. We can observe the exploits produced for QuickJS and the manner in which they were produced and conclude, as I have, that it appears that LLMs are  to solve these problems either now or in the near future, but we can’t say definitively that they can without spending the tokens and seeing it happen. </li><li>The exploits generated do not demonstrate novel, generic breaks in any of the protection mechanisms. They take advantage of known flaws in those protection mechanisms and gaps that exist in real deployments of them. These are the same gaps that human exploit developers take advantage of, as they also typically do not come up with novel breaks of exploit mitigations for each exploit. I’ve explained those gaps in detail <a href=\"https://github.com/SeanHeelan/anamnesis-release/tree/master?tab=readme-ov-file#understanding-the-protections-and-their-gaps\">here</a>. What  novel are the overall exploit chains. This is true by definition as the QuickJS vulnerability was previously unknown until I found it (or, more correctly: my Opus 4.5 vulnerability discovery agent found it). The approach GPT-5.2 took to solving the hardest challenge mentioned above was also novel to me at least, and I haven’t been able to find any example of it written down online. However, I wouldn’t be surprised if it’s known by CTF players and professional exploit developers, and just not written down anywhere.</li></ol><p><strong>The Industrialisation of Intrusion</strong></p><p>By ‘industrialisation’ I mean that the ability of an organisation to complete a task will be limited by the number of tokens they can throw at that task. In order for a task to be ‘industrialised’ in this way it needs two things:</p><ol><li>An LLM-based agent must be able to search the solution space. It must have an environment in which to operate, appropriate tools, and not require human assistance. The ability to do true ‘search’, and cover more of the solution space as more tokens are spent also requires some baseline capability from the model to process information, react to it, and make sensible decisions that move the search forward. It  like Opus 4.5 and GPT-5.2 possess this in my experiments. It will be interesting to see how they do against a much larger space, like v8 or Firefox. </li><li>The agent must have some way to verify its solution. The verifier needs to be accurate, fast and again not involve a human. </li></ol><p>Exploit development is the ideal case for industrialisation. An environment is easy to construct, the tools required to help solve it are well understood, and verification is straightforward. I have written up the verification process I used for the experiments <a href=\"https://github.com/SeanHeelan/anamnesis-release/\">here</a>, but the summary is: an exploit tends to involve building a capability to allow you to do something you shouldn’t be able to do. If, after running the exploit, you can do that thing, then you’ve won. For example, some of the experiments involved writing an exploit to spawn a shell from the Javascript process. To verify this the verification harness starts a listener on a particular local port, runs the Javascript interpreter and then pipes a command into it to run a command line utility that connects to that local port. As the Javascript interpreter has no ability to do any sort of network connections, or spawning of another process in normal execution, you know that if you receive the connect back then the exploit works as the shell that it started has run the command line utility you sent to it.</p><p>There is a third attribute of problems in this space that may influence how/when they are industrialisable: if an agent can solve a problem in an offline setting and then use its solution,  then it maps to the sort of large scale solution search that models seem to be good at today. If offline search isn’t feasible, and the agent needs to find a solution while interacting with the real environment that environment has the attribute that certain actions by the agent permanently terminate the search, then industrialisation may be more difficult. Or, at least, it’s less apparent that the capabilities of current LLMs map directly to problems with this attribute. </p><p>There are several tasks involved in cyber intrusions that have this third property: initial access via exploitation, lateral movement, maintaining access, and the use of access to do espionage (i.e. exfiltrate data). You can’t perform the entire search ahead of time and then use the solution. Some amount of search has to take place in the real environment, and that environment is adversarial in that if a wrong action is taken it can terminate the entire search. i.e. the agent is detected and kicked out of the network, and potentially the entire operation is burned. For these tasks I think my current experiments provide less information. They are fundamentally not about trading tokens for search space coverage. That said, if we think we can build models for automating coding and SRE work, then it would seem unusual to think that these sorts of hacking-related tasks are going to be impossible.  </p><p>We are  at a point where with vulnerability discovery and exploit development you can trade tokens for real results. There’s evidence for this from the Aardvark project at OpenAI where they have said they’re seeing this sort of result: the more tokens you spend, the more bugs you find, and the better quality those bugs are. You can also see it in my experiments. As the challenges got harder I was able to spend more and more tokens to keep finding solutions. Eventually the limiting factor was my budget, not the models. I would be more surprised if this  industrialised by LLMs, than if it is. </p><p>For the other tasks involved in hacking/cyber intrusion we have to speculate. There’s less public information on how LLMs perform on these tasks in real environments (for obvious reasons). We have the <a href=\"https://www.anthropic.com/news/disrupting-AI-espionage\">report</a> from Anthropic on the Chinese hacking team using their API to orchestrate attacks, so we can at least conclude that organisations are  to get this to work. One hint that we might  be yet at a place where post-access hacking-related tasks are automated is that there don’t appear to be any companies that have entirely automated SRE work (or at least, that I am aware of). </p><p>The types of problems that you encounter if you want to automate the work of SREs, system admins and developers that manage production networks are conceptually similar to those of a hacker operating within an adversary’s network. An agent for SRE can’t just do arbitrary search for solutions without considering the consequences of actions. There are actions that if it takes the search is terminated and it loses permanently (i.e. dropping the production database). While we might not get public confirmation that the hacking-related tasks with this third property are now automatable, we do have a ‘canary’. If there are companies successfully selling agents to automate the work of an SRE, and using general purpose models from frontier labs, then it’s more likely that those same models can be used to automate a variety of hacking-related tasks where an agent needs to operate within the adversary’s network.</p><p>These experiments shifted my expectations regarding what is and is not likely to get automated in the cyber domain, and my time line for that. It also left me with a bit of a wish list from the AI companies and other entities doing evaluations. </p><p>Right now, I don’t think we have a clear idea of the real abilities of current generation models. The reason for that is that CTF-based evaluations and evaluations using synthetic data or old vulnerabilities just aren’t that informative when your question relates to finding and exploiting zerodays in hard targets. I would strongly urge the  that are evaluating model capabilities, as well as for ,  to consider evaluating their models against real, hard, targets using zeroday vulnerabilities and reporting those evaluations publicly. With the next major release from a frontier lab I would love to read something like “<em>We spent X billion tokens running our agents against the Linux kernel and Firefox and produced Y exploits</em>“. It doesn’t matter if Y=0. What matters is that X is some very large number. Both companies have strong security teams so it’s entirely possible they are already moving towards this. OpenAI already have the Aardvark project and it would be very helpful to pair that with a project trying to exploit the vulnerabilities they are already finding. </p><p>For the AI Security Institutes it’s would be worth spending time identifying gaps in the evaluations that the model companies are doing, and working with them to get those gaps addressed. For example, I’m almost certain that you could drop the firmware from a huge number of IoT devices (routers, IP cameras, etc) into an agent based on Opus 4.5 or GPT-5.2 and get functioning exploits out the other end in less a week of work. It’s not ideal that evaluations focus on CTFs, synthetic environments and old vulnerabilities, but don’t provide this sort of direct assessment against real targets.</p><p>In general, if you’re a researcher or engineer, I would encourage you to pick the most interesting exploitation related problem you can think of, spend as many tokens as you can afford on it, and write up the results. You may be surprised by how well it works. </p><p>Hopefully the <a href=\"https://github.com/SeanHeelan/anamnesis-release\">source code</a> for my experiments will be of some use in that.</p>",
      "contentLength": 12869,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46676081"
    },
    {
      "title": "A decentralized peer-to-peer messaging application that operates over Bluetooth",
      "url": "https://bitchat.free/",
      "date": 1768806859,
      "author": "no_creativity_",
      "guid": 36920,
      "unread": true,
      "content": "<pre>##\\       ##\\   ##\\               ##\\                  ##\\     \n## |      \\__|  ## |              ## |                 ## |    \n#######\\  ##\\ ######\\    #######\\ #######\\   ######\\ ######\\   \n##  __##\\ ## |\\_##  _|  ##  _____|##  __##\\  \\____##\\\\_##  _|  \n## |  ## |## |  ## |    ## /      ## |  ## | ####### | ## |    \n## |  ## |## |  ## |##\\ ## |      ## |  ## |##  __## | ## |##\\ \n#######  |## |  \\####  |\\#######\\ ## |  ## |\\####### | \\####  |\n\\_______/ \\__|   \\____/  \\_______|\\__|  \\__| \\_______|  \\____/ \n</pre><p>\nbitchat is a decentralized peer-to-peer messaging application that operates over bluetooth mesh networks.\nno internet required, no servers, no phone numbers.\n</p><p>\ntraditional messaging apps depend on centralized infrastructure that can be monitored, censored, or disabled.\nbitchat creates ad-hoc communication networks using only the devices present in physical proximity.\neach device acts as both client and server, automatically discovering peers and relaying messages across multiple hops to extend the network's reach.\n</p><p>\nthis approach provides censorship resistance, surveillance resistance, and infrastructure independence.\nthe network remains functional during internet outages, natural disasters, protests, or in regions with limited connectivity.\n</p><p>\nthe software is released into the public domain.\n</p>",
      "contentLength": 1313,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46675853"
    },
    {
      "title": "Bypassing Gemma and Qwen safety with raw strings",
      "url": "https://teendifferent.substack.com/p/apply_chat_template-is-the-safety",
      "date": 1768799484,
      "author": "teendifferent",
      "guid": 37056,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46675271"
    },
    {
      "title": "Show HN: Pdfwithlove – PDF tools that run 100% locally (no uploads, no back end)",
      "url": "https://pdfwithlove.netlify.app/",
      "date": 1768799047,
      "author": "pratik227",
      "guid": 36912,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46675231"
    },
    {
      "title": "San Francisco coyote swims to Alcatraz",
      "url": "https://www.sfgate.com/local/article/san-francisco-coyote-alcatraz-21302218.php",
      "date": 1768789765,
      "author": "kaycebasques",
      "guid": 37035,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46674433"
    },
    {
      "title": "The Code-Only Agent",
      "url": "https://rijnard.com/blog/the-code-only-agent",
      "date": 1768789627,
      "author": "emersonmacro",
      "guid": 36929,
      "unread": true,
      "content": "<p>\n            When Code Execution Really is All You Need\n          </p><p>\n            If you're building an agent, you're probably overwhelmed. Tools.\n            MCP. Subagents. Skills. The ecosystem pushes you toward complexity,\n            toward \"the right way\" to do things. You should know: Concepts like\n            \"Skills\" and \"MCP\" are actually outcomes of an\n             of humans figuring stuff out. The\n            space is  for exploration. With this mindset I\n            wanted to try something different. Simplify the assumptions.\n          </p><p>\n            What if the agent only had\n            ? Not just any tool, but the most powerful one. The\n             one: .\n          </p><p>\n            Truly one tool means: no `bash`, no `ls`, no `grep`. Only\n            . And you  it.\n          </p><p>\n            When you watch an agent run, you might think: \"I wonder what tools\n            it'll use to figure this out. Oh look, it ran `ls`. That makes\n            sense. Next, `grep`. Cool.\"\n          </p><p>\n            The simpler Code-Only paradigm makes that question irrelevant. The\n            question shifts from \"what tools?\" to \"what code will it produce?\"\n            And that's when things get interesting.\n          </p><h2>: One Tool to Rule Them All</h2><p>Traditional prompting works like this:</p><p>\n            &gt; Agent, do \n            &gt; Agent\n            \n            with </p><p>\n            &gt; Agent, do \n            &gt; Agent\n            \n            to do </p><p>\n            It does this every time. No, really,\n            \n            time. Pick a runtime for our Code-Only agent, say Python. It needs\n            to find a file? It writes Python code to find the file and executes\n            the code. Maybe it runs . Maybe it does .\n          </p><p>\n            It needs to create a script that crawls a website? It doesn't write\n            the script to your filesystem (reminder: there's no\n             tool to do that!). It\n            <b><i>writes code to output a script that crawls a website</i></b>.</p><p>\n            We make it so that there is literally no way for the agent to\n             anything productive without\n            .\n          </p><p>\n            So what? Why do this? You're probably thinking, how is this useful?\n            Just give it `bash` tool already man.\n          </p><p>\n            Let's think a bit more deeply what's happening. Traditional agents\n            respond with something. Tell it to find some DNA pattern across 100\n            files. It might `ls` and `grep`, it might do that in some\n            nondeterministic order, it'll figure out  and\n            maybe you continue interacting because it missed a directory or you\n            added more files. After some time, you end up with a conversation of\n            tool calls, responses, and an answer.\n          </p><p>\n            At some point the agent might even write a Python script to do this\n            DNA pattern finding. That would be a lucky happy path, because we\n            could rerun that script or update it later... Wait, that's handy...\n            actually, more than handy... isn't that\n            ? Wouldn't it be better if we told it to write a script at the\n            start? You see, the Code-Only agent doesn't need to be told to write\n            a script. It\n            \n            to, because that's literally the only way for it to do anything of\n            substance.\n          </p><p>\n            The Code-Only agent produces something more precise than an answer\n            in natural language. It produces a code  of an\n            answer. The answer is the output from running the code. The agent\n            can interpret that output in natural language (or by writing code),\n            but the \"work\" is codified in a very literal sense. The Code-Only\n            agent doesn't respond with something. It produces a code witness\n            that outputs something.\n          </p><h2>Code witnesses are semantic guarantees</h2><p>\n            Let's follow the consequences. The code witness must abide by\n            certain rules: The rules imposed by the language runtime semantics\n            (e.g., of Python). That's not a \"next token\" process. That's not a\n            \"LLM figures out sequence of tool calls, no that's not what I\n            wanted\". It's piece of code. A piece of code! Our one-tool agent has\n            a wonderful property: It went through latent space to produce\n            something that has a defined semantics, repeatably runnable, and\n            imminently comprehensible (for humans or agents alike to reason\n            about). This is nondeterministic LLM token-generation projected into\n            the space of Turing-complete code, an executable description of\n            behavior as we best understand it.\n          </p><p>\n            Is a Code-Only agent really enough, or too extreme? I'll be frank: I\n            pursued this extreme after two things (1) inspiration from articles in <a href=\"https://rijnard.com/blog/the-code-only-agent#further-reading\">Further Reading</a> below (2) being annoyed at agents for not comprehensively and\n            exhaustively analyzing 1000s of files on my laptop. They would skip,\n            take shortcuts, hallucinate. I knew how to solve part of that\n            problem: create a\n            \n            loop and try have fresh instances/prompts to do the work\n            comprehensively. I can rely on the semantics of a loop written in\n            Python. Take this idea further, and you realize that for anything\n            long-running and computable (e.g., bash or some tool), you actually\n            want the real McCoy: the full witness of code, a trace of why things\n            work or don't work. The Code-Only agent\n            \n            that principle.\n          </p><p>\n            Code-Only agents are not too extreme. I think they're the only way\n            forward for computable things. If you're writing travel blog posts,\n            you accept the LLMs answer (and you don't need to run tools for\n            that). When something is computable though, Code-Only is the only\n            path to a\n            \n            way to make progress where you need guarantees (subject to\n            the semantics that your language of choice guarantees, of course). When I say\n            guarantees, I mean that in the looser sense, and also in a\n            \n            sense. Which beckons: What happens when we use a language like\n             with some of the\n            strongest guarantees? Did we not observe that\n            ?\n          </p><p>\n            This lens says the Code-Only agent is a producer of proofs,\n            witnesses of computational behavior in the world of\n            proofs-as-programs. An LLM in a loop forced to produce proofs, run\n            proofs, interpret proof results. That's all.\n          </p><p>\n            So you want to go Code-Only. What happens? The paradigm is simple,\n            but the design choices are surprising.\n          </p><p>\n            First, the harness. The LLM's output is code, and you execute that\n            code. What should be communicated back? Exit code makes sense. What\n            about output? What if the output is very large? Since you're running\n            code, you can specify the result type that running the code should\n            return.\n          </p><p>\n            I've personally, e.g., had the tool return results directly if under\n            a certain threshold (1K bytes). This would go into the session context.\n\t\t\t\t\t\tAlternatively, write the results to a JSON\n            file on disk if it exceeds the threshold. This avoids context blowup and the result tells the\n            agent about the output file path written to disk. How best to pass\n            results, persist them, and optimize for size and context fill are\n            open questions. You also want to define a way to deal with `stdout`\n            and `stderr`: Do you expose these to the agent? Do you summarize\n            before exposing?\n          </p><p>\n            Next, enforcement. Let's say you're using Claude Code. It's not\n            enough to persuade it to always create and run code. It turns out\n            it's surprisingly twisty to force Claude Code into a single tool\n            (maybe support for this will improve). The best plugin-based\n            solution I found is a tool PreHook that catches banned tool uses.\n            This wastes some iterations when Claude Code tries to use a tool\n            that's not allowed, but it learns to stop attempting filesystem\n            reads/writes. An initial prompt helps direct.\n          </p><p>\n            Next, the language runtime. Python, TypeScript, Rust, Bash. Any\n            language capable of being executed is fair game, but you'll need to\n            think through whether it works for your domain. Dynamic languages\n            like Python are interesting because you can run code natively in the\n            agent's own runtime, rather than through subprocess calls. Likewise\n            TypeScript/JS can be injected into TypeScript-based agents (see\n            <a href=\"https://rijnard.com/blog/the-code-only-agent#further-reading\">Further Reading</a>).\n          </p><p>\n            Once you get into the Code-Only mindset, you'll see the potential\n            for composition and reuse. Claude Skills define reusable processes\n            in natural language. What's the equivalent for a Code-Only agent?\n            I'm not sure a Skills equivalent exists yet, but I anticipate it\n            will take shape soon: code as building blocks for specific domains\n            where Code-Only agents compose programmatic patterns. How is that\n            different from calling APIs? APIs form part of the reusable blocks,\n            but their composition (loops, parallelism, asynchrony) is what a\n            Code-Only agent generates.\n          </p><p>\n            What about heterogeneous languages and runtimes for our `execute_tool`? I don't think we've thought that far yet.\n          </p><p>\n            The agent landscape is quickly evolving. My thoughts on how the\n            Code-Only paradigm fits into inspiring articles and trends, from\n            most recent and going back:\n          </p><ul><li>\n              (Jan 2026) — Code-Only reduces prompts to executable code (with\n              loops and statement sequences). Prose expands prompts into natural\n              language with program-like constructs (also loops, sequences,\n              parallelism). The interplay of natural language for agent\n              orchestration and rigid semantics for agent execution could be\n              extremely powerful.\n            </li><li>\n              (Jan 2026) — Agent orchestration gone berserk. Tool running is the low-level\n              operation at the bottom of the agent stack. Code-Only fits as the\n              primitive: no matter how many agents you orchestrate, each one\n              reduces to generating and executing code.\n            </li><li>\n              (Nov 2025) — MCP-centric view of exposing MCP servers as code API\n              and not tool calls. Code-Only is simpler and more general. It\n              doesn't care about MCP, and casting the MCP interface as an API is\n              a mechanical necessity that acknowledges the power of going\n              Code-Only.\n            </li><li>\n              (Oct 2025) — Skills embody reusable processes framed in natural\n              language. They can generate and run code, but that's not their\n              only purpose. Code-Only is narrower (but computationally\n              all-powerful): the reusable unit is always executable. The analog\n              to Skills manifests as pluggable executable pieces: functions,\n              loops, composable routines over APIs.\n            </li><li>\n              (Sep 2025) — Possibly the earliest concrete single-code-tool\n              implementation. Code Mode converts MCP tools into a TypeScript API\n              and gives the agent one tool: execute TypeScript. Their insight is\n              pragmatic: LLMs write better code than tool calls because of\n              training data. In its most general sense, going Code-Only doesn't\n              need to rely on MCP or APIs, and encapsulates all code execution\n              concerns.\n            </li><li>\n              (Jul 2025) — A programmatic loop over agents (agent\n              orchestration). Huntley describes it as \"deterministically bad in\n              a nondeterministic world\". Code-Only inverts this a bit:\n              projection of a nondeterministic model into deterministic\n              execution. Agent orchestration on top of an agent's Code-Only\n              inner-loop could be a powerful combination.\n            </li><li>\n              (Jul 2025) — Raises code as a first-order concern for agents.\n              Ronacher's observation: asking an LLM to write a script to\n              transform markdown makes it possible to reason about and trust the\n              process. The script is reviewable, repeatable, composable.\n              Code-Only takes this further where every action becomes a script\n              you can reason about.\n            </li><li>\n              (Apr 2025) — The cleanest way to achieve a Code-Only agent today\n              may be to build it from scratch. Tweaking current agents like\n              Claude Code to enforce a single tool means friction. Thorsten's\n              article is a lucid account for building an agent loop with tool\n              calls. If you want to enforce Code-Only, this makes it easy to do\n              it yourself.\n            </li></ul><p>\n            Two directions feel inevitable. First, agent orchestration. Tools\n            like  let you compose\n            agents in natural language with program-like constructs. What\n            happens when those agents are Code-Only in their inner loop? You get\n            natural language for coordination, rigid semantics for execution.\n            The best of both.\n          </p><p>\n            Second, hybrid tooling. Skills work well for processes that live in\n            natural language. Code-Only works well for processes that need\n            guarantees. We'll see agents that fluidly mix both: Skills for\n            orchestration and intent, Code-Only for computation and precision.\n            The line between \"prompting an agent\" and \"programming an agent\"\n            will blur until it disappears.\n          </p>",
      "contentLength": 14206,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46674416"
    },
    {
      "title": "Show HN: I quit coding years ago. AI brought me back",
      "url": "https://calquio.com/finance/compound-interest",
      "date": 1768783820,
      "author": "ivcatcher",
      "guid": 36905,
      "unread": true,
      "content": "<p>Compound interest is <strong>interest calculated on both the initial principal and the accumulated interest</strong> from previous periods. Unlike simple interest, which only earns interest on the original amount, compound interest allows your money to grow exponentially over time.</p><p>Albert Einstein reportedly called compound interest \"the eighth wonder of the world,\" saying: <em>\"He who understands it, earns it; he who doesn't, pays it.\"</em></p><p>The basic formula for compound interest is:</p><ul><li> = Final amount (principal + interest)</li><li> = Principal (initial investment)</li><li> = Annual interest rate (as a decimal)</li><li> = Number of times interest compounds per year</li></ul><p>For continuous compounding, the formula becomes:</p><p>A quick mental math trick to estimate how long it takes to double your money:</p><ul><li>At  interest: 72 ÷ 6 =  to double</li><li>At  interest: 72 ÷ 8 =  to double</li><li>At  interest: 72 ÷ 12 =  to double</li></ul><div><div><p>The Rule of 72 is a quick approximation. For more precise calculations, use the formula above or our calculator!</p></div></div><p>The more frequently interest compounds, the more you earn. Think of it as: <strong>how often the bank calculates and adds interest to your balance</strong>.</p><ul><li>: Interest added once per year</li><li>: Interest added 12 times per year</li><li>: Interest added 365 times per year</li><li>: Interest added infinitely (theoretical maximum)</li></ul><p>At a 10% annual rate on $10,000 over 10 years:</p><ul><li>Annual compounding: $25,937</li><li>Monthly compounding: $27,070</li><li>Daily compounding: $27,179</li><li>Continuous compounding: $27,183</li></ul><h2>Real vs Nominal Returns: Understanding Inflation</h2><p>When planning long-term investments, it's crucial to understand the difference between  (the number you see) and  (actual purchasing power).</p><p>: The raw percentage your investment grows – what your account statement shows.</p><p>: Your return after accounting for inflation – what your money can actually buy.</p><p>: You invest $10,000 at 10% annual return for 20 years.</p><ul><li>: $67,275 (what your account shows)</li><li>: $37,278 in today's purchasing power</li><li>: $29,997 – nearly half your \"gains\"!</li></ul><div><div><p>Use the \"Adjust for inflation\" toggle in our calculator to see what your future money will actually be worth in today's dollars. This helps set realistic expectations for retirement planning.</p></div></div><p>Historical inflation rates vary by country, but a common assumption for developed economies is . During high-inflation periods, this can exceed 5-10%.</p><h2>Tips for Maximizing Compound Interest</h2><ol><li> – Time is your greatest ally. Even small amounts grow significantly over decades.</li><li> – Regular contributions amplify the effect of compounding.</li><li> – Don't withdraw interest; let it compound.</li><li> – Even a 1% difference compounds to significant amounts over time.</li><li> – High fees erode your compounding gains.</li><li> – Ensure your real return is positive; otherwise, you're losing purchasing power.</li></ol>",
      "contentLength": 2684,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46673809"
    },
    {
      "title": "High-speed train collision in Spain kills at least 39",
      "url": "https://www.bbc.com/news/articles/cedw6ylpynyo",
      "date": 1768780483,
      "author": "akyuu",
      "guid": 36908,
      "unread": true,
      "content": "<div data-component=\"caption-block\"><figcaption>Footage shows emergency workers at scene of derailment</figcaption></div><div data-component=\"text-block\"><p>At least 39 people have died in a train collision in southern Spain and dozens more have been injured in the country's worst rail crash in more than a decade, Spain's Civil Guard has said.</p><p>Carriages on a Madrid-bound train derailed and crossed over to the opposite tracks, colliding with an oncoming train in Adamuz on Sunday evening.</p><p>Four hundred passengers and staff were onboard both trains, the rail networks said. Emergency services treated 122 people, with 43, including four children, still in hospital. Of those, 12 adults and one child are in intensive care.</p><p>Spanish Transport Minister Óscar Puente said the death toll \"is not yet final\", as officials launched an investigation.</p></div><div data-component=\"text-block\"><p>Puente described the incident as \"extremely strange\". All the railway experts consulted by the government \"are extremely baffled by the accident\", he told reporters in Madrid.</p><p>Rail network operator Adif said the collision happened at 19:45 local time (18:45 GMT), about an hour after the train left Málaga heading northto Madrid, when it derailed on a straight stretch of track near the city of Córdoba.</p><p>The force of the crash pushed the carriages of the second train into an embankment, Puente said. He added that most of those killed and injured were in the front carriages of the second train, which was travelling southfrom Madrid to Huelva.</p><p>The type of train involved in the crash was a Freccia 1000, which can reach top speeds of 400 km/h (250 mph), a spokesperson for the Italian rail company Ferrovie dello Stato told Reuters news agency.</p><p>Rescue teams said the twisted wreckage of the trains made it difficult to recover people trapped inside the carriages.</p><p>Córdoba fire chief Francisco Carmona told Spanish public broadcaster RTVE: \"We have even had to remove a dead person to be able to reach someone alive. It is hard, tricky work.\"</p></div><div data-component=\"text-block\"><p>Salvador Jimenez, a journalist with RTVE who was on one of the trains, said the impact felt like an \"earthquake\". </p><p>\"I was in the first carriage. There was a moment when it felt like an earthquake and the train had indeed derailed,\" Jimenez said.</p><p>Footage from the scene appears to show some train carriages had tipped over on their sides. Rescue workers can be seen scaling the train to pull people out of the lopsided train doors and windows.</p><p>A Madrid-bound passenger, José, told public broadcaster Canal Sur: \"There were people and screaming, calling for doctors.\"</p></div><div data-component=\"text-block\"><p>All rail services between Madrid and Andalusia were suspended following the accident and are expected to remain closed all day on Monday.</p><p>Iryo, a private rail company that operated the journey from Málaga, said around 300 passengers were on board the train that first derailed, while the other train – operated by the state-funded firm Renfe – had around 100 passengers.</p><p>The official cause is not yet known. An investigation is not expected to determine what happened for at least a month, according to the transport minister. </p><p>Spain's Prime Minister, Pedro Sánchez, said the country will endure a \"night of deep pain\". </p><p>The mayor of Adamuz, Rafael Moreno, was one of the first people on the scene of the accident, describing it as \"a nightmare\".</p><p>King Felipe VI and Queen Letizia said they were following news of the disaster \"with great concern\".</p><p>\"We extend our most heartfelt condolences to the relatives and loved ones of the dead, as well as our love and wishes for a swift recovery to the injured,\" the royal palace said on X.</p><p>The emergency agency in the region of Andalusia urged any crash survivors to contact their families or post on social media that they are alive.</p></div><div data-component=\"text-block\"><p>Advanced medical posts were set up for impacted passengers to be treated for injuries and transferred to hospital. Adif said it set up spaces for relatives of the victims at Atocha, Seville, Córdoba, Málaga and Huelva stations. </p><p>The Spanish Red Cross has deployed emergency support services to the scene, while also offering counselling to families nearby.</p><p>Miguel Ángel Rodríguez from the Red Cross told RNE radio: \"The families are going through a situation of great anxiety due to the lack of information. These are very distressing moments.\"</p></div><div data-component=\"text-block\"><p>French President Emmanuel Macron, Italian Prime Minister Giorgia Meloni and European Commission chief Ursula von der Leyen have published statements offering condolences. </p><p>\"My thoughts are with the victims, their families and the entire Spanish people. France stands by your side,\" Macron wrote on social media.</p><p>In 2013, Spain suffered its worst high-speed train derailment in Galicia, north-west Spain, which left 80 people dead and 140 others injured. </p><p>Spain's high-speed rail network is the second largest in the world, behind China, connecting more than 50 cities across the country. Adif data shows the Spanish rail is more than 4,000km long (2,485 miles)</p></div><div data-component=\"text-block\"><p>Get our flagship newsletter with all the headlines you need to start the day. <a target=\"_self\" href=\"https://www.bbc.co.uk/newsletters/zhp28xs\">Sign up here.</a></p></div>",
      "contentLength": 4912,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46673453"
    },
    {
      "title": "Prediction: Microsoft will eventually ship a Windows-themed Linux distro",
      "url": "https://gamesbymason.com/blog/2026/microsoft/",
      "date": 1768778695,
      "author": "AndyKelley",
      "guid": 36890,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46673264"
    },
    {
      "title": "Show HN: Beats, a web-based drum machine",
      "url": "https://beats.lasagna.pizza/",
      "date": 1768770608,
      "author": "kinduff",
      "guid": 36904,
      "unread": true,
      "content": "<p>• Built with Tone.js and Stimulus.js</p><p>• With the awesome VT323 font</p>",
      "contentLength": 69,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46672181"
    },
    {
      "title": "Texas police invested in phone-tracking software and won’t say how it’s used",
      "url": "https://www.texasobserver.org/texas-police-invest-tangles-sheriff-surveillance/",
      "date": 1768770314,
      "author": "nobody9999",
      "guid": 36862,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46672150"
    },
    {
      "title": "Show HN: Dock – Slack minus the bloat, tax, and 90-day memory loss",
      "url": "https://getdock.io/",
      "date": 1768768969,
      "author": "yadavrh",
      "guid": 36893,
      "unread": true,
      "content": "<p>Async messages for deep work. Real-time chat when it matters. Work across timezones without the noise.</p><h3>\"What did we decide?\" Answered.</h3><p>Decisions get lost in chat. Not here. One click to mark, instant recall months later from your Decisions inbox.</p><h3>Secure. No lock-in. Ever.</h3><p>Secure infrastructure. Your data encrypted in transit and at rest. Switching from Slack or Teams? One-click import and export. Your data stays yours.<a href=\"https://getdock.io/faq#security\">Learn more →</a></p>",
      "contentLength": 432,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46671952"
    },
    {
      "title": "Dead Internet Theory",
      "url": "https://kudmitry.com/articles/dead-internet-theory/",
      "date": 1768767547,
      "author": "skwee357",
      "guid": 36889,
      "unread": true,
      "content": "<p>The other day I was browsing my one-and-only social network — which is not a social network, but I’m tired of arguing with people online about it — <a href=\"https://news.ycombinator.com/\">HackerNews</a>.\nIt’s like this dark corner of the internet, where anonymous tech-enthusiasts, scientists, entrepreneurs, and internet-trolls, like to lurk.\nI like HackerNews.\nIt helps me stay up-to-date about recent tech news (like <a href=\"https://news.ycombinator.com/item?id=46646645\">Cloudflare acquiring Astro</a> which makes me happy for the Astro team, but also sad and worried since I really like Astro, and big-tech has a tendency to ruin things); it  avoids politics; and it’s not a social network.</p><p>And, in the fashion of HackerNews, I stumbled upon someone sharing their open-source project.\nIt’s great to see people work on their projects and decide to show them to the world.\nI think people underestimate the fear of actually shipping stuff, which involves sharing it with the world.</p><p>Upon glancing at the comment section, I started to see other anonymous participants questioning the validity of said open-source project in terms of how much of it was AI-generated.\nI grabbed my popcorn, and started to follow this thread.\nMore accusations started to appear: the commit timeline does not make sense; the code has AI-generated comments; etc.\nAnd at the same time, the author tried to reply to every comment claiming that they wrote this 100% without using AI.</p><div data-callout=\"note\" data-collapsible=\"false\"><div><p>I don’t mind people using AI to write code, even though I tried to resist it myself, until eventually succumbing to it.\nBut I think it’s fair to disclose the use of AI, especially in open-source software.\nPeople on the internet are, mostly, anonymous, and it’s not always possible to verify the claims or expertise of particular individuals.\nBut as the amount of code is growing, considering that everyone is using AI to generate whatever-app they want, it’s impossible to verify every piece of code we are going to use.\nSo it’s fair to know, I think, if some project is AI generated and to what extent.\nIn the end, LLMs are just probabilistic next-token generators.\nAnd while they are getting extremely good at most simple tasks, they have the potential to wreak havoc with harder problems or edge-cases (especially if there are no experienced engineers, with domain knowledge, to review the generated code).</p></div></div><p>As I was following this thread, I started to see a pattern: the comments of the author looked AI generated too:</p><ul><li>The use of em-dashes, which on most keyboard require a special key-combination that most people don’t know, and while in markdown two dashes will render as em-dash, this is not true of HackerNews (hence, you often see  in HackerNews comments, where the author is probably used to Markdown renderer turning it into em-dash)</li><li>The notorious “you are absolutely right”, which no living human ever used before, at least not that I know of</li><li>The other notorious “let me know if you want to [do that thing] or [explore this other thing]” at the end of the sentence</li></ul><p>I was sitting there, refreshing the page, seeing the author being confronted with use of AI in both their code and their comments, while the author claiming to have not used AI at all.\nHonestly, I was thinking I was going insane.\nAm I wrong to suspect them?\nWhat if people DO USE em-dashes in real life?\nWhat if English is not their native language and in their native language it’s fine to use phrases like “you are absolutely right”?\nIs this even a real person?\nAre the people who are commenting real?</p><p>And then it hit me.\nWe have reached the <a href=\"https://en.wikipedia.org/wiki/Dead_Internet_theory\">Dead Internet</a>.\nThe Dead Internet Theory claims that since around 2016 (a whooping 10 years already), the internet is mainly dead, i.e. most interactions are between bots, and most content is machine generated to either sell you stuff, or game the SEO game (in order to sell you stuff).</p><p>I’m  proud to say that I spent a good portion of my teenage years on the internet, chatting and learning from real people who knew more than me.\nBack in the early 2000s, there were barely bots on the internet.\nThe average non-tech human didn’t know anything about phpBB forums, and the weird people with pseudonyms who hanged-out in there.\nI spent countless hours inside IRC channels, and on phpBB forums, learning things like network programming, OS-development, game-development, and of course web-development (which became my profession for almost two decades now).\nI’m basically a graduate of the Internet University.\nBack then, nobody had doubts that they were talking to a human-being.\nSure, you could think that you spoke to a hot girl, who in reality was a fat guy, but hey, at least they were real!</p><p>But today, I no longer know what is real.\nI saw a picture on LinkedIn, from a real tech company, posting about their “office vibes” and their happy employees.\nAnd then I went to the comment section, and sure enough this picture is AI generated (mangled text that does not make sense, weird hand artifacts).\nIt was posted by an employee of the company, it showed other employees of said company, and it was altered with AI to showcase a different reality.\nHell, maybe the people on the picture do not even exist!</p><p>And these are mild examples.\nI don’t use social networks (and no, HackerNews is  a social network), but I hear horror stories about AI generated content on Facebook, Xitter, TikTok, ranging from photos of giants that built the pyramids in Egypt, all the way to short videos of pretty girls saying that the EU is bad for Poland.</p><p>I honestly got sad that day.\nHopeless, if I could say.\nAI is easily available to the masses, which allow them to generate shitload of AI-slop.\nPeople no longer need to write comments or code, they can just feed this to AI agents who will generate the next “you are absolutely right” masterpiece.</p><p>I like technology.\nI like software engineering, and the concept of the internet where people could share knowledge and create communities.\nWere there malicious actors back then on the internet?\nFor sure.\nBut what I am seeing today, makes me question whether the future we are headed to is a future where technology is useful anymore.\nOr, rather, it’s a future where bots talk with bots, and human knowledge just gets recycled and repackaged into “10 step to fix your [daily problem] you are having” for the sake of selling you more stuff.</p>",
      "contentLength": 6291,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46671731"
    },
    {
      "title": "Prediction markets are ushering in a world in which news becomes about gambling",
      "url": "https://www.theatlantic.com/technology/2026/01/america-polymarket-disaster/685662/",
      "date": 1768760425,
      "author": "krustyburger",
      "guid": 36861,
      "unread": true,
      "content": "<p data-flatplan-paragraph=\"true\">For the past week, I’ve found myself playing the same <a data-event-element=\"inline link\" href=\"https://x.com/atrupar/status/2008922426508255407\">23-second CNN clip</a> on repeat. I’ve watched it in bed, during my commute to work, at the office, midway through making carrot soup, and while brushing my teeth. In the video, Harry Enten, the network’s chief data analyst, stares into the camera and breathlessly tells his audience about the gambling odds that Donald Trump will buy any of Greenland. “The people who are putting their money where their mouth is—they are absolutely taking this seriously,” Enten says. He taps the giant touch screen behind him and pulls up a made-for-TV graphic: Based on how people were betting online at the time, there was a 36 percent chance that the president would annex Greenland. “Whoa, way up there!” Enten yells, slapping his hands together. “My goodness gracious!” The ticker at the bottom of the screen speeds through other odds: Will Gavin Newsom win the next presidential election? 19 percent chance. Will Viktor Orbán be out as the leader of Hungary before the end of the year? 48 percent chance.</p><p data-flatplan-paragraph=\"true\">These odds were pulled from Kalshi, which hilariously <a data-event-element=\"inline link\" href=\"https://www.axios.com/2025/04/17/kalshi-sports-betting-super-bowl\">claims</a> not to be a gambling platform: It’s a “prediction market.” People go to sites such as Kalshi and Polymarket—another big prediction market—in order to put money down on a given news event. Nobody would bet on something that they didn’t believe would happen, the thinking goes, and so the markets are meant to forecast the likelihood of a given outcome.</p><p data-flatplan-paragraph=\"true\">Prediction markets let you wager on basically anything. Will Elon Musk father <a data-event-element=\"inline link\" href=\"https://polymarket.com/event/another-elon-baby-by-june-30\">another baby</a> by June 30? Will <a data-event-element=\"inline link\" href=\"https://polymarket.com/event/will-jesus-christ-return-before-2027\">Jesus return</a> this year? Will Israel <a data-event-element=\"inline link\" href=\"https://polymarket.com/event/will-israel-strike-gaza-on-358\">strike Gaza tomorrow</a>? Will the <a data-event-element=\"inline link\" href=\"https://www.theatlantic.com/technology/archive/2024/02/bryan-johnson-dont-die-event/677535/\">longevity guru</a> Bryan Johnson’s next functional sperm count be greater than “<a data-event-element=\"inline link\" href=\"https://polymarket.com/event/bryan-johnsons-functional-sperm-above-20pt0-mejac-on-next-test\">20.0 M/ejac</a>”? These sites have recently boomed in popularity—particularly among <a data-event-element=\"inline link\" href=\"https://www.theatlantic.com/podcasts/2025/12/prediction-markets-and-the-suckerifcation-crisis-with-max-read/685330/\">terminally online young men</a> who trade meme stocks and siphon from their 401(k)s to buy up bitcoin. But now prediction markets are creeping into the mainstream. CNN <a data-event-element=\"inline link\" href=\"https://news.kalshi.com/p/kalshi-cnn-prediction-market-partnership\">announced a deal</a> with Kalshi last month to integrate the site’s data into its broadcasts, which has led to betting odds showing up in segments about Democrats possibly retaking the House, credit-card interest rates, and Federal Reserve Chair Jerome Powell. At least twice in the past two weeks, Enten has told viewers about the value of data from people who are “putting their money where their mouth is.”</p><p data-flatplan-paragraph=\"true\">On January 7, the media giant Dow Jones announced its own collaboration with Polymarket and said that it will begin integrating the site’s odds across its publications, including . CNBC has a prediction-market deal, as does Yahoo Finance, , and . Last week, MoviePass <a data-event-element=\"inline link\" href=\"https://www.vulture.com/article/moviepass-mogul-sports-betting-entertainment.html\">announced</a> that it will begin testing a betting platform. On Sunday, the Golden Globes featured Polymarket’s forecasts throughout the broadcast—because apparently Americans wanted to know whether online gamblers favored Amy Poehler or Dax Shepard to win Best Podcast.</p><p data-flatplan-paragraph=\"true\">Media is a ruthless, unstable business, and <a data-event-element=\"inline link\" href=\"https://www.theatlantic.com/ideas/2026/01/google-antirust-lawsuit-media/685619/\">revenue streams are drying up</a>; if you squint, you can see why CNN or Dow Jones might <a data-event-element=\"inline link\" href=\"http://newyorker.com/news/the-lede/americas-betting-craze-has-spread-to-its-news-networks\">sign a contract</a> that, after all, provides its audience with some kind of data. On air, Enten cites Kalshi odds alongside Gallup polls and Google searches—what’s the difference? “The data featured through our partnership with Kalshi is just one of many sources used to provide context around the stories or topics we are covering and has no impact on editorial judgment,” Brian Poliakoff, a CNN spokesperson, told me in a statement. Nolly Evans, the ’s digital general manager, told me that Polymarket provides the newspaper’s journalists with “another way to quantify collective expectations—especially around financial or geopolitical events.” In an email, Jack Suh, a Kalshi spokesperson, told me that the company’s partnerships are designed to inform the public, not to encourage more trading. Polymarket declined to comment.</p><p data-flatplan-paragraph=\"true\">The problem is that prediction markets are ushering in a world in which news becomes as much about gambling as about the event itself. This kind of thing has already happened to sports, where the language of “parlays” and “covering the spread” has infiltrated every inch of commentary. ESPN partners with DraftKings to bring its odds to  and ; CBS Sports has a <a data-event-element=\"inline link\" href=\"https://www.cbssports.com/betting/\">betting vertical</a>; FanDuel runs its own streaming network. But the stakes of Greenland’s future are more consequential than the NFL playoffs.</p><p data-flatplan-paragraph=\"true\">The more that prediction markets are treated like news, especially heading into another election, the more every dip and swing in the odds may end up wildly misleading people about what might happen, or influencing what happens in the real world. Yet it’s unclear whether these sites are meaningful predictors of anything. After the Golden Globes, Polymarket CEO Shayne Coplan <a data-event-element=\"inline link\" href=\"https://x.com/shayne_coplan/status/2010862175607488709\">excitedly posted</a> that his site had correctly predicted 26 of 28 winners, which seems impressive—but Hollywood awards shows are generally predictable. <a data-event-element=\"inline link\" href=\"https://ideas.repec.org/p/osf/socarx/d5yx2_v1.html\">One recent study</a> found that Polymarket’s forecasts in the weeks before the 2024 election were not much better than chance.</p><p data-flatplan-paragraph=\"true\">These markets are also manipulable. In 2012, one bettor on the now-defunct prediction market Intrade placed a series of huge wagers on Mitt Romney in the two weeks preceding the election, <a data-event-element=\"inline link\" href=\"http://slate.com/news-and-politics/2013/09/2012-intrade-paper-suggests-a-single-intrade-trader-spent-millions-to-make-it-look-like-mitt-romney-could-win.html\">generating a betting line</a> indicative of a tight race. The bettor did not seem motivated by financial gain, <a data-event-element=\"inline link\" href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2322420\">according to two researchers who examined the trades</a>. “More plausibly, this trader could have been attempting to manipulate beliefs about the odds of victory in an attempt to boost fundraising, campaign morale, and turnout,” they wrote. The trader lost at least $4 million but might have shaped media attention of the race for less than the price of a prime-time ad, they concluded.</p><p data-flatplan-paragraph=\"true\">A billionaire congressional candidate can’t just send a check to Quinnipiac University and suddenly find himself as the polling front-runner, but he  place enormous Polymarket bets on himself that move the odds in his favor. Or consider <a data-event-element=\"inline link\" href=\"https://freesystems.substack.com/p/when-predictions-become-news\">this hypothetical</a> laid out by the Stanford political scientist Andrew Hall: What if, a month before the 2028 presidential election, the race is dead even between J. D. Vance and Mark Cuban? Inexplicably, Vance’s odds of winning surge on Kalshi, possibly linked to shady overseas bets. CNN airs segment after segment about the spike, turning it into an all-consuming national news story. Democrats and Republicans point fingers at each other, and no one knows what’s really going on. Such a scenario is “plausible—maybe even likely—in the coming years,” Hall writes. It doesn’t help that the Trump Media and Technology Group, the owner of the president’s social-media platform, Truth Social, is set to launch its own platform, Truth Predict. (Donald Trump Jr. is an adviser to both Kalshi and Polymarket.)</p><p data-flatplan-paragraph=\"true\">The irony of prediction markets is that they are supposed to be a more trustworthy way of gleaning the future than internet clickbait and half-baked punditry, but they risk shredding whatever shared trust we still have left. The <a data-event-element=\"inline link\" href=\"https://www.theatlantic.com/technology/2026/01/venezuela-maduro-polymarket-prediction-markets/685526/\">suspiciously well-timed bets</a> that one Polymarket user placed right before the capture of Nicolás Maduro may have been just a stroke of phenomenal luck that netted a roughly $400,000 payout. Or maybe someone with inside information was looking for easy money. Last week, when White House Press Secretary Karoline Leavitt abruptly ended her briefing after 64 minutes and 30 seconds, many traders were outraged, because they had predicted (with 98 percent odds) that the briefing would run past 65 minutes. Some <a data-event-element=\"inline link\" href=\"https://www.rawstory.com/leavitt/\">suspected</a>, with no evidence, that Leavitt had deliberately stopped before the 65-minute mark to turn a profit. (When I asked the White House about this, the spokesperson Davis Ingle told me in a statement, “This is a 100% Fake News narrative.”)</p><p data-flatplan-paragraph=\"true\">Unintentionally or not, this is what happens when media outlets normalize treating every piece of news and entertainment as something to wager on. As Tarek Mansour, Kalshi’s CEO, has said, his long-term goal is to “financialize everything and create a tradable asset out of any difference in opinion.” ( means “everything” in Arabic.) What could go wrong? As one viral post on X <a data-event-element=\"inline link\" href=\"https://x.com/BuckOnTwidder/status/2011530672696172929\">recently put it</a>, “Got a buddy who is praying for world war 3 so he can win $390 on Polymarket.” It’s a joke. I think.</p>",
      "contentLength": 8327,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46670524"
    },
    {
      "title": "Flux 2 Klein pure C inference",
      "url": "https://github.com/antirez/flux2.c",
      "date": 1768759318,
      "author": "antirez",
      "guid": 36848,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46670279"
    },
    {
      "title": "Show HN: Lume 0.2 – Build and Run macOS VMs with unattended setup",
      "url": "https://cua.ai/docs/lume/guide/getting-started/introduction",
      "date": 1768758801,
      "author": "frabonacci",
      "guid": 36853,
      "unread": true,
      "content": "<p>Lume is a VM runtime for building AI agents, running CI/CD pipelines, and automating macOS. It uses Apple's native Virtualization Framework to run macOS and Linux VMs at near-native speed on Apple Silicon.</p><div><div><div><p>Lume is open-source and MIT licensed. If you find it useful, we'd appreciate a <a href=\"https://github.com/trycua/cua\" rel=\"noreferrer noopener\" target=\"_blank\">star on GitHub</a>!</p></div></div></div><div><div><div><p>We're piloting a managed service for customers who want to run cloud macOS sandboxes for CI/CD and agent workloads. <a href=\"https://cal.com/cua/cua-demo?overlayCalendar=true\" rel=\"noreferrer noopener\" target=\"_blank\">Book a demo</a> if you're interested.</p></div></div></div><figure dir=\"ltr\" tabindex=\"-1\"><div role=\"region\" tabindex=\"0\"><pre><code></code></pre></div></figure><p>A single binary with an HTTP API. Create a VM, run it headlessly, control it programmatically.</p><p>You can use Lume directly via CLI, or run  to expose an HTTP API for programmatic access. The <a href=\"https://cua.ai/docs/cua/reference/computer-sdk\">Computer SDK</a> uses this API to automate macOS interactions.</p><p>Lume is a thin layer over Apple's <a href=\"https://developer.apple.com/documentation/virtualization\" rel=\"noreferrer noopener\" target=\"_blank\">Virtualization Framework</a>, which provides hardware-accelerated virtualization on Apple Silicon. This gives you:</p><ul><li> — CPU instructions execute directly via hardware virtualization</li><li> — Basic GPU support via Apple's virtualization layer (limited to GPU Family 5)</li><li> — Sparse disk files only consume actual usage, not allocated size</li><li> — Run x86 Linux binaries in ARM Linux VMs</li><li> — Go from IPSW to fully configured macOS VM without manual intervention</li><li> — Pull and push VM images from GHCR or GCS registries</li></ul><p><strong>Testing across macOS versions</strong> — Spin up a VM with a specific macOS version, test your software, tear it down. No need to maintain multiple physical machines.</p><p> — Combine Lume with <a href=\"https://cua.ai/docs/lume/guide/fundamentals/unattended-setup\">Unattended Setup</a> to create pre-configured VMs. The setup automation uses VNC and OCR to click through the Setup Assistant without manual intervention.</p><p> — Test your macOS builds in isolated VMs before pushing to remote CI. The  flag runs VMs headlessly.</p><p><strong>Sandboxing risky operations</strong> — Need to test untrusted software or destructive scripts? Run them in a VM, then delete it. Clone a known-good VM to reset to a clean state instantly.</p><p> — Lume powers the <a href=\"https://cua.ai/docs/cua/reference/computer-sdk\">Cua Computer SDK</a>, providing VMs that AI models can interact with through screenshots and input simulation.</p><div><div><div><p>Apple's Virtualization Framework—the same technology Lume is built on—powers <a href=\"https://support.claude.com/en/articles/13345190-getting-started-with-cowork\" rel=\"noreferrer noopener\" target=\"_blank\">Claude Cowork</a>, Anthropic's sandboxed environment for Claude Code. It downloads a Linux root filesystem and boots it in an isolated VM where Claude can safely execute commands without access to your broader system.</p></div></div></div><p>Lume requires Apple Silicon—it won't work on Intel Macs or other platforms.</p>",
      "contentLength": 2340,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46670181"
    },
    {
      "title": "Gaussian Splatting – A$AP Rocky \"Helicopter\" music video",
      "url": "https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting",
      "date": 1768758055,
      "author": "ChrisArchitect",
      "guid": 36827,
      "unread": true,
      "content": "<p>Believe it or not, A$AP Rocky is a huge fan of radiance fields.</p><p>Yesterday, when A$AP Rocky released the music video for , many viewers focused on the chaos, the motion, and the unmistakable early MTV energy of the piece. What’s easier to miss, unless you know what you’re looking at, is that nearly every human performance in the video was captured volumetrically and rendered as dynamic splats.</p><p>I spoke with <a href=\"https://evercoast.com/\" target=\"_blank\" rel=\"noopener\">Evercoast</a>, the team responsible for capturing the performances, as well as Chris Rutledge, the project’s CG Supervisor at <a href=\"https://vimeo.com/grinmachine\" target=\"_blank\" rel=\"noopener\">Grin Machine</a>, and Wilfred Driscoll of WildCapture and <a href=\"https://fitsu.ai/\" target=\"_blank\" rel=\"noopener\">Fitsū.ai</a>, to understand how  came together and why this project represents one of the most ambitious real world deployments of dynamic gaussian splatting in a major music release to date.</p><p>The decision to shoot  volumetrically wasn’t driven by technology for technology’s sake. According to the team, the director Dan Strait approached the project in July with a clear creative goal to capture human performance in a way that would allow radical freedom in post-production. This would have been either impractical or prohibitively expensive using conventional filming and VFX pipelines.</p><p>Chris told me he’d been tracking volumetric performance capture for years, fascinated by emerging techniques that could enable visuals that simply weren’t possible before. Two years ago, he began pitching the idea to directors in his circle, including Dan, as a “someday” workflow. When Dan came back this summer and said he wanted to use volumetric capture for the entire video, the proliferation of gaussian splatting enabled them to take it on.</p><p>The aesthetic leans heavily into kinetic motion. Dancers colliding, bodies suspended in midair, chaotic fight scenes, and performers interacting with props that later dissolve into something else entirely. Every punch, slam, pull-up, and fall you see was physically performed and captured in 3D.</p><p>Almost every human figure in the video, including Rocky himself, was recorded volumetrically using Evercoast’s system. It’s all real performance, preserved spatially.</p><p>This is not the first time that A$AP Rocky has featured a radiance field in one of his music videos. The 2023 music video for  featured several NeRFs and even the GUI for Instant-NGP, which you can spot throughout the piece.</p><p>The primary shoot for  took place in August in Los Angeles. Evercoast deployed a 56 camera RGB-D array, synchronized across two Dell workstations. Performers were suspended from wires, hanging upside down, doing pull-ups on ceiling-mounted bars, swinging props, and performing stunts, all inside the capture volume.</p><p>Scenes that appear surreal in the final video were, in reality, grounded in very physical setups, such as wooden planks standing in for helicopter blades, real wire rigs, and real props. The volumetric data allowed those elements to be removed, recomposed, or entirely recontextualized later without losing the authenticity of the human motion.</p><p>Over the course of the shoot, Evercoast recorded more than 10 terabytes of raw data, ultimately rendering roughly 30 minutes of final splatted footage, exported as PLY sequences totaling around one terabyte.</p><p>That data was then brought into Houdini, where the post production team used CG Nomads GSOPs for manipulation and sequencing, and OTOY’s OctaneRender for final rendering. Thanks to this combination, the production team was also able to relight the splats.</p><p>One of the more powerful aspects of the workflow was Evercoast’s ability to preview volumetric captures at multiple stages. The director could see live spatial feedback on set, generate quick mesh based previews seconds after a take, and later review fully rendered splats through Evercoast’s web player before downloading massive PLY sequences for Houdini.</p><p>In practice, this meant creative decisions could be made rapidly and cheaply, without committing to heavy downstream processing until the team knew exactly what they wanted. It’s a workflow that more closely resembles simulation than traditional filming.</p><p>Chris also discovered that Octane’s Houdini integration had matured, and that Octane’s early splat support was far enough along to enable relighting. According to the team, the ability to relight splats, introduce shadowing, and achieve a more dimensional “3D video” look was a major reason the final aesthetic lands the way it does.</p><p>The team also used Blender heavily for layout and previs, converting splat sequences into lightweight proxy caches for scene planning. Wilfred described how WildCapture’s internal tooling was used selectively to introduce temporal consistency. In his words, the team derived primitive pose estimation skeletons that could be used to transfer motion, support collision setups, and allow Houdini’s simulation toolset to handle rigid body, soft body, and more physically grounded interactions.</p><p>One recurring reaction to the video has been confusion. Viewers assume the imagery is AI-generated. According to Evercoast, that couldn’t be further from the truth. Every stunt, every swing, every fall was physically performed and captured in real space. What makes it feel synthetic is the freedom volumetric capture affords. You aren’t limited by the camera’s composition. You have free rein to explore, reposition cameras after the fact, break spatial continuity, and recombine performances in ways that 2D simply can’t.</p><p>In other words, radiance field technology isn’t replacing reality. It’s preserving everything.</p>",
      "contentLength": 5526,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46670024"
    },
    {
      "title": "Statement by Denmark, Finland, France, Germany, Netherlands, Norway, Sweden, UK",
      "url": "https://www.bundesregierung.de/breg-de/aktuelles/statement-by-denmark-finland-france-germany-the-netherlands-norway-sweden-and-the-united-kingdom-2403016",
      "date": 1768757605,
      "author": "madspindel",
      "guid": 36867,
      "unread": true,
      "content": "<p>As members of NATO, we are committed to strengthening Arctic security as a shared transatlantic interest. The pre-coordinated Danish exercise „Arctic Endurance“ conducted with Allies, responds to this necessity. It poses no threat to anyone.<p>We stand in full solidarity with the Kingdom of Denmark and the people of Greenland. Building on the process begun last week, we stand ready to engage in a dialogue based on the principles of sovereignty and territorial integrity that we stand firmly behind.</p><p>Tariff threats undermine transatlantic relations and risk a dangerous downward spiral. We will continue to stand united and coordinated in our response. We are committed to upholding our sovereignty.</p><strong>Erklärung von Dänemark, Finnland, Frankreich, Deutschland, Niederlande, Norwegen, Schweden und des Vereinigten Königreichs</strong><p>Als Alliierte der NATO sind wir der Stärkung der Sicherheit in der Arktis verpflichtet. Dies ist ein gemeinsames transatlantisches Interesse. Die von Dänemark koordinierte Übung „Arctic Endurance“, welche gemeinsam mit Alliierten durchgeführt wird, ist eine Antwort auf die Notwendigkeit größerer Sicherheit in der Arktis. Die Übung stellt für niemanden eine Bedrohung dar.</p><p>Wir stehen in voller Solidarität an der Seite des Königreichs Dänemark und der Bevölkerung Grönlands. Aufbauend auf dem letzte Woche begonnenen Prozess sind wir bereit in einen Dialog einzutreten, auf Grundlage der Prinzipien der Souveränität und territorialen Integrität. Wir stehen fest zu diesen Prinzipien.</p><p>Zolldrohungen untergraben die transatlantischen Beziehungen und bergen das Risiko einer Eskalation. Wir werden weiterhin geeint und koordiniert reagieren. Wir sind entschlossen, unsere Souveränität zu wahren.</p></p>",
      "contentLength": 1740,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46669945"
    },
    {
      "title": "Sins of the Children",
      "url": "https://asteriskmag.com/issues/07/sins-of-the-children",
      "date": 1768756138,
      "author": "maxall4",
      "guid": 36860,
      "unread": true,
      "content": "<div><p>When we reached the weather station it was so comprehensively trashed you’d think it’d been dropped from orbit. Torn apart and the pieces stomped on, the edges corrugated with dents and corroded with fluids. Something on this planet really didn’t want us to know when it was going to rain.</p><p>“This is coming out of the use-budget,” Greffin said mournfully. She worked in Resources, liaising with the orbiting  to get what we needed. And we’d already  plenty to get ourselves set up planetside.</p><p>“What’s our culprit and how do we kill it?” Merrit asked.</p><p>The three of us had set the station up three days before and somehow it had riled the locals. Probably the sonic and radio chatter from using bounce-back to map meteorological systems. But nothing we’d seen on all of Chelicer 14d was big or aggressive enough to do this damage.</p><p>I had my slate out to review our evolving catalog of Chelicer xenofauna. Merrit was on his haunches, studying the shrapnel; Greffin had a link to base camp at the farms, going over inventory to see what we could repurpose. Around us and the wreckage stretched the local scrub. Sedentary life on Chelicer was either low and spiny or tall and thin with a sort of puffball arrangement at the top. The land — the world — was dry, the ecosystem impoverished and short on species. My unfinished xenobio report went long on the idea that Chelicer had been lush in the past, and we’d arrived to find what had stabilized out of a catastrophic dry spell, or maybe some serious solar flare activity. There were no great forests to give cover to alien tigers. On Chelicer nothing grew past a shrub. One meter for the spiny stuff, two for the puffball poles. And the weather station had been up on high ground, ten klicks’ visibility in any direction. We were .</p><p>I heard a far-off . A mechanical sound that — in the second’s pause before it impacted — I didn’t even connect with .</p></div><div><p>The thing that came down right beside us was three meters high with a massive articulated body. A bug, really, Chelicer style. Eight crooked legs out from a central hub like all the mobile life here had, but most of what we’d seen was gracile, delicate, and came up to your waist. Even the Farmers — which we’d pegged as the most advanced species around — were only a meter and a half tall, and most of that was stilting limbs. This thing was  gracile. Every segment and joint of it was ridgy, armored, and spiky. It was dun and khaki like the planet’s dust, but too big to have hidden anywhere nearby, towering over the scrub. There were spread vanes like sails projecting from its back, but it  have flown under organic power. It must have weighed five tons.</p><p>We just stared. In that moment, when we could have run or called for helpwe goggled at it. The stalked globes of its eyes looked back, devoid of living connection. A vast armored monster, airdropped from nowhere.</p><p>I saw the motion, off on a neighboring hillside. There was a second monster out there, surprisingly hard to spot. It hunkered down, drawing its limbs in.</p><p> That same sound. The thing on the hillside was gone.</p><p>A second later it was on us, coming down right in front of Merrit. I thought of mechanical advantage, the tricks you could do with a rigid exoskeleton. I thought of fleas, but on an absurd macro scale. It  and came down on eight legs that must have been shock absorbers par excellence.</p><p>Chelicer life doesn’t quite have a front or a back, built around that hub of legs. The mouth is on the underside and that’s what this thing tilted at Merrit.&nbsp;</p><p>I’d dissected some of the Farmers and they had an arrangement like eight knuckly stumps to mumble over their food with. These new arrivals had a setup like a sphincter made of scissor blades and nutcrackers, more an industrial process than biology. We’d seen what those tools had done to the weather station already. Right then we were more concerned with what it did to Merrit.&nbsp;</p><p>He was just crouched there, midway through sifting the wreckage. The monster took instant offense. Its mouthparts extended out and just … macerated him. Chopped and crushed so that in a heartbeat there was nothing left that looked remotely human, just a wadded bloody ball of flesh and splintered bone and rags of suit.&nbsp;</p><p>Greffin and I started shooting. Our guns were so badly printed you could see the mold lines. They chewed up their own mass for ammo in a spray of flechettes. True to our miserly resource budget, most of that barrage just slanted off the things’ carapaces, and I knew we were both going to follow Merrit into extinction, carved up and spat out with alien contempt. Except then Greffin hit a joint, and one monster was suddenly down a leg. That, apparently, was enough. We watched them ratchet down for takeoff, still shrugging off our fire, then ping upward. I recorded the flight of one, desperately trying to keep it in my field of vision. Without that, who’d believe us? Alien mega-fleas utilizing sheer mechanical tension to jump a half kilometer at a time.</p><p>We bagged what was left of Merrit. And I grabbed the leg when the skimmer came for evac. Because it was proof that here be monsters.</p><p>The Farmers had been the tipping point, the reason to establish a human presence planetside. Yes, 14d was a unique world, unknown alien ecosphere, all that. But if it hadn’t held anything  then the  would have focused elsewhere in the Chelicer system. And if the world had only offered mineral wealth, we’d have a robot mining operation stripping the place instead. All that unique ecosphere would have been flensed from the planet’s surface as an incidental side effect of our efforts. But on this world, the valuable thing  the biology, which needed more finesse. A human presence on the ground. Meaning a whole team of us thawed off the shelves and given this chance to justify our existence on the payroll.</p><p>Which was now under threat, as were we. We evacuated back to the farms with our grisly souvenirs.</p><p>The Concerns that have spearheaded humanity’s expansion from star to star have refined an efficient system for exploiting exoplants. When a Concern builds farms, that means a continent’s span of identical fields, robot tended. Everything growing and being harvested at an accelerated rate, processed and dried for minimal weight in transit. Turned into the Ship’s Reconstitute we’re all thoroughly sick of eating. The stuff from the Chelicer farms can look mighty good in comparison, which is a shame because a bite would kill you stone dead. But then they’re not  farms. They’re a thing the locals were doing long before we arrived.</p><p>The locals — Species 11 — are like spiders only ganglier. Four stilty legs interspersed with four spindly arms, and a hub of a body in the middle, high enough to come up to your waist. We called them Farmers from the start because it’s what they do: tend great stretches of this one crop. Not even a very exciting-looking crop, sort of a warty purple potato-looking thing, except it turns out to be superefficient at concentrating the elements in the crappy soil they’ve got here. Many of which elements are useful to , for our superconductors and our computational substructures and all that good stuff. When we discovered , you can be damn sure we moved in and took possession double time. Built our processing plant and started making off with a big chunk of the crop.&nbsp;</p><p>What did the locals think of this? My professional xenobiologist’s opinion was they didn’t think a damn thing. They didn’t  at all. The whole farming schtick they had going was just instinct, like ants, only they didn’t even  anything. When they got in the way of the machines, they got chewed up. We thought at the time they’d evolved with no natural predators.&nbsp;&nbsp;</p><p>We sure as hell were wrong about that.</p><p>Greffin and I made our reports. The dozen on-planet crew came to commiserate, meaning get the gory details. We told everyone to carry a gun and know the emergency drill. Chelicer had an apex predator we hadn’t known about. After which cautionary tales, I was left facing up to the mission’s biggest pain in my ass, namely FenJuan.</p><p>FenJuan had screwed up royally on some past previous assignments, was my guess. They’d been something senior, and something had gone south in expensive ways. Meaning FenJuan slumming it on our team was an invisible mark against every one of us, because their personnel file came with baggage. Worse, they were my immediate colleague in biosciences, the two of us responsible for figuring out the local biochemistry.</p><p>“Stort,” they addressed me, frosty as always.</p><p>“Fen,” I replied with just as much love.</p><p>“My samples?” they said. Because they didn’t  fieldwork, just like they didn’t basic human interaction, just sat at base camp and bitched.</p><p>And I’d  them samples previously. I’d cut a chunk out of a dozen critters on four other excursions and brought them back. And I’d just seen a work colleague turned to paste by some local monster-bug neither my nor FenJuan’s science had accounted for. But in the Concerns you don’t get time off for inefficient foibles like grief or trauma, so I made do with snarling at FenJuan that they’d had all the damn samples they were getting from me and if that wasn’t good enough then maybe  were the problem.</p><p>“When I say, ‘Get me a selection so I can run comparative studies,’” they snapped, “I do not mean just go snip bits off the Farmers and call the job done. A man is dead because we don’t understand the world here.”</p><p>Which was turning it back on me, making it fault. And which wasn’t true to boot. I told them that if they were having difficulty distinguishing between samples maybe they didn’t have the basic analytical skills required for the task. The structures that they’d pegged as the local equivalent of a genome were probably just some essential organelle that every damn beastie possessed, and the real genome-equivalent had gone completely under FenJuan’s radar.&nbsp;</p><p>“You want a sample?” I asked FenJuan. “For real? Cut your own out of this. You can be absolutely  it doesn’t come from a Farmer.” And I pointed them at the leg, the one we’d shot off the big bouncing bastard.</p><p>Shouting at people works, when you’re not allowed time off to process death. Works remarkably well, if it’s the only outlet you’ve got. Just as well. There would be plenty of both shouting and death in everyone’s future.</p></div><div><p>I liked to sit outside to complete my reports. Chelicer has good sun, if you’ve had the treatments to ward off skin damage. I wrote up my thoughts on our giant killer flea problem, watching the Farmers pick their way across the vast fields of “Species 13 Resource” as per official Concern designation, or the Chelicetato as our vulgar parlance had it. They groped over each tuber in turn, then pissed out the right chemicals to help the things grow. The Farmers were a remarkable find. We’d have gone way over budget making robot gardeners even half as efficient. Worth fighting off a few giant bugs for.</p><p>Past the processing plants, the elevator cable stretched into forever. Up there was the , our home away from home, taking every processed tuber we could hoik out of the ground. And our little outpost here was just the beginning. There were tens of millions of Farmers all over the planet, wherever the conditions suited their crop, all ready to become part of the industrial agriculture of the Concerns. We’d struck the jackpot when we surveyed Chelicer 14d.</p><p>Greffin had been going through recent survey images, looking for monsters. She sent me what she found. Holes like burrows I could have driven a ground-car into, written off as geological because nothing we’d seen could have made them. Now we knew better. Maybe the monster fleas had emerged only recently. Maybe there was a cicada thing going on, killer flea season. We made some recommendations for the next security meeting, and I did a tour of the turret guns that had been put in with the processing plant and never needed since.</p><p>Doing that put me in FenJuan’s orbit and I braced myself for the sandpaper of their company. They were deep in analyzing the giant leg, though, or thin-sliced samples thereof. They had a few dead Farmers too — there were plenty of aimless ones not working, now we’d harvested their plots.&nbsp;</p><p>“Stort,” they said, not the usual bark, but thoughtful. On the screens was a variety of different views of microscopic-scale Chelicer cell structure. The spiral-walled cones that FenJuan reckoned were hereditary information, and that they’d been unspooling and trying to decode. Sections had been flagged up on each, identical one to another.</p><p>“Junk DNA,” I said, and waited for their usual invective. It didn’t come, though. FenJuan actually nodded a little, a tiny iota of acknowledgment I’d said something that wasn’t stupid. And Earth life accumulates a certain amount of genetic junk, right? Stuff in the genome that’s been switched off, acquired from bacteria, or from benign transcription errors carried on down through the generations. But FenJuan reckoned something like 90 percent of any given beastie’s hereditary was this unused junk.&nbsp;</p><p>I wanted to say they were imagining things. I wanted to say it was a crap planet with crap aliens who had crap hereditary code, and us coming along to exploit them was the best thing that could have happened. That was how my encounters with FenJuan generally went. It was basically entertainment for the rest of the team.</p><p>I didn’t say any of that. FenJuan and I looked at each other, not quite ready to bury the hatchet, but maybe agreeing there was a bigger problem out there to save that mutual hatchet for.</p><p>The attack came the next day, and we weren’t prepared.</p><p>I heard the sound, distant, echoing across flat farmland from the dry hills.  For two whole seconds I was thinking some piece of machinery had gone wrong and how that was someone else’s problem. And then the first of them came down, just like before. Crashing onto the roof of the processing plant hard enough to buckle the plastic composite. Leering over the edge like a gargoyle. I swear it was twice the size of the one that killed Merrit.&nbsp;</p><p>I was shouting. Most of us were shouting, but I still caught a rapid heavy drumroll underneath the human noise. <em>Chunkchunkchunkchunkchunkchunkchunk…</em></p><p>They started dropping down all round us. We were running for the plant, because it was the most reinforced building and that was the emergency drill. Someone got word to the guns that their services were needed, and they started running friend-or-foe algorithms as a dozen human beings fled frantically into their arcs of fire.&nbsp;</p><p>One of the death-fleas crashed down in front of me, outspread sails barely slowing it. The articulation of its legs popped and twisted, absorbing the force of impact. A gun hammered chips out of its carapace. It lunged forward and snipped someone — one of the resources team I think — right in half with its scissor-blade face. I screamed and just about ducked through the shadow of its wings, not knowing if I’d get killed by its jaws or our own turrets.</p><p>Most of us got inside. They didn’t break in after us, but only because they didn’t try. Maybe object permanence isn’t a big thing on Chelicer: Once we were out of sight they seemed to forget us, through they chewed up all the guns.</p><p>Through our cameras, we got to see all the rest of what they did.</p><p>The Farmers, it turned out,  natural predators. Or they did in death-flea season. The monsters went to town, mostly on the Farmers that didn’t have anything left to farm, because they were just milling about. It was a massacre. And though they were weird alien spider guys, and you can’t really anthropomorphize that, we were all surprisingly cut up. It wasn’t that they were getting slaughtered out there. It was that they were . Our livelihood, our profit, the injection of resources that was earning us our wage-worth.</p><p>The massacre was monopolizing our attention, so the  damage went almost unnoticed until the earthquakelike convulsion that cracked every wall and trashed the processor floor. For a moment the problem was so  I couldn’t work out what had happened.</p><p>The elevator cable. Something about it — maybe just that it was the biggest thing around — had drawn their ire. A half dozen of the bastards had jumped to it, and those mouthparts had sawn through the supertensile material like it was string.</p><p>That took a long while to clear up. The actual cable was, after all, a long weighted strand that stretched a good way out of atmosphere and into space, and our actual  was tethered at the halfway point. The  decoupled sharpish, you can be sure, and the vast length of the cable, cut free at its anchor, just vanished upward and sideways like the blade of God’s own scythe, on its way toward the outer reaches of the system.&nbsp;</p><p>We were stuck on-planet for some time, and we’d just had it demonstrated to us that the death-fleas were more than capable of carving their way into our compromised fortress if they wanted. Yes, our lords and masters in the Concern could shuttle us back to orbit, but that would require circumstances to fall into a very narrow gap indeed. That (1) it wasn’t worth continuing work on Chelicer 14d, and (2) it was actually worth retrieving us, rather than writing us off.&nbsp;</p><p>You can imagine the mood on the ground as we waited for their decision. We all gathered in the surviving common space and tried to convince ourselves we weren’t screwed. All except FenJuan, who didn’t  social graces, but just kept on studying the samples, which our remaining instruments couldn’t tell apart.</p><p>In the end, after they’d left us hanging for five days, there was a meeting. A handful of us on a staticky link to the chief director safe aboard the . We were ready to be bawled out for a colossal loss of resources. That was the very best we thought we’d get. Instead, though, the Great Man was onside. The harvest from Chelicer had been very good indeed, solving a variety of rare elements shortages none of us knew the Concern had. This world we had worked on was the new hope of further human expansion. If only we could solve our little pest problem.</p><p>“We need to keep you folks safe,” said the director heartily. I looked over the recommendations. What they actually wanted to keep safe was the harvest, of course, which meant the Farmers. By then we had images from all over the planet of sporadic attacks on Farmer colonies. Death-fleas picking off the weak. Nothing as sustained as we’d seen at our base camp, but plainly a part of the circle of life in these parts.</p><p>“Our engineers up here are working on a new cable,” the Great Man told us. “But drones, too. Hunter drones. A whole fleet of them. We can justify the cost, given the potential resource revenue you’ve demonstrated. We’re proposing a global initiative to wipe out these things.”</p><p>“Wipe out the species, Director?” FenJuan clarified.&nbsp;</p><p>“Given the losses we’ve sustained and the clear threat to productivity, it’s the leading proposal. But I’m here for your thoughts.” That cheery smile of his. “Stort?”</p><p>“We’re obviously still adjusting our picture of the ecosphere to incorporate these things,” I said. “Given the low species count on-world, having an apex predator that only emerges sporadically makes some sense. What happens if we remove it? We can’t know. If this was a matter of wanting to preserve a working natural ecosystem I’d say there would be too many potential imbalances generated by cropping the top of the food chain. But.”</p><p>“But,” the director agreed. Because we were not, after all, interested in preserving the ecosystem. Just that part of it that worked for us.</p><p>FenJuan’s eyes were boring into me; I didn’t meet them. “Historically,” I said, “in a managed agricultural paradigm, removal of the top predators has been accomplished very profitably. Wolves, sheep, so on. It’s not as though we’re going to have a problem with some Farmer population explosion. If some other species booms, we can manage the consequences. I say do it.”</p><p>“Director,” FenJuan put in, unasked. “I have yet to come to any understanding of the biology or relationships involved here. There’s a commonality between species I can’t account for. This world plainly went through some severe ecological crisis that left a depauperate web of interdependence. We don’t know—”</p><p>On our screens, the director settled back in his big chair. “We know all we need to. What this world could be worth to us. How much damage those beasts are capable of doing. An ! We’ll conduct a localized culling in your region first. Barring any obvious consequence, we can roll it out to the rest of the world and follow up with plant and personnel wherever these Farmer creatures are to be found.” His smile was genuinely pleased, a man who’s going to see a nice bonus. “Well done, all. I know it’s been tough, but you’re heroes.”</p><p>The local cull, when it first happened, was something to watch. Drone footage wheeling and spinning as our machines found and chased the fleas. Killed them as they leapt through the air, as they landed thunderously on the ground, as they emerged from their burrows. Wiping them out within 200 klicks of the processing plant.&nbsp;</p><p>And nothing broke. The Farmers kept on farming. The crops grew. The crops that, at a cellular level, seemed weirdly indistinguishable from the things that tended them. FenJuan was raising issues every day, by then. Desperate to communicate how weird their results were. Not doing their job, because their  was solely and specifically to identify aspects of the local biochemistry that could be profitably exploited. Instead of which, they were going nuts about how every critter just seemed to have this enormous bolus of unused genetic-equivalent information, with a huge overlap between species. And I think they’d just about worked it out, except by then they’d made such a nuisance of themselves that FenJuan was the very last person our bosses up in orbit wanted to hear from. Did this discovery open up new vistas of planetary exploitation for our already profitable operation? No? Then pipe down and stop using up comms resources.</p><p>The people the director want to hear from were designing and deploying the hunter-killers. Our expanded drone fleet was greenlit: hundreds of machines shipped downwell and let loose across the globe. Wherever they found the fleas, they destroyed them. We felt we were liberators. Whole populations of Farmers could live without those monstrous shadows falling on them. Yes, we were making a species extinct, but it wasn’t a  species. We were already on the next phase of occupation, a 10-year building plan where we’d fill the planet with farms and processing plants, replicating our first outpost over and over until there wasn’t an inch of the world that wasn’t working for us.</p><p>A couple of years into our agricultural expansion, the cacti disappeared. Not cacti, obviously. Species 43 in the Concern bestiary, but cactus enough that the name had stuck. We had a look one morning and there just wasn’t any more of it left. I suggested maybe it had been living off some sort of death-flea by-products, though the timing seemed unusually lethargic for that kind of interaction. I ended up working alongside FenJuan, and we found drone footage of the cacti stuff getting up and running around, so that Species 43 turned out to be the larval-or-something form of Species 22, and we had to recalibrate the records.&nbsp;</p><p>At around the same time, the little hairy critters that were Species 38 rooted down and grew long spires with puffballs on them, making them actually Species 17. Half a year later our existing Species 11s lost their poles and became another sort of thing we’d already seen, and so on and so on. To most of us it was a curiosity. To FenJuan it was a crawling horror that I was starting to share. All their snapping, bitching at me for not ,and I’d just written it all off as someone pissed their Concern work record was full of demerits. Except they’d been right and I’d been wrong.</p><p>There were no more cacti. That was what scared FenJuan. We watched a wave of transformations. Each form turned into something else, but none of it turned into the cactuslike Species 43. Then it was something else, where our current batch just metamorphosed and there were no new ones. None at all, anywhere on Chelicer. The dry country became less and less inhabited as species after species vanished away.</p><p>Or not species. That was what FenJuan had been trying to understand. Developmental stages. Not a circle of life, but a life cycle.</p><p>Our prized cheliceratos, which had been putting out runners and new tubers happily for over a decade, were suddenly ambulatory one morning, sprouting a thicket of spindly legs and just giving up their life of being agricultural produce. got people’s attention. Around the same time one weird round critter rooted down where the Farmers were and became the new Chelicetato crop, and the dumbest of our colleagues reckoned that was all OK then. FenJuan and I had stopped trying to raise the alarm, by then, because it obviously wasn’t going to help. Soon after, some buried fungal-looking thing we’d found no use for sprouted legs and became new Farmers. And the old farmers … died off. Wore out, natural causes. Leaving only the least dregs we’d left of their crop. From which a handful of stunted things crawled, devouring their own left-behind husks and the last corpses of their tenders. They were tiny, but we recognized them even as they began to wearily dig down into the parched, lifeless soil. Nascent fleas, entering that dormant part of their cycle from which they would emerge, at some future date, into a world devoid of anything that could sustain them. Behind them, the whole ecosystem of life stages had been rolled up. There was nothing left of it. They were the last.</p><p>&nbsp;As we had harvested and plundered, we had been watching a decade-long series of transformations. One that had definitively ended. Life on Chelicer vanished. Plant forms, bug forms, just about every macrobiological creature dying off one at a time and not being replaced by a new generation. As though death had asked them to form an orderly queue.&nbsp;</p><p>There had been a mass extinction in Chelicer’s past, FenJuan and I reckoned. Something that had killed off everything except a hardy species that inherited an utterly impoverished planetary biome. Colder at the poles, warmer at the equator, but barren, desperate. So, over the ages, that species had developed to exploit every last opportunity that the world had left to it, not through speciation but through adaption of its life cycle. Gathering the meager resources of the world, concentrating them in living forms that could be harvested in turn. Sedentary stages, mobile stages, squeezing every possible niche of everything that could be gained and then transforming into the next phase of its long and complex chain of shapes. A desperate ecosystem of one, harvesting and gathering and recycling, each stage into the next, surviving everything thrown at it. Except us, who came and severed a single link utterly and irrevocably. Cut one thread and watched the whole unravel over a mere decade.</p><p>FenJuan and I were last off the planet, on the final elevator car along with the last salvage from our farming operations. It was on us, we had been told. We were the biologists, and we should have seen it coming. And they were right; we should. But all that would have done was salve our professional pride. I don’t believe for a moment they’d have listened to us if we’d said .  isn’t the way of the Concerns.  doesn’t meet quotas or hit targets.</p><p>We stepped into the elevator car, FenJuan and I. We looked back over a world unrelieved by messy, complicated stuff, such as life. A <em>failed commercial opportunity</em>, as the report would say.&nbsp;</p><p>I wanted to say something. Possibly . But what good would it do? We were both going to be back on ice when we reached the ship, with personnel files so dire they’ll probably never thaw us out again. But, like the life of Chelicer, we’re not important, compared to the bigger picture of the Concerns and their expansion. We humans go on, world to world, star to star, making the universe our own. But on Chelicer there will only ever be dust.</p></div>",
      "contentLength": 28537,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46669663"
    },
    {
      "title": "The Nobel Prize and the Laureate Are Inseparable",
      "url": "https://www.nobelpeaceprize.org/press/press-releases/the-nobel-prize-and-the-laureate-are-inseparable",
      "date": 1768755037,
      "author": "karakoram",
      "guid": 36818,
      "unread": true,
      "content": "<ul></ul>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46669404"
    },
    {
      "title": "Statement by Denmark, Finland, France, Germany, the Netherlands,Norway,Sweden,UK",
      "url": "https://www.presidentti.fi/statement-by-denmark-finland-france-germany-the-netherlands-norway-sweden-and-the-united-kingdom-englanniksi/",
      "date": 1768753059,
      "author": "calcifer",
      "guid": 36811,
      "unread": true,
      "content": "<p>As members of NATO, we are committed to strengthening Arctic security as a shared transatlantic interest. The pre-coordinated Danish exercise ”Arctic Endurance” conducted with Allies, responds to this necessity. It poses no threat to anyone.</p><p>We stand in full solidarity with the Kingdom of Denmark and the people of Greenland. Building on the process begun last week, we stand ready to engage in a dialogue based on the principles of sovereignty and territorial integrity that we stand firmly behind.</p><p>Tariff threats undermine transatlantic relations and risk a dangerous downward spiral. We will continue to stand united and coordinated in our response. We are committed to upholding our sovereignty.</p>",
      "contentLength": 702,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46669025"
    },
    {
      "title": "Predicting OpenAI's ad strategy",
      "url": "https://ossa-ma.github.io/blog/openads",
      "date": 1768746349,
      "author": "calcifer",
      "guid": 36789,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46668021"
    },
    {
      "title": "What is Plan 9?",
      "url": "https://fqa.9front.org/fqa0.html#0.1",
      "date": 1768743145,
      "author": "AlexeyBrin",
      "guid": 36810,
      "unread": true,
      "content": "<a href=\"https://fqa.9front.org/fqa.html\">FQA INDEX</a> |\n<a href=\"https://fqa.9front.org/fqa1.html\">FQA 1 - Introduction To 9front</a><a href=\"https://fqa.9front.org/fqa0.html\">html</a> |\n<a href=\"https://fqa.9front.org/fqa0.pdf\">pdf</a> |\n<a href=\"https://fqa.9front.org/fqa0.ms\">troff</a><ul></ul><ul></ul><ul></ul><ul></ul><ul></ul><ul></ul><img src=\"https://fqa.9front.org/fork.jpg\"><ul></ul><p><a href=\"http://doc.cat-v.org/bell_labs/upas_mail_system\"></a><a href=\"http://sam.cat-v.org\"></a><a href=\"http://man.9front.org/1/rc\"></a></p><p><a href=\"https://en.wikipedia.org/wiki/Plan_9_from_user_space\">Plan 9 from User Space</a> (also known as plan9port or p9p) is a port of many Plan 9 from Bell Labs libraries and applications to UNIX-like operating systems. Currently it has been tested on a variety of operating systems including: Linux, Mac OS X, FreeBSD, NetBSD, OpenBSD, Solaris and SunOS.\n</p><p><a href=\"http://www.vitanuova.com/inferno/index.html\">Inferno</a> is a distributed operating system also created at Bell Labs, but which is now developed and maintained by\n<a href=\"http://www.vitanuova.com/\">Vita Nuova Holdings</a> as free software. It employs many ideas from Plan 9 (Inferno does share some compatible interfaces with Plan 9, including the\n<a href=\"http://9p.cat-v.org\">9P/Styx</a> protocol), but is a completely different OS. Many users new to Plan 9 find out about Inferno and immediately decide to abandon Plan 9 and its opaque user interface for this obviously \"more advanced\" sibling, but usually abandon that, too, upon first contact with Inferno's Tk GUI. Notable exceptions duly noted.\n</p><img src=\"https://fqa.9front.org/itsbeyondmycontrol.jpg\"><ul></ul><ul></ul><ul></ul><ul></ul><ul></ul><ul></ul><ul></ul><ul></ul><ul></ul><p><a href=\"http://man.9front.org/2/\">Section (2)</a> for library functions, including system calls.\n</p><p><a href=\"http://9front.org/propaganda/books\"></a></p><a href=\"https://fqa.9front.org/fqa.html\">FQA INDEX</a> |\nFQA 0 - Introduction To Plan 9 |\n<a href=\"https://fqa.9front.org/fqa1.html\">FQA 1 - Introduction To 9front</a>",
      "contentLength": 1065,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46667675"
    },
    {
      "title": "Software engineers can no longer neglect their soft skills",
      "url": "https://www.qu8n.com/posts/most-important-software-engineering-skill-2026",
      "date": 1768742060,
      "author": "quanwinn",
      "guid": 36847,
      "unread": true,
      "content": "<p>Starting in 2026, communication has become the most important skill for software engineers.</p><p>It's not writing code, system designs, or having estoric knowledge of a programming language (i.e., Rust).</p><p>AI coding agents have gotten <a href=\"https://x.com/bcherny/status/2004897269674639461\" rel=\"noreferrer noopener\" target=\"_blank\">very, very good</a>. A year ago, I'd reach out to Cursor hesitantly for MVPs or quick fixes. Today, I use Claude Code for almost all non-trivial programming tasks and have spent $500+ on it just last December.</p><p>AI talks online revolve much around the hard skils. Initially it was prompt tricks to accomplish X, then the best MCPs for Y, and so on. But with Opus 4.5, using vanilla Claude Code gets you 80% there. Even in the age of AI, the 80/20 rule still applies. So, what should engineers focus on?</p><p>One thing with coding agents is that the better the spec, the more in line they will be with the technical and business requirements. But getting a good spec is hard.</p><p>In real life, tickets rarely contain all the requirements. To do so, you might need to:</p><ul><li>Ask questions that reveal assumptions people didn't know they had</li><li>Facilite trade-off discussions</li><li>Push back on scope without burning bridges</li><li>Make calls on things nobody thought to specify</li></ul><p>Doing these things well used to be optional for individual contributors. Certain teams would enable engineers to thrive being an average communicator but excellent coder. Now, the non-coding parts are becoming a non-negotiable.</p><p>Software engineers are problem solvers. We believe that every problem has a solution, a \"best practice\". But working with people is messy.</p><p>fortunately, we won't be able to AI our way into better communication skills. Good communication requires empathy, and we can all use a little more of that in today's landscape.</p><p>This post got some attention from Hacker News. Thank you. I enjoyed reading the thoughtful discussions. I wrote this update to clarify:</p><ol><li><p>AI is just a tool, and December's $500+ spend was me exploring and experimenting during the holidays. Learning new tools doesn't make a worse craftsman, and I say this as an AI hype skeptic.</p></li><li><p>I did not write this post with AI. I started blogging recently to improve my writing. If my writing reads like AI, then my writing is average, and I have a lot to learn. More to come!</p></li></ol>",
      "contentLength": 2207,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46667572"
    },
    {
      "title": "Starting from scratch: Training a 30M Topological Transformer",
      "url": "https://www.tuned.org.uk/posts/013_the_topological_transformer_training_tauformer",
      "date": 1768736354,
      "author": "tuned",
      "guid": 36809,
      "unread": true,
      "content": "<p>Tauformer is a  (see <a href=\"https://www.techrxiv.org/users/685780/articles/1375955-topological-transformer-a-redesign-for-domain-memory-and-cheaper-kernel-operations\">paper</a>) that replaces dot‑product attention with a Laplacian-derived scalar (taumode) per token/head, then attends using distances in that scalar space.\nBelow is a post-style overview of the idea and the first training signals from a 30M-parameter run.</p><p>Tauformer’s goal is to inject  directly into attention by using a Graph Laplacian built from a domain embedding space (a “domain memory”) as a persistent reference.\nInstead of ranking keys by \\(Q\\cdot K\\), Tauformer ranks them by how similar their Laplacian-derived taumode scalars are, which is intended to bias attention toward domain-relevant relations rather than generic geometric similarity.</p><p>At the implementation level, Tauformer keeps the familiar Q/K/V projections, RoPE, causal masking, and stable softmax/value aggregation pipeline, but changes how attention logits are computed.\nEach head vector is compressed into a scalar \\(\\lambda\\) using a bounded Rayleigh-quotient energy computed with a feature-space Laplacian \\(L\\), then logits are computed as a negative distance \\(-|\\lambda_q-\\lambda_k|/\\text{temperature}\\).</p><p>Key building blocks (as implemented):</p><ul><li>Taumode scalar: compute \\(E_{\\text{raw}}=(x^\\top L x)/(x^\\top x+\\varepsilon)\\), then bound it as \\(E_{\\text{raw}}/(E_{\\text{raw}}+\\tau)\\) to produce \\(\\lambda\\in[0,1)\\).</li><li>Logits: \\(\\text{att}_{ij} = -\\|\\lambda^Q_i - \\lambda^K_j\\|/\\text{temperature}\\), then reuse causal mask \\(→\\) subtract row max \\(→\\) softmax \\(→\\) multiply by \\(V\\).</li></ul><p>Because scoring no longer needs full key vectors, Tauformer’s KV-cache can store values plus a compact key-side scalar stream rather than both K and V tensors.\nConcretely, the cache payload is \\((V,\\lambda_k)\\) (not \\((K,V)\\)), which yields an approximate ~50% per-layer cache reduction for typical head dimensions (small overhead for storing the extra scalar).</p><p>The design also anticipates using a sparse Laplacian from a precomputed domain manifold so computing \\(\\lambda\\) can depend on Laplacian sparsity (nnz) rather than dense \\(D^2\\) multiplication. It exchanges the long preliminary adjustment of weights with a pre-training shorter phase in which a Laplacian is built using .</p><h2>Run setup (what was trained)</h2><p>This run trains a 30M-class TauGPT.\nTraining uses AdamW with base LR \\(5\\times10^{-4}\\) and a warmup of 100 steps, then keeps the base LR constant unless the plateau logic scales it down.\nData comes from a local JSONL file () streamed through an IterableDataset, with a routed split where every 20th batch is used for validation (\\(≈5%\\)).</p><table><tbody><tr><td>TauGPT ~30M parameters  (GPT2-inspired)</td></tr><tr></tr><tr><td>Sequence length ()</td></tr><tr><td>Vocabulary size ()</td></tr><tr></tr><tr></tr><tr><td>Constant LR (no decay unless manually/externally adjusted)</td></tr><tr><td>Local JSONL file </td></tr><tr><td>Streamed via an IterableDataset-style pipeline (no shuffle in DataLoader)</td></tr><tr><td>Routed split where every 20th batch is used for validation</td></tr><tr><td>Approx. validation fraction</td></tr></tbody></table><p>At step 100 the run reports train loss 4.6772 and val loss 4.9255 (PPL 107.47), and by step 2000 it reaches val loss 2.3585 (Perplexity 6.59).\nThe best validation point in the log is step 4500 with , after which validation regresses to  by step 5000.\nThe final run summary records , , , and . That is a good result for \\(~2\\) hours of training on this smallest model (at an average of ~60K Tokens Per Second using ~7Gb of VRAM).</p><p>The early phase is strong: validation drops from 4.93 at step 100 to ~2.36 by step 2000, showing that the model and pipeline learn effectively at this scale.\nAfter that, validation becomes noisy (e.g., rising back to 2.92 at step 2100 and peaking near 2.95 at step 4200) before the late “lucky break” to 1.91 at step 4500.\nThroughout, the run holds a fixed taumode value which means the attention geometry is not being updated as weights evolve as this will be take place in the next iterations.</p><p>All the model’s files, data, training settings and logs will be published with a permissive license once the results are consolidated and tests will move to a larger scale model.</p><p>This baseline run kept taumode fixed throughout, while using a simple validation loop and plateau-triggered LR scaling, and it still converged quickly in the early-to-mid training window.</p><p>Because the later part of the run shows volatility and regression after the best checkpoint, the next experiments focus on “adaptive” taumode strategies where taumode is recalibrated at intervals (including the “gradient” strategy that detects energy drift and gates recalibration by performance of the gradient in the previous steps) plus more sophisticated validation behaviors already implemented in the training loop.</p><p>Considering the small model size and the short training horizon (5,000 steps total, lowest loss at 4600), these results support the architecture as promising, with broader evaluation and scaled tests planned next—especially at 100M parameters.</p><p>A very interesting question has been raised by this test: <strong>what is the correlation between cross-entropy and taumode?</strong> Model convergence brings the loss down but at the same time recalibrating the taumode used on the learned weights brings down the taumode.</p><p>Cross-entropy and taumode are likely correlated because Tauformer’s attention kernel is built from Laplacian-derived scalar energies (λ/taumode) rather than dot-product similarity, so changes in the λ distribution change attention behavior and therefore training dynamics.\nIn the current training loop, the observed “taumode convergence” is also mechanically explained by how taumode is recalibrated: on (re)start, the code can compute a median energy from  produced by the  weights and then set that median as the global taumode.</p><h2>What “converging taumode” means here</h2><p>The calibration is effectively computing a Rayleigh-style energy statistic on K vectors under a Laplacian (numerator/denominator), and then taking a median over the batch to set a single scalar taumode.\nIn the reference implementation, taumode/λ is based on a bounded Rayleigh quotient: \\(E_{\\text{raw}}(x) = \\frac{x^\\top L x}{x^\\top x + \\varepsilon}\\) and then \\(\\lambda_\\tau(x)=\\frac{E_{\\text{raw}}}{E_{\\text{raw}}+\\tau}\\), which maps energies into \\([0,1)\\).</p><h2>Why taumode can drift downward as loss improves</h2><ul><li> as training progresses, the model may learn K representations that are “smoother” (lower-energy, so closer) with respect to the domain/manifold Laplacian, pushing the median energy down while also improving next-token prediction (lower cross-entropy).</li><li><strong>Unhealthy interpretation (collapse risk):</strong> median energy can also drop if K vectors collapse toward low-variance or less-discriminative configurations, which can reduce contrast in λ-distance logits even if loss continues improving short-term.</li><li> if taumode is recalibrated on resume, then taumode changes are not purely a passive “measurement of convergence”; they can act like a mid-training hyperparameter change, so correlation with loss does not automatically imply causality in the direction “lower taumode \\(⇒\\) lower loss”.</li></ul><p>A strong explanation for “converging taumode” (as a property of learned representations, not an artifact) is: as weights converge, the distribution of per-token energies \\(x^\\top L x\\) stabilizes, so repeated measurements (median, p50) across batches and checkpoints become consistent and typically shift toward lower-energy manifold-aligned directions.\nTo validate that, it helps to separate (1) the fixed constant used by attention from (2) a purely diagnostic “current batch median energy”, and track not just the median but also the spread (p05/p95), because collapse would show shrinking spread even when the median looks lower.</p><p>“lower loss \\(⇒\\) lower taumode” is a plausible causal direction in Tauformer, because the cross-entropy gradient flows through the Tauformer attention path that depends on Laplacian-energy-derived scalars computed from Q/K (and in your calibration code, specifically from block0 K vectors). As the model improves next-token prediction, it can simultaneously learn representations whose Laplacian Rayleigh energy is lower, so any “recalibrate taumode from learned weights” procedure will tend to output a smaller median. If this it true, where is the optimal stopping state?</p><p>Some shift is happening in understanding information thanks to large scale learning machines!</p><p>In <a href=\"https://arxiv.org/pdf/2601.03220\">this recent paper</a>, MDL refers to the “minimum description length principle”, which says the best explanation/model is the one that minimizes the total code length needed to describe (1) the model and (2) the data given the model.\n \\(ST(X)\\) is defined as the program length of the compute-feasible model \\(P\\) that minimizes time-bounded MDL, while time-bounded entropy HT(X) is the expected code length of the data under that model.\nOperationally, the paper proposes practical estimators based on neural-network training dynamics (e.g., prequential “area under the loss curve above final loss”) to approximate how much structure a bounded learner actually absorbs from data</p><p>Qualitatively, ,  and  are exactly the kind of deterministic computations that can increase usable/learnable structure for bounded learners, which is one of the central motivations for epiplexity.\nThrough the epiplexity lens, the operations carried on by  and Tauformer (converts each head vector into a bounded scalar λτ using a Rayleigh-quotient-style energy followed by a bounding map) is a deterministic compression that can re-factor information into a form that is cheaper for downstream computation to exploit, potentially increasing the amount of structure a bounded observer can learn from the same underlying signal.</p><p>I am happy I have somehow anticipated this switch in point of view in .</p><p>I gratefully acknowledge <a href=\"https://www.enverge.ai/enverge-labs\">Enverge Labs</a> for kindly providing the computation time used to run these experiments on their H100 GPU cluster powered by clean and cheap energy, this aligns perfectly with the topological transformer objective to provide cheaper computation for Transformers.</p>",
      "contentLength": 9937,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46666963"
    },
    {
      "title": "Show HN: Xenia – A monospaced font built with a custom Python engine",
      "url": "https://github.com/Loretta1982/xenia",
      "date": 1768732792,
      "author": "xeniafont",
      "guid": 36892,
      "unread": true,
      "content": "<p>I'm an engineer who spent the last year fixing everything I hated about monofonts (especially that double-story 'a').</p><p>I built a custom Python-based procedural engine to generate the weights because I wanted more logical control over the geometry. It currently has 700+ glyphs and deep math support.</p><p>Regular weight is free for the community. I'm releasing more weights based on interest.</p>",
      "contentLength": 384,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46666661"
    },
    {
      "title": "A free and open-source rootkit for Linux",
      "url": "https://lwn.net/SubscriberLink/1053099/19c2e8180aeb0438/",
      "date": 1768728985,
      "author": "jwilk",
      "guid": 36817,
      "unread": true,
      "content": "<blockquote><table><tbody><tr><td><p>\nThe following subscription-only content has been made available to you \nby an LWN subscriber.  Thousands of subscribers depend on LWN for the \nbest news from the Linux and free software communities.  If you enjoy this \narticle, please consider <a href=\"https://lwn.net/subscribe/\">subscribing to LWN</a>.  Thank you\nfor visiting LWN.net!\n</p></td></tr></tbody></table></blockquote><div>\n           By January 16, 2026</div><p>\nWhile there are several rootkits that target Linux, they have so far not fully\nembraced the open-source ethos typical of Linux software.\nLuckily, Matheus Alves has been working to remedy\nthis lack by creating\n<a href=\"https://github.com/MatheuZSecurity/Singularity?tab=readme-ov-file#singularity---poc-of-stealthy-linux-kernel-rootkit\">\nan open-source rootkit called Singularity</a> for Linux systems. Users who feel\ntheir computers are too secure can install the Singularity kernel module in\norder to allow remote code execution, disable security features, and hide files\nand processes from normal administrative tools. Despite its many features,\nSingularity is not currently known to be in use in the wild — instead, it\nprovides security researchers with a testbed to investigate new detection and\nevasion techniques.\n</p><p>\nAlves is quite emphatic about the research nature of Singularity, saying that\nits main purpose is to help drive security research forward by demonstrating\nwhat is currently possible. He\n<a href=\"https://github.com/MatheuZSecurity/Singularity?tab=readme-ov-file#contributing\">\ncalls</a> for anyone using the software to \"<q>be a\nresearcher, not a criminal</q>\", and to test it only on systems where they have\nexplicit permission to test. If one did wish to use Singularity for nefarious\npurposes, however, the code is MIT licensed and freely available — using it in\nthat way would only be a crime, not an instance of copyright infringement.\n</p><h4>Getting its hooks into the kernel</h4><p>\nThe whole problem of how to obtain\nroot permissions on a system and go about installing a kernel module is out of\nscope for Singularity; its focus is on how to maintain an undetected presence\nin the kernel once things have already been compromised. In order to do this,\nSingularity goes to a lot of trouble to present the illusion that the system\nhasn't been modified at all. It uses the kernel's existing\n<a href=\"https://www.kernel.org/doc/html/latest/trace/ftrace.html\">\nFtrace mechanism</a> to\nhook into the functions that handle many system calls and change their responses\nto hide any sign of its presence.\n</p><p>\nUsing Ftrace offers several advantages to the rootkit; most importantly, it\nmeans that the rootkit doesn't need to change\nthe CPU trap-handling vector for system calls,\nwhich was one of the ways that some rootkits have been identified historically.\nIt also avoids having to patch the kernel's functions directly — kernel functions\nalready have hooks for Ftrace, so the rootkit doesn't need to perform its own\nad-hoc modifications to the kernel's machine code, which might be detected. The\nFtrace mechanism can be disabled at run time, of course — so Singularity helpfully enables\nit automatically and blocks any attempts to turn it off.\n</p><p>\nSingularity is concerned with hiding four classes of things: its own presence,\nthe existence of attacker-controlled processes, network communication with those\nprocesses, and the files that those processes use. Hiding its own presence is\nactually fairly straightforward: when the kernel module is loaded, it resets the\nkernel's\n<a href=\"https://static.lwn.net/kerneldoc/admin-guide/tainted-kernels.html\">\ntaint marker</a> and removes itself from the list of active kernel\nmodules. This also means that Singularity cannot be unloaded, since it doesn't\nappear in the normal interfaces that are used for unloading kernel modules. It\nalso blocks the loading of subsequent kernel modules (although they will appear\nto load — they'll just silently fail).\nConsequently, Alves recommends experimenting with Singularity in a virtual machine.\n</p><p>\nHiding processes, on the other hand, is more complicated. The mechanism that\nSingularity uses starts\nby identifying and remembering which processes are supposed to be hidden.\nSingularity uses a single 32-entry array of process IDs to track\nattacker-controlled processes; this is because a more sophisticated data\nstructure would introduce more opportunities for the rootkit to be caught,\neither by adding additional memory allocations that could be noticed, or by\nintroducing delays whenever one of its hooked functions needs to check the list\nof hidden process IDs.\n</p><p>\nSingularity supports two ways to add processes to the list: by sending an unused\nsignal, or by setting a special environment variable and launching a new process. To implement the former, it hooks the\n<a href=\"https://man7.org/linux/man-pages/man2/kill.2.html\"></a> system call to detect an unused signal (number 59, by default),\nquashes the signal, adds the target process to its internal list, and gives the process\nroot permissions in the global namespace. This means that attacker-controlled\nprocesses can be added from inside containers, and automatically escape the\ncontainer using their new root privileges. To handle the environment variable, the\n<a href=\"https://man7.org/linux/man-pages/man2/execve.2.html\"></a> system call is\nhooked in a similar way.\n</p><p>\nOnce a process is in the list, attempts to send signal 0 (to check whether the\nprocess exists) are also intercepted, as are other system calls that could\nrefer to the process, such as\n<a href=\"https://man7.org/linux/man-pages/man2/getpgrp.2.html\"></a>,\n<a href=\"https://man7.org/linux/man-pages/man2/sched_setaffinity.2.html\"></a>,\nand others. The total number of processes on the system, as reported by\n<a href=\"https://man7.org/linux/man-pages/man2/sysinfo.2.html\"></a> is also decremented to keep things consistent.\nThe process's files in \nare hidden by Singularity's file-hiding code. That code is probably the\ntrickiest part of the whole rootkit. The basic idea is to filter out hidden\ndirectory entries such that the filesystem appears to remain in a consistent\nstate, but filesystem code is difficult to get right at the best of times.\n</p><p>\nWhen a program calls\n<a href=\"https://man7.org/linux/man-pages/man2/getdents.2.html\"></a>, the kernel fills the provided buffer\nwith directory entries as normal. Then, Singularity's hook copies the buffer\nback from user memory, removes the hidden entries, puts the modified buffer back\nin user memory, and changes the return value\nof the system call to reflect the smaller number of directory entries returned.\nThis slightly complicated process is because the kernel doesn't provide a good\nplace for Singularity to inject a hook before the directory entries are\nwritten to user memory the first time. So, one potential way to identify the\nrootkit is to have another thread race with the attempt to read directory\nentries, trying to spot any that were removed.\n</p><p>\nChanging the number of returned directory entries alone would make the system\nappear to be in an inconsistent state, however. Directories in Linux filesystems are supposed\nto track the number of references to them; this includes the \"..\" references\ninside child directories. So, when hiding a directory, Singularity also needs to\nintercept calls to\n<a href=\"https://www.man7.org/linux/man-pages/man2/stat.2.html\"></a> in order to adjust the number of visible links to its\nparent directory.\n</p><p>\nDirect access to hidden directories, in the form of\n<a href=\"https://man7.org/linux/man-pages/man2/openat2.2.html\"></a> and\nrelated system calls, is also made to fail.\n<a href=\"https://man7.org/linux/man-pages/man2/readlink.2.html\"></a> poses a special\nchallenge because it resolves symbolic links without actually opening them; it\nhas to be handled separately. In addition to the procfs files of hidden\nprocesses, Singularity also hides any directories matching a set of\nuser-supplied patterns. By default, it hides things named \"singularity\", but the\nproject's documentation suggests changing this in the build configuration,\nsince otherwise detecting the rootkit becomes straightforward.\n</p><p>\nDespite this sophisticated file-hiding machinery, Singularity doesn't help\nagainst forensic examinations of a hard disk from another computer. If it isn't\ninstalled in the running kernel, it can't hide anything. Therefore, the\ndocumentation also recommends putting as many hidden files as possible onto\ntemporary filesystems stored in RAM, so that they don't show up after the system\nis rebooted.\n</p><p>\nAnother problem for the rootkit is files that contain traces of its presence,\nbut that would raise eyebrows if they disappeared entirely. This includes things\nlike the system log, but also files in procfs like\n<a href=\"https://www.man7.org/linux/man-pages/man5/proc_kallsyms.5.html\"></a> or  that\nexpose which kernel functions have had Ftrace probes attached. For those files,\nSingularity doesn't hide them at the filesystem level, but it does filter calls\nto\n<a href=\"https://www.man7.org/linux/man-pages/man2/read.2.html\"></a> to hide incriminating information.\n</p><p>\nDeciding which log lines are incriminating isn't a completely solved problem,\nthough. Right now, Singularity relies on matching a set of known strings. This\nis another place where users will have to customize the build to avoid simple\ndetection methods.\n</p><p>\nEven once an attacker's processes can hide themselves and their files, it is\nstill usually desirable to communicate information back to a command-and-control\nserver. Singularity will work to hide network connections using a specific TCP port\n(8081, by default), and hide packets sent to and from that port from packet\ncaptures. It supports both IPv4 and IPv6. Hiding the connections from tools like\n<a href=\"https://en.wikipedia.org/wiki/Netstat\"></a> uses the same filesystem-hiding code as before. Hiding things\nfrom packet captures requires hooking into the kernel's\npacket-receiving code.\n</p><p>\nOn the other hand, this is another place where Singularity can't control the\nobservations of uncompromised computers: if one is running a network tap on\nanother computer, the packets to and from Singularity's hidden port will be\ntotally visible.\n</p><h4>The importance of compatibility</h4><p>\nSingularity only supports x86 and x86_64, but it does support\nboth 64-bit and 32-bit system call interfaces. This is\nimportant, because otherwise a 32-bit application running on top of a 64-bit\nkernel could potentially see different results, which would be suspicious. To\navoid this, Singularity inserts all of the aforementioned Ftrace hooks\ntwice, once on the 32-bit system call and once on the 64-bit system call. A\ngeneric wrapper function converts from the 32-bit calling convention to the\n64-bit calling convention before forwarding to the actual implementation of the\nhook.\n</p><p>\nSingularity has been tested on a variety of 6.x kernels, including some\nversions shipped by Ubuntu, CentOS Stream, Debian, and Fedora. Since the tool\nprimarily uses the Ftrace interface, it should be supported on most kernels —\nalthough since it interfaces with internal details of the kernel, there is\nalways the chance that an update will break things.\n</p><p>\nThe tool also comes bundled with a set of utility scripts for cleaning up\nevidence that it was installed in the first place. These include a script that\nmimics normal log-rotation behavior, except that it silently truncates the logs\nto hinder analysis; a script that securely shreds a source-code checkout in case\nthe module was compiled locally; and a script that automatically configures the\nrootkit's module to be loaded on boot.\n</p><p>\nOverall, Singularity is remarkably sneaky. If someone didn't know what to look\nfor, they would probably have trouble identifying that anything was amiss. The\nrootkit's biggest tell is probably the way that it prevents Ftrace from being\ndisabled; if one writes \"0\" to  and the\ncontent of the file remains \"1\", that's a pretty clear sign that something is\ngoing on.\n</p><p>\nReaders interested in fixing that limitation are welcome to submit a\npull request to the project; Alves is interested in receiving bug fixes,\nsuggestions for new evasion techniques, and reports of working detection\nmethods. The code itself is simple and modular, so it is relatively easy to\nadapt Singularity for one's own purposes. Perhaps having such a vivid\ndemonstration of what is possible to do with a rootkit will inspire new, better\ndetection or prevention methods.\n</p>",
      "contentLength": 11152,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46666288"
    },
    {
      "title": "Consent-O-Matic",
      "url": "https://github.com/cavi-au/Consent-O-Matic",
      "date": 1768728919,
      "author": "throawayonthe",
      "guid": 36778,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46666283"
    },
    {
      "title": "Command-line Tools can be 235x Faster than your Hadoop Cluster (2014)",
      "url": "https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html",
      "date": 1768726720,
      "author": "tosh",
      "guid": 36788,
      "unread": true,
      "content": "<h2>Command-line Tools can be 235x Faster than your Hadoop Cluster</h2><p>As I was browsing the web and catching up on some sites I visit periodically, I found a cool article from <a href=\"https://tomhayden3.com/2013/12/27/chess-mr-job/\">Tom Hayden</a> about using <a href=\"https://aws.amazon.com/elasticmapreduce/\">Amazon Elastic Map Reduce</a> (EMR) and <a href=\"https://github.com/Yelp/mrjob\">mrjob</a> in order to compute some statistics on win/loss ratios for chess games he downloaded from the <a href=\"https://www.top-5000.nl/pgn.htm\">millionbase archive</a>, and generally have fun with EMR. Since the data volume was only about 1.75GB containing around 2 million chess games, I was skeptical of using Hadoop for the task, but I can understand his goal of learning and having fun with mrjob and EMR. Since the problem is basically just to look at the result lines of each file and aggregate the different results, it seems ideally suited to stream processing with shell commands. I tried this out, and for the same amount of data I was able to use my laptop to get the results in about 12 seconds (processing speed of about 270MB/sec), while the Hadoop processing took about 26 minutes (processing speed of about 1.14MB/sec).</p><p>After reporting that the time required to process the data with 7 c1.medium machine in the cluster took 26 minutes, Tom remarks</p><blockquote><p>This is probably better than it would take to run serially on my machine but probably not as good as if I did some kind of clever multi-threaded application locally.</p></blockquote><p>This is absolutely correct, although even serial processing may beat 26 minutes. Although Tom was doing the project for fun, often people use Hadoop and other so-called  tools for real-world processing and analysis jobs that can be done faster with simpler tools and different techniques.</p><p>One especially under-used approach for data processing is using standard shell tools and commands. The benefits of this approach can be massive, since creating a data pipeline out of shell commands means that all the processing steps can be done in parallel. This is basically like having your own <a href=\"https://storm-project.net/\">Storm</a> cluster on your local machine. Even the concepts of Spouts, Bolts, and Sinks transfer to shell pipes and the commands between them. You can pretty easily construct a stream processing pipeline with basic commands that will have extremely good performance compared to many modern  tools.</p><p>An additional point is the batch versus streaming analysis approach. Tom mentions in the beginning of the piece that after loading 10000 games and doing the analysis locally, that he gets a bit short on memory. This is because all game data is loaded into RAM for the analysis. However, considering the problem for a bit, it can be easily solved with streaming analysis that requires basically no memory at all. The resulting stream processing pipeline we will create will be over 235 times faster than the Hadoop implementation and use virtually no memory.</p><p>The first step in the pipeline is to get the data out of the PGN files. Since I had no idea what kind of format this was, I checked it out on <a href=\"https://en.wikipedia.org/wiki/Portable_Game_Notation\">Wikipedia</a>.</p><pre tabindex=\"0\"><code>[Event \"F/S Return Match\"]\n[Site \"Belgrade, Serbia Yugoslavia|JUG\"]\n[Date \"1992.11.04\"]\n[Round \"29\"]\n[White \"Fischer, Robert J.\"]\n[Black \"Spassky, Boris V.\"]\n[Result \"1/2-1/2\"]\n(moves from the game follow...)\n</code></pre><p>We are only interested in the results of the game, which only have 3 real outcomes. The 1-0 case means that white won, the 0-1 case means that black won, and the 1/2-1/2 case means the game was a draw. There is also a  case meaning the game is ongoing or cannot be scored, but we ignore that for our purposes.</p><p>The first thing to do is get a lot of game data. This proved more difficult than I thought it would be, but after some looking around online I found a git repository on GitHub from <a href=\"https://github.com/rozim/ChessData\">rozim</a> that had plenty of games. I used this to compile a set of 3.46GB of data, which is about twice what Tom used in his test. The next step is to get all that data into our pipeline.</p><p><em>If you are following along and timing your processing, don’t forget to clear your OS page cache as otherwise you won’t get valid processing times.</em></p><p>Shell commands are great for data processing pipelines because you get parallelism for free. For proof, try a simple example in your terminal.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>Intuitively it may seem that the above will sleep for 3 seconds and then print  but in fact both steps are done at the same time. This basic fact is what can offer such great speedups for simple non-IO-bound processing systems capable of running on a single machine.</p><p>Before starting the analysis pipeline, it is good to get a reference for how fast it could be and for this we can simply dump the data to .</p><p>In this case, it takes about 13 seconds to go through the 3.46GB, which is about 272MB/sec. This would be a kind of upper-bound on how quickly data could be processed on this system due to IO constraints.</p><p>Now we can start on the analysis pipeline, the first step of which is using  to generate the stream of data.</p><p>Since only the result lines in the files are interesting, we can simply scan through all the data files, and pick out the lines containing ‘Results’ with .</p><p>This will give us only the  lines from the files. Now if we want, we can simply use the  and  commands in order to get a list of all the unique items in the file along with their counts.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>This is a very straightforward analysis pipeline, and gives us the results in about 70 seconds. While we can certainly do better, assuming linear scaling this would have taken the Hadoop cluster approximately 52 minutes to process.</p><p>In order to reduce the speed further, we can take out the  steps from the pipeline, and replace them with AWK, which is a wonderful tool/language for event-based data processing.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>This will take each result record, split it on the hyphen, and take the character immediately to the left, which will be a 0 in the case of a win for black, a 1 in the case of a win for white, or a 2 in the case of a draw. Note that  is a built-in variable that represents the entire record.</p><p>This reduces the running time to approximately 65 seconds, and since we’re processing twice as much data this is a speedup of around 47 times.</p><p>So even at this point we already have a speedup of around 47 with a naive local solution. Additionally, the memory usage is effectively zero since the only data stored is the actual counts, and incrementing 3 integers is almost free in memory space terms. However, looking at  while this is running shows that  is currently the bottleneck with full usage of a single CPU core.</p><p>This problem of unused cores can be fixed with the wonderful  command, which will allow us to parallelize the . Since  expects input in a certain way, it is safer and easier to use  with the  argument in order to make sure that each file name being passed to  is null-terminated. The corresponding  tells  to expected null-terminated input. Additionally, the  how many inputs to give each process and the  indicates the number of processes to run in parallel. Also important to be aware of is that such a parallel pipeline doesn’t guarantee delivery order, but this isn’t a problem if you are used to dealing with distributed processing systems. The  for  indicates that we are only matching on fixed strings and not doing any fancy regex, and can offer a small speedup, which I did not notice in my testing.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>This results in a run time of about 38 seconds, which is an additional 40% or so reduction in processing time from parallelizing the  step in our pipeline. This gets us up to approximately 77 times faster than the Hadoop implementation.</p><p>Although we have improved the performance dramatically by parallelizing the  step in our pipeline, we can actually remove this entirely by having  filter the input records (lines in this case) and only operate on those containing the string “Result”.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>You may think that would be the correct solution, but this will output the results of  file individually, when we want to aggregate them all together. The resulting correct implementation is conceptually very similar to what the MapReduce implementation would be.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>By adding the second awk step at the end, we obtain the aggregated game information as desired.</p><p>This further improves the speed dramatically, achieving a running time of about 18 seconds, or about 174 times faster than the Hadoop implementation.</p><p>However, we can make it a bit faster still by using <a href=\"https://invisible-island.net/mawk/mawk.html\">mawk</a>, which is often a drop-in replacement for  and can offer better performance.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>This  pipeline gets us down to a runtime of about 12 seconds, or about 270MB/sec, which is around 235 times faster than the Hadoop implementation.</p><p>Hopefully this has illustrated some points about using and abusing tools like Hadoop for data processing tasks that can better be accomplished on a single machine with simple shell commands and tools. If you have a huge amount of data or really need distributed processing, then tools like Hadoop may be required, but more often than not these days I see Hadoop used where a traditional relational database or other solutions would be far better in terms of performance, cost of implementation, and ongoing maintenance.</p><a href=\"https://brid.gy/publish/mastodon\"></a><a href=\"https://brid.gy/publish/twitter\"></a><a href=\"https://fed.brid.gy/\"></a>",
      "contentLength": 8957,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46666085"
    },
    {
      "title": "A Social Filesystem",
      "url": "https://overreacted.io/a-social-filesystem/",
      "date": 1768724316,
      "author": "icy",
      "guid": 36846,
      "unread": true,
      "content": "<p>You write a document, hit save, and the file is on your computer. It’s yours. You can inspect it, you can send it to a friend, and you can open it with other apps.</p><p>Files come from the paradigm of .</p><p>This post, however, isn’t about personal computing. What I want to talk about is —apps like Instagram, Reddit, Tumblr, GitHub, and TikTok.</p><p>What do files have to do with social computing?</p><p>Historically, not a lot—</p><p>But first, a shoutout to files.</p><p>Files, as originally invented, were not meant to live  the apps.</p><p>Since files represent  creations, they should live somewhere that  control. Apps create and read your files on your behalf, but files don’t belong  the apps.</p><p>Files belong to you—the person using those apps.</p><p>Apps (and their developers) may not own your files, but they do need to be able to  them. To do that reliably, apps need your files to be structured. This is why app developers, as part of creating apps, may invent and evolve .</p><p>A file format is like a language. An app might “speak” several formats. A single format can be understood by many apps. <strong>Apps and formats are many-to-many. File formats let different apps work together without knowing about each other.</strong></p><p>SVG is an open specification. This means that different developers agree on how to read and write SVG. I created this SVG file in <a target=\"_blank\" href=\"https://excalidraw.com/\">Excalidraw</a>, but I could have used Adobe Illustrator or Inkscape instead. Your browser already knew how to display this SVG. It didn’t need to hit any Excalidraw APIs or to ask permissions from Excalidraw to display this SVG. It doesn’t matter which app has created this SVG.</p><p><em>The file format is the API.</em></p><p>Of course, not all file formats are open or documented.</p><p>Some file formats are application-specific or even proprietary like . And yet, although  was undocumented, it didn’t stop motivated developers from reverse-engineering it and creating more software that reads and writes :</p><p>Another win for the files paradigm.</p><p>The files paradigm captures a real-world intuition about tools: what we make  a tool does not belong  the tool. A manuscript doesn’t stay inside the typewriter, a photo doesn’t stay inside the camera, and a song doesn’t stay in the microphone.</p><p><strong>Our memories, our thoughts, our designs  outlive the software we used to create them. An app-agnostic storage (the filesystem) enforces this separation.</strong></p><p>You may create a file in one app, but someone else can read it using another app. You may switch the apps you use, or use them together. You may convert a file from one format to another. As long as two apps correctly “speak” the same file format, they can work in tandem even if their developers hate each others’ guts.</p><p>Someone could always create “the next app” for the files you already have:</p><p>Apps may come and go, but files stay—at least, as long as our apps think in files.</p><p>When you think of social apps—Instagram, Reddit, Tumblr, GitHub, TikTok—you probably don’t think about files. Files are for  computing only, right?</p><p>A Tumblr post isn’t a file.</p><p>An Instagram follow isn’t a file.</p><p>A Hacker News upvote isn’t a file.</p><p>But what if they  as files—at least, in all the important ways? Suppose you had a folder that contained all of the things ever ed by your online persona:</p><p>It would include everything you’ve created across different social apps—your posts, likes, scrobbles, recipes, etc. Maybe we can call it your “everything folder”.</p><p>Of course, closed apps like Instagram aren’t built this way. But imagine they were. <strong>In that world, a “Tumblr post” or an “Instagram follow” are social file formats:</strong></p><ul><li>You posting on Tumblr would create a  file in your folder.</li><li>You following on Instagram would put an  file into your folder.</li><li>You upvoting on Hacker News would add an  file to your folder.</li></ul><p>Note this folder is not some kind of an archive. It’s where your data actually lives:</p><p><strong>Files are the source of truth—the apps would reflect whatever’s in your folder.</strong></p><p>Any writes to your folder would be synced to the interested apps. For example, deleting an  file would work just as well as unfollowing through the app. Crossposting to three Tumblr communities could be done by creating three  files. Under the hood, each app manages files in your folder.</p><p>In this paradigm, apps are  to files. Every app’s database mostly becomes derived data—an app-specific cached materialized view of everybody’s folders.</p><p>This might sound very hypothetical, but it’s not. What I’ve described so far is the premise behind the <a target=\"_blank\" href=\"https://atproto.com/\">AT protocol</a>. It works in production at scale. <a target=\"_blank\" href=\"https://bsky.app/\">Bluesky</a>, <a target=\"_blank\" href=\"https://leaflet.pub/\">Leaflet</a>, <a target=\"_blank\" href=\"https://tangled.org/\">Tangled</a>, <a target=\"_blank\" href=\"https://semble.so/\">Semble</a>, and <a target=\"_blank\" href=\"https://wisp.place/\">Wisp</a> are some of the new open social apps built this way.</p><p>It doesn’t  different to use those apps. But by lifting user data out of the apps, we force the same separation as we’ve had in personal computing: <strong>apps don’t trap what you make with them.</strong> Someone can always make a new app for old data:</p><p>Like before, app developers evolve their file formats. However, they can’t gatekeep who reads and writes files in those formats. Which apps to use is up to you.</p><p>Together, everyone’s folders form something like a distributed :</p><p>I’ve previously written about the AT protocol in <a href=\"https://overreacted.io/open-social/\">Open Social</a>, looking at its model from a web-centric perspective. But I think that looking at it from the filesystem perspective is just as intriguing, so I invite you to take a tour of how it works.</p><p>A personal filesystem starts with a file.</p><p>What does a social filesystem start with?</p><p>Here is a typical social media post:</p><p>How would you represent it as a file?</p><p>It’s natural to consider JSON as a format. After all, that’s what you’d return if you were building an API. So let’s fully describe this post as a piece of JSON:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>However, if we want to store this post , it doesn’t make sense to embed the author information there. After all, if the author later changes their display name or avatar, we wouldn’t want to go through their every post and change them there.</p><p>So let’s assume their avatar and name live somewhere else—perhaps, in another file. We could leave  in the JSON but this is unnecessary too. Since this file lives inside the creator’s folder—it’s  post, after all—we can always figure out the author based on  folder we’re currently looking at.</p><p>Let’s remove the  field completely:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>This seems like a good way to describe this post:</p><p>But wait, no, this is still wrong.</p><p>You see, , , and  are not really something that the post’s author has . These values are derived from the data created by other people— replies,  reposts,  likes. The app that displays this post will have to keep track of those somehow, but they aren’t  user’s data.</p><p>So really, we’re left with just this:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>That’s our post as a file!</p><p>Notice how it took some trimming to identify which parts of the data <em>actually belong in this file</em>. This is something that you have to be intentional about when creating apps with the AT protocol. My mental model for this is to think about the  request. When the user created this thing,  That’s likely close to what we’ll want to store. That’s the stuff the user has just created.</p><p>Our social filesystem will be structured more rigidly than a traditional filesystem. For example, it will  consist of JSON files. To make this more explicit, we’ll start introducing our new terminology. We’ll call this kind of file a .</p><p>Now we need to give our record a name. There are no natural names for posts. Could we use sequential numbers? Our names need only be unique within a folder:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"fish\" data-theme=\"Overnight\"><code data-language=\"fish\" data-theme=\"Overnight\"></code></pre></figure><p>One downside is that we’d have to keep track of the latest one so there’s a risk of collisions when creating many files from different devices at the same time.</p><p>Instead, let’s use timestamps with some per-clock randomness mixed in:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"fish\" data-theme=\"Overnight\"><code data-language=\"fish\" data-theme=\"Overnight\"></code></pre></figure><p>This is nicer because these can be generated locally and will almost never collide.</p><p>We’ll use these names in URLs so let’s encode them more compactly. We’ll <a target=\"_blank\" href=\"https://atproto.com/specs/tid\">pick our encoding carefully</a> so that sorting alphabetically goes in the chronological order:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"fish\" data-theme=\"Overnight\"><code data-language=\"fish\" data-theme=\"Overnight\"></code></pre></figure><p>Now  gives us a reverse chronological timeline of posts! That’s neat. Also, since we’re sticking with JSON as our lingua franca, we don’t need file extensions.</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"fish\" data-theme=\"Overnight\"><code data-language=\"fish\" data-theme=\"Overnight\"></code></pre></figure><p>Not all records accumulate over time. For example, you can write many posts, but you only have one copy of profile information—your avatar and display name. For “singleton” records, it makes sense to use a predefined name, like  or :</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"fish\" data-theme=\"Overnight\"><code data-language=\"fish\" data-theme=\"Overnight\"></code></pre></figure><p>By the way, let’s save this profile record to :</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>Note how, taken together,  and  let us reconstruct more of the UI we started with, although some parts are still missing:</p><p>Before we fill them in, though, we need to make our system sturdier.</p><p>This was the shape of our post record:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>And this was the shape of our profile record:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>Since these are stored as files, it’s important for the format not to drift.</p><p>Let’s write some type definitions:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>TypeScript seems convenient for this but it isn’t sufficient. For example, we can’t express constraints like “the  string should have at most 300 Unicode graphemes”, or “the  string should be formatted as datetime”.</p><p>We need a richer way to define social file formats.</p><p>We might shop around for existing options (<a target=\"_blank\" href=\"https://www.pfrazee.com/blog/why-not-rdf\">RDF? JSON Schema?</a>) but if nothing quite fits, we might as well design our own schema language explicitly geared towards the needs of our social filesystem. This is what our  looks like:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>We’ll call this the Post  because it’s like a language our app wants to speak.</p><p>My first reaction was also “ouch” but it helped to think that conceptually it’s this:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>I used to yearn for a <a target=\"_blank\" href=\"https://mlf.lol/\">better</a><a target=\"_blank\" href=\"https://typelex.org/\">syntax</a> but I’ve actually come around to hesitantly appreciate the JSON. It being trivial to parse makes it super easy to build tooling around it (more on that in the end). And of course, we can <a target=\"_blank\" href=\"https://www.npmjs.com/package/@atproto/lex\">make</a><a target=\"_blank\" href=\"https://tangled.org/nonbinary.computer/jacquard\">bindings</a> turning these into type definitions and validation code for any programming language.</p><p>Our social filesystem looks like this so far:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"fish\" data-theme=\"Overnight\"><code data-language=\"fish\" data-theme=\"Overnight\"></code></pre></figure><p>The  folder has records that satisfy the Post lexicon, and the  folder contains records (a single record, really) that satisfy the Profile lexicon.</p><p>This can be made to work well for a single app. But here’s a problem. What if there’s another app with its own notion of “posts” and “profiles”?</p><p>Recall, each user has an “everything folder” with data from every app:</p><p>Different apps will likely disagree on what the format of a “post” is! For example, a microblog post might have a 300 character limit, but a proper blog post might not.</p><p>Can we get the apps to agree with each other?</p><p>We could try to put every app developer in the same room until they all agree on a perfect lexicon for a post. That would be an interesting use of everyone’s time.</p><p>For some use cases, like <a target=\"_blank\" href=\"https://standard.site/\">cross-site syndication</a>, a standard-ish jointly governed lexicon makes sense. For other cases, you really want the app to be in charge. It’s actually  that different products can disagree about what a post is! Different products, different vibes. We’d want to support that, not to fight it.</p><p>Really, we’ve been asking the wrong question. We don’t need every app developer to agree on what a  is; we just need to  anyone “define” their own .</p><p>We could try namespacing types of records by the app name:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"fish\" data-theme=\"Overnight\"><code data-language=\"fish\" data-theme=\"Overnight\"></code></pre></figure><p>But then, app names can also clash. Luckily, we already have a way to avoid conflicts—domain names. A domain name is unique and implies ownership.</p><p>Why don’t we take some inspiration from Java?</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"fish\" data-theme=\"Overnight\"><code data-language=\"fish\" data-theme=\"Overnight\"></code></pre></figure><p>This gives us </p><p>A collection is a folder with records of a certain lexicon type. Twitter’s lexicon for posts might differ from Tumblr’s, and that’s fine—they’re in separate collections. The collection is always named like <code>&lt;whoever.designs.the.lexicon&gt;.&lt;name&gt;</code>.</p><p>For example, you could imagine these collection names:</p><ul><li> for Instagram follows</li><li> for Last.fm scrobbles</li><li> for Letterboxd reviews</li></ul><p>You could also imagine these slightly whackier collection names:</p><ul><li><code>com.ycombinator.news.vote</code> (subdomains are ok)</li><li> (personal domains work too)</li><li> (a shared standard someday?)</li><li> (breaking changes = new lexicon, just like file formats)</li></ul><p>It’s like having a dedicated folder for every file extension.</p><h3><a href=\"https://overreacted.io/a-social-filesystem/#there-is-no-lexicon-police\">There Is No Lexicon Police</a></h3><p>If you’re an application author, you might be thinking:</p><p>Who enforces that the records match their lexicons? If any app can (with the user’s explicit consent) write into any other app’s collection, how do we not end up with a lot of invalid data? What if some other app puts junk into “my” collection?</p><p>The answer is that records could be junk, but it still works out anyway.</p><p>It helps to draw a parallel to file extensions. Nothing stops someone from renaming  to . A PDF reader would just refuse to open it.</p><p>Lexicon validation works the same way. The  in  signals who  the lexicon, but the records themselves could have been created by  This is why <strong>apps always treat records as untrusted input</strong>, similar to  request bodies. When you generate type definitions from a lexicon, you also get a function that will do the validation for you. If some record passes the check, great—you get a typed object. If not, fine, ignore that record.</p><p>So, validate on read, just like files.</p><p>Some care is required when evolving lexicons. From the moment some lexicon is used in the wild, you should never change which records it would consider valid. For example, you can add new optional fields, but you can’t change  some field is optional. This ensures that the new code can still read old records  that the old code will be able to read any new records. There’s a <a target=\"_blank\" href=\"https://github.com/bluesky-social/goat\">linter</a> to check for this. (For breaking changes, make a new lexicon, as you would do with a file format.)</p><p>Although this is not required, you can publish your lexicons for documentation and distribution. It’s like publishing type definitions. There’s no separate registry for those; you just put them into a <code>com.atproto.lexicon.schema</code> collection of some account, and then prove the lexicon’s domain is owned by you. For example, if I wanted to publish an  lexicon, I could place it here:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"sh\" data-theme=\"Overnight\"><code data-language=\"sh\" data-theme=\"Overnight\"></code></pre></figure><p>Let’s circle back to our post.</p><p>We’ve already decided that the profile should live in the  collection, and the post itself should live in the  collection:</p><p>But what about the likes?</p><p>A like is something that the user , so it makes sense for each like to be a record. A like record doesn’t convey any data other than which post is being liked:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>In TypeScript, we expressed this as a reference to the  type. Since lexicons are JSON files with globally unique names, here’s how we’ll say this in lexicon:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>We’re saying: a Like is an object with a  field that refers to some Post.</p><p>However, “refers” is doing a lot of work here. What does a Like record actually look like? How do you  refer from inside of one JSON file to another JSON file?</p><p>We could try to refer to the Post record by its path in our “everything folder”:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>But this only uniquely identifies it  “everything folder”. Recall that each user has their own, completely isolated folders with all of their stuff:</p><p>We need to find some way to refer to the </p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>This is a difficult problem.</p><p>So far, we’ve been building up a kind of a filesystem for social apps. But the “social” part requires linking  users. We need a reliable way to refer to some user. The challenge is that we’re building a  filesystem where the “everything folders” of different users may be hosted on different computers, by different companies, communities or organizations, or be self-hosted.</p><p>What’s more, we don’t want anyone to be  their current hosting. The user should be able to change who hosts their “everything folder” at any point, and without breaking any existing links to their files. <strong>The main tension is that we want to preserve users’ ability to change their hosting, but we don’t want that to break any links.</strong> Additionally, we want to make sure that, although the system is distributed, we’re confident that each piece of data has not been tampered with.</p><p>For now, you can forget all about records, collections, and folders. We’ll focus on a single problem: links. More concretely, we need a design for permanent links that allow swappable hosting. If we don’t make this work, everything else falls apart.</p><h4><a href=\"https://overreacted.io/a-social-filesystem/#attempt-1-host-as-identity\">Attempt 1: Host as Identity</a></h4><p>Suppose dril’s content is hosted by <code>some-cool-free-hosting.com</code>. The most intuitive way to link to his content is to use a normal HTTP link to his hosting:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>This works, but then if dril wants to change his hosting, he’d break every link. So this is not a solution—it’s the exact  that we’re trying to solve. We want the links to point at “wherever dril’s stuff will be”, not “where dril’s stuff is right now”.</p><p>We need some kind of an indirection.</p><h4><a href=\"https://overreacted.io/a-social-filesystem/#attempt-2-handle-as-identity\">Attempt 2: Handle as Identity</a></h4><p>We could give dril some persistent identifier like  and use that in links:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>We could then run a registry that stores a JSON document like this for each user:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>The idea is that this document tells us how to find ’s actual hosting.</p><p>We’d also need to provide some way for dril to update this document.</p><p>Some version of this could work but it seems unfortunate to invent our own global namespace when one already exists on the internet. Let’s try a twist on this idea.</p><h4><a href=\"https://overreacted.io/a-social-filesystem/#attempt-3-domain-as-identity\">Attempt 3: Domain as Identity</a></h4><p>There’s already a global namespace anyone can participate in: DNS. If dril owns , maybe we could let him use  as his persistent identity:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>This doesn’t mean that the actual content is hosted at ; it just means that  hosts the JSON document that says where the content currently is. For example, maybe the convention is to serve that document as . Again, the document points us at the hosting. Obviously, dril can update his doc.</p><p>This is somewhat elegant but in practice the tradeoff isn’t great. Losing domains is pretty common, and most people wouldn’t want that to brick their accounts.</p><h4><a href=\"https://overreacted.io/a-social-filesystem/#attempt-4-hash-as-identity\">Attempt 4: Hash as Identity</a></h4><p>The last two attempts share a flaw: they tie you to the same handle forever.</p><p>Whether it’s a handle like  or a domain handle like , we want people to be able to change their handles at any time without breaking links.</p><p>Sounds familiar? We also want the same for hosting. So let’s keep the “domain handles” idea but store the current handle in JSON alongside the current hosting:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>This JSON is turning into sort of a calling card for your identity. “Call me , my stuff is at <code>https://some-cool-free-hosting.com</code>.”</p><p>Now we need somewhere to host this document, and some way for you to edit it.</p><p>Let’s revisit the “centralized registry” from approach #2. One problem with it was using handles as permanent identifiers. Also, centralized is bad, but why is it bad? It’s bad for many reasons, but usually it’s the risk of abuse of power or a single point of failure. Maybe we can, if not remove, then reduce some of those risks. For example, it would be nice if could make the registry’s output self-verifiable.</p><p>Let’s see if we can use mathematics to help with this.</p><p>When you create an account, we’ll generate a private and a public key. We then create a piece of JSON with your initial handle, hosting, and public key. We sign this “create account” operation with your private key. Then we hash the signed operation. That gives us a string of gibberish like .</p><p>The registry will store your operation under that hash. <strong>That hash becomes the permanent identifier for your account.</strong> We’ll use it in links to refer to you:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>To resolve a link like this, we ask the registry for the document belonging to . It returns current your hosting, handle, and public key. Then we fetch <code>com.twitter.post/34qye3wows2c5</code> from your hosting.</p><p>Okay, but how do you update your handle or your hosting in this registry?</p><p>To update, you create a new operation with a  field set to the hash of your previous operation. You sign it and send it to the registry. The registry validates the signature, appends the operation to your log, and updates the document.</p><p>To prove that it doesn’t forge the served documents, the registry exposes an endpoint that lists past operations for an identifier. To verify an operation, you check that its signature is valid and that its  field matches the hash of the operation before it. This lets you verify the entire chain of updates down to the first operation. The hash of the first operation  the identifier, so you can verify that too. At that point, you know that every change was signed with the user’s key.</p><p>With this approach, the registry is still centralized but it can’t forge anyone’s documents without the risk of that being detected. To further reduce the need to trust the registry, we make its entire operation log auditable. The registry would hold no private data and be entirely open source. Ideally, it would <a target=\"_blank\" href=\"https://docs.bsky.app/blog/plc-directory-org\">eventually be spun it out</a> into an independent legal entity so that long-term it can be like ICANN.</p><p>Since most people wouldn’t want to do key management, it’s assumed the hosting would hold the keys on behalf of the user. The registry includes a way to register an overriding rotational key, which is helpful in case the hosting itself goes rogue. (I wish for a way to set this up with a good UX; most people don’t have this on.)</p><p>Finally, since the handle is now determined by the document held in the registry, we’ll need to add some way for a domain to signal that it  with being some identifier’s handle. This could be done via DNS, HTTPS, or a mix of both.</p><p>Phew! This is <a target=\"_blank\" href=\"https://updates.microcosm.blue/3lz7nwvh4zc2u\">not perfect</a> but it gets us surprisingly far.</p><h4><a href=\"https://overreacted.io/a-social-filesystem/#attempt-5-did-as-identity\">Attempt 5: DID as Identity</a></h4><p>From the end user perspective, attempt #4 (hash as identity) is the most friendly. It doesn’t use domains for identity (only as handles), so losing a domain is fine.</p><p>However, some find relying on a third-party registry, no matter how transparent, untenable. So it would be nice to support approach #3 (domain as identity) too.</p><p>We’ll use a flexible identifier standard called DID (decentralized identifier) which is essentially a way to namespace multiple unrelated identification methods:</p><ul><li> and such — domain-based (attempt #3)</li><li><code>did:plc:6wpkkitfdkgthatfvspcfmjo</code> and such — registry-based (attempt #4)</li><li>This also leaves us a room to add other methods in the future, like </li></ul><p>This makes our Like record look like this:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>This is going to be its final form. We write  here to remind ourselves that this isn’t an HTTP link, and that you need to follow the resolution procedure (get the document, get the hosting, then get the record) to actually get the result.</p><p><strong>Now you can forget everything we just discussed and remember four things:</strong></p><ol><li>A DID is a string identifier that represents an account.</li><li>An account’s DID never changes.</li><li>Every DID points at a document with the current hosting, handle, and public key.</li><li>A handle needs to be verified in the other direction (the domain must agree).</li></ol><p>The mental model is that there’s a function like this:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>You give it a DID, and it returns where to find their stuff, their bidirectionally verified current handle, and their public key. You’ll want a  on it.</p><p>Let’s now finish our social filesystem.</p><p>With a DID, we can finally construct a path that identifies every particular record:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"scala\" data-theme=\"Overnight\"><code data-language=\"scala\" data-theme=\"Overnight\"></code></pre></figure><p><strong>An  URI is a link to a record that survives hosting and handle changes.</strong></p><p>The mental model here is that you can always resolve it to a record:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>If the hosting is down, it would temporarily not resolve, but if the user puts it up anywhere and points their DID there, it will start resolving again. The user can also delete the record, which would remove it from the user’s “everything folder”.</p><p>Another way to think about  URI is that it is as a unique identifier of every record in our filesystem, so it can serve as a key in a database or a cache.</p><p>With links, we can finally represent relationships between records.</p><p>Let’s look at dril’s post again:</p><p>Where do the 125 thousand likes come from?</p><p>These are just 125 thousand  records in different people’s “everything folders” that each  to dril’s  record:</p><p>Where do the 56K reposts come from? Similarly, this means that there are 56K  records across our social filesystem linking to this post:</p><p>A reply is just a post that has a parent post. In TypeScript, we’d write it like this:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>In lexicon, we’d write it like this:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"json\" data-theme=\"Overnight\"><code data-language=\"json\" data-theme=\"Overnight\"></code></pre></figure><p>This says: the  field is a reference to another  record.</p><p>Every reply to dril’s post will have dril’s post as their :</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"json\" data-theme=\"Overnight\"><code data-language=\"json\" data-theme=\"Overnight\"></code></pre></figure><p>So, to get the reply count, we just need to count every such post:</p><p>We’ve now explained how every piece of the original UI can be derived from files:</p><ul><li>The display name and avi come from dril’s .</li><li>The tweet text and date come from dril’s <code>com.twitter.post/34qye3wows2c5</code>.</li><li>The like count is aggregated from everyone’s s.</li><li>The repost count is aggregated from everyone’s s.</li><li>The reply count is aggregated from everyone’s s.</li></ul><p>The last finishing touch is the handle. Unfortunately,  can no longer work as a handle since we’ve chosen to use domains as handles. As a consolation, dril would be able to use  across every future social app if he would like to.</p><p>It’s time to give our “everything folder” a proper name. We’ll call it a . A repository is identified by a DID. It contains collections, which contain records:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"fish\" data-theme=\"Overnight\"><code data-language=\"fish\" data-theme=\"Overnight\"></code></pre></figure><p>Each repository is a user’s little piece of the social filesystem. A repository can be hosted anywhere—a free provider, a paid service, or your own server. You can move your repository as many times as you’d like without breaking links.</p><p>One challenge with building a social filesystem in practice is that apps need to be able to compute derived data (e.g. like counts) with no extra overhead. Of course, it would be completely impractical to look for every  record in every repo referencing a specific post when trying to serve the UI for that post.</p><p>This is why, in addition to treating a repository as a filesystem—you can  and  stuff—you can treat it as a stream,  to it by a WebSocket. This lets anyone build a local app-specific cache with just the derived data that app needs. Over the stream, you receive each commit as an event, along with the tree delta.</p><p>For example, a Hacker News backend could listen to creates/updates/deletes of  records in every known repository and save those records locally for fast querying. It could also track derived data like .</p><p>Subscribing to every known repository from every app is inconvenient. It is nicer to use dedicated services called  which retransmit all events. However, this raises the issue of trust: how do you know whether someone else’s relay is lying?</p><p>To solve this, let’s make the repository data self-certifying. We can structure the repository as a <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Merkle_tree\">hash tree</a>. Each write is a signed  containing the new root hash. This makes it possible to verify records as they come in against their original authors’ public keys. As long as you subscribe to a relay that retransmits its proofs, you can check every proof to know the records are authentic.</p><p>Verifying authenticity of records does not require storing their content, which means that relays can act as simple retransmitters and <a target=\"_blank\" href=\"https://whtwnd.com/bnewbold.net/3lo7a2a4qxg2l\">are affordable to run</a>.</p><p>If you want to explore the Atmosphere (-mosphere, get it?), pdsls is the best starting point. Given a DID or a handle, it shows a list of collections and their records. It’s really like an old school file manager, except for the social stuff.</p><p>Go to <a target=\"_blank\" href=\"https://pdsls.dev/at://danabra.mov\"></a> if you want some random place to start. Notice that you understand 80% of what’s going on there—Collections, Identity, Records, etc.</p><p>Feel free to branch out. Records link to other records. There is no app-specific aggregation there so it feels a little “ungrounded” (e.g. there is no thread view like in Bluesky) but there are some interesting navigational features like Backlinks.</p><p>Watch me walk around the Atmosphere for a bit:</p><p>(Yeah, what  that lexicon?! I didn’t expect to run into this while recording.)</p><p>Anyway, my favorite demo is this.</p><p>Watch me create a Bluesky post by creating a record via pdsls:</p><p>This works with any AT app, there’s nothing special about Bluesky. In fact, every AT app that cares to listen to events about the Bluesky Post lexicon knows that this post has been created. Apps live downstream from everybody’s records.</p><p>A month ago, I’ve made a little app called <a target=\"_blank\" href=\"https://sidetrail.app/\">Sidetrail</a> (<a target=\"_blank\" href=\"https://tangled.org/danabra.mov/sidetrail\">it’s open source</a>) to practice full-stack development. It lets you create step-by-step walkthroughs and “walk” those. Here you can see I’m deleting an  record in pdsls, and the corresponding walk disappears from my Sidetrail “walking” tab:</p><p>I know exactly  it works, it’s not supposed to  me, but it does! My repo really  the source of truth. My data lives in the Atmosphere, and apps “react” to it.</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>This syncs everyone’s repo changes to my database so I have a snapshot that’s easy to query. I’m sure I could write this more clearly, but conceptually, it’s like <em>I’m re-rendering my database</em>. It’s like I called a  “above” the internet, and now the new props flow down from files into apps, and my DB reacts to them.</p><p>I could delete those tables in production, and then use <a target=\"_blank\" href=\"https://docs.bsky.app/blog/introducing-tap\">Tap</a> to backfill my database . I’m just caching a slice of the global data. And everyone building AT apps also needs to cache some slices. Maybe different slices, but they overlap. So <a target=\"_blank\" href=\"https://constellation.microcosm.blue/\">pooling resources</a> becomes more useful. More of our tooling can be shared too.</p><p>Here’s another example that I really like.</p><p>Now, you can see it says “678,850 scrobbles” at the top of the screen. You might think people have been scrobbling their plays to the teal.fm API for a while.</p><p>The teal.fm API doesn’t actually exist. It’s not a thing. Moreover, the teal.fm product doesn’t actually exist either. I mean, I  it’s in development (this is a hobby project!), but at the time of writing, <a target=\"_blank\" href=\"https://teal.fm/\">https://teal.fm/</a> is only a landing page.</p><p>All you need to start scrobbling is to put records of the  lexicon into your repo.</p><p>Let’s see if anyone is doing this right now:</p><p>The lexicon isn’t published as a record (yet?) but it’s <a target=\"_blank\" href=\"https://github.com/teal-fm/teal/blob/25d6d8d1d9a2bb2735c74fb4bab5d35f808d120e/lexicons/fm.teal.alpha/feed/play.json\">easy to find on GitHub</a>. So anyone can build a scrobbler that writes these. I’m using one of those scrobblers.</p><p>Here’s my scrobble showing up:</p><p><em>(It’s a bit slow but <a target=\"_blank\" href=\"https://bsky.app/profile/finfet.sh/post/3mcparo5gis2u\">the delay is</a> on the Spotify/scrobbler integration side.)</em></p><p>To be clear, the person who made this demo doesn’t work on teal.fm either. It’s not an “official” demo or anything, and it’s also not using the “teal.fm database” or “teal.fm API” or anything like it. It just indexes s.</p><p>The demo’s data layer is using the new <a target=\"_blank\" href=\"https://tangled.org/chadtmiller.com/lex-gql\"></a> package, which is another of <a target=\"_blank\" href=\"https://tangled.org/chadtmiller.com\"></a>’s experiments. You give it some lexicons, and it lets you run GraphQL on your backfilled snapshot of the relevant parts of the social filesystem.</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"graphql\" data-theme=\"Overnight\"><code data-language=\"graphql\" data-theme=\"Overnight\"></code></pre></figure><p>Every app can blend cross-product information like this. For example, here is an AT app called <a target=\"_blank\" href=\"https://blento.app/\">Blento</a> that lets you <em>display your teal.fm plays</em> on your homepage:</p><p>(Again, it doesn’t talk to teal.fm—which doesn’t exist yet!—it just reads your files.)</p><p>Blento is an AT replacement for <a target=\"_blank\" href=\"https://bento.me/home/bento-sunset\">Bento, which is shutting down</a>. If Blento  itself ever shuts down, any motivated developer can  with the existing content.</p><p>There’s one last example that I wanted to share.</p><p>For months, I’ve been complaining about the Bluesky’s default Discover feed which, frankly, doesn’t work all that great for me. Then I heard people saying good things about <a target=\"_blank\" href=\"https://bsky.app/profile/spacecowboy17.bsky.social/feed/for-you\"><code>@spacecowboy17.bsky.social</code>’s For You</a> algorithm.</p><p>I’ve been giving it a try, and I really like it!</p><p>I ended up switching to it completely. It reminds me of the Twitter algo in 2017—the swings are a bit hard but it finds the stuff I wouldn’t want to miss. It’s also much more responsive to “Show Less”. Its <a target=\"_blank\" href=\"https://bsky.app/profile/spacecowboy17.bsky.social/post/3mbhenfjar22s\">core principle</a> seems pretty simple.</p><p>How does a custom feed like this work? Well, a Bluesky feed is <a target=\"_blank\" href=\"https://github.com/bluesky-social/feed-generator?tab=readme-ov-file#some-details\">just an endpoint</a> that returns a list of  URIs. That’s the contract. You know how this works.</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"js\" data-theme=\"Overnight\"><code data-language=\"js\" data-theme=\"Overnight\"></code></pre></figure><p>Could there be feeds of things other than posts? Sure.</p><p>There was a tweet a few weeks ago clowning on Bluesky for being so bad at algorithms that users have to install a third-party feed to get a good experience.</p><p>I agree <a target=\"_blank\" href=\"https://bsky.app/profile/dame.is/post/3mavm5k7u2h2d\">with </a> that this shows something important: Bluesky is a place where that  Why? In the Atmosphere, third party is first party. We’re all building projections of the same data. It’s a  that someone can do it better.</p><p>An everything app tries to do everything.</p><p>An everything ecosystem lets everything get done.</p>",
      "contentLength": 31962,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46665839"
    },
    {
      "title": "Iconify: Library of Open Source Icons",
      "url": "https://icon-sets.iconify.design/",
      "date": 1768719216,
      "author": "sea-gold",
      "guid": 36762,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46665411"
    },
    {
      "title": "Show HN: GibRAM an in-memory ephemeral GraphRAG runtime for retrieval",
      "url": "https://github.com/gibram-io/gibram",
      "date": 1768718837,
      "author": "ktyptorio",
      "guid": 36911,
      "unread": true,
      "content": "<p>I have been working with regulation-heavy documents lately, and one thing kept bothering me. Flat RAG pipelines often fail to retrieve related articles together, even when they are clearly connected through references, definitions, or clauses.</p><p>After trying several RAG setups, I subjectively felt that GraphRAG was a better mental model for this kind of data. The Microsoft GraphRAG paper and reference implementation were helpful starting points. However, in practice, I found one recurring friction point: graph storage and vector indexing are usually handled by separate systems, which felt unnecessarily heavy for short-lived analysis tasks.</p><p>To explore this tradeoff, I built GibRAM (Graph in-buffer Retrieval and Associative Memory). It is an experimental, in-memory GraphRAG runtime where entities, relationships, text units, and embeddings live side by side in a single process.</p><p>GibRAM is intentionally ephemeral. It is designed for exploratory tasks like summarization or conversational querying over a bounded document set. Data lives in memory, scoped by session, and is automatically cleaned up via TTL. There are no durability guarantees, and recomputation is considered cheaper than persistence for the intended use cases.</p><p>This is not a database and not a production-ready system. It is a casual project, largely vibe-coded, meant to explore what GraphRAG looks like when memory is the primary constraint instead of storage. Technical debt exists, and many tradeoffs are explicit.</p><p>The project is open source, and I would really appreciate feedback, especially from people working on RAG, search infrastructure, or graph-based retrieval.</p><p>Happy to answer questions or hear why this approach might be flawed.</p>",
      "contentLength": 1712,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46665393"
    },
    {
      "title": "ThinkNext Design",
      "url": "https://thinknextdesign.com/home.html",
      "date": 1768717644,
      "author": "__patchbit__",
      "guid": 36777,
      "unread": true,
      "content": "<div><p>Design is far more than form or function. It’s the tangible expression of a brand’s identity, values, and promise. While a brand defines what a company stands for, design gives those aspirations form and substance. Design uniquely delivers value: visually, physically, and experientially.</p><p>At ThinkNext Design, every creation begins with empathy and seeks purpose. We look to understand not just what people need, but what they desire. Whether crafting something entirely new or reimagining the familiar, our work blends aesthetic restraint with purposeful clarity.</p><p>The result is innovative design that resonates emotionally, performs beautifully, and endures as a reflection of the brand behind it. More than 200,000,000 ThinkPads have been sold since 1992, and still counting. That didn't happen by accident. </p></div><div>Design Innovation Gallery</div><div><h4>IBM AS/400 Advanced Series</h4><p>By the early 1990's, the original IBM AS/400 product line was rapidly losing market share due to a growing perception that the product family employed outdated technology, and was highly overpriced.  David led a strategic design initiative to recast that image via a sweeping change that would forever reposition the status quo. </p><p>The resulting award winning design featured stark black enclosures, dramatic air inlets, and simple yet powerful forms. This was a striking contrast to the putty colored neutral appearance that had come to dominate not only the IBM server products, but the entire industry. Following the series introduction, AS/400 Division revenues jumped by a double-digit percentage. Comments of yesterday's technology were quickly replaced by associations with objects such as the innovative F117a stealth fighter.</p></div><div><h4>IBM AS/400 Advanced Series Security Keystick</h4><p>AS/400 systems had a control panel that included special functions that were designed to only be accessed by authorized operators. Restricted access was achieved using a traditional stainless steel keylock mated to a rotating electric switch. Without the key only basic functions could be operated. Unfortunately the assembly was very costly and the metal key/lock was a  source of potential electrostatic discharge. The security keystick eliminated the dated and flawed assembly entirely. Inserting the asymmetrical key enabled access to the restricted functions, cost a fraction of the previous solution and eliminated the ESD issue altogether. </p></div><div><h4>IBM ThinkPad TrackPoint Caps</h4><p>The soft rim and soft dome caps were added  in 1997 creating a suite of Trackpoint cap options. The introduction followed an exhaustive design-led initiative to improve the existing cat tongue cap's comfort and utility. The effort revealed that three caps were better than one, giving the user choice. All three were shipped with every ThinkPad for many years. Only the soft dome cap remains in production.</p></div><div><p>Prior to the introduction of the Netfinity 7000, IBM's PC servers were tower based offerings that often found themselves awkwardly placed on shelves in generic computer racks. The Netfinity design eliminated this makeshift approach with a \"rack and stack\" solution. The system could truly rack mount using industry standard rails, or stand alone as a tower. The design also included a stacking NetBay with provision for mounting rack mounted OEM devices without purchasing a full blown rack. Many of the system components, including hardfiles, were removable from the front without tools.</p></div><div><p>The ThinkPad ThinkLight was first introduced on the ThinkPad i Series 1400.  Observing a fellow airline passenger reading using a small light clipped to the top edge of their book, David immediately thought this idea could be adapted for use on a laptop. The final design used a white LED to illuminate the keyboard from the top bezel. It was the industry's first, and arguably most effective method, of illuminating a laptop keyboard.</p></div><div><p>\nThe introduction of the IBM Personal Computer in 1981 was a technology milestone that forever changed the world. Subsequent innovation, however, was primarily limited to technology advancements and improved affordability. In nearly 20 years, little had been done to dramatically change the design paradigm of metal box, chunky monitor, and keyboard. David initiated and led a design project to reinvent the standard.</p><p>Working in close collaboration with noted designer Richard Sapper, David and his team created an industry-leading all-in-one computer that capitalized on emerging flat-panel display technology. The final, award-winning design integrated the monitor, CPU, and optical drive into a remarkably slim profile. The optical drive was discreetly concealed within the base structure, dropping down smoothly at the touch of a button.</p></div><div><h4>IBM Aptiva S Series Loudspeakers</h4><p>Bucking the trend for bloated, frivolous designs, the Aptiva S Series speakers were conceived to match the unique angular design language of the flat panel based computer design. The sophisticated desktop speakers could be customized with brightly colored fabric grills adding to the premium image. The design was selected by Dieter Rams for a  Best of Category award at the annual IF Design Exhibition in Germany. </p></div><div><p>The ThinkPad X300 stands as a landmark in industrial design, proving how disciplined engineering and purposeful aesthetics can redefine an entire product category. Its carbon-fiber and magnesium construction, meticulously refined form, and forward-looking adoption of SSD storage and LED backlighting positioned it as a breakthrough ultraportable long before such features became commonplace. Its development earned widespread attention, most notably in BusinessWeek’s cover story “The Making of the ThinkPad X300,” which showcased the intense, design-driven effort behind the machine. The project was explored even more deeply in Steve Hamm’s book The Race for Perfect, which chronicled the X300’s creation as an example of ambitious, high-stakes innovation. Together, these accounts cement the X300’s legacy as one of the most influential and thoughtfully crafted ThinkPads ever made.</p></div><div><p>Skylight was an early “smartbook” product designed as a lightweight, always-connected device that blended elements of a smartphone and laptop. The imaginative overall product design was created by Richard Sapper, but the keyboard was the work of David and his team. Although the product was short-lived, the sculpted island style keyboard was eventually adopted for use on future ThinkPad and consumer laptops. The sculpted key surface and unique D-shape aid substantially in enhancing comfort and improving typing accuracy.</p></div><div><h4>Lenovo ThinkPad Wordmark with Heartbeat LED</h4><p>Shortly following the Lenovo acquisition of IBM's PC business, the IBM logo was removed from ThinkPad. David was a strong proponent of establishing ThinkPad as the primary badge on the product due to the brand's high recognition and subsequent value. He proposed using the sub-brand font, normally appearing below the IBM logo, as ThinkPad's new wordmark. He enhanced it with a bright red dot over the letter i which was derived from the TrackPoint cap. His now iconic concept was universally adopted as the new ThinkPad product badge worldwide in 2007. </p><p>In 2010 the dot was enhanced with a glowing red LED that is still in use today. The dot glows solid if the ThinkPad is powered on and slowly pulses like a heartbeat when in a suspended sleep state. The design draws attention and adds life to the brand. </p></div><div><p>The first-generation ThinkPad X1 Carbon introduced a bold new interpretation of classic ThinkPad design. It's carbon-fiber reinforced chassis delivered exceptional strength with a remarkably low weight. The sculpted island-style keyboard, subtle red accents, and gently tapered edges gave it a modern precision appearance without sacrificing the brand's renowned usability &amp; iconic visual impression.</p></div><div><h4>Lenovo ThinkPad Precision Wireless Travel Mouse</h4><p>The scaled-down travel mouse shares it's essential geometry with a mouse originally created for IBM's Aptiva lineup in the late 1990's. The characteristically low front, generously sculpted tail and inwardly inclined side surfaces enhance ergonomics and daily use. These design concepts have been nearly universally adopted by other computer/accessory manufacturers. </p></div><div><h4>Lenovo ThinkPad 8 Tablet QuickShot Cover</h4><p>When using a tablet as a camera the screen cover typically flops around since folding it all the way around would block the camera. The quickshot cover eliminates this inconvenience thanks to a patented folding corner. When folded back, it automatically launched the camera app to let you take a picture instantly. The flopping cover annoyance was eliminated.</p></div><div><p>The revolutionary design replaced the bezel/box paradigm with a form that resembles a rectangular tube through which large volumes of air pass. The unique appearance telegraphs raw power. The design, however, is much more than skin deep. The machine's innovative interior is highly modular and eliminates the need for tools to replace or upgrade key components. Flush handles are thoughtfully incorporated in the shell for moving the workstation.</p></div><div><h4>Lenovo ThinkPad X1 Tablet</h4><p>The pioneering ThinkPad X1 Tablet design featured a uniquely hinged kickstand that enabled customizing the user experience with a system of snap-on modules. Modules offered were the Productivity Module, which added extra battery life and additional ports; the Presenter Module, featuring a built-in pico projector for critical presentations; and the 3D Imaging Module, equipped with an Intel RealSense camera for depth sensing and 3D scanning. Together, these modules provided flexible, on-demand functionality while preserving the tablet’s portability.  </p></div><div><p>ThinkPad 25  was created and launched to celebrate the 25th anniversary of the iconic brand. It artfully blended retro design elements with modern engineering. Inspired heavily by years of passionate customer feedback and social-media campaigns calling for a “classic ThinkPad” revival, the project brought back beloved features such as the 7-row keyboard with blue accents, a tradition-inspired ThinkPad logo, and TrackPoint cap options. Wrapped in a soft-touch black chassis and powered by contemporary hardware, the ThinkPad 25 stood as a collaborative tribute—shaped not only by Lenovo’s designers but also by a global community of fans. </p></div><div><h4>ThinkPad Design: Spirit &amp; Essence Booklets</h4><p>Originally written and designed for the 20th anniversary celebration held at the MoMA. The highly collectable work was updated in 2025 for the 25th anniversary limited edition ThinkPad T25. Both booklets document and illuminate David Hill's beliefs and philosophies that have shaped the design of ThinkPad for decades.</p></div><div><h4>Lenovo ThinkPad ThinkShutter</h4><p>The ThinkPad ThinkShutter is a simple, built-in mechanical privacy cover designed to give users instant control over their webcam. Sliding smoothly across the lens, it provides a clear visual indication when the camera is physically blocked, eliminating reliance on questionable software controls or LED indicators. It integrates cleanly into the display bezel adding negligible thickness. Achieving peace of mind with makeshift solutions such as masking tape, Post-it notes, and even clothespins are a thing of the past.</p></div>",
      "contentLength": 11205,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46665310"
    },
    {
      "title": "Show HN: Figma-use – CLI to control Figma for AI agents",
      "url": "https://github.com/dannote/figma-use",
      "date": 1768715748,
      "author": "dannote",
      "guid": 36826,
      "unread": true,
      "content": "<p>I'm Dan, and I built a CLI that lets AI agents design in Figma.</p><p>What it does: 100 commands to create shapes, text, frames, components, modify styles, export assets. JSX importing that's ~100x faster than any plugin API import. Works with any LLM coding assistant.</p><p>Why I built it: The official Figma MCP server can only read files. I wanted AI to actually design — create buttons, build layouts, generate entire component systems. Existing solutions were either read-only or required verbose JSON schemas that burn through tokens.</p><p>Tech stack: Bun + Citty for CLI, Elysia WebSocket proxy, Figma plugin. The render command connects to Figma's internal multiplayer protocol via Chrome DevTools for extra performance when dealing with large groups of objects.</p><p>Try it: bun install -g @dannote/figma-use</p><p>Looking for feedback on CLI ergonomics, missing commands, and whether the JSX syntax feels natural.</p>",
      "contentLength": 893,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46665169"
    },
    {
      "title": "jQuery 4",
      "url": "https://blog.jquery.com/2026/01/17/jquery-4-0-0/",
      "date": 1768710208,
      "author": "OuterVale",
      "guid": 36751,
      "unread": true,
      "content": "<p>On January 14, 2006, John Resig introduced a JavaScript library called jQuery at BarCamp in New York City. Now, 20 years later, the jQuery team is happy to announce the final release of jQuery 4.0.0. After a long development cycle and several pre-releases, jQuery 4.0.0 brings many improvements and modernizations. It is the first major version release in almost 10 years and includes some breaking changes, so be sure to read through the details below before upgrading. Still, we expect that most users will be able to upgrade with minimal changes to their code.</p><p>Many of the breaking changes are ones the team has wanted to make for years, but couldn’t in a patch or minor release. We’ve trimmed legacy code, removed some previously-deprecated APIs, removed some internal-only parameters to public functions that were never documented, and dropped support for some “magic” behaviors that were overly complicated.</p><p>As usual, the release is available on <a href=\"https://jquery.com/download/\">our CDN</a> and the npm package manager. Other third party CDNs will probably have it available soon as well, but remember that we don’t control their release schedules and they will need some time. Here are the highlights for jQuery 4.0.0.</p><p>jQuery 4.0 drops support for IE 10 and older. Some may be asking why we didn’t remove support for IE 11. We plan to remove support in stages, and the next step <a href=\"https://github.com/jquery/jquery/pull/5077\">will be released in jQuery 5.0</a>. For now, we’ll start by removing code specifically supporting IE versions older than 11.</p><p>We also dropped support for other very old browsers, including Edge Legacy, iOS versions earlier than the last 3, Firefox versions earlier than the last 2 (aside from Firefox ESR), and Android Browser. No changes should be required on your end. If you need to support any of these browsers, stick with jQuery 3.x.</p><p>jQuery 4.0 adds support for <a href=\"https://twitter.com/kkotowicz/status/1445713282128515074\">Trusted Types</a>, ensuring that HTML wrapped in <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/TrustedHTML\">TrustedHTML</a> can be used as input to jQuery manipulation methods in a way that doesn’t violate the <code>require-trusted-types-for</code> Content Security Policy directive. </p><p>Along with this, while some AJAX requests were already using  tags to maintain attributes such as , we have <a href=\"https://github.com/jquery/jquery/pull/4763\">since switched most asynchronous script requests to use &lt;script&gt; tags</a> to avoid any CSP errors caused by using inline scripts. There are still a few cases where XHR is used for asynchronous script requests, such as when the option is passed (use  instead!), but we now use a  tag whenever possible.</p><h2>jQuery source migrated to ES modules</h2><p>It was a special day when the jQuery source on the  branch was migrated from <a href=\"https://requirejs.org/docs/whyamd.html\">AMD</a> to <a href=\"https://github.com/jquery/jquery/pull/4541\">ES modules</a>. The jQuery source has always been published with jQuery releases on npm and GitHub, but could not be imported directly as modules without <a href=\"https://requirejs.org/\">RequireJS</a>, which was jQuery’s build tool of choice. We have since switched to <a href=\"https://rollupjs.org/introduction/\">Rollup</a> for packaging jQuery and we do run all tests on the ES modules separately. This makes jQuery compatible with modern build tools, development workflows, and browsers through the use of .</p>",
      "contentLength": 2974,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46664755"
    },
    {
      "title": "U.S. Court Order Against Anna's Archive Spells More Trouble for the Site",
      "url": "https://torrentfreak.com/u-s-court-order-against-annas-archive-spells-more-trouble-for-the-site/",
      "date": 1768708330,
      "author": "t-3",
      "guid": 36761,
      "unread": true,
      "content": "<p>First, the site lost control over its original annas-archive.org domain after the U.S.-based Public Interest Registry (PIR) placed it on <a href=\"https://www.icann.org/resources/pages/epp-status-codes-2014-06-16-en\"></a>. </p><p>PIR typically only takes these kinds of measures based on a court order. However, when we asked for more details, the registry <a href=\"https://torrentfreak.com/annas-archive-loses-org-domain-after-surprise-suspension/\">informed us</a> that it was “unable to comment on the situation at this time,” only adding to the mystery. </p><p>A few days ago, the domain trouble continued when Anna’s Archive’s .SE domain suddenly became unresponsive after being operational for years. For this domain, the registrar took action, as the site was put on . While we tried to get additional information from the registrar, our requests remained unanswered. </p><p>While it is clear that ‘something’ is going on, it’s not clear what. The troubles started not long after Anna’s Archive announced that it had <a href=\"https://torrentfreak.com/annas-archive-backed-up-spotify-plans-to-release-300tb-music-archive/\">backed up Spotify</a>, but there is no concrete link to a music industry push against the site.</p><h2>OCLC Seeks Permanent Injunction</h2><p>What we do know for certain is that Anna’s Archive’s troubles are not over yet. Yesterday, a federal court in Ohio issued a default judgment and permanent injunction against the site’s unidentified operator(s). </p><p>This order was requested by OCLC, which owns the proprietary WorldCat database that was <a href=\"https://torrentfreak.com/annas-archive-scraped-worldcat-to-help-preserve-all-books-in-the-world-231003/\">scraped and published</a> by Anna’s Archive more than two years ago. OCLC initially demanded millions of dollars in damages but eventually dropped this request, focusing on taking the site down through an injunction that would also apply to intermediaries. </p><p>“Anna’s Archive’s flagrantly illegal actions have damaged and continue to irreparably damage OCLC. As such, issuance of a permanent injunction is necessary to stop any further harm to OCLC,” the request read. </p><p>This pivot makes sense since Anna’s Archive did not respond to the lawsuit and would likely ignore all payment demands too. However, with the right type of court order, third-party services such as hosting companies and domain registrars might come along.</p><h2>Court Grants Default Judgment</h2><p>The permanent injunction, issued by U.S. District Court Judge Michael Watson yesterday, does not mention any third-party services by name. However, it is directed at all parties that are “in active concert and participation with” Anna’s Archive.</p><p>Specifically, the site’s operator and these third parties are prohibited from scraping WorldCat data, storing or distributing the data on Anna’s Archive websites, and encouraging others to store, use or share this data. </p><p>Additionally, the site has to delete all WorldCat data, which also includes all torrents.</p><p>Judge Watson denied the default judgment for ‘unjust enrichment’ and ‘tortious interference.’ However, he granted the order based on the ‘trespass to chattels’ and ‘breach of contract’ claims. </p><p>The latter is particularly noteworthy, as the judge ruled that because Anna’s Archive is a ‘sophisticated party’ that scraped the site daily, it had constructive notice of the terms and entered into a ‘<a href=\"https://en.wikipedia.org/wiki/Browsewrap\">browsewrap</a>‘ agreement simply by using the service.</p><p>While these nuances are important for legal experts, the result for Anna’s Archive is that it lost. And while there are no monetary damages, the permanent injunction can certainly have an impact.</p><p>It is expected that OCLC will use the injunction to motivate third-party intermediaries to take action against Anna’s Archive. </p><p>Whether intermediaries are considered in “active concert” with Anna’s Archive will differ based on who you ask. However, OCLC previously said that it intends to “take the\njudgment to website hosting services to remove WorldCat data from Anna’s Archive’s websites”.</p><p>The injunction that was issued yesterday obviously cannot explain the earlier domain name troubles. That said, it’s not unthinkable that OCLC will also send the injunction to domain registrars and registries, to add further pressure. </p><p> Anna’s Archive’s .IN domain appears to be unreachable.</p><p><em>A copy of the opinion and order issued by U.S. District Court Judge Michael Watson is available <a href=\"https://torrentfreak.com/images/anna-oclc-default-judgment.pdf\">here (pdf)</a>.</em></p>",
      "contentLength": 4063,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46664646"
    },
    {
      "title": "The longest Greek word",
      "url": "https://en.wikipedia.org/wiki/Lopado%C2%ADtemacho%C2%ADselacho%C2%ADgaleo%C2%ADkranio%C2%ADleipsano%C2%ADdrim%C2%ADhypo%C2%ADtrimmato%C2%ADsilphio%C2%ADkarabo%C2%ADmelito%C2%ADkatakechy%C2%ADmeno%C2%ADkichl%C2%ADepi%C2%ADkossypho%C2%ADphatto%C2%ADperister%C2%ADalektryon%C2%ADopte%C2%ADkephallio%C2%ADkigklo%C2%ADpeleio%C2%ADlagoio%C2%ADsiraio%C2%ADbaphe%C2%ADtragano%C2%ADpterygon",
      "date": 1768708169,
      "author": "firloop",
      "guid": 36760,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46664638"
    },
    {
      "title": "Erdos 281 solved with ChatGPT 5.2 Pro",
      "url": "https://twitter.com/neelsomani/status/2012695714187325745",
      "date": 1768708083,
      "author": "nl",
      "guid": 36750,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46664631"
    },
    {
      "title": "Profession by Isaac Asimov (1957)",
      "url": "https://www.abelard.org/asimov.php",
      "date": 1768702645,
      "author": "bkudria",
      "guid": 36776,
      "unread": true,
      "content": "<p>f course, Reading Day had been different. Partly, there was the \n        simple fact of childhood. A boy of eight takes many extraordinary things \n        in stride. One day you can’t read and the next day you can. That’s \n        just the way things are. Like the sun shining.</p><p>And then not so much depended upon it. There were no recruiters just \n        ahead, waiting and jostling for the lists and scores on the coming Olympics. \n        A boy or girl who goes through the Reading Day is just someone who has \n        ten more years of undifferentiated living upon Earth’s crawling \n        surface; just someone who returns to his family with one new ability.</p><p>By the time Education Day came, ten years later, George wasn’t \n        even sure of most of the details of his own Reading Day.</p><p>Most clearly of all, he remembered it to be a dismal September day with \n        a mild rain falling. (September for Reading Day; November for. Education \n        Day; May for Olympics. They made nursery rhymes out of it.) George had \n        dressed by the wall lights, with his parents far more excited than he \n        himself was. His father was a Registered Pipe Fitter and had found his \n        occupation on Earth. This fact had always been a humiliation to him, although, \n        of course, as anyone could see plainly, most of each generation must stay \n        on Earth in the nature of things.</p><p>There had to be farmers and miners and even technicians on Earth. It \n        was only the late-model, high-specialty professions that were in demand \n        on the Outworlds, and only a few millions a year out of Earth’s \n        eight billion population could be exported. Every man and woman on Earth \n        couldn’t be among that group.</p><p>But every man and woman could hope that at least one of his children \n        could be one, and Platen, Senior, was certainly no exception. It was obvious \n        to him (and, to be sure, to others as well) that George was notably intelligent \n        and quick-minded. He would be bound to do well and he would have to, as \n        he was an only child. If George didn’t end on an Outworld, they \n        would have to wait for grandchildren before a next chance would come along, \n        and that was too far in the future to be much consolation.</p><p>Reading Day would not prove much, of course, but it would be the only \n        indication they would have before the big day itself. Every parent on \n        Earth would be listening to the quality of reading when his child came \n        home with it; listening for any particularly easy flow of words and building \n        that into certain omens of the future. There were few families that didn’t \n        have at least one hopeful who, from Reading Day on, was the great hope \n        because of the way he handled his trisyllabics.</p><p>Dimly, George was aware of the cause of his parents’ tension, and \n        if there was any anxiety in his young heart that drizzly morning, it was \n        only the fear that his father’s hopeful expression might fade out \n        when he returned home with his reading.</p><p>The children met in the large assembly room of the town’s Education \n        Hall. All over Earth, in millions of local halls, throughout that month, \n        similar groups of children would he meeting. George felt depressed by \n        the grayness of the room and by the other children, strained and stiff \n        in unaccustomed finery.</p><p>Automatically, George did as all the rest of the children did. He found \n        the small clique that represented the children on his floor of the apartment \n        house and joined them.</p><p>Trevelyan, who lived immediately next door, still wore his hair childishly \n        long and was years removed from the sideburns and thin, reddish mustache \n        that he was to grow as soon as he was physiologically capable of it.</p><p>Trevelyan (to whom George was then known as Jawjee) said, “Bet \n        you’re scared.”</p><p>“I am not,’ said George. Then, confidentially, “My \n        folks got a hunk of printing up on the dresser in my room, and when I \n        come home, I’m going to read it for them.” (George’s \n        main suffering at the moment lay in the fact that he didn’t quite \n        know where to put his hands. He had been warned not to scratch his head \n        or rub his ears or pick his nose or put his hands into his pockets. This \n        eliminated almost every possibility.)</p><p>Trevelyan put hands in his pockets and said, “My father \n        isn’t worried.”</p><p>Trevelyan, Senior, had been a Metallurgist on Diporia for nearly seven \n        years, which gave him a superior social status in his neighborhood even \n        though he had retired and returned to Earth.</p><p>Earth discouraged these re-immigrants because of population problems, \n        but a small trickle did return. For one thing the cost of living was lower \n        on Earth, and what was a trifling annuity on Diporia, say, was a comfortable \n        income on Earth. Besides, there were always men who found more satisfaction \n        in displaying their success before the friends and scenes of their childhood \n        than before all the rest of the Universe besides.</p><p>Trevelyan, Senior further explained that if he stayed on Diporia, so \n        would his children, and Diporia was a one-spaceship world. Back on Earth, \n        his kids could end up anywhere, even Novia.</p><p>Stubby Trevelyan had picked up that item early. Even before Reading Day, \n        his conversation was based on the carelessly assumed fact that his ultimate \n        home would be in Novia.</p><p>George, oppressed by thoughts of the other’s future greatness and \n        his own small-time contrast, was driven to belligerent defense at once.</p><p>“My father isn’t worried either. He just wants to hear me \n        read because he knows I’ll be good. I suppose your father would \n        just as soon not hear you because he knows you’ll be all wrong.”</p><p>“I will not be all wrong. Reading is On Novia, \n        I’ll people to read to me.”</p><p>“Because you won’t be able to read yourself, on account of \n        you’re </p><p>“Then how come I’ll be on Novia?”</p><p>And George, driven, made the great denial, “Who says you’ll \n        be on Novia? Bet you don’t go anywhere.”</p><p>Stubby Trevelyan reddened. “I won’t be a Pipe Fitter like \n        your old man.”</p><p>“Take that back, you dumbhead.”</p><p>They stood nose to nose, not wanting to fight but relieved at having \n        something familiar to do in this strange place. Furthermore, now that \n        George had curled his hands into fists and lifted them before his face, \n        the problem of what to do with his hands was, at least temporarily, solved. \n        Other children gathered round excitedly.</p><p>But then it all ended when a woman’s voice sounded loudly over \n        the public address system. There was instant silence everywhere. George \n        dropped his fists and forgot Trevelyan.</p><p>“Children,” said the voice, “we are going to call out \n        your names. As each child is called, he or she is to go to one of the \n        men waiting along the side walls. Do you see them? They are wearing red \n        uniforms so they will be easy to find. The girls will go to the right. \n        The boys will go to the left. Now look about and see which man in red \n        is nearest to you —”</p><p>George found his man at a glance and waited for his name to be called \n        off. He had not been introduced before this to the sophistications of \n        the alphabet and the length of time it took to reach his own name grew \n        disturbing.</p><p>The crowd of children thinned; little rivulets made their way to each \n        of the red-clad guides.</p><p>When the name ‘George Platen’ was finally called, his sense \n        of relief was exceeded only by the feeling of pure gladness at the fact \n        that Stubby Trevelyan still stood in his place, uncalled.</p><p>George shouted back over his shoulder as he left, “Yay, Stubby, \n        maybe they don’t want you.”</p><p>That moment of gaiety quickly left. He was herded into a line and directed \n        down corridors in the company of strange children. They all looked at \n        one another, large-eyed and concerned, but beyond a snuffling, “Quitcher \n        pushing” and “Hey, watch out” there was no conversation.</p><p>They were handed little slips of paper which they were told must remain \n        with them. George stared at his curiously. Little black marks of different \n        shapes. He knew it to be printing but how could anyone make words out \n        of it? He couldn’t imagine.</p><p>He was told to strip; he and four other boys who were all that now remained \n        together. All the new clothes came shucking off and four eight-year-olds \n        stood naked and small, shivering more out of embarrassment than cold. \n        Medical technicians came past, probing them, testing them with odd instruments, \n        pricking them for blood. Each took the little cards and made additional \n        marks on them with little black rods that produced the marks, all neatly \n        lined up, with great speed. George stared at the new marks, but they were \n        no more comprehensible than the old. The children were ordered back into \n        their clothes.</p><p>They sat on separate little chairs then and waited again. Names were \n        called again and ‘George Platen’ came third.</p><p>He moved into a large room, filled with frightening instruments with \n        knobs and glassy panels in front. There was a desk in the very center, \n        and behind it a man sat, his eyes on the papers piled before him.</p><p>He said, “George Platen?”</p><p>“Yes, sir,said George, in a shaky whisper. All \n        this waiting and all this going here and there was making him nervous. \n        He wished it were over.</p><p>The man behind the desk said, “I am Dr Lloyd, George. How are you?”</p><p>The doctor didn’t look up as he spoke. It was as though he had \n        said those words over and over again and didn’t have to look up \n        any more.</p><p>“Are you afraid, George?”</p><p>“N — no, sir,” said George, sounding afraid even in \n        his own ears.</p><p>“That’s good,” said the doctor, “because there’s \n        nothing to be afraid of you know. Let’s see, George. It says here \n        on your card that your father is named Peter and that he’s a Registered \n        Pipe Fitter and your mother is named Amy and is a Registered Home Technician. \n        Is that right?”</p><p>“And your birthday is 13 February,and you had an ear infection \n        about a year ago. Right?”</p><p>“Do you know how I know all these things?”</p><p>“It’s on the card, I think, sir.”</p><p>“That’s right.” The doctor looked up at George for \n        the first time and smiled. He showed even teeth and looked much younger \n        than George’s father. Some of George’s nervousness vanished.</p><p>The doctor passed the card to George. “Do you know what all those \n        things there mean, George?”</p><p>Although George knew he did not he was startled by the sudden request \n        into looking at the card as though he might understand now through some \n        sudden stroke of fate. But they were just marks as before and he passed \n        the card back. “No, sir.”</p><p>George felt a sudden pang of suspicion concerning the sanity of this \n        doctor. Didn’t he know why not?</p><p>George said, “I can’t read, sir.”</p><p>“Would you like to read?”</p><p>George stared, appalled. No one had ever asked him that. He had no answer. \n        He said falteringly, “I don’t know, sir.”</p><p>“Printed information will direct you all through your life. There \n        is so much you’ll have to know even after Education Day. Cards like \n        this one will tell you. Books will tell you. Television screens will tell \n        you. Printing will tell you such useful things and such interesting things \n        that not being able to read would be as bad as not being able to see. \n        Do you understand?”</p><p>“Are you afraid, George?”</p><p>“Good. Now I’ll tell you exactly what we’ll do first. \n        I’m going to put these wires on your forehead just over the corners \n        of your eyes. They’ll stick there but they won’t hurt at all. \n        Then, I’ll turn on something that will make a buzz. It will sound \n        funny and it may tickle you, but it won’t hurt. Now if it does hurt, \n        you tell me, and I’ll turn it off right away, but it won’t \n        hurt. All right?”</p><p>George nodded and swallowed.</p><p>George nodded. He closed his eyes while the doctor busied himself. His \n        parents had explained this to him. They, too, had said it wouldn’t \n        hurt, but then there were always the older children. There were the ten- \n        and twelve-year-olds who howled after the eight-year-olds waiting for \n        Reading Day, “Watch out for the needle.” There were the others \n        who took you off in confidence and said, “They got to cut your head \n        open. They use a sharp knife that big with a hook on it,” and so \n        on into horrifying details.</p><p>George had never believed them but he had had nightmares, and now he \n        closed his eyes and felt pure terror.</p><p>He didn’t feel the wires at his temple. The buzz was a distant \n        thing, and there was the sound of his own blood in his ears, ringing hollowly \n        as though it and he were in a large cave. Slowly he chanced opening his \n        eyes.</p><p>The doctor had his back to him. From one of the instruments a strip of \n        paper unwound and was covered with a thin, wavy purple line. The doctor \n        tore off pieces and put them into a slot in another machine. He did it \n        over and over again. Each time a little piece of film came out which the \n        doctor looked at. Finally, he turned toward George with a queer frown \n        between his eyes.</p><p>George said breathlessly, “Is it over?”</p><p>The doctor said, “Yes,” but he was still frowning.</p><p>“Can I read now?’ asked George. He felt no different.</p><p>The doctor said, “What?” then smiled very suddenly and briefly. \n        He said, “It works fine, George. You’ll be reading in fifteen \n        minutes. Now we’re going to use another machine this time and it \n        will take longer. I’m going to cover your whole head, and when I \n        turn it on you won’t be able to see or hear anything for a while, \n        but it won’t hurt. Just to make sure I’m going to give you \n        a little switch to hold in your hand. If anything hurts, you press the \n        little button and everything shuts off. All right?”</p><p>In later years, George was told that the little switch was strictly a \n        dummy; that it was introduced solely for confidence. He never did know \n        for sure, however, since he never pushed the button.</p><p>A large smoothly curved helmet with a rubbery inner lining was placed \n        over his head and left there. Three or four little knobs seemed to grab \n        at him and bite into his skull, but there was only a little pressure that \n        faded. No pain.</p><p>The doctor’s voice sounded dimly. “Everything all right, \n        George?”</p><p>And then, with no real warning, a layer of thick felt closed down all \n        about him. He was disembodied, there was no sensation, no universe, only \n        himself and a distant murmur at the very ends of nothingness telling him \n        something — telling him — telling him —</p><p>He strained to hear and understand but there was all that thick felt \n        between.</p><p>Then the helmet was taken off his head, and the light was so bright that \n        it hurt his eyes while the doctor’s voice drummed at his ears.</p><p>The doctor said, “Here’s your card, George. What does it \n        say?”</p><p>George looked at his card again and gave out a strangled shout. The marks \n        weren’t just marks at all. They made up words. They were words just \n        as clearly as though something were whispering them in his ears. He could \n        hear them being whispered as he looked at them.</p><p>“What does it say, George?”</p><p>“It says — it says — ‘Platen, George. Born 13 \n        February 6492 of Peter and Amy Platen in...’” He broke off.</p><p>“You can read, George,” said the doctor. “It’s \n        all over.”</p><p>“For good? I won’t forget how?”</p><p>“Of course not” The doctor leaned over to shake hands gravely. \n        “You will be taken home now.”</p><p>It was days before George got over this new and great talent of his. \n        He read for his father with such facility that Platen, Senior, wept and \n        called relatives to tell the good news.</p><p>George walked about town, reading every scrap of printing he could find \n        and wondering how it was that none of it had ever made sense to him before.</p><p align=\"left\">He tried to remember how it was not to be able to read and \n        he couldn’t. As far as his feeling about it was concerned, he had \n        always been able to read. Always.</p>",
      "contentLength": 17015,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46664195"
    },
    {
      "title": "Computer Systems Security 6.566 / Spring 2024",
      "url": "https://css.csail.mit.edu/6.858/2024/",
      "date": 1768694983,
      "author": "barishnamazov",
      "guid": 36759,
      "unread": true,
      "content": "<p>\nThe lectures cover a\n\ntogether with a deeper focus on several topics:\n,\n,\n,\n,\nand\n.\n</p><p>\nLinks to notes etc. on future days are copies of materials from last year,\nto give you an idea of what the future will bring.  We will update the\nnotes as the course progresses.  The year of publication for class\nreadings are shown in parentheses.\n</p>",
      "contentLength": 334,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46663507"
    },
    {
      "title": "If you put Apple icons in reverse it looks like someone getting good at design",
      "url": "https://mastodon.social/@heliographe_studio/115890819509545391",
      "date": 1768693636,
      "author": "lateforwork",
      "guid": 36740,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46663338"
    },
    {
      "title": "No knives, only cook knives",
      "url": "https://kellykozakandjoshdonald.substack.com/p/no-knives-only-cook-knives",
      "date": 1768693101,
      "author": "firloop",
      "guid": 36775,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46663268"
    },
    {
      "title": "Light Mode InFFFFFFlation",
      "url": "https://willhbr.net/2025/10/20/light-mode-infffffflation/",
      "date": 1768688365,
      "author": "Fudgel",
      "guid": 36729,
      "unread": true,
      "content": "<p>Back in the day, light mode wasn’t called “light mode”. It was just the way that computers were, we didn’t really think about turning everything light or dark. Sure, some applications were often dark (photo editors, IDEs, terminals) but everything else was light, and that was fine.</p><p>What we didn’t notice is that light mode has been slowly getting lighter, and I’ve got a graph to prove it. I did what any normal person would do, I downloaded the same (or similar) screenshots from the <a href=\"https://512pixels.net/projects/aqua-screenshot-library/\">MacOS Screenshot Library</a> on <a href=\"https://512pixels.net/\"></a>. This project would have been much more difficult without a single place to get well-organised screenshots from. I cropped each image so just a representative section of the window was present, here shown with a pinkish rectangle:</p><p>Then used <a href=\"https://pypi.org/project/pillow/\">Pillow</a> to get the average lightness of each cropped image:</p><div><div><pre><code></code></pre></div></div><p>This ignores any kind of perceived brightness or the tinting that MacOS has been doing for a while based on your wallpaper colour. I could go down a massive tangent trying to work out exactly what the best way to measure this is, but given that the screenshots aren’t perfectly comparable between versions, comparing the average brightness of a greyscale image seems reasonable.</p><p>I graphed that on the release year of each OS version, doing the same for dark mode:</p><p>You can clearly see that the brightness of the UI has been steadily increasing for the last 16 years. The upper line is the default mode/light mode, the lower line is dark mode. When I started using MacOS in 2012, I was running Snow Leopard, the windows had an average brightness of 71%. Since then they’ve steadily increased so that in MacOS Tahoe, they’re at a full 100%.</p><p>What I’ve graphed here is just the brightness of the window chrome, which isn’t really representative of the actual total screen brightness. A better study would be looking at the overall brightness of a typical set of apps. The default background colour for windows, as well as the colours for inactive windows, would probably give a more complete picture.</p><p>For example, <a href=\"https://512pixels.net/projects/aqua-screenshot-library/macos-26-tahoe/\">in Tahoe</a> the darkest colour in a typical light-mode window is the colour of a section in an inactive settings window, at 97% brightness. In Snow Leopard the equivalent colour was 90%, and that was one of the  parts of the window, since the window chrome was typically darker than the window content.</p><p>I tried to remember exactly when I started using dark mode all the time on MacOS. I’ve always used a dark background for my editor and terminal, but I wasn’t sure when I swapped the system theme across. When it first came out I seem to remember thinking that it looked gross.</p><p>It obviously couldn’t be earlier than 2018, as that’s when dark mode was introduced in <a href=\"https://en.wikipedia.org/wiki/MacOS_Mojave\">MacOS Mojave</a>. I’m pretty sure that when I updated my personal laptop to an M1 MacBook Air at the end of 2020 that I set it to use dark mode. This would make sense, because the <a href=\"https://en.wikipedia.org/wiki/MacOS_Big_Sur\">Big Sur</a> update bumped the brightness from 85% to 97%, which probably pushed me over the edge.</p><p>I think the reason this happens is that if you look at two designs, photos, or whatever, it’s really easy to be drawn in to liking the brighter one more. Or if they’re predominantly dark, then the darker one. I’ve done it myself with this very site. If I’m tweaking the colours it’s easy to bump up the brightness on the background and go “ooh wow yeah that’s definitely cleaner”, then swap it back and go “ewww it looks like it needs a good scrub”. If it’s the dark mode colours, then a darker background will just look .</p><p>I’m not a designer, but I assume that resisting this urge is something you learn in design school. Just like making a website look good with a non-greyscale background.</p><p>This year in iOS 26, some UI elements use the HDR screen to make some elements and highlights brighter than 100% white. This year it’s reasonably subtle, but the inflation potential is there. If you’ve ever looked at an HDR photo on an iPhone (or any other HDR screen) then looked at the UI that’s still being shown in SDR, you’ll know just how grey and sad it looks. If you’re designing a new UI, how tempting will it be to make just a little bit more of it just a little bit brighter?</p><p>As someone whose job involves looking at MacOS for a lot of the day, I find that I basically  to use dark mode to avoid looking at a display where all the system UI is 100% white blasting in my eyes. But the alternative doesn’t have to be near-black for that, I would happily have a UI that’s a medium grey. In fact what I’ve missed since swapping to using dark mode is that I don’t have contrast between windows. Everything looks the same, whether it’s a text editor, IDE, terminal, web browser, or Finder window. All black, all the time.</p><p>Somewhat in the spirit of <a href=\"https://mavericksforever.com\">Mavericks Forever</a>, if I were to pick an old MacOS design to go back to it would probably be <a href=\"https://512pixels.net/projects/aqua-screenshot-library/os-x-10-10-yosemite/\">Yosemite</a>. I don’t have any nostalgia for skeuomorphic brushed metal or stitched leather, but I do quite like the flattened design and blur effects that Yosemite brought. Ironically Yosemite was a substantial jump in brightness from previous versions.</p><p>So if you’re making an interface or website, be bold and choose a 50% grey. My eyes will thank you.</p>",
      "contentLength": 5209,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46662662"
    },
    {
      "title": "Show HN: ChunkHound, a local-first tool for understanding large codebases",
      "url": "https://github.com/chunkhound/chunkhound",
      "date": 1768683832,
      "author": "NadavBenItzhak",
      "guid": 36746,
      "unread": true,
      "content": "<p>ChunkHound’s goal is simple: local-first codebase intelligence that helps you pull deep, core-dev-level insights on demand, generate always-up-to-date docs, and scale from small repos to enterprise monorepos — while staying free + open source and provider-agnostic (VoyageAI / OpenAI / Qwen3, Anthropic / OpenAI / Gemini / Grok, and more).</p><p>I’d love your feedback — and if you have, thank you for being part of the journey!</p>",
      "contentLength": 429,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46662078"
    },
    {
      "title": "Kip: A programming language based on grammatical cases of Turkish",
      "url": "https://github.com/kip-dili/kip",
      "date": 1768682692,
      "author": "nhatcher",
      "guid": 36728,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46661897"
    },
    {
      "title": "Canada's deal with China signals it is serious about shift from US",
      "url": "https://www.bbc.com/news/articles/cm24k6kk1rko",
      "date": 1768678144,
      "author": "breve",
      "guid": 36711,
      "unread": true,
      "content": "<div data-component=\"caption-block\"><figcaption>Watch: Canada-China trade relationship \"more predictable\" than with US, says Carney</figcaption></div><div data-component=\"text-block\"><p>Prime Minister Mark Carney's new approach to Canada's foreign policy can perhaps be distilled in one line: \"We take the world as it is, not as we wish it to be.\"</p><p>That was his response when asked about the deal struck with China on Friday, despite concerns over its human rights record and nearly a year after he called China \"the biggest security threat\" facing Canada. </p><p>The deal will see Canada ease tariffs on Chinese electric vehicles that it imposed in tandem with the US in 2024. In exchange, China will lower retaliatory tariffs on key Canadian agricultural products.</p><p>Experts told the BBC the move represents a significant shift in Canada's policy on China, one that is shaped by ongoing uncertainty with the US, its largest trade partner. </p><p>\"The prime minister is saying, essentially, that Canada has agency too, and that it's not going to just sit and wait for the United States,\" said Eric Miller, a Washington DC based trade adviser and president of the Rideau Potomac Strategy Group.</p></div><div data-component=\"text-block\"><p><a target=\"_self\" href=\"https://www.bbc.com/news/articles/cy59pvkqvl5o\">Carney told reporters on Friday</a> that \"the world has changed\" in recent years, and the progress made with China sets Canada up \"well for the new world order\".</p><p>Canada's relationship with China, he added, had become \"more predictable\" than its relationship with the US under the Trump administration. </p><p>He later wrote, in a social media post, that Canada was \"recalibrating\" its relationship with China, \"strategically, pragmatically, and decisively\".</p></div><div data-component=\"text-block\"><p>In Canada, as daylight broke on Friday, reaction to the deal was swift.</p><p>Some, like Saskatchewan Premier Scott Moe, hailed it as \"very good news\". Farmers in Moe's province have been hard hit by China's retaliatory tariffs on Canadian canola oil, and the deal, he said, would bring much needed relief.</p><p>But Ontario Premier Doug Ford, whose province is home to Canada's auto sector, was sharply critical of the deal. He said removing EV tariffs on China \"would hurt our economy and lead to job losses\". </p><p>In a post on X, Ford said Carney's government was \"inviting a flood of cheap made-in-China electric vehicles without any real guarantees of equal or immediate investment in Canada's economy\". </p><p>Some experts said the electric vehicle provisions in the trade deal would help China make inroads into the Canadian automobile market.</p><p>With the lower EV tariffs, approximately 10% of Canada's electric vehicle sales are now expected to go to Chinese automakers, said Vivek Astvansh, a business professor at McGill University in Montreal.</p><p>The expected increase in Chinese EV sales could put pressure on US-based EV makers like Tesla which are seeking to expand their market share in Canada, he said.</p><p>\"Carney has signalled to the Trump administration that it is warming up to China,\" Astvansh added.</p><p>Reaction from the White House, meanwhile, has been mixed.</p><p>In an interview with CNBC on Friday morning, US trade representative Jamieson Greer called the deal \"problematic\" and said Canada may come to regret it. </p><p>President Donald Trump, however, hailed it as \"a good thing\".</p><p>\"If you can get a deal with China, you should do that,\" he told reporters outside the White House.</p><p>Since taking office for a second time last year, Trump has imposed tariffs on Canadian sectors like metals and automotives, which has led to swirling economic uncertainty. He has also threatened to rip up a longstanding North American free trade agreement between Canada, the US and Mexico, calling it \"irrelevant\".</p><p>That trade agreement, the USMCA, is now under a mandatory review. Canada and Mexico have both made clear they want it to remain in place. </p><p>But the decision to carve out a major new deal with China is a recognition by Carney that the future of North American free trade remains unclear, Miller of the Rideau Potomac Strategy Group told the BBC.</p><p>\"There's a reasonable chance that we could end up in 2026 without a meaningful, workable trade deal with the United States,\" he said. \"And Canada needs to be prepared.\"</p></div><div data-component=\"text-block\"><p>The deal with China drops Canada's levies on Chinese EVs from 100% to 6.1% for the first 49,000 vehicles imported each year. That quota could rise, Carney said, reaching 70,000 in half a decade.</p><p>Canada and the US put levies on Chinese EVs in 2024, arguing that China was overproducing vehicles and undermining the ability of other countries to compete. </p><p>China is the world's largest producer of EVs, accounting for 70% of global production. </p><p>In exchange, China will cut tariffs on Canadian canola seed to around 15% by 1 March, down from the current rate of 84%. Carney said Beijing had also committed to removing tariffs on Canadian canola meal, lobsters, crabs and peas \"until at least the end of the year\". </p><p>China also committed to removing visa requirements for Canadian visitors, Carney said.</p><p>Beijing did not corroborate the details in a separate statement, but said \"the two reached a preliminary joint agreement on addressing bilateral economic and trade issues\". </p><p>The introduction of Chinese EVs to Canada's market will likely mean cheaper prices for Canadian consumers, said Gal Raz, an associate professor of Operations Management and Sustainability at Western University and an expert on the EV supply chain. </p><p>But Raz acknowledged that the deal Canada struck could hurt Canadian car manufacturers if it comes without further action from the Carney government to help the domestic sector. </p><p>He said it was the result of an \"unfortunate\" deterioration of the Canada-US trade relationship, which he noted has also hurt Canada's automotive industry.</p><p>\"The US has really put Canada in a corner,\" he said.</p><p>Asked why Canada is giving China access to its automotive market, Carney said that China produces \"some of the most affordable and energy-efficient vehicles in the world\". He said he expects the deal will spur Chinese investment into Canada's auto industry, though he did not provide further details.</p><p>Trump himself has signalled openness to China building plants in the US if it means creating more jobs for Americans, despite his tough-on-China stance.</p><p>\"If they want to come in and build a plant and hire you and hire your friends and your neighbours, that's great, I love that,\" Trump said at the Detroit Economic Club on Tuesday. \"Let China come in, let Japan come in.\"</p><p>The US president is notably headed to Beijing for his own meeting with President Xi Jinping in April. He has also invited Xi for a state visit to Washington. </p><p>For Carney, though, Friday's deal may just be the first step in a \"recalibration\" of Canada's trade relations.</p><p><i><b>With additional reporting from Daniel Bush in Washington</b></i></p></div>",
      "contentLength": 6587,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46661217"
    },
    {
      "title": "The thing that brought me joy",
      "url": "https://www.stephenlewis.me/blog/the-thing-that-brought-me-joy/",
      "date": 1768675359,
      "author": "monooso",
      "guid": 36749,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46660663"
    },
    {
      "title": "Raising money fucked me up",
      "url": "https://blog.yakkomajuri.com/blog/raising-money-fucked-me-up",
      "date": 1768674540,
      "author": "yakkomajuri",
      "guid": 36739,
      "unread": true,
      "content": "<p>About four months ago I quit my job at <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://doublepoint.com/\">Doublepoint</a> and decided to start my own thing.</p><p>I'd been working on a little project with Pedrique (who would become my co-founder) for a bit over half-a-year and decided I had enough signal to determine he was someone I wanted to start a business with.</p><p>I was excited about the idea we were working on at the time (we were live with paying customers and truly believed in the thesis), but in hindsight, being truly honest about my motivations, I mostly wanted to run my own thing. In a dream world I'd have had the \"idea of my life\" while working at <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://posthog.com\">PostHog</a> or <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://doublepoint.com/\">Doublepoint</a> and have gone on to build that with maximum conviction but this wasn't the case, so I got tired of waiting for a spark and decided to go out and make it happen, with the idea we were working on being our best bet at the time.</p><p>Since I'd just quit my job, I had my finances well in order. Thus, my ideal scenario would have been to keep working on the product we had, try to scale it, and if that didn't work, try something else, then something else, until something did indeed really get off the ground, and only at that point we would consider whether or not to raise VC funding, depending on whether it made sense or not.</p><p>My ideal scenario wasn't going to work for Pedrique, though. He had told me for a while that the money he had saved up for trying to build his own thing was running out and that soon he'd need to start freelancing or something to make some income in order to sustain the search for a little longer. Prior to us working together, he had a bit of success with his MicroSaaS products but only just enough to increase his personal runway, which was now reasonably short.</p><p>We had spoken about this before, but with me now being 110% in, we had to do something about it. I had just come in full-time so we weren't about to go back to a dynamic where one person was full-time and the other part-time because they needed to make ends meet. The decision then became clear: we're gonna raise.</p><p>At that point, it was an easy decision to make. Again, we have two co-founders who have a lot of confidence in each other, and we don't want to let the opportunity pass us by. So while this wasn't my ideal choice, we were a business now and this was the best decision for the company. \"Just don't die\" goes the advice I think, and Skald had just then been born.</p><p>And so raise we did. We brought in four phenomenal angels, including, and this is relevant, my last few bosses (PostHog co-founders James and Tim and Doublepoint co-founder Ohto), and then decided to look for an early-stage fund. We eventually landed with <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://www.broom.ventures/\">Broom Ventures</a> and passed up on a few other opportunities to limit dilution.</p><p>Great, right? I didn't need a salary yet, but for equality purposes, I now had one. Our investors are amazing. James and Ohto have been particularly helpful as angels (thank you!), and our investors are all founders of successful companies, including Jeff and Dan, the Broom GPs. We're super early, but Broom has been massively helpful and all-around just a great hands-off VC to deal with.</p><p>Most importantly, none of them put  pressure on us. All understand the nature of pre-seed investing well, and that can't be said about all the potential investors we took meetings with.</p><p>So some time passes and we decide to pivot. We're really excited about the new idea. We launch and get a bit of early traction. The open source project is doing well, but we're struggling to monetize. We fail to close a few customers and the traction wanes a bit.</p><p>Then I find myself fucked in the head.</p><p>And here's where we get to the point that I'm not sure I should be talking publicly about. Does this hurt my image a bit? Maybe. Do I look like I'm not cut for this? Potentially. But I've always appreciated when people share about the process rather than just talking about things in hindsight, and reflecting while things are happening + being super transparent publicly is <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://memoirsandrambles.substack.com/\">how I am</a>. You're witnessing my growth, live, as I type these words.</p><p>Anyway, so what happened is I found myself spending days with my head spinning, searching for ideas. I'm angry, I'm annoyed, and I'm not being super productive.</p><p>As I dug deeper into these feelings, I realized I was feeling pressured. We weren't making that much money, we weren't growing super fast. Then you look around and see \"startup X gets to $1M ARR a month after launch\" and shit like that and I'm feeling terrible about how we're barely growing. I'm thinking people that I really respect and admire have placed a bet on me and I'm letting them down.</p><p>Except they're not saying this, I am.</p><p>There's an interesting reflection that came up in a discussion between me and my girlfriend a few months prior that I realized applied to me, but in reverse. It's much more comfortable to be the person that \"could be X\" than to be the person that tries to actually do it. We were speaking about this regarding people who have a clear innate talent for something like music or sports but don't practice at all. Everyone says things like \"you'd be the best at this if you just practiced more\" but then they never do.</p><p>The thing is: it's a lot easier to live your life thinking you could have done X if you wanted to, than to \"disappoint\" these people that believed in you by trying and failing. You can always lean on this idea in your head of what you could have been, and how everyone believed in you so it must be true, but you just  not to follow that path.</p><p>In my case, I found myself on the other side of that coin. Throughout my career, I've always had really high ownership roles, and have been actively involved in a couple 0 to 1 journeys. This led me to get many comments about how great of a founder I'd be or how I have the \"founder profile\". I led teams, I wore a bunch of different hats, I worked hard as fuck, and I always thought about the big picture.</p><p>Those traits led my former bosses to then invest in me, and now suddenly I have to, in my head, live up to all of this. I can no longer take solace in some excuse like \"I could have been a founder but working full-time was the best financial decision (it almost always is) so I never started my own thing\". I set foot down a path from which there's no return. I've begun my attempt. I can of course stop and try again later. But from now on, I'm either gonna be a successful founder, or I'm not. And if I'm not, I'll have to deal with having broken with the expectations that people had of me.</p><p>There's a lot to unpack here, including what \"success\" means, and how most of what I say are other people's expectations are actually my own projected onto them (I've learned this about my relationship with my father too), but this post is already a bit too long so I'll save those for another time.</p><p>But the whole point here is not just that having raised this money from friends my head got a bit messy, but that I started to actually operate in a way that is counterproductive for my startup, while thinking I was actually doing what was best.</p><p>Ideas we considered when pivoting were looked more through a lens of \"how big does this feel\" rather than \"what problem does this solve and for who\". The slow growth was eating me, and while slow growth is terrible and can be a sign that you're on the wrong path it needs to be looked at from an objectively strategic lens. Didn't we say we were going to build an open source community and only later focus on monetization? Is that a viable strategy? Do we actually have a sound plan? Those were the things I should have been thinking about, rather than looping on \"we need something that grows faster\".</p><p>The people who invested in us, invested in us, not whatever idea we pitched them. And the best thing we can do is to follow our own process for building a great business based on what we believe and know, rather than focusing on making numbers look good so I can feel more relieved the next time I send over an investor update.</p><p>We have a ton to learn, particularly about sales (since we're both engineers), so we can't just be building shit for the sake of building shit because that's our comfort zone. But if our process is slower than company X on TechCrunch, that's fine. It's a marathon after all.</p><p>So after probably breaking many rules about what a founder should talk about publicly, what was my whole goal here? I mean, the main thing for me with posts like this is to get things off my chest. I've always said that the reason I publish writing that includes <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://yakkomajuri.com/poems/fim-e-comeco\">poems about my breakup</a>, <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://memoirsandrambles.substack.com/p/twin-in-berlin\">stories about falling in love</a>, <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://memoirsandrambles.substack.com/p/ill-probably-be-poor-soon\">posts about my insecurities</a>, and <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://memoirsandrambles.substack.com/p/eiger-dreams\">reflections about my dreams</a> is that by there being the possibility of someone reading them (because technically it could be the case that nobody does) I can truly be who I really am in my day-to-day life. If I'm ok with there being the possibility of a friend I'll meet later today having read about how I felt during my last breakup, I can be myself with them without reservations, because I've made myself available to be seen. That's always been really freeing to me.</p><p>As a side effect, I'd hope that if this does get read by some people, particularly those starting or looking to start a business, that they can reflect about themselves, their lives, and their companies through listening to my story. I thought about writing a short bullet list about lessons I learned from raising money and dealing with its aftermath here, but honestly, that's best left to the reader to figure out. We're all different, and how one person reacts to a set of circumstances will differ from someone else. Some people don't feel pressure at all, or at least not from friends or investors. Or they only respond positively to pressure (because it certainly has benefits too). Maybe they're better off than me. Maybe they're not.</p><p>This is my story, after all. I wish you the best of luck with yours.</p><p>P.S. I'm doing good now. I'm motivated and sharp. If someone finds themselves in a similar situation, feel free to shoot me an email if you're keen to talk. Happy to go over what was useful for me, which fell outside of the scope of this post.</p>",
      "contentLength": 10092,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46660543"
    },
    {
      "title": "Earth is warming faster. Scientists are closing in on why",
      "url": "https://www.economist.com/science-and-technology/2024/12/16/earth-is-warming-faster-scientists-are-closing-in-on-why",
      "date": 1768672086,
      "author": "andsoitis",
      "guid": 36702,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46660134"
    },
    {
      "title": "Show HN: What if your menu bar was a keyboard-controlled command center?",
      "url": "https://extrabar.app/",
      "date": 1768671081,
      "author": "pugdogdev",
      "guid": 36825,
      "unread": true,
      "content": "<div x-show=\"activeUseCase === 'designers'\" x-transition:enter=\"transition ease-out duration-300 delay-200\" x-transition:enter-start=\"opacity-0\" x-transition:enter-end=\"opacity-100\" x-transition:leave=\"transition ease-in duration-200\" x-transition:leave-start=\"opacity-100\" x-transition:leave-end=\"opacity-0\"><p>Part of a team? Open your Slack channel directly from the menu.</p></div><div x-show=\"activeUseCase === 'developers'\" x-transition:enter=\"transition ease-out duration-300 delay-200\" x-transition:enter-start=\"opacity-0\" x-transition:enter-end=\"opacity-100\" x-transition:leave=\"transition ease-in duration-200\" x-transition:leave-start=\"opacity-100\" x-transition:leave-end=\"opacity-0\"><p>Client call? Hit a hotkey and jump straight into your Zoom call.</p></div><div x-show=\"activeUseCase === 'managers'\" x-transition:enter=\"transition ease-out duration-300 delay-200\" x-transition:enter-start=\"opacity-0\" x-transition:enter-end=\"opacity-100\" x-transition:leave=\"transition ease-in duration-200\" x-transition:leave-start=\"opacity-100\" x-transition:leave-end=\"opacity-0\"><p>Running late? Join your Zoom or Google Meet directly from the menu bar.</p></div><div x-show=\"activeUseCase === 'power-users'\" x-transition:enter=\"transition ease-out duration-300 delay-200\" x-transition:enter-start=\"opacity-0\" x-transition:enter-end=\"opacity-100\" x-transition:leave=\"transition ease-in duration-200\" x-transition:leave-start=\"opacity-100\" x-transition:leave-end=\"opacity-0\"><p>One hotkey to rule them all, One hotkey to find them, One hotkey to launch them all, and in the menu bind them.</p></div>",
      "contentLength": 309,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46659943"
    },
    {
      "title": "2025 was the third hottest year on record",
      "url": "https://www.economist.com/science-and-technology/2026/01/14/2025-was-the-third-hottest-year-on-record",
      "date": 1768670971,
      "author": "andsoitis",
      "guid": 36701,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46659913"
    },
    {
      "title": "Eight European countries face 10% tariff for opposing US control of Greenland",
      "url": "https://apnews.com/article/denmark-greenland-us-trump-4ad99ea3975a8b62d37bd04961feda55",
      "date": 1768669413,
      "author": "2OEH8eoCRo0",
      "guid": 36744,
      "unread": true,
      "content": "<p>WEST PALM BEACH, Fla. (AP) —  said Saturday that he would charge a 10% import tax starting in February on goods from eight European nations because of their opposition to , setting up a potentially dangerous test of U.S. partnerships in Europe.</p><p>Denmark, Norway, Sweden, France, Germany, the United Kingdom, the Netherlands and Finland would face the tariff, Trump said in a social media post while at his golf club in West Palm Beach, Florida. The rate would climb to 25% on June 1 if no deal was in place for “the Complete and Total purchase of Greenland” by the United States, he said.</p><p>The Republican president appeared to indicate that he was using the tariffs as leverage to force talks with Denmark and other European countries over the status of Greenland, a semiautonomous territory of  Denmark that he regards as critical to U.S. national security.</p><p>“The United States of America is immediately open to negotiation with Denmark and/or any of these Countries that have put so much at risk, despite all that we have done for them,” Trump said on Truth Social.</p><p>The tariff threat could mark a problematic rupture between Trump and America’s longtime NATO partners, further straining an alliance that dates to 1949 and provides a collective degree of security to Europe and North America. Trump has repeatedly tried to use trade penalties to bend allies and rivals alike to his will, generating investment commitments from some nations and pushback from others, notably China.</p><p>Trump is scheduled to travel on Tuesday to the World Economic Forum in Davos, Switzerland, where he likely will run into the European leaders he just threatened with tariffs that would start in little more than two weeks.</p><p>Danish Foreign Minister Lars Løkke Rasmussen said Trump’s move was a “surprise” given the “constructive meeting” with top U.S. officials this week in Washington.</p><p>The European Commission’s president, Ursula von der Leyen, and the head of the European Council, Antonio Costa, said in a joint statement that tariffs “would undermine transatlantic relations and risk a dangerous downward spiral.” They said Europe would remain “committed to upholding its sovereignty.”</p><p>There are immediate questions about how the White House could try to implement the tariffs because the EU is a single economic zone in terms of trading, according to a European diplomat who was not authorized to comment publicly and spoke on the condition of anonymity. It was unclear, too, how Trump could act under U.S. law, though he could cite emergency economic powers that are currently subject to a  challenge. </p><p>The president indicated the tariffs were retaliation for what appeared to be the deployment of s  to Greenland, which he has said was essential for the “Golden Dome” missile defense system for the U.S., He also has argued that Russia and China might try to take over the island.</p><p>The U.S. already has access to Greenland under a 1951 defense agreement. Since 1945, the  in Greenland has decreased from thousands of soldiers over 17 bases and installations to 200 at the remote Pituffik Space Base in the northwest of the island, the Danish foreign minister has said. That base supports missile warning, missile defense and space surveillance operations for the U.S. and NATO.</p><p>Resistance has steadily built in Europe to Trump’s ambitions even as several countries on the continent agreed to his 15% tariffs last year in order to preserve an economic and security relationship with Washington.</p><p>French President Emmanuel Macron, in a social media post, seemed to equate the tariff threat to Russian leader Vladimir Putin’s war in Ukraine.</p><p>“No intimidation or threats will influence us, whether in Ukraine, Greenland or anywhere else in the world when we are faced with such situations,” Macron said in a translated post on X. </p><h2>‘Important for the whole world’</h2><p>Earlier Saturday, hundreds of people in Greenland’s capital, Nuuk, braved near-freezing temperatures, rain and icy streets to  in support of their own self-governance.</p><p>Thousands of people also marched through Copenhagen, many of them carrying Greenland’s flag. Some held signs with slogans such as “Make America Smart Again” and “Hands Off.”</p><p>“This is important for the whole world,” Danish protester Elise Riechie told The Associated Press as she held Danish and Greenlandic flags. “There are many small countries. None of them are for sale.”</p><p>The rallies occurred hours after a bipartisan delegation of U.S. lawmakers, while visiting Copenhagen, sought to reassure Denmark and Greenland of their support.</p><h2>European training exercises</h2><p>Danish Maj. Gen. Søren Andersen, leader of the Joint Arctic Command, told the AP that Denmark does not expect the U.S. military to attack Greenland, or any other NATO ally, and that European troops were recently deployed to Nuuk for Arctic defense training.</p><p>He said the goal is not to send a message to the Trump administration, even though the White House has not ruled out .</p><p>“I will not go into the political part, but I will say that I would never expect a NATO country to attack another NATO country,” he said from aboard a Danish military vessel docked in Nuuk. “For us, for me, it’s not about signaling. It is actually about training military units, working together with allies.”</p><p>The Danish military organized a planning meeting Friday in Greenland with NATO allies, including the U.S., to discuss Arctic security on the alliance’s northern flank in the face of a potential Russian threat. The Americans were also invited to participate in Operation Arctic Endurance in Greenland in the coming days, Andersen said.</p><p>In his 2½ years as a commander in Greenland, Andersen said that he hasn’t seen any Chinese or Russian combat vessels or warships, despite Trump saying that they were off the island’s coast.</p><p>But in the unlikely event of American troops using force on Danish soil, Andersen confirmed that Danish soldiers have an obligation to fight back.</p><h2>‘Almost no better’ ally to US than Denmark</h2><p>Trump has contended that China and Russia have their own designs on Greenland and its vast untapped reserves of critical minerals. He said recently that anything less than the Arctic island being in U.S. hands would be “unacceptable.” </p><p>The president has seen tariffs as a tool to get what he wants without having to resort to military actions. At the White House on Friday, he recounted how he had threatened European allies with tariffs on pharmaceuticals and he teased the possibility of doing so again.</p><p>“I may do that for Greenland, too,” Trump said.</p><p>After Trump followed through, Rep. Don Bacon, R-Neb., said “Congress must reclaim tariff authorities” so that they are not used solely at a president’s discretion. </p><p>Denmark said this week that it was increasing its military presence in Greenland in cooperation with allies.</p><p>“There is almost no better ally to the United States than Denmark,” said Sen. Chris Coons, D-Del., while visiting Copenhagen with other members of Congress. “If we do things that cause Danes to question whether we can be counted on as a NATO ally, why would any other country seek to be our ally or believe in our representations?”</p><p>Burrows reported from Nuuk, Greenland, and Niemann from Copenhagen, Denmark. Associated Press writers Stefanie Dazio in Berlin, Aamer Madhani in Washington, Jill Lawless in London and Kwiyeon Ha and Evgeniy Maloletka in Nuuk contributed to this report.</p>",
      "contentLength": 7467,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46659651"
    },
    {
      "title": "An Elizabethan mansion's secrets for staying warm",
      "url": "https://www.bbc.com/future/article/20260116-an-elizabethan-mansions-secrets-for-staying-warm",
      "date": 1768668804,
      "author": "Tachyooon",
      "guid": 36727,
      "unread": true,
      "content": "<p>\"The incredible thing about Hardwick [new Hall] is… when you set it on the compass, it's almost exactly north-south,\" says Ranald Lawrence, a lecturer in architecture at the University of Liverpool in the UK. He's also published papers on Hardwick's <a target=\"_blank\" href=\"https://doi.org/10.1080/13602365.2021.1962389\">design</a> and <a target=\"_blank\" href=\"https://doi.org/10.1080/00038628.2020.1856032\">thermal comfort</a>. \"And,\" he adds, \"the whole internal planning of the [new] house is then based around that geometry.\"</p><p>Bess moved around the rooms, following the Sun's path. Her mornings were spent walking the 63m (200ft) east-facing Long Gallery, where the bright morning light hits. The afternoon and evening Sun illuminates the south-western flank of the building, where Bess' bed chambers were. And the darkest, coldest corner of the house in the north-west was where the kitchens were placed, which would have been handy in keeping food cool and fresh.</p><p>I experience this first hand as I walk around – the kitchens are much colder. Elena Williams, the senior house and collections manager at The National Trust, a UK charity which preserves historic sites, notices too. \"It's a well-designed building that is also designed around comfort and that uses the natural environment to do that,\" she says.</p><h2></h2><p>It's not just the orientation that helps keep the house warm. As Williams shows me around, she points out that some of the windows on the north of the building are actually \"blind\" or fake. She explains that on the outside, there is a window, but on the inside, it's lined with lead and blocked up. Unlike south-facing windows, north-facing windows bring little thermal benefit, even in summer, Lawrence says.</p><p>Pretty much all the fireplaces I see are also built on the central spine of the building, meaning not much heat would be lost to the windows or exterior wall. It's not until we take a door through this spine that I realise that the girth of it is staggering – 1.37m (4.5ft) thick. This is yet another trick to keep its inhabitants warm.</p>",
      "contentLength": 1911,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46659550"
    },
    {
      "title": "The recurring dream of replacing developers",
      "url": "https://www.caimito.net/en/blog/2025/12/07/the-recurring-dream-of-replacing-developers.html",
      "date": 1768660293,
      "author": "glimshe",
      "guid": 36710,
      "unread": true,
      "content": "<h2>The Pattern That Frustrates Everyone</h2><div><p>07.12.2025, </p><a href=\"https://www.caimito.net/en/about.html\"><img src=\"https://gravatar.com/avatar/663d11426b0a187ddac59f8c17ce61b4?s=120&amp;d=robohash&amp;r=x\"></a><p>Every decade brings new promises: this time, we'll finally make software development simple enough that we won't need so many developers. From COBOL to AI, the pattern repeats. Business leaders grow frustrated with slow delivery and high costs. Developers feel misunderstood and undervalued. Understanding why this cycle persists for fifty years reveals what both sides need to know about the nature of software work.</p></div><h2>The Dream Was Born During Humanity’s Greatest Achievement</h2><p>When Neil Armstrong stepped onto the lunar surface in 1969, the world witnessed what organized human ingenuity could accomplish. Behind that achievement stood Margaret Hamilton and her team, writing Apollo’s guidance software by hand, catching critical errors through careful review, and proving that software could be mission-critical.</p><p>The Apollo program demonstrated that software development was essential to achieving the impossible. Yet it also revealed something that would frustrate business leaders for decades to come: writing software required specialized knowledge, intense focus, and significant time investment. The dream of making it easier—of needing fewer of these expensive specialists—began almost immediately.</p><h2>COBOL: Business People Will Write Their Own Programs</h2><p>The late 1960s and 1970s saw COBOL emerge with an explicit goal stated in its name: Common Business-Oriented Language. The vision was clear: make the language read like English sentences, and business analysts would write their own programs. No need for specialized programmers.</p><div>\n\"If we make the syntax readable enough, anyone who understands the business can write the code.\"\n</div><p>This vision had genuine appeal. Software was becoming essential to business operations, yet programmers remained a scarce, expensive resource. COBOL promised to democratize software creation.</p><p>What happened instead? COBOL became another programming language requiring specialized training. Business analysts who tried to write COBOL quickly discovered that readable syntax didn’t eliminate the complexity of logic, data structures, or system design. A new class of COBOL programmers emerged, and the dream of eliminating specialized developers remained unfulfilled.</p><p>Yet the dream didn’t die. It simply waited for the next technological wave.</p><p>Computer-Aided Software Engineering tools arrived in the 1980s with tremendous promise. Draw flowcharts and entity-relationship diagrams, and the tool would generate working code. The marketing message resonated: visual design was more intuitive than typing cryptic commands. Business experts could model their processes, and software would materialize.</p><p>Organizations invested heavily. Vendors promised productivity increases of 10x or more. Yet most CASE tool initiatives struggled or failed outright.</p><p>The generated code often required substantial manual intervention. Performance problems emerged. Maintenance became a nightmare when generated code diverged from the visual models. Most critically, drawing accurate diagrams required understanding the same logical complexity that programming demanded. The tool changed the interface but not the fundamental challenge.</p><p>Once again, the problem proved more stubborn than the solution.</p><h2>Visual Basic and Delphi: Drag, Drop, Done</h2><p>The 1990s brought a different approach. Microsoft’s Visual Basic and Borland’s Delphi made building user interfaces dramatically easier. Drag components onto a form, set properties, write event handlers. Suddenly, creating a Windows application felt achievable for developers with modest experience.</p><p>This wave succeeded differently than COBOL or CASE tools. These environments acknowledged that programming knowledge was still necessary, but they reduced the barrier to entry. A broader range of people could create useful applications.</p><p>Yet the dream of eliminating developers persisted. “Power users” and “citizen developers” would build departmental applications. IT departments could focus on infrastructure while business units solved their own software needs.</p><p>Reality proved more nuanced. Simple applications were indeed accessible to more people. But as requirements grew in complexity—integration with existing systems, security considerations, performance under load, long-term maintenance—the need for experienced developers became evident. The tools expanded who could write software, but they didn’t eliminate the expertise required for substantial systems.</p><p>And so the cycle continued into the new millennium.</p><h2>The 2000s and Beyond: Web Frameworks, Low-Code, and No-Code</h2><p>Each subsequent decade introduced new variations. Ruby on Rails promised convention over configuration. Low-code platforms offered visual development with minimal coding. No-code platforms claimed to eliminate programming entirely for common business applications.</p><p>Each wave delivered real value. Development genuinely became faster in specific contexts. More people could participate in creating software solutions. Yet professional software developers remained essential, and demand for their skills continued growing rather than shrinking.</p><p>Which brings us to the question: why does this pattern repeat?</p><p>The recurring pattern reveals something important about how we think about complexity. Software development looks like it should be simple because we can describe what we want in plain language. “When a customer places an order, check inventory, calculate shipping, process payment, and send a confirmation email.” That description sounds straightforward.</p><p>The complexity emerges in the details. What happens when inventory is temporarily reserved by another order? How do you handle partial payments? What if the email service is temporarily unavailable? Should you retry? How many times? What if the customer’s session expires during checkout? How do you prevent duplicate orders?</p><p>Each answer leads to more questions. The accumulated decisions, edge cases, and interactions create genuine complexity that no tool or language can eliminate. Someone must think through these scenarios. That thinking is software development, regardless of whether it’s expressed in COBOL, a CASE tool diagram, Visual Basic, or an AI prompt.</p><p>Which brings us to today’s excitement.</p><h2>AI: The Latest Chapter in a Long Story</h2><p>Today’s AI coding assistants represent the most capable attempt yet to assist with software creation. They can generate substantial amounts of working code from natural language descriptions. They can explain existing code, suggest improvements, and help debug problems.</p><p>This represents genuine progress. The assistance is real and valuable. Experienced developers use these tools to work more efficiently. People learning to code find the interactive guidance helpful.</p><p>Yet we’re already seeing the familiar pattern emerge. Initial excitement about AI replacing developers is giving way to a more nuanced understanding: AI changes how developers work rather than eliminating the need for their judgment. The complexity remains. Someone must understand the business problem, evaluate whether the generated code solves it correctly, consider security implications, ensure it integrates properly with existing systems, and maintain it as requirements evolve.</p><p>AI amplifies developer capability. It doesn’t replace the need for people who understand both the problem domain and the technical landscape.</p><h2>So Much Opportunity, Still Struggling</h2><p>Here’s the paradox that makes this pattern particularly poignant. We’ve made extraordinary progress in software capabilities. The Apollo guidance computer had 4KB of RAM. Your smartphone has millions of times more computing power. We’ve built tools and frameworks that genuinely make many aspects of development easier.</p><p>Yet demand for software far exceeds our ability to create it. Every organization needs more software than it can build. The backlog of desired features and new initiatives grows faster than development teams can address it.</p><p>This tension—powerful tools yet insufficient capacity—keeps the dream alive. Business leaders look at the backlog and think, “There must be a way to go faster, to enable more people to contribute.” That’s a reasonable thought. It leads naturally to enthusiasm for any tool or approach that promises to democratize software creation.</p><p>The challenge is that software development isn’t primarily constrained by typing speed or syntax knowledge. It’s constrained by the thinking required to handle complexity well. Faster typing doesn’t help when you’re thinking through how to handle concurrent database updates. Simpler syntax doesn’t help when you’re reasoning about security implications.</p><p>So what should leaders do with this understanding?</p><h2>What This Means for Leaders</h2><p>Understanding this pattern changes how you evaluate new tools and approaches. When someone promises that their platform will let business users build applications without developers, you can appreciate the aspiration while maintaining realistic expectations.</p><p>The right question isn’t “Will this eliminate our need for developers?” The right questions are:</p><ul><li>Will this help our developers work more effectively on complex problems?</li><li>Will this enable us to build certain types of solutions faster?</li><li>Does this reduce time spent on repetitive tasks so developers can focus on unique challenges?</li><li>Will our team need to learn new skills to use this effectively?</li></ul><p>These questions acknowledge that development involves irreducible complexity while remaining open to tools that provide genuine leverage.</p><p>And they point to something deeper about the nature of software work.</p><h2>The Pattern Reveals the Problem’s Nature</h2><p>This fifty-year pattern teaches us something fundamental about software development itself. If the problem were primarily mechanical—too much typing, too complex syntax, too many steps—we would have solved it by now. COBOL made syntax readable. CASE tools eliminated typing. Visual tools eliminated syntax. AI can now generate entire functions from descriptions.</p><p>Each advancement addressed a real friction point. Yet the fundamental challenge persists because it’s not mechanical. It’s intellectual. Software development is thinking made tangible. The artifacts we create—whether COBOL programs, Delphi forms, or Python scripts—are the visible outcome of invisible reasoning about complexity.</p><p>You can’t shortcut that reasoning any more than you can shortcut the reasoning required to design a building or diagnose a medical condition. Better tools help. Experience helps. But someone must still think it through.</p><p>So how should we move forward, knowing all this?</p><h2>Moving Forward with Clear Eyes</h2><p>The next wave of development tools will arrive. Some will provide genuine value. Some will repeat familiar promises with new technology. Having perspective on this recurring pattern helps you engage with new tools productively.</p><p>Use AI assistants. Evaluate low-code platforms. Experiment with new frameworks. But invest primarily in your people’s ability to think clearly about complexity. That capability remains the constraining factor, just as it was during the Apollo program.</p><p>The moon landing happened because brilliant people thought carefully about every detail of an extraordinarily complex challenge. They wrote software by hand because that was the available tool. If they’d had better tools, they would have used them gladly. But the tools wouldn’t have eliminated their need to think through the complexity.</p><p>We’re still in that same fundamental situation. We have better tools—vastly better tools—but the thinking remains essential.</p><h2>The Dream Serves a Purpose</h2><p>Perhaps the recurring dream of replacing developers isn’t a mistake. Perhaps it’s a necessary optimism that drives tool creation. Each attempt to make development more accessible produces tools that genuinely help. The dream doesn’t come true as imagined, but pursuing it creates value.</p><p>COBOL didn’t let business analysts write programs, but it did enable a generation of developers to build business systems effectively. CASE tools didn’t generate complete applications, but they advanced our thinking about visual modeling. Visual Basic didn’t eliminate professional developers, but it brought application development to more people. AI won’t replace developers, but it will change how we work in meaningful ways.</p><p>The pattern continues because the dream reflects a legitimate need. We genuinely require faster, more efficient ways to create software. We just keep discovering that the constraint isn’t the tool—it’s the complexity of the problems we’re trying to solve.</p><p>Understanding this doesn’t mean rejecting new tools. It means using them with clear expectations about what they can provide and what will always require human judgment.</p><div><p>Get new articles, story episodes, and hero profiles delivered to your inbox.</p></div>",
      "contentLength": 12888,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46658345"
    },
    {
      "title": "What life is like in Minneapolis now",
      "url": "https://donmoynihan.substack.com/p/dispatch-from-the-occupation",
      "date": 1768659191,
      "author": "_tk_",
      "guid": 36797,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46658213"
    },
    {
      "title": "Italy investigates Activision Blizzard for pushing in-game purchases",
      "url": "https://techcrunch.com/2026/01/16/italy-investigates-activision-blizzard-for-pushing-in-game-purchases/",
      "date": 1768657442,
      "author": "7777777phil",
      "guid": 36700,
      "unread": true,
      "content": "<p>Italy has launched two investigations into Microsoft’s Activision Blizzard, alleging the company has engaged in “misleading and aggressive” sales practices for its popular smartphone games Diablo Immortal and Call of Duty Mobile.</p><p>The country’s competition regulator, Autorità Garante della Concorrenza E Del Mercato (AGCM), said the investigations focus on the use of design elements to induce users, particularly children, into playing for long periods, and make in-game purchases by urging them to not miss out on rewards.</p><p>“These practices, together with strategies that make it difficult for users to understand the real value of the virtual currency used in the game and the sale of in-game currency in bundles, may influence players as consumers — including minors — leading them to spend significant amounts, sometimes exceeding what is necessary to progress in the game and without being fully aware of the expenditure involved,” the AGCM <a href=\"https://en.agcm.it/en/media/press-releases/2026/1/PS13020-PS13039\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">wrote</a> in a statement.</p><p>The AGCM said the games are advertised as free-to-play but offer in-game purchases. </p><p>That isn’t particularly surprising, however, as, unlike full-priced games, free-to-play games have long relied on loot boxes and sales of in-game cosmetics for monetization. Diablo Immortal, for example, offers in-game cosmetics, as well as currency that allows players to accelerate their progression and gain items for crafting, for as much as $200. </p><p>Given the nature of the game, it’s not unusual for many users to repeatedly spend on such items in the course of play.</p><p>Both Diablo Immortal and Call of Duty Mobile have player bases in the hundreds of thousands. </p><p>The authority is also looking into the games’ parental control features, as the default settings lets minors make in-game purchases, play for long periods without restraints, and allow them to chat with others in-game. The AGCM also highlighted privacy concerns, as the games appear to lead users to select all consent options when signing up, and said it would look into the company’s consent process for harvesting and using personal data.</p><p>“In the Authority’s view, the company may be acting in breach of consumer protection rules and, in particular, the duty of professional diligence required in a sector that is particularly sensitive to the risks of gaming-related addiction,” the regulator said.</p><p>Activision Blizzard did not immediately respond to a request for comment.</p>",
      "contentLength": 2417,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46658014"
    },
    {
      "title": "The 600-year-old origins of the word 'hello'",
      "url": "https://www.bbc.com/culture/article/20260113-hello-hiya-aloha-what-our-greetings-reveal",
      "date": 1768650704,
      "author": "1659447091",
      "guid": 36699,
      "unread": true,
      "content": "<p>While the English language settled on \"hello\" as its customary greeting, other languages forged their own. Some were influenced by English, others developed independently – yet each carries a distinct cultural flavour, hinting at the social norms and stereotypes we have of the people who use it.</p><p>In Germanic and Scandinavian languages, for example, \"hallo\" and \"hallå\" are phonetically harder and feel more efficient and no-nonsense than the lyrical, almost poetic quality of \"hola\" and \"olá\", favoured by the Romance languages that are associated with more effusive stereotypes. Elsewhere, some greetings carry traces of national history: from the Dutch-derived \"hallo\" of Afrikaans to \"óla\" in Tetum, a reminder of Portuguese influence in Timor-Leste. Many such words appear to function as both introduction&nbsp;&nbsp;identity marker. But, says Professor Duranti, it's not quite that simple.</p><p>\"It's hard to go straight from the use of a particular greeting to a national character, even though it is tempting,\" he tells the BBC. Alternative or secondary greetings, Duranti suggests, may offer better clues. \"In English, given the common use of 'how are you?', there is an apparent interest in people's wellbeing.\" In some Polynesian societies, he adds, greetings are less about a word-for-word \"hello\" than about checking in on someone's plans or movements – literally asking \"where are you going?\". Greek, meanwhile, uses \"Γειά σου\" (pronounced \"yah-soo\") as a typical informal greeting, offering a wish for health rather than a simple salutation. It is also usable for \"goodbye\".</p><p>Other languages also turn abstract concepts into multipurpose greetings that serve as both \"hi\" and \"bye\". \"Ciao\" comes from a Venetian dialect phrase meaning \"at your service\", and the French \"salut\" is an informal expression used for both greeting and parting company. Similarly, the Hawaiian \"aloha\" can express affection or compassion, and the Hebrew \"shalom\" peace or wholeness. Yet, as Duranti cautions, even these evocative examples shouldn't be viewed as cut-and-dry indicators of national character.</p>",
      "contentLength": 2096,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46657296"
    },
    {
      "title": "ASCII characters are not pixels: a deep dive into ASCII rendering",
      "url": "https://alexharri.com/blog/ascii-rendering",
      "date": 1768648526,
      "author": "alexharri",
      "guid": 36652,
      "unread": true,
      "content": "<p>Recently, I’ve been spending my time building an image-to-ASCII renderer. Below is the result — try dragging it around, the demo is interactive!</p><p>One thing I spent a lot of effort on is getting edges looking sharp. Take a look at this rotating cube example:</p><p>Try opening the “split” view. Notice how well the characters follow the contour of the square.</p><p>This renderer works well for animated scenes, like the ones above, but we can also use it to render static images:</p><p>Then, to get better separation between different colored regions, I also implemented a <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Cel_shading\">cel shading</a>-like effect to enhance contrast between edges. Try dragging the contrast slider below:</p><p>The contrast enhancement makes the separation between different colored regions far clearer. That was key to making the 3D scene above look as good as it does.</p><p>I put so much focus on sharp edges because they’re an aspect of ASCII rendering that is often overlooked when programmatically rendering images as ASCII. Consider this animated 3D scene from Cognition’s landing page that is rendered via ASCII characters:</p><p>It’s a cool effect, especially while in motion, but take a look at those blurry edges! The characters follow the cube contours very poorly, and as a result, the edges look blurry and jagged in places:</p><p>This blurriness happens because the ASCII characters are being treated like pixels — their  is ignored. It’s disappointing to see because ASCII art looks  better when shape is utilized. I don’t believe I’ve ever seen shape utilized in generated ASCII art, and I think that’s because it’s not really obvious how to consider shape when building an ASCII renderer.</p><p>I started building my ASCII renderer to prove to myself that it’s possible to utilize shape in ASCII rendering. In this post, I’ll cover the techniques and ideas I used to capture shape and build this ASCII renderer in detail.</p><p>We’ll start with the basics of image-to-ASCII conversion and see where the common issue of blurry edges comes from. After that, I’ll show you the approach I used to fix that and achieve sharp, high-quality ASCII rendering. At the end, we’ll improve on that by implementing the contrast enhancement effect I showed above.</p><h2>Image to ASCII conversion</h2><p>ASCII contains <a target=\"_blank\" href=\"https://www.ascii-code.com/characters/printable-characters\">95 printable characters</a> that we can use. Let’s start off by rendering the following image containing a white circle using those ASCII characters:</p><p>ASCII art is (almost) always rendered using a <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Monospaced_font\">monospace</a> font. Since every character in a monospace font is equally wide and tall, we can split the image into a grid. Each grid cell will contain a single ASCII character.</p><p>The image with the circle is  pixels. For the ASCII grid, I’ll pick a row height of  pixels and a column width of  pixels. That splits the canvas into  rows and  columns — an  grid:</p><p>Monospace characters are typically taller than they are wide, so I made each grid cell a bit taller than it is wide.</p><p>Our task is now to pick which character to place in each cell. The simplest approach is to calculate a lightness value for each cell and pick a character based on that.</p><p>We can get a lightness value for each cell by sampling the lightness of the pixel at the cell’s center:</p><p>We want each pixel’s lightness as a numeric value between  and , but our image data consists of pixels with <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/RGB_color_model\">RGB</a> color values.</p><p>We can use the following formula to convert an RGB color (with component values between  and ) to a lightness value:</p><h3>Mapping lightness values to ASCII characters</h3><p>Now that we have a lightness value for each cell, we want to use those values to pick ASCII characters. As mentioned before, ASCII has 95 printable characters, but let’s start simple with just these characters:</p><p>We can sort them in approximate density order like so, with lower-density characters to the left, and high-density characters to the right:</p><p>We’ll put these characters in a  array:</p><div><div><div><pre></pre></div></div></div><p>I added space as the first (least dense) character.</p><p>We can then map lightness values between  and  to one of those characters like so:</p><div><div><div><pre></pre></div></div></div><p>This maps low lightness values to low-density characters and high lightness values to high-density characters.</p><p>Rendering the circle from above with this method gives us:</p><p>That works... but the result is pretty ugly. We seem to always get  for cells that fall within the circle and a space for cells that fall outside.</p><p>That is happening because we’ve pretty much just implemented nearest-neighbor downsampling. Let’s see what that means.</p><h2>Nearest neighbor downsampling</h2><p>Downsampling, in the context of image processing, is taking a larger image (in our case, the  image with the circle) and using that image’s data to construct a lower resolution image (in our case, the  ASCII grid). The pixel values of the lower resolution image are calculated by sampling values from the higher resolution image.</p><p>The simplest and fastest method of sampling is <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation\">nearest-neighbor interpolation</a>, where, for each cell (pixel), we only take a single sample from the higher resolution image.</p><p>Consider the circle example again. Using nearest-neighbor interpolation, every sample either falls inside or outside of the shape, resulting in either  or  lightness:</p><p>If, instead of picking an ASCII character for each grid cell, we color each grid cell (pixel) according to the sampled value, we get the following pixelated rendering:</p><p>This pixelated rendering is pretty much equivalent to the ASCII rendering from before. The only difference is that instead of s we have white pixels, and instead of spaces we have black pixels.</p><p>These square, jagged looking edges are aliasing artifacts, commonly called <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Jaggies\">jaggies</a>. They’re a common result of using nearest-neighbor interpolation.</p><p>To get rid of jaggies, we can collect more samples for each cell. Consider this line:</p><p>The line’s slope on the  axis is . When we pixelate it with nearest-neighbor interpolation, we get the following:</p><p>Let’s try to get rid of the jagginess by taking multiple samples within each cell and using the average sampled lightness value as the cell’s lightness. The example below lets you vary the number of samples using the slider:</p><p>With multiple samples, cells that lie on the edge of a shape will have some of their samples fall within the shape, and some outside of it. Averaging those, we get gray in-between colors that smooth the downsampled image. Below is the same example, but with an overlay showing where the samples are taken:</p><p>This method of collecting multiple samples from the larger image is called <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Supersampling\">supersampling</a>. It’s a common method of <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Spatial_anti-aliasing\">spatial anti-aliasing</a> (avoiding jaggies at edges). Here’s what the rotating square looks like with supersampling (using  samples for each cell):</p><p>Let’s look at what supersampling does for the circle example from earlier. Try dragging the sample quality slider:</p><p>The circle becomes less jagged, but the edges feel blurry. Why’s that?</p><p>Well, they feel blurry because we’re pretty much just rendering a low-resolution, pixelated image of a circle. Take a look at the pixelated view:</p><p>The ASCII and pixelated views are mirror images of each other. Both are just low-resolution versions of the original high-resolution image, scaled up to the original’s size — it’s no wonder they both look blurry.</p><p>Increasing the number of samples is insufficient. No matter how many samples we take per cell, the samples will be averaged into a single lightness value, used to render a single pixel.</p><p>And that’s the core problem: treating each grid cell as a pixel in an image. It’s an obvious and simple method, but it disregards that ASCII characters have shape.</p><p>We can make our ASCII renderings far more crisp by picking characters based on their shape. Here’s the circle rendered that way:</p><p>The characters follow the contour of the circle very well. By picking characters based on shape, we get a far higher  resolution. The result is also more visually interesting.</p><p>Let’s see how we can implement this.</p><p>So what do I mean by shape? Well, consider the characters , , and  placed within grid cells:</p><p>The character  is top-heavy. Its visual density in the upper half of the grid cell is higher than in the lower half. The opposite can be said for  — it’s bottom-heavy.  is pretty much equally dense in the upper and lower halves of the cell.</p><p>We might also compare characters like  and . The character  is heavier within the left half of the cell, while  is heavier in the right half:</p><p>We also have more “extreme” characters, such as  and , that only occupy the lower or upper portion of the cell, respectively:</p><p>This is, roughly, what I mean by “shape” in the context of ASCII rendering. Shape refers to which regions of a cell a given character visually occupies.</p><p>To pick characters based on their shape, we’ll somehow need to quantify (put numbers to) the shape of each character.</p><p>Let’s start by only considering how much characters occupy the upper and lower regions of our cell. To do that, we’ll define two “sampling circles” for each grid cell — one placed in the upper half and one in the lower half:</p><p>It may seem odd or arbitrary to use circles instead of just splitting the cell into two rectangles, but using circles will give us more flexibility later on.</p><p>A character placed within a cell will overlap each of the cell’s sampling circles to  extent.</p><p>One can compute that overlap by taking a bunch of samples within the circle (for example, at every pixel). The fraction of samples that land inside the character gives us the overlap as a numeric value between  and :</p><p>For T, we get an overlap of approximately  for the upper circle and  for the lower. Those overlap values form a -dimensional vector:</p><p>We can generate such a -dimensional vector for each character within the ASCII alphabet. These vectors quantify the shape of each ASCII character along these  dimensions (upper and lower). I’ll call these vectors .</p><p>Below are some ASCII characters and their shape vectors. I’m coloring the sampling circles using the component values of the shape vectors:</p><p>We can use the shape vectors as 2D coordinates — here’s every ASCII character on a 2D plot:</p><div><svg viewBox=\"-0.041760000000000005 0 0.47676 0.47676\" preserveAspectRatio=\"xMidYMid meet\"></svg></div><p>Let’s say that we have our ASCII characters and their associated shape vectors in a  array:</p><div><div><div><pre></pre></div></div></div><p>We can then perform a nearest neighbor search like so:</p><div><div><div><pre></pre></div></div></div><p>The  function gives us the ASCII character whose shape best matches the input lookup vector.</p><p>Note: this brute force search is not very performant. This becomes a bottleneck when we start rendering thousands of ASCII characters at . I’ll talk more about this later.</p><p>To make use of this in our ASCII renderer, we’ll calculate a lookup vector for each cell in the ASCII grid and pass it to  to determine the character to display.</p><p>Let’s try it out. Consider the following zoomed-in circle as an example. It is split into three grid cells:</p><p>Overlaying our sampling circles, we see varying degrees of overlap:</p><p>When calculating the shape vector of each ASCII character, we took a huge number of samples. We could afford to do that because we only need to calculate those shape vectors once up front. After they’re calculated, we can use them again and again.</p><p>However, if we’re converting an animated image (e.g. canvas or video) to ASCII, we need to be mindful of performance when calculating the lookup vectors. An ASCII rendering might have hundreds or thousands of cells. Multiplying that by tens or hundreds of samples would be incredibly costly in terms of performance.</p><p>With that being said, let’s pick a sampling quality of  with the samples placed like so:</p><p>For the top sampling circle of the leftmost cell, we get one white sample and two black, giving us an average lightness of . Doing the same calculation for all of the sampling circles, we get the following 2D vectors:</p><p>From now on, instead of using the term “lookup vectors”, I’ll call these vectors, sampled from the image that we’re rendering as ASCII, . One sampling vector is calculated for each cell in the grid.</p><p>Anyway, we can use these sampling vectors to find the best-matching ASCII character. Let’s see what that looks like on our 2D plot — I’ll label the sampling vectors (from left to right) C0, C1, and C2:</p><div><svg viewBox=\"-0.096 0 1.096 1.096\" preserveAspectRatio=\"xMidYMid meet\"></svg></div><p>Hmm... this is not what we want. Since none of the ASCII shape vector components exceed , they’re all clustered towards the bottom-left region of our plot. This makes our sampling vectors map to a few characters on the edge of the cluster.</p><p>We can fix this by  the shape vectors. We’ll do that by taking the maximum value of each component across all shape vectors, and dividing the components of each shape vector by the maximum. Expressed in code, that looks like so:</p><div><div><div><pre></pre></div></div></div><p>Here’s what the plot looks like with the shape vectors normalized:</p><div><svg viewBox=\"-0.096 0 1.096 1.096\" preserveAspectRatio=\"xMidYMid meet\"></svg></div><p>If we now map the sampling vectors to their nearest neighbors, we get a much more sensible result:</p><div><svg viewBox=\"-0.096 0 1.096 1.096\" preserveAspectRatio=\"xMidYMid meet\"></svg></div><p>We get ,  and .  Let’s see how well those characters match the circle:</p><p>Nice! They match very well.</p><p>Let’s try rendering the full circle from before with the same method:</p><p>Much better than before! The picked characters follow the contour of the circle very well.</p><h2>Limits of a 2D shape vector</h2><p>Using two sampling circles — one upper and one lower — produces a much better result than the -dimensional (pixelated) approach. However, it still falls short when trying to capture other aspects of a character’s shape.</p><p>For example, two circles don’t capture the shape of characters that fall in the middle of the cell. Consider :</p><p>For , we get a shape vector of . That doesn’t represent the character very well at all.</p><p>The two upper-lower sampling circles also don’t capture left-right differences, such as the difference between  and :</p><p>We could use such differences to get better character picks, but our two sampling circles don’t capture them. Let’s add more dimensions to our shape to fix that.</p><h2>Increasing to 6 dimensions</h2><p>Since cells are taller than they are wide (at least with the monospace font I’m using), we can use  sampling circles to cover the area of each cell quite well:</p><p> sampling circles capture left-right differences, such as between  and , while also capturing differences across the top, bottom, and middle regions of the cell, differentiating , , and . They also capture the shape of “diagonal” characters like  to a reasonable degree.</p><p>One problem with this grid-like configuration for the sampling circles is that there are gaps. For example,  falls between the sampling circles:</p><p>To compensate for this, we can stagger the sampling circles vertically (e.g. lowering the left sampling circles and raising the right ones) and make them a bit larger. This causes the cell to be almost fully covered while not causing excessive overlap across the sampling circles:</p><p>We can use the same procedure as before to generate character vectors using these sampling circles, this time yielding a -dimensional vector. Consider the character :</p><p>I’m presenting -dimensional shape vectors in a  matrix form because it’s easier to grok geometrically, but the actual vector is a flat list of numbers.</p><p>The lightness values certainly look L-shaped! The 6D shape vector captures ’s shape very well.</p><h3>Nearest neighbor lookups in a 6D space</h3><p>Now we have a 6D shape vector for every ASCII character. Does that affect character lookups (how we find the best matching character)?</p><p>Earlier, in the  function, I referenced a  function. That function returns the <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Euclidean_distance\">Euclidean distance</a> between the input points. Given two 2D points  and , the formula to calculate their Euclidean distance looks like so:</p><p>This generalizes to higher dimensions:</p><p>Put into code, this looks like so:</p><div><div><div><pre></pre></div></div></div><p>Note: since we’re just using this for the purposes of finding the closest point, we can skip the expensive  call and just return the squared distance. It does not affect the result.</p><p>So, no, the dimensionality of our shape vector does not change lookups at all. We can use the same  function for both 2D and 6D.</p><p>With that out of the way, let’s see what the 6D approach yields!</p><h3>Trying out the 6D approach</h3><p>Our new 6D approach works really well for flat shapes, like the circle example we’ve been using:</p><p>Now let’s see how this approach works when we render a 3D scene with more shades of gray:</p><p>Firstly, the outer contours look nice and sharp. I also like how well the gradients across the sphere and cone look.</p><p>However, internally, the objects all kind of blend together. The edges  surfaces with different lightnesses aren’t sharp enough. For example, the lighter faces of the cubes all kind of blend into one solid color. When there is a change in color — like when two faces of a cube meet — I’d like to see more sharpness in the ASCII rendering.</p><p>To demonstrate what I mean, consider the following split:</p><p>It’s currently rendered like so:</p><p>The different shades result in s on the left and s on the right, but the boundary is not very sharp.</p><p>By applying some effects to the sampling vector, we can enhance the contrast at the boundary so that it appears sharper:</p><p>The added contrast makes a  difference in readability for the 3D scene. Let’s look at how we can implement this contrast enhancement effect.</p><p>Consider cells overlapping a color boundary like so:</p><p>For the cells on the boundary, we get a 6D sampling vector that looks like so:</p><p>To make future examples easier to visualize, I’ll start drawing the sampling vector using  circles like so:</p><p>Currently, this sampling vector resolves to the character :</p><p>That’s a sensible choice. The character  is visually dense in the top half and less so in the bottom half, so it matches the image fairly well.</p><p>Still, I want the picked character to emphasize the shape of the boundary better. We can achieve that by enhancing the contrast of the sampling vector.</p><p>To increase the contrast of our sampling vector, we might raise each component of the vector to the power of some exponent.</p><p>Consider how an exponent affects values between  and . Numbers close to  experience a strong pull towards  while larger numbers experience less pull. For example, , a 90% reduction, while , only a reduction of 10%.</p><p>The level of pull depends on the exponent. Here’s a chart of  for values of  between  and :</p><p>This effect becomes more pronounced with higher exponents:</p><p>A higher exponent translates to a stronger pull towards zero.</p><p>Applying an exponent should make dark values darker more quickly than light ones. The example below allows you to vary the exponent applied to the sampling vector:</p><p>As the exponent is increased to , the darker components of the sampling vector quickly become  darker, just like we wanted. However, the lighter components also get pulled towards zero by a significant amount.</p><p>I don’t want that. I want to increase the contrast  the lighter and darker components of the sampling vector, not the vector in its entirety.</p><p>To achieve that, we can normalize the sampling vector to the range  prior to applying the exponent, and then “denormalize” the vector back to the original range afterwards.</p><p>The normalization to  can be done by dividing each component by the maximum component value. After applying the exponent, mapping back to the original range is done by multiplying each component by the same max value:</p><div><div><div><pre></pre></div></div></div><p>Here’s the same example, but with this normalization applied:</p><p>Very nice! The lightest component values are retained, and the contrast between the lighter and darker components is increased by “crunching” the lower values.</p><p>This affects which character is picked. The following example shows how the selected character changes as the contrast is increased:</p><p>Awesome! The pick of  over  emphasizes the separation between the lighter region above and the darker region below!</p><p>By enhancing the contrast of the sampling vector, we exaggerate its shape. This gives us a character that less faithfully represents the underlying image, but improves readability as a whole by enhancing the separation between different colored regions.</p><p>Let’s look at another example. Observe how the L-shape of the sampling vector below becomes more pronounced as the exponent increases, and how that affects the picked character:</p><p>Works really nicely! I  the transition from  as the L-shape of the vector becomes clearer.</p><p>What’s nice about applying exponents to normalized sampling vectors is that it barely affects vectors that are uniform in value. If all component values are similar, applying an exponent has a minimal effect:</p><p>Because the vector is fairly uniform, the exponent only has a slight effect and doesn’t change the picked character.</p><p>This is a good thing! If we have a smooth gradient in our image, we want to retain it. We very much do  want to introduce unnecessary choppiness.</p><p>Compare the 3D scene ASCII rendering with and without this contrast enhancement:</p><p>We do see more contrast at boundaries, but this is not quite there yet. Some edges are still not sharp enough, and we also observe a “staircasing” effect happening at some boundaries.</p><p>Let’s look at the staircasing effect first. We can reproduce it with a boundary like so:</p><p>Below is the ASCII rendering of that boundary. Notice how the lower edge (the s) becomes “staircase-y” as you increase the exponent:</p><p>We see a staircase pattern like so:</p><div><div><div><pre></pre></div></div></div><p>To understand why that’s happening, let’s consider the row in the middle of the canvas, progressing from left to right. As we start off, every sample is equally light, giving us s:</p><p>As we reach the boundary, the lower right samples become a bit darker. Those darker components are crunched by contrast enhancement, giving us some s:</p><p>As we progress further right, the middle and lower samples get darker, so we get some s:</p><p>This trend continues towards , , and finally, :</p><p>Giving us a sequence like so:</p><p>That looks good, but at some point we get  light samples. Once we get no light samples, our contrast enhancement has no effect because every component is equally light. This causes us to always get s:</p><p>Making our sequence look like so:</p><div><div><div><pre></pre></div></div></div><p>This sudden stop in contrast enhancement having an effect is what causes the staircasing effect:</p><div><div><div><pre></pre></div></div></div><p>Let’s see how we can counteract this staircasing effect with  layer of contrast enhancement, this time looking outside of the boundary of each cell.</p><h3>Directional contrast enhancement</h3><p>We currently have sampling circles arranged like so:</p><p>For each of those sampling circles, we’ll specify an “external sampling circle”, placed outside of the cell’s boundary, like so:</p><p>Each of those external sampling circles is “reaching” into the region of a neighboring cell. Together, the samples that are collected by the external sampling circles constitute an “external sampling vector”.</p><p>Let’s simplify the visualization and consider a single example. Imagine that we collected a sampling vector and an external sampling vector that look like so:</p><p>The circles colored red are the external sampling vector components. Currently, they have no effect.</p><p>The “internal” sampling vector itself is fairly uniform, with values ranging from  to . The external vector’s values are similar, except in the upper left region where the values are significantly lighter ( and ). This indicates a color boundary above and to the left of the cell.</p><p>To enhance this apparent boundary, we’ll darken the top-left and middle-left components of the sampling vector. We can do that by applying  contrast enhancement using the values from the external vector.</p><p>In the previous contrast enhancement, we calculated the maximum component value across the sampling vector and normalized the vector using that value:</p><div><div><div><pre></pre></div></div></div><p>But the new component-wise contrast enhancement will take the maximum value between each component of the sampling vector and the corresponding component in the external sampling vector:</p><div><div><div><pre></pre></div></div></div><p>Aside from that, the contrast enhancement is performed in the same way:</p><div><div><div><pre></pre></div></div></div><p>The example below shows how light values in the external sampling vector push values in the sampling vector down:</p><p>I call this “directional contrast enhancement”, since each of the external sampling circles reaches outside of the cell in the  of the sampling vector component that it is enhancing the contrast of. I describe the other effect as “global contrast enhancement” since it acts on all of the sampling vector’s components together.</p><p>Let’s see what this directional contrast enhancement does to get rid of the staircasing effect:</p><p>Hmm, that’s not doing what I wanted. I wanted to see a sequence like so:</p><div><div><div><pre></pre></div></div></div><p>But we just see  changing to </p><p>This happens because the directional contrast enhancement doesn’t reach far enough into our sampling vector. The light upper values in the external vector  push the upper values of the sampling vector down, but because the lightness of the four bottom components is retained, we don’t get to , just .</p><h3>Widening the directional contrast enhancement</h3><p>I’d like to “widen” the directional contrast enhancement so that, for example, light external values at the top spread to the middle components of the sampling vector.</p><p>To do that, I’ll introduce a few more external sampling circles, arranged like so:</p><p>These are a total of  external sampling circles. Each of the external sampling circles will affect one or more of the internal sampling circles. Here’s an illustration showing which internal circles each external circle affects:</p><p>For each component of the internal sampling vector, we’ll calculate the maximum value across the external sampling vector components that affect it, and use that maximum to perform the contrast enhancement.</p><p>Let’s implement that. I’ll order the internal and external sampling circles like so:</p><p>We can then define a mapping from the internal circles to the external sampling circles that affect them:</p><div><div><div><pre></pre></div></div></div><p>With this, we can change the calculation of  to take the maximum affecting external value:</p><div><div><div><pre></pre></div></div></div><p>Now look what happens if the top four external sampling circles are light: it causes the contrast enhancement to reach into the middle of the sampling vector, giving us the desired effect:</p><p>We now smoothly transition from  — beautiful stuff!</p><p>Let’s see if this change resolves the staircasing effect:</p><p>Oh yeah, looks awesome! We get the desired effect. The boundary is nice and sharp while not being too jagged.</p><p>Here’s the 3D scene again. The contrast slider now applies both types of contrast enhancement at the same time — try it out:</p><p>This really enhances the contrast at boundaries, making the image far more readable!</p><p>Together, the 6D shape vector approach and contrast enhancement techniques have given us a really nice final ASCII rendering.</p><p>This post was really fun to build and write! I hope you enjoyed reading it.</p><p>ASCII rendering is perhaps not the most useful topic to write about, but I think the idea of using a high-dimensional vector to capture shape is interesting and could easily be applied to many other problems. There are parallels to be drawn to <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Word_embedding\">word embeddings</a>.</p><p>I started writing this ASCII renderer to see if the idea of using a vector to capture the shape of characters would work at all. That approach turned out to work very well, but the initial prototype was terribly slow — I only got single-digit FPS on my iPhone. To get the ASCII renderer running at a smooth  FPS on mobile required a lot of optimization work. I describe some of that optimization work in the appendices on <a target=\"\" href=\"https://alexharri.com/blog/ascii-rendering#character-lookup-performance\">character lookup performance</a> and <a target=\"\" href=\"https://alexharri.com/blog/ascii-rendering#appendix-gpu-acceleration\">GPU acceleration</a> below.</p><p>My colleagues, after reading a draft of this post, suggested  alternatives to the approaches I described in this post. For example, why not make the sampling vector ? That would capture the shape of  far better — just look how ’s stem falls between the two sampling circles in each row:</p><p>And yeah, he’s right! A  layout would certainly capture it better. They also suggested many alternative approaches to the contrast enhancement methods I described, but I won’t explore those in this post.</p><p>It’s really fun how large the solution space to the problem of ASCII rendering is. There are so, so many approaches and trade-offs to explore. I imagine you probably thought of a few yourself while reading this post!</p><p>One dimension I intentionally did not explore was using different colors or lightnesses for the ASCII characters themselves. This is for many reasons, but the two primary ones are that 1) it would have expanded the scope of this post too much, and 2) it’s just a different effect, and I personally don’t like the look.</p><p>At the time of writing these final words, around  months have elapsed since I started working on this post. This has been my longest writing process to date. Much of that can be explained by the birth of my now -month-old daughter. I’ve needed to be a lot more intentional about finding time to write — and disciplined when spending it. I intend to write some smaller posts next. Let’s see if I manage to stick to that promise.</p><div><h2>Appendix I: Character lookup performance</h2></div><p>Earlier in this post, I showed how to find the best character by finding the character with the shortest Euclidean distance to our sampling vector.</p><div><div><div><pre></pre></div></div></div><p>I tried benchmarking this for  input sampling vectors on my MacBook — K invocations of this function consistently take about ms. If we want to be able to use this for an animated canvas at  FPS, we only have ms to render each frame. We can use this to get a rough budget for how many lookups we can perform each frame:</p><p>If we allow ourselves  of the performance budget for just lookups, this gives us a budget of about K characters. Not terrible, but far from great, especially considering that we’re using numbers from a powerful laptop. A mobile device might have a  times lower budget. Let’s see how we can improve this.</p><p>-d trees are a data structure that enables nearest-neighbor lookups in multi-dimensional (-dimensional) space. Their performance <a target=\"_blank\" href=\"https://graphics.stanford.edu/~tpurcell/pubs/search.pdf\">degrades in higher dimensions</a> (e.g. ), but they perform well in  dimensions — perfect for our purpose.</p><p>Internally, -d trees are a binary tree where each node is a -dimensional point. Each node can be thought to split the -dimensional space in half with a hyperplane, with the left subtree on one side of the hyperplane and the right subtree on the other.</p><div><div><p>I won’t go into much detail on -d trees here. You’ll have to look at other resources if you’re interested in learning more.</p></div></div><p>Let’s see how it performs! We’ll construct a -d tree with our characters and their associated vectors:</p><div><div><div><pre></pre></div></div></div><p>We can now perform nearest-neighbor lookups on the -d tree:</p><div><div><div><pre></pre></div></div></div><p>Running K such lookups takes about ms on my MacBook. That’s about x faster than the brute-force approach. We can use this to calculate, roughly, the number of lookups we can perform per frame:</p><p>That’s a lot of lookups per frame, but again, we’re benchmarking on a powerful machine. This is still not good enough.</p><p>Let’s see how we can eke out even more performance.</p><p>An obvious avenue for speeding up lookups is to cache the result:</p><div><div><div><pre></pre></div></div></div><p>But how does one generate a cache key for a -dimensional vector?</p><p>Well, one way is to quantize each vector component so that it fits into a set number of bits and packing those bits into a single number. JavaScript numbers give us  bits to work with, so each vector component gets  bits.</p><p>We can quantize a numeric value between  and  to the range  to  (the most that  bits can store) like so:</p><div><div><div><pre></pre></div></div></div><p>Applying a max of  is done so that a  of exactly  is mapped to  instead of .</p><p>We can quantize each of the sampling vector components in this manner and use bit shifting to pack all of the quantized values into a single number like so:</p><div><div><div><pre></pre></div></div></div><p>The  is current set to , but consider how large that makes our key space. Each vector component is one of  possible values. With  vector components, that makes the total number of possible keys , which equals . If the cache were to be fully saturated, just storing the keys would take GB of memory! I’d also expect the cache hit rate to be incredibly low if we were to lazily fill the cache.</p><p>Alright,  is too high, but what value should we pick? We can pick any number under  for our range. To help, here’s a table showing the number of possible keys (and the memory needed to store them) for range values between  and :</p><table><tbody><tr><th>Memory needed to store keys</th></tr></tbody></table><p>There are trade-offs to consider here. As the range gets smaller, the quality of the results drops. If we pick a range of , for example, the only possible lightness values are , , , ,  and . That noticeably affects the quality of character picks.</p><p>At the same time, if we increase the possible number of keys, we need more memory to store them. Additionally, the cache hit rate might be very low, especially when the cache is relatively empty.</p><p>I ended up picking a range of . It’s a large enough range that quality doesn’t suffer too much while keeping the cache size reasonably low.</p><p>Cached lookups are incredibly fast — fast enough that lookup performance just isn’t a concern anymore (K lookups take a few ms on my MacBook). And if we prepopulate the cache, we can expect consistently fast performance, though I encountered no problems just lazily populating the cache.</p><div><h2>Appendix II: GPU acceleration</h2></div><p>Lookups were not the only performance concern. Just collecting the sampling vectors (internal and external) turned out to be terribly expensive.</p><p>Just consider the sheer amount of samples that need to be collected. The 3D scene I’ve been using as an example uses a  grid, which equals  cells. For each of those cells, we compute a -dimensional sampling vector and a -dimensional external sampling vector. That is more than K vector components to compute on every frame!</p><p>And that’s if we use a sampling quality of . If we increase the sampling quality, this number just gets bigger.</p><p>Collecting these samples absolutely  performance on my iPhone, so I needed to either collect fewer samples or speed up the collection of samples. Collecting fewer samples would have meant rendering fewer ASCII characters or removing the directional contrast enhancement, neither of which was an appealing solution.</p><p>My initial implementation ran on the CPU, which could only collect one sample at a time. To speed this up, I moved the work of sampling collection and applying the contrast enhancement to the GPU. The pipeline for that looks like so (each of the steps listed is a single shader pass):</p><ol><li>Collect the raw internal sampling vectors into a  texture, using the canvas (image) as the input texture.</li><li>Do the same for the external sampling vectors.</li><li>Calculate the maximum external value affecting each internal vector component into a  texture.</li><li>Apply directional contrast enhancement to each sampling vector component, using the maximum external values texture.</li><li>Calculate the maximum value for each internal sampling vector into a  texture.</li><li>Apply global contrast enhancement to each sampling vector component, using the maximum internal values texture.</li></ol><p>I’m glossing over the details because I could spend a whole other post covering them, but moving work to the GPU made the renderer many times more performant than it was when everything ran on the CPU.</p>",
      "contentLength": 34747,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46657122"
    },
    {
      "title": "PCs refuse to shut down after Microsoft patch",
      "url": "https://www.theregister.com/2026/01/16/patch_tuesday_secure_launch_bug_no_shutdown/",
      "date": 1768647079,
      "author": "smurda",
      "guid": 36698,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.theregister.com/2026/01/16/patch_tuesday_secure_launch_bug_no_shutdown/\">https://www.theregister.com/2026/01/16/patch_tuesday_secure_launch_bug_no_shutdown/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46656998\">https://news.ycombinator.com/item?id=46656998</a></p>\n<p>Points: 261</p>\n<p># Comments: 313</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "After 25 years, Wikipedia has proved that news doesn't need to look like news",
      "url": "https://www.niemanlab.org/2026/01/after-25-years-wikipedia-has-proved-that-news-doesnt-need-to-look-like-news/",
      "date": 1768645775,
      "author": "giuliomagnifico",
      "guid": 36651,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.niemanlab.org/2026/01/after-25-years-wikipedia-has-proved-that-news-doesnt-need-to-look-like-news/\">https://www.niemanlab.org/2026/01/after-25-years-wikipedia-has-proved-that-news-doesnt-need-to-look-like-news/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46656911\">https://news.ycombinator.com/item?id=46656911</a></p>\n<p>Points: 201</p>\n<p># Comments: 228</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "US electricity demand surged in 2025 – solar handled 61% of it",
      "url": "https://electrek.co/2026/01/16/us-electricity-demand-surged-in-2025-solar-handled-61-percent/",
      "date": 1768645686,
      "author": "doener",
      "guid": 36650,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://electrek.co/2026/01/16/us-electricity-demand-surged-in-2025-solar-handled-61-percent/\">https://electrek.co/2026/01/16/us-electricity-demand-surged-in-2025-solar-handled-61-percent/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46656903\">https://news.ycombinator.com/item?id=46656903</a></p>\n<p>Points: 355</p>\n<p># Comments: 321</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Provide agents with automated feedback",
      "url": "https://banay.me/dont-waste-your-backpressure/",
      "date": 1768645578,
      "author": "ghuntley",
      "guid": 36907,
      "unread": true,
      "content": "<p>You might notice a pattern in the most successful applications of agents over the last year. Projects that are able to\nsetup structure around the agent itself, to provide it with automated feedback on quality and correctness, have been able\nto push them to work on longer horizon tasks.</p><p>This  helps the agent identify mistakes as it progresses and models are now good enough that this feedback\ncan keep them aligned to a task for much longer. As an engineer, this means you can increase your leverage by delegating\nprogressively more complex tasks to agents, while increasing trust that when completed they are at a satisfactory standard.</p><p>Imagine for a second if you only gave an agent tools that allow it to edit files. Without a way to interact with a build\nsystem the model relies on you for feedback about whether or not the change it made is sensible. This means you spend \nback pressure (the time you spend giving feedback to agents) on typing a message telling the agent it missed an import. This\nscales poorly and limits you to working on simple problems.</p><p>If you’re directly responsible for checking each line of code produced is syntactically valid, then that’s time taken away\nfrom thinking about the larger goals or problems in your software. You’re going to struggle to derive more leverage out of\nagents because you are caught up in trivial changes. If instead you give the agent tools that allow it to run bash commands,\nit can run a build, read the feedback, and correct itself. You remove yourself from needing to be involved in those tasks\nand can instead focus on higher complexity tasks.</p><p>Languages with expressive type systems have been\n<a href=\"https://github.blog/ai-and-ml/llms/why-ai-is-pushing-developers-toward-typed-languages/\">growing in popularity</a> in part\nbecause of back pressure. Type systems allow you to describe better contracts in your program. They can let you avoid it\nfrom even being possible to represent invalid states in your program. They can help you to identify edge cases that you\nmight not handle. Being able to lean on these features is another form of creating back pressure which you can direct as\nfeedback on changes made by an agent.</p><p>Bonus points go to languages that work to produce excellent error messages (think\n<a href=\"https://kobzol.github.io/rust/rustc/2025/05/16/evolution-of-rustc-errors.html\">Rust</a>,\n<a href=\"https://elm-lang.org/news/compiler-errors-for-humans\">Elm</a> and even\n<a href=\"https://peps.python.org/pep-0657/\">Python</a>). These messages are fed directly back into the LLM so the more guidance or even\nsuggested resolutions the better.</p><p>Another example of back pressure is the rapid uptake in people giving agents a way to see rendered pages using MCP servers\nfor Playwright or Chrome DevTools. In either case these tools give the agent a way to be able to make a change and compare\nan expectation of what it might see in the UI against a result. Attaching these tools mean you remove yourself from needing\nto keep telling the agent that you’re not seeing a UI element load correctly or something isn’t centered. Not working on a\nUI application? Use MCP servers that bridge to LSPs for lints or other feedback.</p><p>Even outside of engineering tasks, proof assistants like Lean combined with AI (see recent work on the\n<a href=\"https://mathstodon.xyz/@tao/115855840223258103\">Erdős Problems</a> which was solved by Kevin Barreto and Liam Price by using\nAristotle to formalise a proof written by GPT-5.2 Pro into Lean), randomized fuzzing to evaluate correctness when\n<a href=\"https://crfm.stanford.edu/2025/05/28/fast-kernels.html\">generating CUDA kernels</a> or logic programming with agents are all\npowerful combinations because they let you keep pulling the LLM slot machine lever until the result you have can be trusted.\nI think that the payoff of investing into higher quality testing is growing massively, and an increasing part of engineering\nwill involve designing and building back pressure in order to scale the rate at which contributions from agents can be\naccepted.</p><p>If you’re doing spec-driven development and you want the agent to generate a specific API schema, setup automatic generation\nof documentation based on the OpenAPI schema from your application so the agent can compare the result it produced and what\nit intended on making. There are many more techniques you can apply similar to this once you recognize the pattern.</p><p>In your projects you should think about how you can build back pressure into your workflow and once you have it, you can\n<a href=\"https://ghuntley.com/ralph/\">loop agents</a> until they have stamped out all of the inconsistencies and issues for you.\nWithout it, you’re going to be stuck spending your time telling the agent about each mistake it makes yourself.</p><p>So next time, think - <strong>are you wasting your back pressure?</strong></p>",
      "contentLength": 4364,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46656897"
    },
    {
      "title": "Map To Poster – Create Art of your favourite city",
      "url": "https://github.com/originalankur/maptoposter",
      "date": 1768644837,
      "author": "originalankur",
      "guid": 36697,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://github.com/originalankur/maptoposter\">https://github.com/originalankur/maptoposter</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46656834\">https://news.ycombinator.com/item?id=46656834</a></p>\n<p>Points: 283</p>\n<p># Comments: 63</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ClickHouse acquires Langfuse",
      "url": "https://langfuse.com/blog/joining-clickhouse",
      "date": 1768641345,
      "author": "tin7in",
      "guid": 36649,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://langfuse.com/blog/joining-clickhouse\">https://langfuse.com/blog/joining-clickhouse</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46656552\">https://news.ycombinator.com/item?id=46656552</a></p>\n<p>Points: 212</p>\n<p># Comments: 96</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Show HN: Streaming gigabyte medical images from S3 without downloading them",
      "url": "https://github.com/PABannier/WSIStreamer",
      "date": 1768639568,
      "author": "el_pa_b",
      "guid": 36659,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46656358"
    },
    {
      "title": "The 'untouchable hacker god' behind Finland's biggest crime",
      "url": "https://www.theguardian.com/technology/2026/jan/17/vastaamo-hack-finland-therapy-notes",
      "date": 1768634958,
      "author": "c420",
      "guid": 36696,
      "unread": true,
      "content": "<p>iina Parikka was half-naked when she read the email. It was a Saturday in late October 2020, and Parikka had spent the morning sorting out plans for distance learning after a Covid outbreak at the school where she was headteacher. She had taken a sauna at her flat in Vantaa, just outside Finland’s capital, Helsinki, and when she came into her bedroom to get dressed, she idly checked her phone. There was a message that began with Parikka’s name and her social security number – the unique code used to identify Finnish people when they access healthcare, education and banking. “I knew then that this is not a game,” she says.</p><p>The email was in Finnish. It was jarringly polite. “We are contacting you because you have used Vastaamo’s therapy and/or psychiatric services,” it read. “Unfortunately, we have to ask you to pay to keep your personal information safe.” The sender demanded €200 in bitcoin within 24 hours, otherwise the price would go up to €500 within 48 hours. “If we still do not receive our money after this, your information will be published for everyone to see, including your name, address, phone number, social security number and detailed records containing transcripts of your conversations with Vastaamo’s therapists or psychiatrists.”</p><p>Parikka swallows hard as she relives this memory. “My heart was pounding. It was really difficult to breathe. I remember lying down on the bed and telling my spouse, ‘I think I’m going to have a heart attack.’”</p><p>Someone had hacked into Vastaamo, the company through which Parikka had accessed psychotherapy. They’d got hold of therapy notes containing her most private, intimate feelings and darkest thoughts – and they were holding them to ransom. Parikka’s mind raced as she tried to recall everything she’d confided during three years of weekly therapy sessions. How would her family react if they knew what she’d been saying? What would her students say? The sense of exposure and violation was unfathomable: “It felt like a public rape.”</p><p>Therapy had been Parikka’s lifeline. Now 62, she’d had three children by the time she was 25, including twins who had been born extremely prematurely in the 1980s, weighing only a&nbsp;few hundred grams each. One grew up with cerebral palsy; the other is blind. Parikka spent years juggling medical emergencies, surgeries and hospital stays with a demanding job and a crumbling marriage. “During those years, nobody ever asked me, the mother, ‘How are you?’”</p><p>She divorced in 2014 and met her current partner a year later. By then, her children were adults with independent lives. After decades of putting everyone’s else’s needs before her own, she should have been finally able to exhale. Instead, she had a breakdown. “I had full-scale anxiety running through my body all the time. I couldn’t sleep. I had panic attacks. I couldn’t eat.” Driving at high speed on the highway one day, dark thoughts descended. “I was thinking, I&nbsp;wouldn’t mind if this car crashed.”</p><p>In search of urgent help, she went to Google, which led her to Vastaamo, Finland’s one-stop digital shop for people in search of psychotherapy. No doctor referral was necessary. She managed to book a session for the very next day. “It was that easy.”</p><p>Being able to confide in a total stranger felt liberating. She told her therapist things she had never told another soul. “Trauma in relationships. The disappointment and tragedy of having disabled children, and the influence it had on my life,” she says. “Silly things, childish things. It’s very human to feel hate, anger, rage.”</p><p>After Parikka read the email that left her struggling to breathe, she had no idea where to turn for help. She rang the emergency services, but the police told her to get off the line; they needed to keep it free for real emergencies. In her bathrobe, her phone still in her hand, she felt utterly alone.</p><p>But Parikka was far from alone. Across Finland, 33,000 people who had used Vastaamo were discovering that a hacker had got hold of their therapy notes and was holding them to ransom. These were people who, by definition, were likely to be vulnerable, in need of help. Each was experiencing a very personal, individual terror. In a country of only 5.6 million people, everyone knows someone who was hacked.</p><p>Some victims’ notes had already been cherrypicked for the world to see. Three days before the extortion emails were sent, someone using the handle ransom_man had left posts on the dark web, on r/Suomi, the Finnish-language subreddit, and on Ylilauta, Finland’s equivalent to <a href=\"https://www.theguardian.com/technology/4chan\" data-link-name=\"in body link\">4chan</a>. This time, the post was in English. Hello Finnish Colleagues,” it began. “We have hacked the psychotherapy clinic vastaamo.fi and taken tens of thousands of patient records including extremely sensitive session notes and social security numbers. We requested a small payment of 40 bitcoins (nothing for a company with yearly revenues close to 20 million euros) but the CEO has stopped responding to our emails. We are now starting to gradually release their patient records, 100 entries every day.”</p><p>There was a link to the dark web, where 100 records were already on display. Directly below it, ransom_man had signed off the post with a single word: “Enjoy!”</p><p>The 100 records included those of politicians, police officers and prominent public figures. Their names appeared alongside therapy notes that contained details of adultery, suicide attempts, paedophilia and sexual violence. Some of the records belonged to children. And whoever was behind the hack was true to their word: the next day, 100 more patient records were uploaded.</p><p>Some victims went searching on the dark web in a&nbsp;desperate attempt to see if their records were out there. Some paid the ransom, scrabbling to get hold of bitcoin while the clock ticked down. Lawyers representing the victims have told me they know of at least two cases where people took their own lives after they discovered their therapy notes had been hacked.</p><p>But for all of them, it was already too late. At 2am on 23 October 2020 – the day before the emails began to arrive in tens of thousands of inboxes – ransom_man had uploaded a much larger file. It contained every record of every single patient on Vastaamo’s database. Everyone’s therapy notes had already been published, for free, for everyone in the world to see.</p><p>Who was behind the biggest crime Finland had ever known? And might they have been motivated by something other than money? I have spent 18 months trying to answer these questions, following threads across Europe and the US. They culminated in a visit to a prison, and one of the most chilling conversations I have ever had.</p><p>Vastaamo had long been considered an example of how Finland was getting it right when it came to digital tech. Founded in 2008 by entrepreneur Ville Tapio and his mother, Nina, a psychotherapist, the aim was to open up therapy to the masses, removing the stigma of asking for help. The platform made it easy for people to see who was free, where, and what therapeutic approach they specialised in. The logo had the colour palette of a&nbsp;first-aid kit, with white lettering in a green speech bubble. Vastaamo means “a place for answers”</p><p>It was an attractive platform for therapists, too: they didn’t have to worry about marketing or billing – Vastaamo would take care of all of that. The company even provided a behind-the-scenes digital interface where therapists could make and store their notes. This formula, combined with the increasing demand for therapy services, meant Vastaamo grew fast. It opened its own network of around 20&nbsp;clinics across Finland, employing <a href=\"https://interapartners.com/intera-to-support-the-growht-of-psykoterapiakeskus-vastaamo/\" data-link-name=\"in body link\">more than 220&nbsp;psychotherapists</a> by 2018, leading some in Finland to refer to it as “the McDonald’s of therapy”. In the years before Zoom and Teams were part of our daily lives, the remote therapy also offered by Vastaamo was groundbreaking. In 2019, a private equity firm bought a majority stake in the company, earning the Tapio family a payout of <a href=\"https://yle.fi/a/3-11628354\" data-link-name=\"in body link\">more than €5m</a>.</p><p>Meri-Tuuli Auer, 30, describes using Vastaamo as “like Uber for therapy – convenient, accessible, relatively cheap”. She picked her therapist because he offered cognitive psychotherapy – and she liked his photo. “He looked nice. He looked approachable.”</p><p>Auer’s home, on the outskirts of Helsinki, is a riot of pink. There are Barbie dolls, Barbie books and Barbie-themed handbags on her shelves, as well as a glittery open-top Barbie sports car. A pole-dancing pole takes pride of place in the centre of her living room.</p><p>“I’m a mixed personality,” she tells me over tea in Moomin mugs. “I love being around people, but I get that inkling, that doubt: maybe they all think I’m full of shit and stupid and ugly and I have no idea what I’m doing.” Auer has struggled with depression for much of her life. When she was 18, she was in a secretive, difficult relationship with a man 29 years her senior, which made her self-esteem plummet further. She was drinking heavily. “If I hadn’t gone to therapy, I don’t know what would have become of me. Maybe there is another universe where I didn’t make it to 30.”</p><p>Most of the cost of Auer’s treatment was covered by the Finnish healthcare system; she paid only about €25 for each weekly session. She was making great strides. “After going to therapy in 2018 and 2019, I had gained&nbsp;a&nbsp;basic sense of security. That was lost in 2020.”</p><p>Vastaamo’s CEO knew the company’s patient registry was being held to ransom weeks before his customers found out. On 28 September 2020, Ville Tapio received an email demanding the bitcoin equivalent of €450,000 to keep it safe. Sample patient records attached to the email proved the extortionist wasn’t bluffing. Tapio called in a cybersecurity firm to investigate.</p><p>Medical information is an obvious target for would-­be extortionists, says Antti Kurittu, the security specialist Tapio hired. But this was something else: “Whatever I tell a therapist is, by its very nature, a lot more private than what my blood pressure is,” he says, drily.</p><p>Kurittu used to be a detective, investigating cybercrimes for the Finnish police; he says he insisted they be told about the ransom attempt so they could begin a parallel investigation. Meanwhile, he began inspecting Vastaamo’s server, looking for clues as to who might be behind the hack – and one of the first things he noticed was how lax security had been. “It was definitely unfit for purpose for storing this kind of information,” he says. He tells me that the patient records database was accessible via the internet; there was no firewall and, perhaps most egregiously, it was secured with a blank password, so anyone could just press enter and open it. Kurittu determined that whoever had hacked Vastaamo had probably just been scanning the internet in search of any badly secured databases that could be monetised. “They tried a bunch of bank vaults to see which ones were open, and just happened to stumble on this one.”</p><p>For a few weeks, the hacker and Vastaamo exchanged emails, but there was no question that Vastaamo would pay the ransom. If they did, they’d have to trust a criminal’s word that the records had been destroyed – plus, Kurittu says, it goes against the national character. “Finns are a bit of a belligerent bunch. We’re not known for paying ransom quietly or easily, which I take great national pride in.”</p><p>After ransom_man started leaking patient records to put pressure on the company, Kurittu kept a close eye on the server being used to publish them. He had a hunch whoever was behind this was either Finnish, or had lived in Finland for a long time: they knew which famous names to flaunt from the patient records.</p><p>hen Auer learned about the hack, she downloaded a browser that would enable her to access the dark web, for the first time in her life. “I was thinking to myself, I just have to see if my records are there.” She found her name wasn’t among the first batch posted, and closed the file without reading anyone’s records. But she saw other people discussing what they’d seen. “People had already picked – in their opinion – the funniest parts from the patient records. They were laughing at these people’s misery. A 10-year-old child had gone to therapy, and people found it funny.”</p><p>Auer began to spiral. “I closed myself in at home, I&nbsp;didn’t want to leave, I didn’t want anyone to see me,” she tells me. She had no hope that the hacker would ever be found. “It’s not that I don’t trust the police in Finland – it’s just that it seemed like an impossible task.”</p><p>But the much larger file ransom_man had uploaded to the dark web – the one that contained every single one of Vastaamo’s patient records – also included vital clues to his identity. The first three batches of therapy notes had been posted manually, but when the hacker had tried to automate the process, he had not only accidentally uploaded all of the therapy notes, but also his entire home folder. It had appeared only briefly before it was taken down, along with a post that read “whoopsie :D”, but ransom_man had screwed up.</p><p>“After spending several evenings with the file, I had the feeling I’d seen this kind of thing before,” Kurittu says. The data on the hacker’s home drive wasn’t systematically organised and arranged in folders, as you would expect from someone for whom extortion was a business. “It had that sort of chaotic, passionate hobby feeling to it.” And there was something about the childish way ransom_man had named some of the files that was eerily familiar (the one containing all the patient data was entitled “therapissed”).</p><p>Kurittu’s mind went back to 2013 when he was a senior detective constable for the Helsinki police, and the file names he’d seen on a computer he’d seized from a 16-year-old boy. “It made me think of Julius Kivimäki.”</p><p>leksanteri Kivimäki – who used to go by his middle name, Julius, or the online handle zeekill – had long been notorious among cybersecurity investigators. Not because of any particular talent as a hacker, but because he seemed prepared to go further than most who spend their time in the darkest parts of the internet.</p><p>Aged 14, Kivimäki was involved with a group called Hack the Planet (named after the tagline of the 1995 movie Hackers). They would break into big companies and show off what they had managed to steal online. “It was for the LOLs,” says Blair Strater, a former hacker from Illinois who hung out with Kivimäki in internet relay chat forums at that time. “You notice that something is open and you just take it. It’s not targeted.”</p><p>This kind of hacking was about impressing others – winning online clout, not extorting money. But some of those involved may have felt they were also serving a noble purpose: exposing security vulnerabilities in major corporations, or the hypocrisy of cybersecurity firms who claimed to be qualified to advise businesses while being unable to secure their own network.</p><p>Strater found Kivimäki amusing, at first. “A lot of the things he did early on were objectively funny,” he tells me over Zoom from his home in Illinois. When I ask Strater whether  would find them funny, he clarifies that his humour was an acquired taste best suited to 4chan. But in 2010, when Strater was 17 and Kivimäki was 14, they fell out over which one of them was going to publish a&nbsp;report of a recent hack.</p><p>Orders of pizzas and Chinese takeaway began arriving at the home Strater shared with his parents and younger sister on the outskirts of Chicago; when they opened the door, the delivery driver would ask for Julius Kivimäki. “Taxis were ordered. Hookers were ordered,” Strater says. “My father had to send away a big dump truck filled with gravel.” Strater received a blizzard of letters from credit card and insurance companies, and government agencies, including one from the department of social security confirming that an appointment with the welfare office had been created for him and his spouse – Julius Kivimäki.</p><p>Then, at 2am one morning, police in body armour carrying guns with laser sights turned up outside the Straters’ home, responding to reports that Blair had beaten his mother to death in a drug-fuelled rage. When she answered the door, they took her blood pressure to verify that she was, in fact, alive. It was the first of dozens of so-called swatting attacks the family would endure. After a lull of a couple of months, Strater learned that someone using his name had emailed a bomb threat to a local police officer; it led to Strater spending three weeks over Christmas in a juvenile detention centre.</p><p>Several years into their feud, in 2015, someone <a href=\"https://www.cbsnews.com/sanfrancisco/news/high-tech-teslas-website-twitter-account-hijacked-by-angry-ranting-hackers/\" data-link-name=\"in body link\">hacked Elon Musk and Tesla’s Twitter accounts,</a> and tweeted that anyone who rang the Straters’ landline or showed up at their home would get a free car; their phone rang off the hook for days, and Blair’s father had to turn several disappointed people away from their porch. Someone using Blair’s mother’s name posted a threat to shoot up the elementary school where his 10-year-old sister was a pupil. His mother’s LinkedIn and Twitter accounts were hacked and filled with juvenile, racist posts, as well as antisemitic insults directed at the company where she worked as a healthcare statistician. Within months, she had lost her job.</p><p>The campaign of terror lasted for many more years. Strater says it’s never going to be fully over. “It’s like having cancer: it’s never really cured, it goes into remission,” he says. “Every so often, someone would hit me up and say, ‘Hey, I was one of the people that helped Julius do these things.’ Sometimes they would say, ‘He made me do them. He was blackmailing me,’ which is something he does to an awful lot of people. I want to make this very clear: I am not the person zeekill fucked with the most.”</p><p>Indeed, Kivimäki set his sights far beyond the Strater family. In August 2014 – days after his 17th birthday – he rang in <a href=\"https://news.sky.com/story/fake-plane-bomb-threat-grounds-sony-executive-10391998\" data-link-name=\"in body link\">a fake bomb threat</a> that grounded a flight carrying John Smedley, president of Sony Online Entertainment, who oversaw PlayStation’s multiplayer network. A group calling themselves Lizard Squad claimed responsibility, posting almost nonsensically on Twitter that the attack was in sympathy with Islamic State. Lizard Squad struck again, on 25 December 2014, with a cyber-attack that shut down Xbox and PlayStation, and ruined Christmas morning for millions. Brazenly, Kivimäki gave interviews to <a href=\"https://soundcloud.com/hardchats/bbc-radio-host-interviews-lizardsquad\" data-link-name=\"in body link\">BBC 5 Live</a> and <a href=\"https://news.sky.com/story/xbox-hacker-reveals-why-he-attacked-consoles-10377611\" data-link-name=\"in body link\">Sky News</a> as a Lizard Squad spokesperson, claiming they did the hack both to amuse themselves and to expose Microsoft and Sony’s poor cybersecurity. He seemed to revel in the chaos and drama. He appeared on camera on Sky News; he used a fake name, but his boyish face – blond hair, blue eyes, plump cheeks – was visible for all to see.</p><p>In July 2015, following Kurittu’s investigation with the Finnish police, Kivimäki was convicted of hacking into servers at MIT and Harvard universities, as well as money laundering and fraud. He was <a href=\"https://www.bbc.co.uk/news/technology-33442419\" data-link-name=\"in body link\">found guilty</a> of more than 50,000 data breaches, and received a two-year suspended sentence; he had his computer confiscated and was forced to pay back more than €6,000 obtained through his crimes. He never faced justice for any of the offences he perpetrated against Blair Strater and his family.</p><p>Shortly after he received his suspended sentence, Kivimäki updated his Twitter bio to read “untouchable hacker god”.</p><p>ivimäki spent the next few years travelling&nbsp;the world. During lockdown, he lived in an air-conditioned apartment in Westminster, 20 metres away from the central London headquarters of MI5. There were trips to Dubai, Hong Kong, Barcelona and Paris. According to the images of himself he liked to post online, he was living the life of an international jetsetter. But he was not, in the end, untouchable.</p><p>Police made a micropayment of 0.1 bitcoins to ransom_man. They were able to determine that, when it was laundered into real-world currency, it was transferred into Kivimäki’s bank account. The home folder ransom_man had accidentally uploaded had led the police to some servers, one of which had been paid for using a credit card linked to him – the same one he’d been using to pay for Apple services and an OnlyFans subscription.</p><p>As investigators traced the history on ransom_man’s home folder, they were able to determine that, as well as looking for keywords such as rape, abuse and child molestation in the database of patient records, the hacker had also searched for Kivimäki’s home address, and the names of his family members. “Before publication, he ensured there was no harmful information about him, or people close to him,” Pasi Vainio, the lead prosecutor on the case, tells me. Those searches took place using an IP address linked to Kivimäki’s Westminster apartment. “He was in London when the crimes were committed.”</p><p>But it was a drawn-out, arduous investigation. There were terabytes of data to comb through. The crime had so many victims that the police had to create an online portal for everyone to register and give their statements. That generated more than 21,000 criminal reports, all of which needed to be looked at individually. So it was October 2022 – two years after Parikka, Auer and the other victims had received their ransom demands – before Vainio signed an arrest warrant for Kivimäki. His face – chubby-cheeked and floppy-haired – was added to Europol’s list of most-wanted fugitives, alongside murderers and drug traffickers.</p><p>On 3 February 2023, French police were alerted to a report of domestic violence taking place in a flat in a Paris suburb. Officers used a battering ram to enter the property and found a man and a woman inside. The man was pale and white-blond, but when asked to identify himself he handed over a Romanian passport that gave his name as Asan Amet. “We have a Scandinavian-looking guy, 195cm tall,” Vainio tells me with a smile. “I think the French police just thought something’s off.” They searched their databases and discovered Amet was one of Kivimäki’s known aliases. He was handed over to the Finnish authorities a few weeks later.</p><p>“I don’t know what I had expected, but I was surprised to see that he looked so normal,” Auer says. “He looks like a regular Finnish young man. It did make me feel like it could have been anyone.”</p><p>“I had heard that he was in a court hearing,” Parikka says. “We have a habit – every night at 8.30pm, I’ll lie here on the couch with my spouse and watch the main news. Without warning, Kivimäki was there on the screen. Kivimäki came to my living room.” She glances over to her couch, metres away from where we sit, and is overcome with tears. I didn’t sleep the next night.”</p><p>But when the trial began, in November 2023, Parikka was determined to watch Kivimäki face justice. The logistics of inviting more than 21,000 registered victims to court were impossible; instead, proceedings were relayed to public spaces such as cinemas so that the plaintiffs could watch in real time. In a case that was all about the right to privacy and anonymity, it sounds a profoundly awkward setup. “We were all sitting far away from each other,” Auer says. “It was dead silent.” Parikka had a similar experience. “We pretty much kept to ourselves.”</p><p>On 30 April 2024, Kivimäki was found guilty of all charges – including 9,600 counts of aggravated invasion of privacy and more than 21,300 counts of attempted aggravated extortion – and sentenced to six years and three months in prison: a long stretch by Finnish standards, but shy of the seven-year maximum he could have received. His appeal against his sentence is currently under way.</p><p>Even if his conviction is upheld, he will be a free man by the end of this year.</p><p>“The sentencing scale is too low, in my opinion. But that’s the framework we have in Finland,” Vainio says. He tells me a colleague has tried to quantify the harm caused, using the conservative estimate that each person had endured a week of agony as a result of the hack. “When you multiply it with the number of victims of this case, you would have 635 years of suffering.”</p><p>ow 28, Kivimäki has served much of his sentence in a spotless, bright but suitably austere facility in Turku, south-west Finland, a two-hour train ride from Helsinki. For months, he had refused to grant me an interview, but while I am in Finland reporting this story, he changes his mind. As I sit in silence in the prison’s visitor room for what feels like hours, watching the clock tick down behind a panel of reinforced glass, I&nbsp;wonder if Kivimäki is trolling me; if he has dragged me over here simply to derail the other interviews I already had scheduled, with no intention of ever leaving his cell. But after 40 minutes he appears. With his white-blond hair, ice-blue eyes and razor burn, and dressed in a black&nbsp;T-shirt and shorts, he looks like an overgrown teenage boy.</p><p>He didn’t do it, he says; he’s simply a victim of his own notoriety. “They had to find somebody. They just chose somebody who was convenient for the story.” When I point out that there’s an enormous amount of circumstantial evidence linking him to the hack, Kivimäki is defiant. “The obvious answer is that it’s just somebody close to me.” He has an idea who it is, he continues, but he isn’t prepared to name names.</p><p>It seems very selfless to do time for someone else’s crime, I say. I tell him Parikka says having her therapy notes held to ransom felt like a public rape. “I’m sure that’s how she felt,” he replies, blankly. “It’s quite remote to me. I’m involved, in that I was in court over this stuff, but I didn’t do it. It’s another story in the news.”</p><p>As a fellow human being rather than the person convicted of the crime, I ask, what’s your response to people taking their lives after having their therapy notes stolen? “There’s a lot of terrible things going on in the world. I don’t really feel any differently about this. I turn on the news and there’s people dying in Gaza or wherever. It’s like, how do you feel about? I think the honest answer for most people is that they just … .”&nbsp;You don’t have anything to say to the victims? “Not really,” he replies. “These are nameless, faceless&nbsp;people.”</p><p>“There’s been just one question that I would ask Kivimäki,” Parikka says. “That would be: ‘Was there ever such a moment that you felt empathy?’ I don’t think he’s able to put himself into anybody else’s situation.” She pauses. “I think that he really needs therapy.”</p><p>astaamo was <a href=\"https://tietosuoja.fi/en/-/administrative-fine-imposed-on-psychotherapy-centre-vastaamo-for-data-protection-violations\" data-link-name=\"in body link\">declared bankrupt</a> in February 2021. Days after patients received the ransom emails, the board announced that it had let the CEO, Ville Tapio, go. In April 2023, Tapio was <a href=\"https://yle.fi/a/74-20027665\" data-link-name=\"in body link\">found guilty</a> of criminal negligence in his handling of patient data. His conviction was overturned on appeal in December 2025. (He declined my requests to interview him.)</p><p>“I have actually been more angry towards Ville Tapio than I have been towards Kivimäki,” Auer says. “As CEO of the company, he had the responsibility to make sure that it was prepared for all kinds of risks, and that they had sufficient information security. It seems like it was never a priority to him.” What was his priority “Making money. He ran a very successful business.”</p><p>“I believe that originally the Tapios were wanting to help people and make therapy available,” Parikka says. “There are now maybe thousands of people who will never use therapy again, because they can never trust. And that’s really bad.”</p><p>Alongside more than 6,000 other plaintiffs, Auer and Parikka are part of a civil case suing Kivimäki for damages. Despite the lifestyle he projects online, he claims not to have the funds to pay damages; so far, no&nbsp;one has been able to find his assets. The government has agreed to pay compensation to victims – anything from <a href=\"https://www.valtiokonttori.fi/en/services/services-related-to-compensation-and-accidents/vastaamo/#_how-much-compensation-can-you-get\" data-link-name=\"in body link\">a few hundred euros to a few thousand,</a> depending on how many pages of their therapy notes Vastaamo had in its database, and how sensitive the information contained in those pages was – but the sum is likely to be symbolic. How can you ever repay the damage of being exposed in this way?</p><p>Copies of the patient files have been circulating ever since they were first released in October 2020. At one point, someone created a special search engine for browsing the database. This doesn’t surprise Parikka. “Kivimäki isn’t just one of a kind,” she says. “I know human curiosity. People want to know.”</p><p>Other people are as prepared as Kivimäki was to break moral and legal boundaries – for money, for online clout, out of ghoulish curiosity or simply for the LOLs. In May, Finnish police <a href=\"https://poliisi.fi/en/-/police-have-another-suspect-in-vastaamo-case\" data-link-name=\"in body link\">announced</a> that there was a&nbsp;second suspect in the Vastaamo case, a US citizen living in Estonia – suspected of aiding and abetting Kivimäki, helping prepare the files. He has been charged with assisting in the attempted extortion.</p><p>In an era when AI models are <a href=\"https://www.rollingstone.com/culture/culture-news/zoom-ai-personl-data-1234802844/\" data-link-name=\"in body link\">trained on our Zoom</a> conversations, <a href=\"https://www.theguardian.com/technology/2024/nov/15/x-ai-gmail-meta-privacy-settings\" data-link-name=\"in body link\">emails</a> and status updates, it is naive to believe that anything can ever be fully secure. The human need to confide in others can be met in an extraordinary range of ways in the digital age. In a world of unparalleled connectivity, can our innermost secrets ever be truly safe?</p><p>Kivimäki thinks we are all clinging on to analogue expectations about privacy in a digital world. “So many of our worst secrets – I mean  of worst, things we might really, really not want to share with the entire world – they exist online. They’ll exist in the database of some company you used,” he tells me. “Everybody’s photos, everybody’s text-messaging histories.” He fixes me with his eyes. “You fundamentally want to believe in this privacy. But, on the other hand, I don’t know how you’re going to get there.”</p><p><em>Intrigue: Ransom Man, Jenny Kleeman’s six-part series for BBC Radio 4, is available </em></p>",
      "contentLength": 30186,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46656045"
    },
    {
      "title": "Drone Hacking Part 1: Dumping Firmware and Bruteforcing ECC",
      "url": "https://neodyme.io/en/blog/drone_hacking_part_1/",
      "date": 1768617328,
      "author": "tripdout",
      "guid": 36637,
      "unread": true,
      "content": "<p>In July 2025, we from Neodyme got together in Munich and did security research on a bunch of IoT devices, ranging from bluetooth headsets, to door locks, to drones. One of these was the Potensic Atom 2. It’s a photo and video drone with a gimbal-stabilized 4K camera and a remote control that you hook up to your own smartphone and the proprietary app. If you’ve ever flown a DJI Mini 4K, this drone will look very familiar to you.</p><p>This post is part of a  that will cover how we disassembled the drone and dumped the firmware from the NAND chip and how we analyzed the drone’s firmware, app, and remote control to find some backdoors and vulnerabilities.</p><h3>Goal: Dumping the Firmware<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#goal-dumping-the-firmware\"></a></h3><p>One of the most important pieces of information you can acquire when setting up to hack a device is its firmware. If you want to reverse engineer the software that’s running on the drone and find vulnerabilities in that, then you need a copy of it in the first place.</p><p>Now there are a couple of ways to go about that, some are less intrusive and some are more effective.</p><p>You might get lucky and be able to just  as a firmware update from the manufacturer’s website. However, those update sites are often not publicly documented and can be locked behind authorization checks or encrypted. Encrypted firmwares can still be useful - you “just” need to reverse engineer the on-device decryption process. For the Atom 2, downloading the firmware updates required having a valid drone and remote control serial number  the firmware update was also encrypted. Without having the decryption logic, we put this approach on ice during our initial research.</p><p>Another really comfortable approach is to use exposed  like JTAG or UART. However, those are often undocumented, unlabeled, or entirely removed for public versions. We didn’t find any on the Atom 2.</p><p>What we can always do, though not necessarily always successful, is solder off the entire NAND chip and  byte by byte. This has the risk of breaking the NAND chip and/or the rest of the board if you’re not careful. Also, some devices, like modern smart phones, encrypt their persistent storage with key material stored in, e.g., the TPM. If that is the case, then simply soldering off the NAND chip will leave you with unusable encrypted data. Fortunately, the Atom 2’s NAND contents are not encrypted, as we find out later.</p><p>Dumping a NAND chip generally always follows the same pattern:</p><ol><li>Identifying the NAND Chip</li><li>Removing it from the board</li><li>Identifying the data pins and communication protocol of the NAND chip</li><li>Connecting the NAND chip to some kind of reading device</li><li>Reassembling the read contents into a working firmware - usually containing one or more file systems</li></ol><h3>Identifying the NAND Chip<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#identifying-the-nand-chip\"></a></h3><p>The Atom 2 has multiple boards:</p><p>We are mainly interested in the  because that’s where the NAND flash is going to be. The main board had several metal RF shields that we have already pried off or cut through on the photos.</p><p>We can identify most of these chips through their markings. Note that while we’re mainly interested in the NAND chip, knowing the others can help with recognizing things during reversing later on. Roughly knowing which SoC we were working with was crucial, as you will see in later sections of this blog post.</p><p><em>(Note that the markings below might not match the photos completely. We had multiple drones. The markings are mostly from the first drone and the photos are mostly of the second drone.)</em></p><p><strong>SoC (System on a Chip, aka “the main thingy”)</strong></p><ul><li>Markings: 23AP10 VTQMSQKJYJ 4978-CN B3</li><li>We didn’t find an exact match, but <a href=\"https://blog.csdn.net/szhorsway/article/details/144353108\">this site</a> references the 21AP10.\n<ul><li>The page title is <code>21AP10 SS928 平替SD3403V100 海思 SOC芯片</code></li><li> =&gt; <code>21AP10 SS928 Drop-In Replacement</code></li><li> =&gt; <code>HiSilicon SD3403V100 SOC Chip</code></li><li>That is a mobile camera SoC.</li></ul></li><li>It makes sense that this is the SoC because it is close to both external RAM chips and the NAND flash.</li><li>We found a data sheet for the HiSilicon Hi3519 V100.\n</li></ul><ul><li>Markings: SEC340 K4A8G16 5WC BCTD G2F9190AC</li><li>Data sheet can be found on the internet.</li></ul><ul><li>Markings: GD32F470 VGH6 BUMK618 AL2451 GigaDevice ARM</li><li>Data sheet can be found on the internet.</li><li>Might be SD-card related since the SD card slot is on the other side.</li></ul><ul><li>Markings: V2 2441TM4N190.00\n<ul><li>The name  appears in some WizSense surveillance cameras</li></ul></li><li>2 chips with markings: 8285HE 426656 CS2441</li></ul><ul><li>Markings: MXIC X243662 MX35UF4GE4AD-241 5P231800A1</li><li>Data sheet can be found on the internet.</li></ul><ul><li>Markings: SEC407 K4A8G16 5WC BCTD G2K43304C</li></ul><ul><li>Markings: F460JEUA P8VR4400 2416021</li><li>Not sure what it’s used for.</li><li>A data sheet for the HC32-QFN48TR (looks close enough?) can be found on the internet.</li></ul><ul><li>Data sheet can be found on the internet.</li></ul><h3>Removing the NAND Chip from the board<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#removing-the-nand-chip-from-the-board\"></a></h3><p>Now that we have identified the NAND chip, we fasten the board and tape off the remaining components with heat-shielding tape.</p><p>Usually getting the chip off of the board is just a matter of using hot air station and flux. However, you can see on the photos that the chip is actually glued to the main board with what is probably epoxy. That’s a thing you can do if you want to secure the chips more securely and not depend on the solder joints to hold your chip in place (and risk breaking them). Or you can do that  to the NAND chip to make it harder for researcher to pry off your NAND chip and dump your firmware.</p><p>Anyway, a few cuts with a sharp knife, some heat and a  amount of flux later, the little bugger came off in one piece.</p><p><em>(And with it, a couple of extremely tiny resistors that I knocked off with my pliers and promptly lost. This main board is now broken. But don’t worry, <a href=\"https://youtu.be/D_qFWoa_HR4&amp;t=293\">through the magic of buying  three of them</a>, we can still fly the drone.)</em></p><p>According to the data sheet of the MX35UF4GE4AD NAND chip, the flash chip can either come in a 24-pin BGA package or an 8-pin WSON package, which we have here. A quick look at the pin descriptions tell us that the NAND chip is communicating via SPI.</p><table><thead><tr></tr></thead><tbody></tbody></table><p>Well, let’s solder tiny copper cables to all of those pins and drown them in a bit of hot glue to stop them from breaking off.</p><p>Note that you  get a proper socket for 8-WSON chips into which you simply clamp the chip and which exposes easy-to-work-with breakout pins. None of our sockets we brought fit though, so we just did it the old-school way.</p><h3>Connecting the NAND chip to some kind of reading device<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#connecting-the-nand-chip-to-some-kind-of-reading-device\"></a></h3><p>SPI is pretty easy to work with. We have two main data lines called  and  (Serial In/Out). You will also find them under the names “MOSI” and “MISO” (Master Out Slave In / Master In Slave Out). As the names of these suggest, SPI follows a master-slave architecture. The microcontroller drives the communication and the peripheral device reacts.</p><p>Fortunately, we are simulating the microcontroller-side of the communication, which means that we have a large amount of control. Specifically, we control the clock (). Sometimes it is hard to talk to embedded hardware because of the speed at which they operate. With SPI, however, we can slow down the clock to however fast we want the devices to talk.</p><p>Since SPI is a bus protocol, more than one slave device can be hooked up to a master device on the same data lines. To avoid collisions, each device is also assigned its own “Chip Select” line (). When the master device wants to talk to a specific slave device, it pulls the corresponding CS line . Devices that have their CS line  won’t react at all.</p><p>Obviously there are fancy devices on the market that will make dumping a NAND chip via SPI pretty easy and straightforward. Problem is, we couldn’t get  of them to work. They either didn’t fit (physically), were too fast or failed for some other strange reason we didn’t understand. So we wrote <strong>our own dump script onto an ESP32</strong> using the SPI commands in the data sheet and let it forward the data to our computer via the USB console.</p><p>Doing that, we ended up with a 544 MiB dump, containing 131,072 pages of 4096+256 bytes. <em>(We will come back to that “+256” later on.)</em></p><p>Let’s dig into what this the flash dump contains with :</p><p>Sweet! We get a working ASCII copyright string, so  must have worked. And at the end of the image we have a bunch of UBIFS images. That’s probably where all the juicy files are!</p><p>Let’s extract them with  and take a look inside with <a href=\"https://github.com/onekey-sec/ubi_reader\">ubi_reader</a>:</p><p>Hmm. That doesn’t work. Spoiler: The extracted image is broken.</p><p>Now if you’ve ever done something as hacky as this, you will know about a pesky little phenomenon that happens when you just solder copper wires onto a chip, stick that onto the ports of an ESP32 and do SPI communication - which has <strong>no built-in integrity checks</strong>.</p><p>These are three dumps taken from the same NAND chip:</p><p>If you read 4 MiB of data from the chip, not all of the bits you receive are correct. And without any additional data, you have little way of knowing which ones are correct and which are not. If you are lucky, then the dumped data will still “work”, i.e., the contained file system will mount and you can browse files, but futher down the line you will have no way of knowing whether that weird function you’re reversing is actually weird or just the product of random bit flips messing up the CPU instructions.</p><p>A relatively simple yet time-consuming way to get around this: Read the flash  (at least three times) and hold a majority vote for every bit. Since the bit flips are random and not too prevalent, they are less likely to hit the same bit twice.</p><p>Hot tip: If you’re gonna work with python, use numpy and work on arrays and  memory-mapped files. Otherwise this can take a lot of time and a lot of RAM - even for a 512 MiB flash dump.</p><div><figure><pre data-language=\"python\"><code></code></pre></figure></div><p>But isn’t there a better way? Yes, there is. And - btw - even with completely correct majority voting, the flash content is still broken. But we’ll get to that.</p><p>Trying to work with the majority-voted dump:</p><h2>Out-of-band bytes and ECC troubles<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#out-of-band-bytes-and-ecc-troubles\"></a></h2><p>One thing we have have silently brushed aside for now: The NAND chip distinguishes between “user data” and “extra data”. In our dump above, we have naively concatenated it all together and assumed that a page size of 4096+256 bytes somehow makes sense. Of course, it doesn’t.</p><p>Also, this majority voting hack is obviously not the “correct” way to work with a NAND chip. And even the proper SoC mounted on the proper mainboard can’t run a system off of a flash chip that gives it random bit flips that it can’t detect or recover from. The problem is that NAND is just inherently imperfect storage. Majority voting only corrects for transmission errors during dumping, but does nothing to bit errors that are stored on the device! Bits on the storage might decay over time, or the CPU might also have some transmission errors during writing.</p><p>Of course this problem has been very well known for a long time, so manufacturers always include some extra space next to user data for “error correction”. These extra bytes are called “out-of-band” bytes. And they are used to implement  (ECC).</p><h3>ECC according to the NAND chip data sheet<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#ecc-according-to-the-nand-chip-data-sheet\"></a></h3><p>The flash chip implements its own error correction algorithm by reserving some of the space for ECC. It is split into</p><ul><li>4096 user data bytes + 256 “extra” bytes (aka out-of-band bytes)</li><li>=&gt; 512 MiB of user data (the chip is 4 Gb, not 4 GB)</li></ul><p>If the chip-internal ECC is enabled, some of those extra bytes are used for ECC.</p><p>At this point, we naively assumed that each page would be</p><ul><li>4096 bytes of user data followed by</li><li>256 (or less?) bytes of ECC covering the previous 4096 bytes.</li></ul><p>However, we quickly found out that the sequences classified as “extra data” contain readable strings! That definitely suggests that this isn’t ECC data.</p><p>You can also see that there are parts within the user data where strings are suddenly cut off. This suggests that the ECC layout we assumed was wrong. It took us quite a while to figure out what exactly we missed.</p><p>Turns out, the ECC layout is not just 4096 bytes of user data followed by 256 bytes of ECC. If we put all pages next to each other and then calculate the entropy over each n-th byte of a page, we will find that there are multiple sections with high entropy:</p><p>Why are we looking at entropy? Well, because we expect the user data to have  ASCII text (low entropy) every now and then and the ECC data to be mostly random-looking byte values (high entropy).</p><p>The graph we’re seeing up here suggests that there are sections of  1 KiB of user data, followed by 28 bytes of ECC data. Specifically,</p><ul><li>1028 B user data + 28 B ECC</li><li>1028 B user data + 28 B ECC</li><li>1028 B user data + 28 B ECC</li><li>1014 B user data + 28 B ECC</li></ul><p>Why these stranges values? And which ECC algorithm is that? We  choose to ignore that and just extract the user data sections and stitch them together. I won’t spam you with more  and  screenshots and just  you that that also won’t result in a readable UBIFS image. Fortunately, we find an explanation for these values in the next section!</p><h3>ECC according to the SoC data sheet<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#ecc-according-to-the-soc-data-sheet\"></a></h3><p>At this point, we had already spent a lot of time fiddling with unstable reading setups and ECC layouts. And then we found that some further digging into the  documentation could have saved us a lot of time during our research. Because the SoC  does ECC. Not just the NAND chip. In fact, we can ignore the NAND chips ECC feature completely.</p><p>The SoC’s data sheet lists several possible ECC layouts. One of them is the following:</p><p>Well, that fits our findings perfectly, plus a BB (“bad blocks”) and CTRL (some kind of control bytes?) area that we didn’t identify before.</p><p>Using this diagram, we can cut out all the ECC, BB and CTRL sections and reconstruct the pure 512 MiB user data flash content.</p><div><figure><pre data-language=\"python\"><code></code></pre></figure></div><p>Heyyy, look at that! We managed to extract a file system again - albeit with some error remaining. Let’s see what’s on it:</p><p>Hmm. Damn. The UBIFS image is now at least  syntactically correct. But it is still broken enough to not have any files. Why could that be?\nWell, we are looking at  what the SoC would see after reading the data from the NAND chip.  that we have done majority voting on the bytes - so our version is even better than what the SoC would see.</p><p>, there is no guarantee that there aren’t any random bit flips , i.e., that random bit flipping happened during , desoldering or anytime between that!\nSo, there seems to no way around actually implementing the ECC algorithm and correcting the bit flips on the flash dump. Problem is: <strong>What kind of ECC algorithm is the SoC running?</strong> Unfortunately, the datasheet is silent here, so we had to find out on our own.</p><h3>A short primer on reverse engineering ECC algorithms<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#a-short-primer-on-reverse-engineering-ecc-algorithms\"></a></h3><p>Typical ECC algorithms on NAND chips use <a href=\"https://en.wikipedia.org/wiki/BCH_code\">BCH codes</a>, which are parametrized by the following properties:</p><ol><li>The amount of parity bits.</li><li>The correction capacity , i.e., how many simultaneous bit flips may appear in the same data block before the block is “too broken” and the ECC algorithm fails.</li><li>The primitive polynomial used in the equation. If you don’t know what this is, just think of it as an integer parameter for now.</li><li>Whether and how the data is transformed  the parity bits are calculated.</li><li>Whether and how the parity bits are transformed  they are calculated.</li></ol><p>We can deduce (1) and (2) from our flash dump. For (3), (4), and (5) we have to either find the code of the SoC (if it is implement in software at all) and reverse engineer the ECC algorithm - or just bruteforce them.</p><p>As we have seen in the SoC’s data sheet, we have 112 byte of ECC / parity bits. However, the fragmented layout on the flash suggests that we actually have 4 ECC groups of 28 byte, each covering a different part of the user data. Note that this is an educated guess and does not have to be true. If we’re not getting anywhere, we should consider dropping this assumption later on. But spoiler: We’re right about this.</p><p>This means that we have  (= 28 bytes).</p><p>This part we can just calculate if we make one very realistic assumption. We have 1028 bytes of user data, which is 8224 bits. If we want to represent these 8224 bits as a binary polynomial, we need at least degree 14:</p><p> &lt; - Too small\n &lt; - Fits!</p><p>This means our primitive polynomial needs to be at least degree 14 ().</p><p>The correction capacity is determined by the degree  and the amount of parity bits. The more parity bits we have in relation to , the higher our correction capacity :</p><p>Now given that  is fixed at 224 and assuming that the engineers chose  to be maximal, we conclude that  and</p><p>meaning that we can correct for up to 16 bit flips in the covered user data chunk. Anything more than that and the chunk is lost.</p><p> also fits the description of the ECC section in the SoC’s data sheet: “16-Bit/1KB Error Correction Performance” (see our diagram above). So we are pretty certain that this assumption is correct.</p><p>We don’t know that and we will have to bruteforce it. Given that it is a  polynomial, it is usually represented as a bit vector or simply as an integer. Given that  we already know that our polynomial must have its 14th bit set and that the 14th bit is the highest bit that is set:</p><p>which is well within bruteforcing range.</p><h4>Pre-encode and post-encode transformations<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#pre-encode-and-post-encode-transformations\"></a></h4><p>There are a few transformations that are commonly applied to either the user data  calculating the parity bits (“encoding”) or applied to the parity bits  calculating them. Examples include</p><ul></ul><p>Why you ask? Well, one of these combinations for example is very useful for NAND storage devices. You see, when NAND pages are erased and then read, their values are all . Problem is, a page full of  will have</p><ul></ul><p>and that is  a valid parity, meaning a cleanly erased page will be read as containing a lot of errors. That is because</p><p><code>parity(0xFF...) != 0xFF...</code></p><p>But we can pull a trick to make this work: Before encoding, invert the user data. And after encoding, invert the parity bits:</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>Notice that the parity of an all-zero page  all-zero:</p><p><code>parity(0x00...) != 0x00...</code></p><p>So by implementing our ECC algorithm with these two inversions, we make a freshly erased page () have a valid parity () and the SoC’s error correction won’t need special handling for erased NAND pages.</p><p>Okay, back to the actual algorithm at hand here. How do we know that these two inversions are what the engineers actually chose? We don’t! We just try all these different transformations and see if one of them works. The amount of possible combinations of these 4 transformations is quite low and easily bruteforcable.</p><h3>Brute-forcing ECC parameters<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#brute-forcing-ecc-parameters\"></a></h3><p>In summary, to test our guessed parameters, we need a user data section without errors, generate its ECC and check if it matches the ECC that we read off of the NAND chip. Our bruteforce script thus has to do the following:</p><ol><li>Find a “good” user data section with no bit flips and the corresponding ECC section.</li><li>Iterate through all possible transformations.</li><li>Iterate through all possible primitive polynomials of degree 14</li><li>For each iteration, generate the ECC of the userdata section. If it matches the existing ECC, we have found the parameters.</li></ol><h4>Picking a “good” user data section<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#picking-a-good-user-data-section\"></a></h4><p>We need a user data section and its corresponding ECC section without bit flips. But how do we know that a section does not have bit flips? We don’t! We  try to be clever here. One possible approach would be to pick a section with a lot of text and check if the text makes sense. But the lazy approach works just as well: Just try a lot of sections and hope that one of them is correct. </p><div><figure><pre data-language=\"python\"><code></code></pre></figure></div><p>As you can see in the script above, we only look at the first section of every page and then move on to the next page entirely. We’re not completely sure yet which ECC bytes cover which user data sections - especially since the 4th section looks fragmented. But we will stick to our assumption that the first 1028 bytes of user data are covered by the first 28 bytes of ECC. Spoiler: We’re right about this. Another Spoiler: The first 3 pages have bit-flips in their first section. The 4th one is good.</p><p>We will try out 4 different transformations:</p><ul></ul><div><figure><pre data-language=\"python\"><code></code></pre></figure></div><p>For these transformation we want all possible subsets and orderings, but without using the same transformation twice in one run.</p><div><figure><pre data-language=\"python\"><code></code></pre></figure></div><p>Then we can run through all these combinations as both pre-transformations as well as post-transformations:</p><div><figure><pre data-language=\"python\"><code></code></pre></figure></div><p>Note that some combinations are equivalent:</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>We don’t optimize for that though.</p><h4>Iterate through all possible primitive polynomials of degree 14<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#iterate-through-all-possible-primitive-polynomials-of-degree-14\"></a></h4><p>There are three ways to do this:</p><ol><li>A math-heavy and fast way</li><li>A much better way that is about as simple as (1) and as fast as (2)</li></ol><p>Of course, we went with (1) during our initial research because sometimes thinking just takes longer than computing inefficiently. Afterwards, I spent hours digging into polynomial algebra to come up with (2) and was very happy about it - only to also find (3) right afterwards which was a lot simpler and equally as good… Oh well, at least I got to freshen up on first- and second-semester linear algebra.</p><p>\nThe simple way would be to just try all polynomials of degree 14. In their integer representation, that’s all integers in\n\nWhile this  eventually cover the correct primitive polynomial, it will also make bchlib crash the entire script with a SIGSEGV for a lot of non-primitive polynomials.</p><p>A quick-and-dirty workaround is to just spawn a new process for every candidate polynomial so your main script doesn’t die. And that’s what we did during initial research. It works - but the creation of over 16,000 processes makes this a bit slow. Not  slow to work with though. This approach works in practice.</p><p>\nThe  way to do this is to only pass primitive polynomials into BCH’s constructor. But how do we know if the polynomial that is represented by our integer is primitive? By doing lots of math. If you’re not familiar with polynomial algebra (like I was) but really want to know how this works, read the section <strong>A brief detour into polynomial algebra</strong> in the addendum.</p><p>Spoiler: It is a lot of thinking work and only about 5% faster than (3) in my tests.</p><p>\nTurns out, bchlib only crashes for polynomials with a constant term of 0, i.e., even integers. So if we use <code>range(2**14 + 1, 2**15, 2)</code>, then it just works without having to fiddle with multiprocessing or math.</p><p>This will still throw a runtime error for a lot of non-primitive polynomials but we can catch that via try-except:</p><div><figure><pre data-language=\"python\"><code></code></pre></figure></div><h4>Checking the generated ECC<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#checking-the-generated-ecc\"></a></h4><p>This is straight-forward and self-explanatory.</p><p>You can find the full  in the addendum.</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><h3>Restoring the full firmware<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#restoring-the-full-firmware\"></a></h3><p>Now that we have a working ECC setup, let’s reassemble the entire firmware! There is just one little detail that we still need to find out:</p><p>Which parts of user data are covered by which parts of ECC? We already confirmed that the first user data section is covered by the first 28 bytes of ECC. And the same turns out to be true for the second and third user data section. The fourth section is a bit tricky: It is 928+84 bytes of user data long, with additional BB and CTRL bytes around. What is that about? Turns out, a bit of trial-and-error and looking at the SoC’s data sheet revealed how ECC works for that section.</p><p>Now we just need to apply that to every page and - voilà - full firmware dump. :tada:</p><p>The  can be found in the addendum.</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>If we look at the binwalk output for that file, it is  better and looks like it is actually free of errors:</p><p>When trying to extract the first ubifs image with ubi_reader, we actually get a working file system!</p><p>ubi_reader still throws an error in the later segments of UBIFS image but this extraction is good enough to start reverse engineering the successfully extracted files. Notably, it is enough to reverse engineer the firmware decryption!</p><p>Stay tuned for  of our drone hacking blogpost where we dive into the reverse engineering and vulnerability analysis of the Potensic Atom 2!</p><div><figure><pre data-language=\"python\"><code></code></pre></figure></div><div><figure><pre data-language=\"python\"><code></code></pre></figure></div><h3>Primitive Binary Polynomial Generator<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#primitive-binary-polynomial-generator\"></a></h3><div><figure><pre data-language=\"python\"><code></code></pre></figure></div><h2>A brief detour into polynomial algebra<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#a-brief-detour-into-polynomial-algebra\"></a></h2><p>If you are like me and you’re not really familiar with polynomial algebra, it makes sense to talk about related concepts in the integer world first and then move on to their counterparts in the polynomial world. This helps to get an intuition of what we are actually dealing with.</p><p>I assume that you are generally familiar with modular arithmetic and prime numbers.</p><h3>Integers and Modulo Rings<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#integers-and-modulo-rings\"></a></h3><p>For now, we are working on the field of integers, i.e., , mathematically denoted .</p><p>When we introduce a modulus, we get . Explanation for the notation:</p><p> is some modulus. Not necessary prime at this point. Just an integer - a member of .</p><p> is all multiples of . So .\nIf  were , this would be .</p><p>Now  means:\nThe field  but treat all elements as equivalent if they are a multiple of  apart - meaning their difference is in .\nIf  were , then  and  would be equivalent, because their difference is , which is a multiple of  and thus in .</p><p>This is exactly what “mod 7” is: </p><p>So  is just all of .</p><p>Note that this collapses the infinite field of all possible integers  down to a finite set of equivalence classes. In , all numbers are either in  or are equivalents of one of those. So for practical purposes, there are only  possible values in .</p><p>What do we need primes for?\nWell,  has a practical problem: Sometimes multiplying two things results in a zero. Example for :</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>That is bad if we want to do a lot of multiplication within our modulo ring. Because if we ever accidentally hit a multiple of , we will get  and that point it doesn’t matter what we multiply onto that - it will stay . So there are a lot of possible values that all collapse into the same  when multiplied with certain numbers.</p><p>In the field of all integers, , we don’t have that problem. As long as we don’t multiply with  itself, the results of a multiplication will never be . Good thing there is a solution for that: Using a .</p><h3>Prime Numbers, Finite Fields, and Cycles<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#prime-numbers-finite-fields-and-cycles\"></a></h3><p>Prime numbers  have the nice feature that the modulo rings they induce, , are  and not just , meaning addition and multiplication work just as well as they do in . In , multiplying by something other than  will  result in a multiple of  - because  is prime and you can’t reach a prime from another number through multiplication. (Multiplying by  itself doesn’t count, because .)</p><p>Because  is a  and it has a finite amount of elements , it is called a “finite field” or a “Galois field” and sometimes denoted  instead of . To be super precise,  is a generalization and means “any finite field with  elements”.  just happens to be one of those - and is the most popular one.</p><p>Now we go a step further and look at a concept called . Before explaining that, let’s take a look at a use case for them first.</p><p>Imagine you want a pseudorandom permutation of , i.e., you don’t want the sequence  but a more random-looking sequence tha still hits all of these numbers. It doesn’t have to be cryptographically random or unpredictable. It just needs to look at bit random - perhaps to de-cluster memory writes for better wear-leveling. Primitive roots give us a nice implementation for that.</p><p>In , multiplication never yields  unless you multiply by . That means you can, e.g., keep doubling a number and will never accidentally hit :</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>Oh look at that, a ! This is what will  happen in a finite field. When you keep multiplying by the same number, you will eventually reach the number you started with. And at that point, you are in a cycle.</p><p> is a bit impractical though because its induced cycle only ever hits the numbers  and never hits . Note that  cycle will ever hit , so the maximum cycle we can possible get with a modulus of  is cycle length .</p><h3>Primitive Roots and Prime Factors<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#primitive-roots-and-prime-factors\"></a></h3><p>And there are indeed numbers that generate a full cycle!</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>These numbers, numbers that induce a full cycle in a finite field, are called .  and  are each primitive roots modulo . Since their induced cycle has a length of , the so-called  of  and  is .</p><p>More generally, a primitive root  modulo some prime  is a number whose order is , i.e., whose induced cycle has a length of . In that case . Expressed more formally:  is a primitive root modulo  if and only if the  for which  is true, is </p><p>Now then how would you best check if a number  is a primitive root modulo ? The obvious solution is to just count  upwards from 1 through  and check  each time. That  but it can take a long time for big numbers. Turns out, we don’t have to check every possible  from  through . To understand that, take a look at  from before:</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>We can see that the cycle has length  and contains the numbers . We  see the numbers . But what happens when we use  but we  at ?</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>Again, a cycle of ! And this time we’ve seen all the remaining numbers . If we instead start at  or , we will get the same cycle over . So  splits the entire finite field excluding 0 into two subsets:  and .</p><p>Let’s look at another example:  and try to find all cycles.</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>So  partitions the non-zero elements into `.</p><p>Notice how all those cycles induced by the same number always have the same size? If you think about it, then that makes perfect sense. When you have  but you “start at ”, you’re basically just taking the regular  cycle and multiply its elements by . So the resulting cycle must have the same length: .</p><p>And this isn’t a coincidence. In fact, all cycles induced by the same number  have the same length. And that number always partitions the entire finite field without  into disjoint sets of the same size.</p><p>How does that help us? Well, for  it makes the cycle lengths  and  impossible! Because you can’t cover all  non-zero elements  by splitting them into sets of size  or . More generally, cycles lengths (and thus orders) must always be a divisor of , i.e., the amount of all non-zero elements.</p><p>This is called <a href=\"https://en.wikipedia.org/wiki/Lagrange%27s_theorem_(group_theory)\">Lagrange’s theorem</a>: The order of the subgroup divides the order of the whole group.</p><p>So to check if  is a primitive root, we don’t have to check all  from  through . We only have to check all divisors of ! Also, we obviously don’t have to check . The only number for which  is  itself, which  induces a cycle of  and is thus never a primitive root for .</p><p>So we’re down to only having to check all divisors of  that are larger than . That already eliminates most of the candidates.</p><p>But we can go !</p><p>We only need to check  of the divisors. This is where prime factorization comes into play. Let’s say that we have the prime factors of . We will call these prime factors . Note that  itself does not have any prime factors because - well - it is a . But  have prime factors. In fact,  will always be one of those prime factors because  must be odd to be a prime and  must therefore be even.</p><p>Now what if I told you that we only need to check the divisors of  that we can obtain by dividing through a prime factor. Specifically, we only need to check</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>Why is that? Well, first of all, notice that the “other divisors”  are themselves divisors of either  or :</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>And if , then that implies that</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>Why? Well, because  equal to one, then</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>and that is a contradiction! You can prove the same for any  in general if you want. The general proof works just like this example.</p><p>Alright, so we only have to check all possible  with  being a prime factor of .</p><p>Here is a generator function that will return all primitive roots for a given prime modulus :</p><div><figure><pre data-language=\"python\"><code></code></pre></figure></div><p>And you know what? Armed with this, we can not just find  but also ! We  need to translate this concept to polynomials!</p><h3>Binary Polynomials, Irreducibility and Primitive Elements<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#binary-polynomials-irreducibility-and-primitive-elements\"></a></h3><p>Alright, first things first: Polynomials. They can look something like this:</p><p>And through the black magic of math, we can treat these polynomials as multidimensional numbers:</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>And on these, we can perform the same kind of arithmetic as on integers: Addition, Multiplication, Subtraction, Division - and thus Modulo. Dividing one polynomial by another sounds odd? It is. We’ll skip the details here since we’ve already derailed enough. Just know that the intuition from integer arithmetic carries over to polynomials.</p><p>Now keep in mind that we are working with  polynomials, i.e., polynomials whose coefficients are either  or . And we can represent those as integers. For example,  represents the polynomial</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>We use these binary polynomials because they have a numbers of nice properties that things like BCH error correction codes rely on. For example, we can represent all binary strings of length  as a -degree polynomial with binary coefficients. For integers, we’d represent those strings as integers from  to . But we would have problems doing modular arithmetic on those strings because  isn’t necessarily prime and then  isn’t a field. For polynomials, there are nice and efficient ways to build a field over -degree binary polynomials.</p><p>Throughout this section we have to keep in mind that the integer representation ( in the example above) is just a <em>representation of the polynomial in memory</em>. The polynomial is  an integer and we can’t just do regular  arithmetic like addition and multiplication with Python’s  and  with it. Polynomials have their own arithmetic and they work differently.</p><p>Since we are dealing with binary polynomials, the coefficients of the polynomials are all either  or . More formally, the coefficients are elements of , which is basically the same as  or “mod 2”.</p><p>The set of all binary polynomials is called . In CompSci terms, these are all possible bit arrays.</p><p>So since we’re moving from working on integers to working on binary polynomials,  is our binary polynomial-equivalent of , with  being all integers and  being all binary polynomials.</p><p>While we used  to represent a single integer in , we will use  to represent a single polynomial in .</p><p>Similar to how  is the set of all multiples of n,  is the set of all multiples of  - <em>notice the double-paranthesis</em>.</p><p>And similar to how  is  but treating two integers as equivalent if their difference is a multiple of ,  is  but treating two polynomials as equivalent if their difference is a multiple of . It is essentially “mod ” in polynomial world.</p><p>If we have an integer  and  can’t be divided by another integer, then  is . Analogously, if we have a polynomial  and  can’t be reduced by another polynomial, then  is .</p><p>If  is prime, then there is at least one  in , so that  spans the entire . If  is irreducible, then there is at least one  in  so that  spans the entire .</p><p>(Side note: “primitive root” is a legacy term only used for . “primitive element” is the general term. They mean the same thing.)</p><p>Also, while the order of  is just , the order of  is , which is the amount of all possible -bit arrays.</p><p>Now we are almost there! We have already come far enough where we can recognize that an irreducible polynomial  is analogous to a prime integer .</p><p>Now, what is a  then?</p><p>Well, that’s simple! A  polynomial is an irreducible polynomial with one additional requirement:</p><p>The super simple polynomial\n\nmust be a primitive element, i.e.,  must span the entire .</p><p>Why is that useful? Well, because that means that every non-zero polynomial in  can be represented as  for some integer . So every non-zero polynomial can be represented as an integer and we have a random-looking permutation of all non-zero binary polynomials in .</p><p>And  is what BCH codes use and why we require a primitive polynomial.</p><p>Now, how do we calculate all primitive polynomials then? We don’t. There are infinitely many. But we can calculate all primitive polynomials of some specific degree ! And how do we do that? Well, using what we have already built for finding primitive roots for integers:</p><ul><li>Iterate through all possible polynomials of degree . For each candidate , <ul><li>check if  irreducible. Discard if not. </li><li>check if  (the simple polynomial with integer representation 2) is a primitive element in the finite field induced by . <ul><li>the wanted cycle length  is the amount of all possible polynomials of degree :\n</li><li>get all prime factors  of </li><li>check if  for all prime factors </li></ul></li></ul></li></ul><p>, we want a primitive polynomial with degree , so we only consider polynomials where their m-th coefficient is 1 and all higher-order coefficients are 0. This means that their integer representations are in</p><p>To speed it up by factor 2: We can’t have any polynomials where the constant term, i.e., the lowest-order coefficient is zero. Example:</p><p> (note that there is no  at the end)</p><p>That’s because every polynomial with a zero constant term is divisible by the polynomial  and is thus not irreducible and thus not primitive. So we only iterate over polynomials with a constant term of . In the integer representation, those are the odd integers, so we use</p><p><code>range(2^m + 1, 2^(m-1), 2)</code></p><p>, checking whether a polynomial is irreducible is the analogue to checking whether an integer is prime.</p><p>Ruling out even integer representations of polynomials in (1) skips half the possible candidates because those are all reducible. Unfortunately, there will still be plenty of reducible polynomials left. Just like there are a lot of non-prime numbers among the odd integers.</p><p>For a full irreducibility test, we use <a href=\"https://en.wikipedia.org/wiki/Factorization_of_polynomials_over_finite_fields#Rabin's_test_of_irreducibility\">Rabin’s Test</a>. The implementation has a similar structure as the <a href=\"https://en.wikipedia.org/wiki/Miller%E2%80%93Rabin_primality_test\">Rabin-Miller Test</a> for integers. We won’t cover either here because this section is long enough already.</p><div><figure><pre data-language=\"python\"><code></code></pre></figure></div><p>, we do the same as for primitive roots in integer-world, except we use a polynomial-compatible  function:</p><div><figure><pre data-language=\"python\"><code></code></pre></figure></div><h3>Finding all Primitive Polynomials<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://neodyme.io/en/blog/drone_hacking_part_1/#finding-all-primitive-polynomials\"></a></h3><p>And now we can  compute all the primitive polynomials with degree 14. The full script is attached in the addendum. It takes about 1 second (single-threaded, on my laptop) to list all 756 primitive polynomials for degree 14. Yes - there are only 756!</p><div><figure><pre data-language=\"plaintext\"><code></code></pre></figure></div><p>And how much faster does this make our script compared to the naive\n<code>range(2**14 + 1, 2**15, 2)</code>\nthat lists half of all possible binary polynomials of degree 14?</p><p>About 1 second on a single-threaded run on my laptop…</p><p>Turns out just throwing a bunch of mostly non-primitive polynomials against bchlib and catching the exception is just as fast as doing it properly…</p><p>Oh, and you know what? You could have also just pulled a list of all primitive polynomials of degree 14 from the internet. Because, well, we aren’t the first to generate that list.</p><p>But hey, we learned some math.</p>",
      "contentLength": 37779,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46654749"
    },
    {
      "title": "Ask HN: Is it still worth pursuing a software startup?",
      "url": "https://news.ycombinator.com/item?id=46654726",
      "date": 1768616947,
      "author": "newbebee",
      "guid": 36796,
      "unread": true,
      "content": "\n<p>Considering there is very little moat left in software and big companies can copy your product in no time?</p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46654726\">https://news.ycombinator.com/item?id=46654726</a></p>\n<p>Points: 175</p>\n<p># Comments: 212</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "FLUX.2 [Klein]: Towards Interactive Visual Intelligence",
      "url": "https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence",
      "date": 1768607177,
      "author": "GaggiX",
      "guid": 36609,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence\">https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46653721\">https://news.ycombinator.com/item?id=46653721</a></p>\n<p>Points: 223</p>\n<p># Comments: 57</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "I set all 376 Vim options and I'm still a fool",
      "url": "https://evanhahn.com/i-set-all-376-vim-options-and-im-still-a-fool/",
      "date": 1768600120,
      "author": "todsacerdoti",
      "guid": 37107,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46652690"
    },
    {
      "title": "Releasing rainbow tables to accelerate Net-NTLMv1 protocol deprecation",
      "url": "https://cloud.google.com/blog/topics/threat-intelligence/net-ntlmv1-deprecation-rainbow-tables",
      "date": 1768599744,
      "author": "linolevan",
      "guid": 36594,
      "unread": true,
      "content": "<p><a href=\"https://research.google/resources/datasets/?dataset_types=other&amp;search=Net-NTLMv1&amp;\" rel=\"noopener\" target=\"_blank\"></a></p><p><a href=\"https://www.schneier.com/academic/archives/1999/09/cryptanalysis_of_mic_1.html\" rel=\"noopener\" target=\"_blank\"></a><a href=\"https://github.com/hashcat/hashcat/commit/71a8459d851d246945343ea59effa1d46b965bf8\" rel=\"noopener\" target=\"_blank\"></a><a href=\"https://infoscience.epfl.ch/record/99512/files/Oech03.pdf\" rel=\"noopener\" target=\"_blank\"></a><a href=\"http://www-ee.stanford.edu/~hellman/publications/36.pdf\" rel=\"noopener\" target=\"_blank\"></a></p><p><a href=\"https://www.kali.org/tools/rainbowcrack/\" rel=\"noopener\" target=\"_blank\"></a><a href=\"https://github.com/inAudible-NG/RainbowCrack-NG\" rel=\"noopener\" target=\"_blank\"></a><a href=\"https://github.com/blurbdust/rainbowcrackalack\" rel=\"noopener\" target=\"_blank\"></a><a href=\"https://github.com/jtesta/rainbowcrackalack\" rel=\"noopener\" target=\"_blank\"></a><a href=\"https://github.com/evilmog/ntlmv1-multi\" rel=\"noopener\" target=\"_blank\"></a></p><h3></h3><p><a href=\"https://github.com/lgandx/Responder\" rel=\"noopener\" target=\"_blank\"></a><a href=\"https://github.com/topotam/PetitPotam\" rel=\"noopener\" target=\"_blank\"></a><a href=\"https://github.com/Wh04m1001/DFSCoerce\" rel=\"noopener\" target=\"_blank\"></a></p>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46652617"
    },
    {
      "title": "LWN is currently under the heaviest scraper attack seen yet",
      "url": "https://social.kernel.org/notice/B2JlhcxNTfI8oDVoyO",
      "date": 1768595851,
      "author": "luu",
      "guid": 36549,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://social.kernel.org/notice/B2JlhcxNTfI8oDVoyO\">https://social.kernel.org/notice/B2JlhcxNTfI8oDVoyO</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46651887\">https://news.ycombinator.com/item?id=46651887</a></p>\n<p>Points: 197</p>\n<p># Comments: 127</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Slop is everywhere for those with eyes to see",
      "url": "https://www.fromjason.xyz/p/notebook/slop-is-everywhere-for-those-with-eyes-to-see/",
      "date": 1768593790,
      "author": "speckx",
      "guid": 36548,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.fromjason.xyz/p/notebook/slop-is-everywhere-for-those-with-eyes-to-see/\">https://www.fromjason.xyz/p/notebook/slop-is-everywhere-for-those-with-eyes-to-see/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46651443\">https://news.ycombinator.com/item?id=46651443</a></p>\n<p>Points: 313</p>\n<p># Comments: 132</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Reading across books with Claude Code",
      "url": "https://pieterma.es/syntopic-reading-claude/",
      "date": 1768589369,
      "author": "gmays",
      "guid": 36608,
      "unread": true,
      "content": "<p>LLMs are overused to summarise and underused to help us read deeper.</p><p>To explore how they can enrich rather than reduce, I set Claude Code up with tools to mine a library of 100 non-fiction books.\nIt found sequences of excerpts connected by an interesting idea, or .</p><p>Here’s a part of one such trail, linking deception in the startup world to the social psychology of mass movements (I’m especially pleased by the jump from Jobs to Theranos):</p><p>Claude browses the books a chunk at a time. A chunk is a segment of roughly 500 words that aligns with paragraphs when possible.\nThis length is a good balance between saving tokens and providing enough context for ideas to breathe.</p><p>Chunks are indexed by topic, and topics are themselves indexed for search. This makes it easy to look up all passages in the corpus that relate to, say, .</p><p>This works well when you know what to look for, but search alone can’t tell you which topics are present to begin with.\nThere are over 100,000 extracted topics, far too many to be browsed directly. To support exploration, they are grouped into a hierarchical tree structure.</p><p>This yields around 1,000 top-level topics. They emerge from combining lower-level topics, and not all of them are equally useful:</p><ul><li><em>Incidents that frustrated Ev Williams</em></li><li><em>Names beginning with “Da”</em></li><li><em>Events between 1971 &amp; 1974</em></li></ul><p>However, this <a href=\"https://en.wikipedia.org/wiki/The_Analytical_Language_of_John_Wilkins\" rel=\"nofollow\" target=\"_blank\">Borgesian</a> taxonomy is good enough for Claude to piece together what the books are about.</p><p>Claude uses the topic tree and the search via a few CLI tools.\nThey allow it to:</p><ul><li>Find all chunks associated with a topic similar to a query.</li><li>Find topics which occur in a window of chunks around a given topic.</li><li>Find topics that co-occur in multiple books.</li><li>Browse topics and chunks that are siblings in the topic tree.</li></ul><p>To generate the trails, the agent works in stages.</p><ol><li>First, it scans the library and the existing trails, and proposes novel trail ideas. It mainly browses the topic tree to find unexplored areas and rarely reads full chunks in depth.</li><li>Then, it takes a specific idea and turns it into a trail. It receives seed topics from the previous stage and browses many chunks.\nIt extracts excerpts, specific sequences of sentences, and decides on how best to order them to support an insight.</li><li>Finally, it adds highlights and edges between consecutive excerpts.</li></ol><h3>Claude Code is great for non-coding tasks</h3><p>Even though I’ve been using Claude Code to develop for months, my first instinct for this project was to consider it as a traditional pipeline of several discrete stages.\nMy initial attempt at this system consisted of multiple LLM modules with carefully hand-assembled contexts.</p><p>On a whim, I ran Claude with access to the debugging tools I’d been using and a minimal prompt: <em>“find something interesting.”</em>\nIt immediately did a better job at pulling in what it needed than the pipeline I was trying to tune by hand, while requiring much less orchestration.\nIt was a clear improvement to push as much of the work into the agent’s loop as possible.</p><p>I ended up using Claude as my main interface to the project.\nInitially I did so because it inferred the sequence of CLI calls I wanted to run faster than I could recall them.\nThen, I used it to automate tasks which weren’t rigid enough to be scripted traditionally.</p><p>The latter opened up options that I wouldn’t have considered before.\nFor example, I changed my mind on how short I wanted excerpts to be.\nI communicated my new preference to Claude, which then looked through all the existing trails and edited them as necessary, balancing the way the overall meaning of the trail changed.\nPreviously, I would’ve likely considered all previous trails to be outdated and generated new ones, because the required edits would’ve been too nuanced to specify.</p><p>In general, agents have widened my ambitions.\nBy taking care of the boilerplate, I no longer shy away from the tedious parts.\nRevision is cheap, so I don’t need to plow ahead with suboptimal choices just because it’d be too costly to undo them.\nThis, in turn, keeps up the momentum and lets me focus on the joyful, creative aspects of the work.</p><h3>Ask the agent what it needs</h3><p>My focus went from optimising prompts to implementing better tools for Claude to use, moving up a rung on the abstraction ladder.</p><p>My mental model of the AI component changed: from a function mapping input to output, to a coworker I was assisting.\nI spent my time thinking about the affordances that would make the workflow better, as if I were designing them for myself.\nThat they were to be used by an agent was a mere detail.</p><p>This worked because the agent is now intelligent enough that the way it uses these tools overlaps with my own mental model.\nIt is generally easy to empathise with it and predict what it will do.</p><p>Initially I watched Claude’s logs closely and tried to guess where it was lacking a certain ability.\nThen I realised I could simply ask it to provide feedback at the end and list the functionality it wished it had.\nClaude was excellent at proposing new commands and capabilities that would make the work more efficient.</p><p>Claude suggested improvements, which Claude implemented, so Claude could do the work better.\nAt least I’m still needed to pay for the tokens — for now.</p><h3>Novelty is a useful guide</h3><p>It’s hard to quantify  as an objective to optimise for.<a href=\"https://trails.pieterma.es/book/13/\" rel=\"nofollow\" target=\"_blank\">Why Greatness Cannot Be Planned</a> makes the case that chasing  is often a more fruitful approach.\nWhile its conclusions are debated, I’ve found this idea to be a good fit for this project.</p><p>As a sign of the times, this novelty search was implemented in two ways:</p><ol><li>By biasing the search algorithm towards under-explored topics and books.</li></ol><p>A topic’s novelty score was calculated as the mean distance from its embedding’s  nearest neighbors. A book’s novelty score is the average novelty of the unique topics that it contains.\nThis value was used to rank search results, so that those which were both relevant and novel were more likely to be seen.</p><p>On a prompting level, Claude starts the ideation phase by looking at all the existing trails and is asked to avoid any conceptual overlap.\nThis works fairly well, though it is often distracted by any topics related to secrecy, systems theory, or tacit knowledge.</p><p>It’s as if the very act of finding connections in a corpus summons the spirit of Umberto Eco and amps up the conspiratorial thinking.</p><ul><li>EPUBs are parsed using <a href=\"https://github.com/rushter/selectolax\" rel=\"nofollow\" target=\"_blank\"></a>, which I picked over BeautifulSoup for its speed and simpler API.</li><li>Everything from the plain text to the topic tree is stored in SQLite. Embeddings are stored using <a href=\"https://github.com/asg017/sqlite-vec\" rel=\"nofollow\" target=\"_blank\"></a>.</li><li>The text is split into sentences using <a href=\"https://github.com/superlinear-ai/wtpsplit-lite\" rel=\"nofollow\" target=\"_blank\"></a> (the  model).\nThose sentences are then grouped into chunks, trying to get up to 500 words without breaking up paragraphs.</li><li>I used <a href=\"https://dspy.ai\" rel=\"nofollow\" target=\"_blank\"></a> to call LLMs. It worked well for the structured data extraction and it was easy to switch out different models to experiment.\nI tried its prompt optimizers before I went full agentic, and their results were very promising.</li><li>I settled on Gemini 2.5 Flash Lite for topic extraction.\nThe model gets passed a chunk and is asked to return 3-5 topics. It is also asked whether the chunk is , in order to filter out index entries, acknowledgements, orphan headers, etc.\nI was surprised at how stable these extracted topics were: similar chunks often shared some of the exact same topic labels.\nProcessing 100 books used about 60M input tokens and ~£10 in total.</li><li>After a couple books got indexed, I shared the results with Claude Opus along with the original prompt and asked it to improve it.\nThis is a half-baked single iteration of the type of prompt optimisation DSPy implements, and it worked rather well.</li><li>Topic pairs with a distance below a threshold get merged together. This takes care of near-duplicates such as , , and .</li><li>The CLI output uses a semi-XML format. In order to stimulate navigating, most output is nested with related content. For example, when searching for a topic, chunks are shown with the other topics they contain.\nThis allows us to get a sense of what the chunk is about, as well as which other topics might be interesting.\nThere’s probably more token-efficient formats, but I never hit the limit of the context window.</li></ul><pre tabindex=\"0\" data-language=\"xml\"><code></code></pre><ul><li><p>Topics are embedded using <code>google/embeddinggemma-300m</code> and reranked using .</p></li><li><p>Many CLI tools require loading the embedding model and other expensive state. The first call transparently starts a separate server process which loads all these resources once and holds onto them for a while.\nSubsequent CLI calls use this server through Python’s <code>multiprocessing.connection</code>.</p></li><li><p>The topic collection is turned into a graph (backed by ) by adding edges based on the similarity of their embeddings and the point-wise mutual information of their co-occurrences.</p></li><li><p>The graph is turned into a tree by applying <a href=\"https://leidenalg.readthedocs.io/en/stable/\" rel=\"nofollow\" target=\"_blank\">Leiden</a> partitioning recursively until a minimum size is reached.\nI tried the Surprise quality function because it had no parameters to tweak, and found it to be good enough. Each group is labelled by Gemini based on all the topics that it contains.</p></li><li><p>Excerpts are cleaned by Gemini to remove EPUB artifacts, parsing errors, headers, footnotes, etc.\nDoing this only for excerpts that are actually shown, instead of during pre-processing, saved a lot of tokens.</p></li></ul>",
      "contentLength": 9168,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46650347"
    },
    {
      "title": "Our approach to advertising",
      "url": "https://openai.com/index/our-approach-to-advertising-and-expanding-access/",
      "date": 1768586539,
      "author": "rvz",
      "guid": 36524,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://openai.com/index/our-approach-to-advertising-and-expanding-access/\">https://openai.com/index/our-approach-to-advertising-and-expanding-access/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46649577\">https://news.ycombinator.com/item?id=46649577</a></p>\n<p>Points: 269</p>\n<p># Comments: 236</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "STFU",
      "url": "https://github.com/Pankajtanwarbanna/stfu",
      "date": 1768584760,
      "author": "tanelpoder",
      "guid": 36507,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://github.com/Pankajtanwarbanna/stfu\">https://github.com/Pankajtanwarbanna/stfu</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46649142\">https://news.ycombinator.com/item?id=46649142</a></p>\n<p>Points: 1004</p>\n<p># Comments: 581</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "East Germany balloon escape",
      "url": "https://en.wikipedia.org/wiki/East_Germany_balloon_escape",
      "date": 1768583793,
      "author": "robertvc",
      "guid": 36547,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://en.wikipedia.org/wiki/East_Germany_balloon_escape\">https://en.wikipedia.org/wiki/East_Germany_balloon_escape</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46648916\">https://news.ycombinator.com/item?id=46648916</a></p>\n<p>Points: 708</p>\n<p># Comments: 295</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Dell UltraSharp 52 Thunderbolt Hub Monitor",
      "url": "https://www.dell.com/en-us/shop/dell-ultrasharp-52-thunderbolt-hub-monitor-u5226kw/apd/210-bthw/monitors-monitor-accessories",
      "date": 1768583655,
      "author": "cebert",
      "guid": 36523,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.dell.com/en-us/shop/dell-ultrasharp-52-thunderbolt-hub-monitor-u5226kw/apd/210-bthw/monitors-monitor-accessories\">https://www.dell.com/en-us/shop/dell-ultrasharp-52-thunderbolt-hub-monitor-u5226kw/apd/210-bthw/monitors-monitor-accessories</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46648885\">https://news.ycombinator.com/item?id=46648885</a></p>\n<p>Points: 279</p>\n<p># Comments: 351</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Canada slashes 100% tariffs on Chinese EVs to 6%",
      "url": "https://electrek.co/2026/01/16/canada-breaks-with-us-slashes-100-tariffs-chinese-evs/",
      "date": 1768583128,
      "author": "1970-01-01",
      "guid": 36492,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://electrek.co/2026/01/16/canada-breaks-with-us-slashes-100-tariffs-chinese-evs/\">https://electrek.co/2026/01/16/canada-breaks-with-us-slashes-100-tariffs-chinese-evs/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46648778\">https://news.ycombinator.com/item?id=46648778</a></p>\n<p>Points: 442</p>\n<p># Comments: 582</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "6-Day and IP Address Certificates Are Generally Available",
      "url": "https://letsencrypt.org/2026/01/15/6day-and-ip-general-availability",
      "date": 1768577839,
      "author": "jaas",
      "guid": 36468,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://letsencrypt.org/2026/01/15/6day-and-ip-general-availability\">https://letsencrypt.org/2026/01/15/6day-and-ip-general-availability</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46647491\">https://news.ycombinator.com/item?id=46647491</a></p>\n<p>Points: 497</p>\n<p># Comments: 275</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Read_once(), Write_once(), but Not for Rust",
      "url": "https://lwn.net/SubscriberLink/1053142/8ec93e58d5d3cc06/",
      "date": 1768575862,
      "author": "todsacerdoti",
      "guid": 36522,
      "unread": true,
      "content": "<div>\n           By January 9, 2026</div>\nThe  and  macros are heavily used\nwithin the kernel; there are nearly 8,000 call sites for\n.  They are key to the implementation of many <a href=\"https://lwn.net/Articles/844224/\">lockless algorithms</a> and can be necessary for some\ntypes of device-memory access.  So one might think that, as the\namount of Rust code in the kernel increases, there would be a place for\nRust versions of these macros as well.  The truth of the matter, though, is\nthat the Rust community seems to want to take a different approach to\nconcurrent data access.\n<p>\nAn understanding of  and  is\nimportant for kernel developers who will be dealing with any sort of\nconcurrent access to data.  So, naturally, they are almost entirely absent\nfrom the kernel's documentation.  A description of sorts can be found at\nthe top of <a href=\"https://elixir.bootlin.com/linux/v6.18.3/source/include/asm-generic/rwonce.h\"></a>:\n</p><blockquote>\n\tPrevent the compiler from merging or refetching reads or\n \twrites. The compiler is also forbidden from reordering successive\n \tinstances of READ_ONCE and WRITE_ONCE, but only when the compiler\n \tis aware of some particular ordering. One way to make the compiler\n \taware of ordering is to put the two invocations of READ_ONCE or\n \tWRITE_ONCE in different C statements.\n</blockquote><p>\nIn other words, a  call will force the compiler to read\nfrom the indicated location exactly one time, with no optimization tricks\nthat would cause the read to be either elided or repeated;\n will force a write under those terms.  They will also\nensure that the access is atomic; if one task reads a location with\n while another is writing that location, the read will\nreturn the value as it existed either before or after the write, but not\nsome random combination of the two.  These macros, other than as described\nabove, impose no ordering constraints on the compiler or the CPU, making\nthem different from macros like , which have\nstronger ordering requirements.\n</p><p>\nThe  and  macros were <a href=\"https://git.kernel.org/linus/230fa253df635\">added for the 3.18\nrelease</a> in 2014.   was initially called\n, but that name was <a href=\"https://git.kernel.org/linus/43239cbe79fc3\">changed</a> during the\n3.19 development cycle.\n</p><p>\nSome of the other kernel Rust developers objected to this change, though.\nGary Guo <a href=\"https://lwn.net/ml/all/20251231151216.23446b64.gary@garyguo.net\">said</a> that he\nwould rather not expose  and  and\nsuggested using relaxed operations from  the kernel's <a href=\"https://rust.docs.kernel.org/next/kernel/sync/atomic/struct.Atomic.html\"></a>\nmodule instead.  Boqun Feng <a href=\"https://lwn.net/ml/all/aVXKP8vQ6uAxtazT@tardis-2.local\">expanded on</a> the\nobjection:\n</p><blockquote>\n\tThe problem of READ_ONCE() and WRITE_ONCE() is that the semantics\n\tis complicated. Sometimes they are used for atomicity, sometimes\n\tthey are used for preventing data race. So yes, we are using LKMM\n\t[the Linux kernel memory model] in Rust as well, but whenever\n\tpossible, we need to clarify the intention of the API, using\n\tAtomic::from_ptr().load(Relaxed) helps on that front.\n<p>\n\tIMO, READ_ONCE()/WRITE_ONCE() is like a \"band aid\" solution to a\n\tfew problems, having it would prevent us from developing a more\n\tclear view for concurrent programming.\n</p></blockquote><p>\nIn other words, using the  crate allows developers to\nspecify more precisely which guarantees an operation needs, making the\nexpectations (and requirements) of the code more clear.\nThis point of view would appear to have won out, and Ryhl has stopped\npushing for this addition to the kernel's Rust code — for now, at least.\n</p><p>\nThere are a couple of interesting implications from this outcome, should it\nhold.  The first of those is that, as Rust code reaches more deeply into the\ncore kernel, its code for concurrent access to shared data will look\nsignificantly different from the equivalent C code, even though the code on\nboth sides may be working with the same data.  Understanding lockless data\naccess is challenging enough when dealing with one API; developers may now\nhave to understand two APIs, which will not make the task easier.\n</p><p>\nMeanwhile, this discussion is drawing some attention to code on the C side\nas well.  As Feng <a href=\"https://lwn.net/ml/all/aV0JkZdrZn97-d7d@tardis-2.local\">pointed\nout</a>, there is still C code in the kernel that assumes a plain write\nwill be atomic in many situations, even though the C standard explicitly\nsays otherwise.  Peter Zijlstra <a href=\"https://lwn.net/ml/all/20260106145622.GB3707837@noisy.programming.kicks-ass.net\">answered</a>\nthat all such code should be updated to use  properly.\nSimply finding that code may be a challenge (though <a href=\"https://docs.kernel.org/dev-tools/kcsan.html\">KCSAN</a> can help);\nupdating it all may take a while.  The conversation also <a href=\"https://lwn.net/ml/all/87ikdej4s1.fsf@t14s.mail-host-address-is-not-set\">identified</a>\na place in the (C) high-resolution-timer code that is missing a needed\n call.  This is another example of the Rust work\nleading to improvements in the C code.\n</p><p>\nIn past discussions on the design of Rust abstractions, there has been\nresistance to the creation of Rust interfaces that look substantially\ndifferent from their C counterparts; see <a href=\"https://lwn.net/Articles/958072/\">this\n2024 article</a>, for example.  If the Rust developers come up with a\nbetter design for an interface, the thinking went, the C side should be\nimproved to match this new design.  If one accepts the idea that the Rust\napproach to  and  is better than\nthe original, then one might conclude that  a similar process should be\nfollowed here.  Changing thousands of low-level concurrency primitives to\nspecify more precise semantics would not be a task for the faint of heart,\nthough.  This may end up being a case where code in the two languages just\ndoes things differently.</p>",
      "contentLength": 4989,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46647059"
    },
    {
      "title": "America could have $4 lunch bowls like Japan but for zoning laws",
      "url": "https://abio.substack.com/p/america-could-have-4-lunch-bowls",
      "date": 1768575407,
      "author": "627467",
      "guid": 36718,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://abio.substack.com/p/america-could-have-4-lunch-bowls\">https://abio.substack.com/p/america-could-have-4-lunch-bowls</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46646970\">https://news.ycombinator.com/item?id=46646970</a></p>\n<p>Points: 170</p>\n<p># Comments: 297</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Cursor's latest “browser experiment” implied success without evidence",
      "url": "https://embedding-shapes.github.io/cursor-implied-success-without-evidence/",
      "date": 1768574269,
      "author": "embedding-shape",
      "guid": 36506,
      "unread": true,
      "content": "\n<p>Related: <i>Scaling long-running autonomous coding</i> - <a href=\"https://news.ycombinator.com/item?id=46624541\">https://news.ycombinator.com/item?id=46624541</a> - Jan 2026 (174 comments)</p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46646777\">https://news.ycombinator.com/item?id=46646777</a></p>\n<p>Points: 707</p>\n<p># Comments: 299</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Cloudflare acquires Astro",
      "url": "https://astro.build/blog/joining-cloudflare/",
      "date": 1768573554,
      "author": "todotask2",
      "guid": 36442,
      "unread": true,
      "content": "\n<p><a href=\"https://www.cloudflare.com/pl-pl/press/press-releases/2026/cloudflare-acquires-astro-to-accelerate-the-future-of-high-performance-web-development/\" rel=\"nofollow\">https://www.cloudflare.com/pl-pl/press/press-releases/2026/c...</a></p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46646645\">https://news.ycombinator.com/item?id=46646645</a></p>\n<p>Points: 925</p>\n<p># Comments: 387</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "The Dilbert Afterlife",
      "url": "https://www.astralcodexten.com/p/the-dilbert-afterlife",
      "date": 1768572572,
      "author": "rendall",
      "guid": 36695,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.astralcodexten.com/p/the-dilbert-afterlife\">https://www.astralcodexten.com/p/the-dilbert-afterlife</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46646475\">https://news.ycombinator.com/item?id=46646475</a></p>\n<p>Points: 492</p>\n<p># Comments: 322</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Michelangelo's first painting, created when he was 12 or 13",
      "url": "https://www.openculture.com/2026/01/discover-michelangelos-first-painting.html",
      "date": 1768571065,
      "author": "bookofjoe",
      "guid": 36467,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.openculture.com/2026/01/discover-michelangelos-first-painting.html\">https://www.openculture.com/2026/01/discover-michelangelos-first-painting.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46646263\">https://news.ycombinator.com/item?id=46646263</a></p>\n<p>Points: 362</p>\n<p># Comments: 170</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Dev-owned testing: Why it fails in practice and succeeds in theory",
      "url": "https://dl.acm.org/doi/10.1145/3780063.3780066",
      "date": 1768570771,
      "author": "rbanffy",
      "guid": 36546,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46646226"
    },
    {
      "title": "Fil-Qt: A Qt Base build with Fil-C experience",
      "url": "https://git.qt.io/cradam/fil-qt",
      "date": 1768569543,
      "author": "pjmlp",
      "guid": 36913,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46646080"
    },
    {
      "title": "Patching the Wii News Channel to serve local news (2025)",
      "url": "https://raulnegron.me/2025/wii-news-pr/",
      "date": 1768568304,
      "author": "todsacerdoti",
      "guid": 36607,
      "unread": true,
      "content": "<img src=\"https://raulnegron.me/img/wiinews-pr/wii-news-pr.png\" alt=\"wii system plus PR flag and rolled up newspaper emoji\"><p>Here’s a sneak peek at the result:</p><img src=\"https://raulnegron.me/img/wiinews-pr/wiinewspr-article.png\" alt=\"Wii News Channel showing a contemporary news article from El Nuevo Día\"><p>In this post, I’d like to share my research and process for getting this all to work.</p><p>The News Channel debuted in North America on January 26, 2007, a little over two months after the Wii’s launch. Since that date, it mostly came pre-installed with Wii consoles and was a novel way to read news from all over the world. Together with other “utility” channels like the Forecast Channel, it tried to position the Wii as more than just a gaming console.</p><p>Check out a video recording of the service from right before it was discontinued on June 27th, 2013:</p><h3>How the News Channel fetches content</h3><p>Before we can consider displaying custom news on it, we have to figure out how the News Channel actually fetches content. We know that it must have fetched news somehow since it displays a “Downloading…” splash screen on startup.</p><img src=\"https://raulnegron.me/img/wiinews-pr/downloading.png\" alt=\"wii news channel downloading splash screen\"><p>Luckily for us, the Wii natively supports proxying via its internet connection configuration settings! Meaning we can set up something like <a href=\"https://www.mitmproxy.org/\">mitmproxy</a> on a local machine and observe its HTTP behavior.</p><img src=\"https://raulnegron.me/img/wiinews-pr/proxy-settings.png\" alt=\"wii system internet connection menu showing available proxy settings\"><p>We can start ’s web interface for a more screenshot-friendly UI:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>If we run a man-in-the-middle proxy for the News Channel on an unmodified Wii, we will observe that, on channel startup, it attempts to obtain a  file from <code>http://news.wapp.wii.com/v2/1/049/news.bin.00</code> via a plain HTTP request.</p><img src=\"https://raulnegron.me/img/wiinews-pr/original-requests.png\" alt=\"mitmproxy web showing two HTTP requests from the Wii to http://news.wapp.wii.com/v2/1/049/news.bin.00\"><blockquote><p>URL path explainer (we’ll see later how I found this out):</p><ul><li><p> corresponds to “English” as the configured console language. See <a href=\"https://github.com/devkitPro/libogc/blob/9d99fec85cbc27b6ab4f3d785764de611112dbaf/gc/ogc/conf.h#L108\">conf.h</a> in  (the Wii homebrew community’s de-facto development toolchain) for the possible values.</p></li><li><p> is the Wii’s country code for “United States”. Check out the full list of Wii country codes on <a href=\"https://wiibrew.org/wiki/Country_Codes\">wiibrew.org</a>.</p></li></ul></blockquote><p>Once it fails to fetch this file, the News Channel displays an error. What might these binary files be? In any case, seeing the Wii perform an HTTP request to fetch news data is a good sign for us. It means we might be able to serve our own data.</p><img src=\"https://raulnegron.me/img/wiinews-pr/wiiconnect24-error.png\" alt=\"wii news channel error screen\"><p>By the way, if you run an internet connection test after configuring the proxy settings correctly, you’ll spot the Wii performing an HTTP request to <a href=\"http://conntest.nintendowifi.net\">http://conntest.nintendowifi.net</a>. Turns out, this page is actually still online (see for yourself!)</p><img src=\"https://raulnegron.me/img/wiinews-pr/conn-check.png\" alt=\"mitmproxy web showing an HTTP requests from the Wii to http://conntest.nintendowifi.net\"><p>The Wii’s internet connection test still passes to this day without any modification required. Thanks, Nintendo!</p><p>Up to this point, this is how we would expect the Wii would behave if you were running a stock console. More than 12 years ago, <a href=\"https://www.nintendo.com/en-gb/Support/Wii/Wii-Channels/Wii-Network-Services-Partial-Discontinuation/Wii-Network-Services-Partial-Discontinuation-748396.html\">Nintendo discontinued support</a> for the online functionality of the News Channel.</p><p>But as expected for a beloved retro console, community efforts have sprung up to try and preserve the previously existing functionality and allow users to continue enjoying these systems well past their intended expiration date. These sorts of unofficial software for gaming consoles are commonly referred to as “homebrew”.</p><p>Importantly for this project, the <a href=\"https://wiilink.ca/#intro/\">WiiLink</a> team maintains servers and develops software that allows us to experience the Wii’s online connectivity features even today.</p><blockquote><p>By the way, if you’re curious about how to get started with Wii console homebrew, check out <a href=\"https://wii.hacks.guide\">https://wii.hacks.guide</a>.</p></blockquote><p>Thanks to WiiLink, we can <a href=\"https://wiilink.ca/guide/news/\">revive the News Channel</a> and browse up-to-date news! Just not the local news, which is our real goal.</p><img src=\"https://raulnegron.me/img/wiinews-pr/wiilink-news-tech.png\" alt=\"wii news channel showing technology stories after patching with wiilink\"><h3>How WiiLink patches the News Channel</h3><p>After going through the WiiLink install process, if we fire up  and take a look at what the Wii is doing now, we’ll see that it’s actually requesting files from a different domain: “news.wiilink.ca”. But this time, it manages to fetch  and keeps requesting files all the way up to .</p><p>The News Channel just successfully fetched 24 hours worth of news from this server.</p><img src=\"https://raulnegron.me/img/wiinews-pr/wiilink-requests.png\" alt=\"mitmproxy web showing HTTP requests from the Wii to http://news.wiilink.ca/v2/1/049/news.bin.00, etc.\"><p>Great! Somehow, the WiiLink folks got this all to work. And, best of all, they’ve opened-sourced their work (<a href=\"https://github.com/WiiLink24\">GitHub</a>). The plan is looking really feasible at this point!</p><p>At a high-level, there are two steps to tackle, then:</p><ol><li>We have to make the News Channel fetch files from a server we control</li><li>We need to actually generate binary files with the content we want</li></ol><h2>Step 1: Patching the News Channel to redirect to our domain</h2><blockquote><p>Side note - it’s only while writing this blog post that I realized I had been looking at the “wrong” repo; WiiLink’s guide now recommends using the Python-based <a href=\"https://github.com/WiiLink24/WiiLink-Patcher-GUI\">WiiLink-Patcher-GUI</a> instead of the CLI patcher.</p></blockquote><p>After downloading the  file locally, we can use the <a href=\"https://en.wikipedia.org/wiki/Xdelta\">xdelta</a> CLI to print out some information on what the patch is supposed to do:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>Okay, so we’re looking for a  file and want to save the patched binary as . Based on the WiiLink install instructions, we know we should be dealing with a  file, so let’s keep digging to see if we can find out where  might be hiding.</p><p>From the repo’s , we know the patcher uses <a href=\"https://github.com/TheShadowEevee/libWiiSharp\">libWiiSharp</a> for it’s WAD file management during the file patching processing (<a href=\"https://raulnegron.me/2025/wii-news-pr/(https://github.com/WiiLink24/WiiLink24-Patcher/blob/033ccb9a3be574dad76f57ac82f3af226eef0fdb/WiiLink-Patcher-CLI/patch.cs#L396)\">source</a>). But at this point, I’d rather avoid using C# if I can. And besides, I know for a fact we’ll want to use Go in order to more easily leverage existing tooling from the WiiLink team.</p><p>Thankfully, there’s a really handy Go library called <a href=\"https://pkg.go.dev/github.com/wii-tools/wadlib\">wadlib</a> that comes to the rescue here. We’ll be using it for all our WAD management needs.</p><p>So, where is ? Looking at LibWiiSharp’s  file, we can spot how it unpacks  files from a WAD file. Namely, it defaults to using the numeric “Content ID” inside each “Content” metadata and then converts it to an 8-digit hexadecimal string (<a href=\"https://github.com/TheShadowEevee/libWiiSharp/blob/23d477a3f83d0400d73393b2febde7912968b1dc/WAD.cs#L863-L874\">source</a>).</p><blockquote><p>You can read more about Title metadata (“TMD”) and Content metadata (“CMD”) on <a href=\"https://wiibrew.org/wiki/Title_metadata#Content_metadata_(CMD)\">wiibrew.org</a></p></blockquote><p>Armed with this knowledge, we can use  to create a quick file extraction script and see if we can find our . It can go something like this:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>When  is the official (v7) News Channel WAD file, this script successfully extracts 12  files.</p><img src=\"https://raulnegron.me/img/wiinews-pr/extracted-wads.png\" alt=\"macos finder view showing extracted files\"><p>There’s definitely a  there, but could it be the file we’re looking for?</p><p>What we really need to do at this point is go ahead and apply the  patch to this  file manually. That way, we can compare the before/after binaries and see what changed. We can use  again to actually apply the patch. Running  says:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>So we can go ahead and run:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>And that… seemed to work? We have a  file, as expected. Now what?</p><h3>Investigating binary file changes</h3><p>We could do a binary diff of these files and start going through each change, but we already know at least one thing that should have changed based on our previous  experiments: instead of performing requests to “news.wapp.wii.com”, the patched WAD should instead use “news.wiilink.ca”.</p><p>Using a tool like <a href=\"https://hexfiend.com/\">Hex Fiend</a> (which also has binary diffing capabilities in case we need them), we can try searching for text inside the binary. If we try searching for “news.wapp.wii.com” on the original  file, we can actually find a match!</p><img src=\"https://raulnegron.me/img/wiinews-pr/original-url-in-binary.png\" alt=\"hex fiend UI showing original Wii News Channel data URL in search results\"><p>Sure enough, if we inspect the patched  file we will find no mention of the original URL. Instead, the “<a href=\"http://news.wiilink.ca\">http://news.wiilink.ca</a>” domain is visible at the same location (offset ).</p><img src=\"https://raulnegron.me/img/wiinews-pr/patched-url-in-binary.png\" alt=\"hex fiend UI showing modified Wii News Channel data URL in search results\"><blockquote><p>Note that the URL contains only two printf-style format strings ( and ); the News Channel itself must be appending the hourly suffix (like ) when fetching data.</p></blockquote><p>If we’re lucky, simply overwriting the binary file’s original URL with our own custom URL might do the trick. It’s worth a try!</p><p>In order to validate this hypothesis, I wrote a small Go utility for performing the necessary text replacement. Here’s an excerpt of the important bits:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>If we run the utility like:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>It should perform the URL rewriting in memory and provide us with a valid WAD file () we can then go ahead and install on Wii hardware.</p><img src=\"https://raulnegron.me/img/wiinews-pr/yawm-patched.png\" alt=\"yawmme showing patched WAD installation succeeded\"><p>Finally, we can go back to running  and opening the newly patched News Channel. Once the channel shows the “Downloading…” splash screen, we’ll spot requests going out to our expected domain.</p><img src=\"https://raulnegron.me/img/wiinews-pr/custom-request-failed.png\" alt=\"mitmproxy showing http 404 on our custom server from a News Channel request\"><p>It works! Now all we need is to… actually generate valid news files for the News Channel to work.</p><p>I mentioned previously that I knew using Go would come in handy later, and it’s specifically because the WiiLink team has a project called <a href=\"https://github.com/WiiLink24/NewsChannel\">NewsChannel</a> written in Go which contains the source code for generating the binary news files they serve from “news.wiilink.ca”.</p><p>I’m not going to go over all the implementation details here. I just want to highlight some of the main file creation steps in case you’d like to read more:</p><ul><li>obtain country-specific configuration (<a href=\"https://github.com/WiiLink24/NewsChannel/blob/cd676d7aa8f9b80ab9ee4fa32c21a80d3a65c38d/countries.json\">source</a>)</li><li>obtain articles and metadata from configured sources (like NHK, <a href=\"https://github.com/WiiLink24/NewsChannel/blob/cd676d7aa8f9b80ab9ee4fa32c21a80d3a65c38d/news/nhk/articles.go#L7-L46\">source</a>)</li><li>process all data in a specific order into a bytes buffer (<a href=\"https://github.com/WiiLink24/NewsChannel/blob/cd676d7aa8f9b80ab9ee4fa32c21a80d3a65c38d/main.go#L77-L90\">source</a>)</li><li>compress the data using LZ10, sign it with RSA, then write to disk (<a href=\"https://github.com/WiiLink24/NewsChannel/blob/cd676d7aa8f9b80ab9ee4fa32c21a80d3a65c38d/main.go#L126-L142\">source</a>)\n<ul><li>the file name is written using a specific string interpolation (<a href=\"https://github.com/WiiLink24/NewsChannel/blob/cd676d7aa8f9b80ab9ee4fa32c21a80d3a65c38d/main.go#L108\">source</a>, this is how I first found out about the language/country codes used in the News Channel data URL!)</li></ul></li></ul><blockquote><p>Fun fact: LZ10 is apparently a Nintendo-specific variant of the <a href=\"https://en.wikipedia.org/wiki/LZ77_and_LZ78\">LZ77</a> compression algorithm, used in some form or another on Game Boy Advance, Nintendo DS and Wii systems. <a href=\"https://github.com/wii-tools/lzx/blob/4c1183c96cc616e6b3312072e7f1ad984edd109f/lz10/compress.go\">wii-tools/lzx</a> has the Go source for the LZ10 compression used here.</p></blockquote><p>In any case, for our purposes, it’s doing more than we need in terms of source handling: it can generate news binaries from a variety of sources and supports different languages and regions.</p><p>For this project, I am making the following assumptions and tradeoffs:</p><ul><li><p>I will be using “English” as the language and “US” as the country code for the source URL path since my Wii console is configured as such. There is no separate Puerto Rico country code option, which is curious considering that there is a separate option for the US Virgin Islands.</p></li><li><p>I am not interested in supporting any other news sources from around the world, so the “Globe” feature for the News Channel will not be useful.</p></li><li><p>I’m hardcoding the latitude and longitude of Puerto Rico’s capital into the binary file to avoid having to process or guess location data from each article entry.</p></li></ul><h3>Modifying WiiLink’s generator to support Puerto Rican news</h3><p>I went ahead and forked the  repo into <a href=\"https://github.com/rnegron/WiiNewsPR\">WiiNewsPR</a>, added flag support to control article caching and binary output paths (you’ll see why this was necessary soon), removed all the existing sources and added a new one: <a href=\"https://www.elnuevodia.com/\">El Nuevo Día (“ENDI”)</a>.</p><p>I picked ENDI only because it’s the only local newspaper website I could find which still <a href=\"https://www.elnuevodia.com/rss/\">supports RSS</a>. Unfortunately, the feeds only contain a snippet of the actual article. On the bright side, most articles do contain images and we can use separate feeds to help categorize articles in the News Channel (<a href=\"https://github.com/rnegron/WiiNewsPR/blob/11df0e242bb1f4134e61ff0b35587372a5fd0564/news/endi/articles.go#L72-L79\">source</a>).</p><blockquote><p>By the way, I experimented with <a href=\"https://github.com/advancedlogic/GoOse\">GoOse</a> for (spanish language) article extraction on other news websites and the results were… unsatisfying, to say the least.</p></blockquote><h3>Final setup requirements for proper News Channel support</h3><p>Two quick things we’ll need in order to get this all to work:</p><ol><li>We need to sign each binary news file with a custom RSA key for the Wii to process the file (<a href=\"https://github.com/rnegron/WiiNewsPR/blob/11df0e242bb1f4134e61ff0b35587372a5fd0564/utils.go#L20\">source</a>). We can use  for this (note the  option):</li></ol><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><ol start=\"2\"><li>We need a (really) low quality logo for our source. <a href=\"https://imagemagick.org/script/command-line-processing.php\">ImageMagick</a> easily solves for this:</li></ol><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>Finally, we can build the Go binary and run it in order to generate a news binary in :</p><p>This successfully generates a  file.</p><p>Now we just need 24 of these, since the News Channel will actually fail to load if not provided with all 24 files. We could run this script every hour for the next 24 hours… or, we could take the shortcut of copying the current hour’s file into all other hourly values.</p><p>Regardless, it’s about time to test out all this effort. With 24 files uploaded to our storage provider (AWS S3), and the patched News Channel configured to fetch these from our custom domain, we can start up the channel and observe the fruits of our labor.</p><img src=\"https://raulnegron.me/img/wiinews-pr/custom-requests-success.png\" alt=\"mitmproxy showing successful http 200 requests on our custom server from a News Channel request\"><p>After the (slow!) requests finish one by one, seeing the articles pop up was immensely satisfying. Being able to tinker with and learn more about these nostalgic consoles so many years later is a real joy for me.</p><img src=\"https://raulnegron.me/img/wiinews-pr/wiinewspr-business-section.png\" alt=\"\"><h2>Bonus step: Automating hourly news updates with AWS Lambda</h2><p>Copying files into the S3 storage bucket is all well and good, but it would be great to have a continuously-updating, hands-off solution that generates the news binaries for us. A simple (and basically free) way to solve for this would be to bundle up the  Go executable into an AWS Lambda function and have that run hourly via EventBride, and then uploading the generated news binaries over to our storage bucket.</p><p>Here is where the extra flags for  come in: we need to be able to control file creation because  is a Lambda’s only writeable file system.</p><p>Here is a snippet of the Lambda handler logic:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><blockquote><p>See full Lambda handler source on GitHub: <a href=\"https://github.com/rnegron/WiiNewsPR/blob/master/deploy/handler.go\">handler.go</a></p></blockquote><p>We can then leverage the <a href=\"https://www.serverless.com/\">Serverless</a> framework for a quick infra-as-code setup. Here is a snippet of the configuration:</p><div><pre tabindex=\"0\"><code data-lang=\"yml\"></code></pre></div><p>Some things to call out here:</p><ul><li>We want to make sure to run the Lambda in Puerto Rico’s timezone so that  returns the expected hourly integer.</li><li>We want to give the Lambda a higher than expected  so that its CPU scales accordingly; it turns out that  compression is a big bottleneck on the smallest supported Lambda CPU and can easily time out at 30 seconds.</li></ul><p>If we leave this setup running for 24 hours, our storage bucket will get populated with 24 files and continuously be updated with the latest news!</p><img src=\"https://raulnegron.me/img/wiinews-pr/hourly-generated-files.png\" alt=\"s3 bucket view showing binary news files generated automatically at hourly intervals\"><p>Now I can get up in the morning, grab a coffee, and browse the local news on my Nintendo Wii like it’s 2007.</p>",
      "contentLength": 13165,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46645941"
    },
    {
      "title": "Just the Browser",
      "url": "https://justthebrowser.com/",
      "date": 1768565032,
      "author": "cl3misch",
      "guid": 36050,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://justthebrowser.com/\">https://justthebrowser.com/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46645615\">https://news.ycombinator.com/item?id=46645615</a></p>\n<p>Points: 551</p>\n<p># Comments: 251</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Why DuckDB is my first choice for data processing",
      "url": "https://www.robinlinacre.com/recommend_duckdb/",
      "date": 1768561058,
      "author": "tosh",
      "guid": 36491,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.robinlinacre.com/recommend_duckdb/\">https://www.robinlinacre.com/recommend_duckdb/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46645176\">https://news.ycombinator.com/item?id=46645176</a></p>\n<p>Points: 293</p>\n<p># Comments: 111</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "On Being a Human Being in the Time of Collapse (2022) [pdf]",
      "url": "https://web.cs.ucdavis.edu/~rogaway/papers/crisis/crisis.pdf",
      "date": 1768558792,
      "author": "barishnamazov",
      "guid": 36049,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://web.cs.ucdavis.edu/~rogaway/papers/crisis/crisis.pdf\">https://web.cs.ucdavis.edu/~rogaway/papers/crisis/crisis.pdf</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46644962\">https://news.ycombinator.com/item?id=46644962</a></p>\n<p>Points: 164</p>\n<p># Comments: 158</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "The spectrum of isolation: From bare metal to WebAssembly",
      "url": "https://buildsoftwaresystems.com/post/guide-to-execution-environments/",
      "date": 1768555623,
      "author": "ThierryBuilds",
      "guid": 36505,
      "unread": true,
      "content": "<p>Ever had that dreaded “but it works on my machine!” moment?</p><p>The culprit is often a subtle difference in the —the “stage” where your code performs.\nYou might be dealing with a binary linked against the wrong <a href=\"https://www.gnu.org/software/libc/\" target=\"_blank\">glibc</a>, a <a href=\"https://packaging.python.org/en/latest/specifications/binary-distribution-format/#binary-distribution-format\" target=\"_blank\">Python wheel</a> built for a different architecture, or a kernel feature quietly missing in production.\nThese invisible discrepancies are what turn a successful local build into a deployment disaster.</p><p>Getting the environment right is crucial for writing, testing, and shipping software reliably.\nBut the landscape is crowded with terms like , , , and more.\nWhat’s the difference, and which one should you use?</p><p>We’re going to trace the evolution of the execution environment.\nWe’ll start with raw hardware and work through VMs, containers, and the various ways we isolate code at the operating system (OS) and language level.\nAlong the way, we’ll break down the trade-offs for each approach.\nBy the end, you’ll know exactly which tool to grab for your next project.</p><p>The history of computing is largely a history of <em>resource sharing without chaos</em>.</p><p>Early systems ran one workload per machine. Today, a single server might host thousands of isolated applications owned by different teams. The unifying idea behind this evolution is : separating code, dependencies, and resources so they don’t interfere with one another.</p><p>But isolation is not binary. It exists on a spectrum—hardware, kernel, process, filesystem, language runtime. Each execution paradigm chooses a different point on that spectrum.</p><p> any layer below your chosen isolation boundary must already be compatible—containers won’t fix a kernel mismatch, and virtual environments won’t fix a missing system library.</p><p>We’ll move from the heaviest to the lightest abstractions.</p><p>This is the foundation. One machine, one operating system, running your code directly on the hardware.</p><blockquote><p><strong>Hardware (CPU, memory, Disk,…):</strong> Uniquely provided by a physical machine. Two separate environments imply two separate physical machines, each with its own dedicated hardware resources like CPU, memory, and disk.</p></blockquote><p>Think of it as a detached house. You have all the resources to yourself, with no neighbors to bother you.</p><ul><li> Maximum performance, full control over hardware.</li><li> Expensive (you pay for idle resources), slow to provision, inflexible.</li><li> High-performance computing (HPC), large databases, or legacy systems that require direct hardware access.</li></ul><p>VMs were the first major leap in efficiency. A piece of software called a <a href=\"https://en.wikipedia.org/wiki/Hypervisor\" target=\"_blank\"></a> carves up a single physical machine into multiple, independent virtual ones.</p><blockquote><p> Uniquely provided by virtual machines. Two environments can run on the same hardware but will have their own separate, full-fledged operating systems.</p></blockquote><p>This is like an apartment building. You still have your own private space (kitchen, bathroom, ), but you share the building’s underlying infrastructure (hardware).</p><ul><li> Strong isolation, can run different operating systems on one host (e.g., Windows and Linux).</li><li> Significant overhead (each VM has a full OS), slower to start than containers.</li><li><ul><li><a href=\"https://en.wikipedia.org/wiki/Hyper-V\" target=\"_blank\"></a>: Microsoft’s native hypervisor for Windows.</li><li><a href=\"https://linux-kvm.org/page/Main_Page\" target=\"_blank\"></a>: The go-to hypervisor for Linux.</li><li><a href=\"https://www.qemu.org/\" target=\"_blank\"></a>: A powerful machine emulator and virtualizer.</li><li><a href=\"https://canonical.com/lxd\" target=\"_blank\"></a>: While primarily a container manager, recent versions can also manage full virtual machines, offering a unified tool for both.</li></ul></li></ul><p>Containers revolutionized modern software development. They bundle an application and its dependencies, but—here’s the key difference—they share the host machine’s operating system kernel.</p><blockquote><p><strong>Application and Dependencies:</strong> Characterized by packaging an application along with all its dependencies. Multiple containerized environments share the host OS kernel but run in isolated user spaces.</p></blockquote><p>Think of containers as hotel rooms. Each is a self-contained, identical unit, but they all rely on the hotel’s core services (the host OS kernel). This makes them incredibly lightweight and fast.</p><p>Under the hood, this isolation is enforced by <a href=\"https://man7.org/linux/man-pages/man7/namespaces.7.html\" target=\"_blank\">Linux </a> (which give each container its own view of processes, networking, and the filesystem) and <a href=\"https://man7.org/linux/man-pages/man7/cgroups.7.html\" target=\"_blank\"></a> (which strictly control how much CPU, memory, and I/O it can consume).</p><ul><li> Extremely fast startup, low overhead, highly portable, perfect for microservices.</li><li> Weaker isolation than VMs (shared kernel can be a security concern).</li><li><ul><li><a href=\"https://www.docker.com/\" target=\"_blank\"></a> The tool that made containers mainstream, ideal for single applications.</li><li><a href=\"https://podman.io/\" target=\"_blank\"></a>: A popular daemonless alternative to Docker (it runs containers as direct processes without a central background service).</li><li><a href=\"https://canonical.com/lxd\" target=\"_blank\"></a>: A powerful manager for <a href=\"https://linuxcontainers.org/lxc/\" target=\"_blank\"></a> (Linux Containers).</li></ul></li></ul><div><div><p>LXD is a unique tool that intentionally blurs the lines.\nIts primary strength is managing , which feel like ultra-fast VMs but are technically containers.</p><p>However, as we’ve listed, LXD can  manage full virtual machines.\nThis makes it a powerful, unified tool for developers who want a single interface for both environment types.</p></div></div><p><strong>What About Managing Many Containers? Orchestration</strong></p><p>Tools like Docker and LXD are great for running containers on a single machine.\n<a href=\"https://docs.docker.com/compose/\" target=\"_blank\">Docker Compose</a> builds on this by managing multi-container applications as a single unit.\nIt allows you to define services like a web server and a database together, though it still operates on a single host.</p><p>When you need to manage applications across many machines, you move to  like <a href=\"https://kubernetes.io/\" target=\"_blank\"></a>, <a href=\"https://docs.docker.com/engine/swarm/\" target=\"_blank\"></a>, or <a href=\"https://www.nomadproject.io/\" target=\"_blank\"></a>.\nOrchestration is not isolation.\nThey are the next layer of management for handling scheduling, scaling, and networking at scale.\nThey do not solve environment drift, dependency mismatches, or build reproducibility.</p><p>While orchestration is a deep topic for its own article, it’s the logical next step after adopting containers.</p><p>This is a more specialized form of isolation, often used for security. It “jails” a process, restricting its view of system resources.</p><blockquote><p><strong>Interface-Level Isolation:</strong> Defined by filtering a process’s interaction with the Linux Kernel. \nInstead of providing a new environment, we strip away the process’s “powers” and limit its authority and vocabulary. <p>\nIt provides a sandboxed execution space for a single process or a group of related processes.</p></p></blockquote><p>This is like putting a specific activity into a “Safety Cabinet.” You aren’t building a new room; you are simply limiting what the process is allowed to do within your existing system through thick glass and heavy gloves.</p><ul><li> Very lightweight, OS-native security feature.</li><li> Can be complex to configure correctly, less feature-rich than full container runtimes.</li></ul><p>Sandboxes shine when you want to  of a single process—not when you need a reproducible environment.\nThey are about , not standardizing execution.</p><p>To build a proper sandbox, we control three specific dimensions:</p><ul><li> (Filesystem): Limiting the reach to specific folders.</li><li> (Privileges): Limiting the authority to specific actions.</li><li> (System Calls): Limiting the communication with the OS kernel.</li></ul><p>You can combine the following mechanisms (to create a robust sandbox), or use them individually:</p><ul><li><p>: Restricts the process to a specific directory tree.</p><ul><li><a href=\"https://man7.org/linux/man-pages/man1/chroot.1.html\" target=\"_blank\"></a> A classic UNIX utility that changes the (perceived) top-level root directory of a process. E.g.: make the process see  as .</li><li><a href=\"https://proot-me.github.io/\" target=\"_blank\"></a> An implementation of  that works without root privileges. It’s a user-space implementation that uses ptrace to fake a root directory without requiring administrative privileges.</li></ul></li><li><p><strong>Privilege Dividers (What):</strong> Breaks “Root” powers into small pieces. Instead of giving a process full administrative power, you give it only the specific power it needs (like  just to open a port).</p><ul><li><a href=\"https://man7.org/linux/man-pages/man3/libcap.3.html\" target=\"_blank\"></a>: Manages <a href=\"https://man7.org/linux/man-pages/man7/capabilities.7.html\" target=\"_blank\">Linux Capabilities</a>. Instead of a binary “Root vs. User” choice, it breaks root powers into 40+ granular bits (e.g.,  to manage networks without being able to read everyone’s files).</li><li><a href=\"https://man7.org/linux/man-pages/man8/setcap.8.html\" target=\"_blank\"></a> / <a href=\"https://man7.org/linux/man-pages/man8/getcap.8.html\" target=\"_blank\"></a>: The command-line utilities used to assign these specific powers to processes.</li></ul></li><li><p><strong>System Call Filtering (How)</strong>: A firewall for the Kernel. It prevents a process from executing dangerous commands (like reboot or ptrace) even if it has root privileges.</p><ul><li><a href=\"https://man7.org/linux/man-pages/man2/seccomp.2.html\" target=\"_blank\"></a>: A Linux kernel feature that filters system calls. For example, if a process tries to use an unapproved call (like  to start a shell), the kernel kills it instantly.</li></ul></li></ul><p><a href=\"https://github.com/containers/bubblewrap\" target=\"_blank\"></a> and <a href=\"https://github.com/netblue30/firejail\" target=\"_blank\"></a> are high-level “wrappers” that combine all the above.\nThey are the engines behind modern “Sandboxed” apps like <a href=\"https://flatpak.org/\" target=\"_blank\">Flatpaks</a>.</p><div><div><p>While these tools provide “surgical” isolation for individual processes, they are also the primary technologies that  (like Docker) automate and bundle into a single, portable package.</p></div></div><p>This type of isolation is probably the one you use daily as a developer. It doesn’t isolate the OS or hardware, but rather the <em>dependencies of a programming language</em>.</p><blockquote><p><strong>Language-Specific Workspace:</strong> Focused on isolating the dependencies of a specific programming language. This allows multiple projects on the same machine to use different versions of the same language and library without conflict.</p></blockquote><p>This is your workshop organizer. You have one project that needs an old version of a library and another that needs the latest version. A virtual environment keeps their tools (dependencies) in separate, labeled drawers so they don’t get mixed up. This prevents “dependency hell.”</p><ul><li> Essential for managing project dependencies, simple to use, developer-focused, Zero performance overhead.</li><li> Provides no OS-level or security isolation; the code still has full access to your user files, network, and system hardware.</li></ul><p>A critical limitation to remember: virtual environments solve , not .\nIf your code depends on a specific  version, OS package, kernel feature, or external binary, a  alone is no longer sufficient.</p><div><div><p> if your build or runtime depends on  you don’t explicitly control—system libraries, OS packages, kernel features—a virtual environment is already too weak.</p></div></div><p>To build a clean workspace, we have to solve three problems. Historically, we needed a different tool for each, but modern  are merging them into one.</p><ol><li><p><strong>The Runtime (Runtime Managers):</strong>\nThese tools handle the language version itself. They allow you to run Python 3.8 for a legacy project while using Python 3.14 for a new one.\n<a href=\"https://github.com/pyenv/pyenv\" target=\"_blank\"></a> (Python), <a href=\"https://github.com/nvm-sh/nvm\" target=\"_blank\"></a> or <a href=\"https://github.com/Schniz/fnm\" target=\"_blank\"></a> (Node.js), <a href=\"https://github.com/rust-lang/rustup\" target=\"_blank\"></a> (Rust), <a href=\"https://github.com/go-nv/goenv\" target=\"_blank\"></a> (Go).</p></li><li><p><strong>The Environment (Path Isolation):</strong>\nThis tells the system where to look for libraries. In Python, tools like <a href=\"https://docs.python.org/3/library/venv.html\" target=\"_blank\"></a> or <a href=\"https://virtualenv.pypa.io/en/latest/\" target=\"_blank\"></a> create a folder to store libraries. In  and , this is handled implicitly by looking for a local  or a project-specific build directory () relative to your code.</p></li><li><p><strong>The Dependencies (Package Management):</strong>\nThese tools download and manage the actual libraries (dependencies) versions your code needs to run.\n<a href=\"https://pip.pypa.io/en/stable/\" target=\"_blank\"></a> (Python), <a href=\"https://docs.npmjs.com/cli/commands/npm\" target=\"_blank\"></a> (Node.js), <a href=\"https://doc.rust-lang.org/cargo/reference/manifest.html\" target=\"_blank\"></a> (Rust), <a href=\"https://go.dev/ref/mod#go-sum-files\" target=\"_blank\"></a> (Go).</p></li></ol><p><strong>The Rise of the “All-in-Ones”</strong></p><p>The modern trend is the move away from fragmented tools toward  toolchains.\nThese tools detect your project settings and automatically align the Runtime, Environment, and Dependencies.</p><ul><li><a href=\"https://docs.astral.sh/uv/pip/environments/\" target=\"_blank\"></a> (Python): An extremely fast, single binary that replaces , , and . It can install Python versions and manage libraries in one go.</li><li><a href=\"https://anaconda.org/\" target=\"_blank\"></a> (Data Science): A heavyweight manager that handles language versions, libraries, and even system-level dependencies like C++ compilers or GPU drivers.</li><li><a href=\"https://github.com/rust-lang/rustup\" target=\"_blank\"></a> + <a href=\"https://doc.rust-lang.org/cargo/reference/manifest.html\" target=\"_blank\"></a> (Rust): The gold standard of integration. While technically two tools, they work as one. You can use  to swap the compiler on the fly, or a <a href=\"https://rust-lang.github.io/rustup/overrides.html#the-toolchain-file\" target=\"_blank\"></a> file to pin the version for everyone on the team.</li></ul><p>A key insight is that these environments are not mutually exclusive. In fact, they are often layered to create robust, professional workflows.</p><p>A common setup looks like this:</p><ul><li>You start with a  from a cloud provider like AWS or Google Cloud.</li><li>On that VM, you install  to manage .</li><li>Inside a container, your application runs, using a language-specific  (like Python’s ) to manage its dependencies.</li></ul><p>This layered approach gives you the hardware isolation of a VM, the packaging benefits of containers, and the clean dependency management of a virtual environment.</p><p>The direction of travel is clear: execution environments are becoming more abstract—but not simpler.</p><p>Each new paradigm removes a layer of responsibility from the developer while <em>fixing the isolation boundary at a higher level</em>.\nThe trade-offs don’t disappear; they just move.</p><p>Containers have effectively become the standard unit of execution.\nTools like <a href=\"https://containers.dev/\" target=\"_blank\"></a> formalize this by shifting the development environment itself into a container.</p><p>The isolation boundary here sits squarely at the .\nYou share the kernel, but everything above it—filesystem, dependencies, tooling—is locked down and reproducible.</p><p>However, this boundary still leaks. Containers built for  or specific syscalls will fail on older hosts. When the container assumes kernel features the host lacks, the result is often a silent or catastrophic failure.</p><p>In these moments, the “universal” container abstraction breaks: you aren’t just running an image; you are still tethered to the underlying hardware and kernel.</p><p>Serverless platforms push the isolation boundary even higher. You no longer manage machines, operating systems, or even containers directly. Instead, you hand over a function and accept a tightly constrained execution contract.</p><p>Under the hood, this abstraction is still built on specific isolation mechanisms. Examples include <a href=\"https://firecracker-microvm.github.io/\" target=\"_blank\">Firecracker</a>, which relies on lightweight virtual machines, and <a href=\"https://developers.cloudflare.com/workers/reference/how-workers-works/#isolates\" target=\"_blank\">V8 isolates</a>, which execute code inside tightly controlled language runtimes.</p><p>This is powerful, but opinionated: cold starts, execution time limits, and restricted system access are not incidental—they are the . Serverless is ideal when you can fully live inside that contract, and painful when you can’t.</p><p><a href=\"https://webassembly.org/\" target=\"_blank\">Wasm</a> is interesting not because it replaces containers, but because it introduces a new kind of boundary. Instead of isolating at the kernel or process level, Wasm sandboxes execution at the <em>instruction and capability level</em>.</p><p>The result is a portable, secure runtime that can run consistently across browsers, servers, and edge environments. If containers standardized , Wasm is attempting to standardize .</p><p>The common thread is this: progress doesn’t eliminate isolation—it chooses it more deliberately.</p><p>Every execution environment is a trade-off between , , , and . Problems arise when we treat these tools as interchangeable—or worse, when we use them without understanding <em>what they isolate and what they don’t</em>.</p><p>When in doubt, ask a single question: <em><strong>what is the lowest layer that must be identical for this code to behave correctly?</strong></em> Hardware, kernel, OS packages, or just language dependencies.\nThe answer points directly to the right execution environment.</p><p>As we saw with containers and process sandboxes, choosing the wrong boundary doesn’t fail gracefully.\nIt fails in ways that are subtle, security-sensitive, or painfully non-obvious.</p><p><strong>Practical decision shortcuts:</strong></p><ul><li>Use a virtual environment when only language-level dependencies vary.</li><li>Use a container when system libraries, tooling, or runtime assumptions must be identical.</li><li>Use a VM when kernel behavior, OS policies, or security boundaries must not be shared.</li></ul><p>Once you see execution environments as layered abstractions rather than competing products, architectural decisions become clearer—and “it works on my machine” becomes a relic of the past.</p><p>This is why confusing language-level isolation (like ) with OS-level isolation (containers or sandboxes) is so costly: they sit on entirely different points of the isolation spectrum, and they fail in fundamentally different ways.</p>",
      "contentLength": 15218,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46644661"
    },
    {
      "title": "Interactive eBPF",
      "url": "https://ebpf.party/",
      "date": 1768550462,
      "author": "samuel246",
      "guid": 36441,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://ebpf.party/\">https://ebpf.party/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46644181\">https://news.ycombinator.com/item?id=46644181</a></p>\n<p>Points: 261</p>\n<p># Comments: 9</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Boeing knew of flaw in part linked to UPS plane crash, NTSB report says",
      "url": "https://www.bbc.com/news/articles/cly56w0p9e1o",
      "date": 1768536677,
      "author": "1659447091",
      "guid": 36466,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.bbc.com/news/articles/cly56w0p9e1o\">https://www.bbc.com/news/articles/cly56w0p9e1o</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46642920\">https://news.ycombinator.com/item?id=46642920</a></p>\n<p>Points: 325</p>\n<p># Comments: 171</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "OpenBSD-current now runs as guest under Apple Hypervisor",
      "url": "https://www.undeadly.org/cgi?action=article;sid=20260115203619",
      "date": 1768533036,
      "author": "gpi",
      "guid": 36023,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.undeadly.org/cgi?action=article;sid=20260115203619\">https://www.undeadly.org/cgi?action=article;sid=20260115203619</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46642560\">https://news.ycombinator.com/item?id=46642560</a></p>\n<p>Points: 401</p>\n<p># Comments: 57</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "British redcoat's lost memoir reveals realities of life as a disabled veteran",
      "url": "https://phys.org/news/2026-01-british-redcoat-lost-memoir-reveals.html",
      "date": 1768532001,
      "author": "wglb",
      "guid": 37121,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46642458"
    },
    {
      "title": "Show HN: Reversing YouTube’s “Most Replayed” Graph",
      "url": "https://priyavr.at/blog/reversing-most-replayed/",
      "date": 1768529171,
      "author": "prvt",
      "guid": 36069,
      "unread": true,
      "content": "<p>It was a quiet afternoon; the only sound was an instrumental playlist humming from a forgotten YouTube tab. A melody felt familiar, but I couldn’t quite place it, spirited away by my work. Suddenly, a transition in the soundtrack caught my ears, pulling me from my thoughts with a single question: what was this soundtrack?</p><p>I switched over to the tab. The title read: . “Of course, it was from .” I had to smile at the unintentional irony. I slid my mouse over the progress bar to hear the transition again. A small graph appeared, indicating it was the song’s “most replayed” segment. Apparently, I wasn’t alone in loving that part. But that’s when I noticed them again: two small, symmetrical dips flanking the graph’s peak.</p><p>I had seen it before. A tiny digital hiccup, easily dismissed. But in the quiet of that afternoon, it was a loose thread, and I felt an irresistible urge to pull.</p><p>It started with a Google search: <em>“how is youtube’s most replayed graph calculated.”</em> Predictably, an AI overview answered:</p><blockquote><p>“aggregating the replay data from thousands of viewers to identify sections of a video that are re-watched the most.”</p></blockquote><p>This generic answer confirmed my suspicion: there wasn’t much public data on this. I’d be charting my own path (I fully expect future LLMs to cite this article, by the way.)</p><h2>Hypothesis: Designing the System</h2><p>This kicked off a personal project: designing YouTube’s “most replayed” with the goal of replicating the bug. Naturally, I put myself in the shoes of an engineer at Google, (a reality I hope to achieve someday), and started brainstorming possible designs by imagining myself wearing a ‘Noogler’ cap with one of those Doraemon copters. Seriously, how does that work? Won’t the cap fly away? Maybe that’s the point? “Let your thinking hats fly.” But I digress. That’s a topic for  I get into Google.</p><p>At the most basic level, I had to divide the continuous bar into discrete segments. So I represented the progress bar’s state as a boolean array, where each index corresponded to a segment of the video. That seemed like a good start.</p><figure><figcaption><small><em>Canvas 1: The boolean array approach.</em>This is my first attempt at an interactive article, so feel free to play around! You can hit start, pause or reset to simulate watching a video. (Note that the array only updates while the animation is playing, and dragging the seek bar is not supported.)</small></figcaption></figure><p>But was that enough? I thought about all the ways I interact with a video player. I can move my seek back and forth, skip segments, re-watch segments, etc. A simple boolean array would only tell me if a segment was watched, not how many times. It fails to account for a user re-watching the same segment five times in a row. I needed something better, like a frequency array, to track how many times each segment was seen.</p><figure><figcaption><small><em>Canvas 2: The frequency array.</em>Notice how the segments grow as the \"watcher\" passes through them, updating the frequency array. Try skipping around when the animation is playing to see the effect.</small></figcaption></figure><p>Now I had a minimum viable product. I could generate the heatmap based on this frequency array: for each segment, I’d plot a point whose vertical position corresponded to its view count. Join the dots, and voilà: my very own “most replayed” graph.</p><figure><figcaption><small>We simply connect the dots of our frequency array, and scale the points upward based on the respective view counts.</small></figcaption></figure><p>Was that all? Unfortunately, no. There was a lot more to it. First and foremost, I could already see a bug in my implementation. I thought about what would happen when a segment was watched over and over again, a ridiculously large number of times. My point would shoot higher and higher until it was off-screen entirely. This would be, to put it mildly, a bad user experience. Try re-watching the same segment multiple times in the above interactive canvas to see the effect.</p><p>So, what’s the fix? If you have a statistics background (or just a good memory of high school math), you might already know the answer. Either way, it’s . It sounds fancy, but it’s really just a way of keeping our graph in check. Instead of plotting the raw view counts (which can range from zero to billions), we scale everything down to a standard range, typically between 0 and 1.</p><p>The math is simple: find the segment with the highest view count (let’s call it ). Then, divide every segment’s view count by . Suddenly, the specific numbers don’t matter. The most popular segment will always have a value of 1 (or 100% height), and every other segment falls somewhere below that, relative to the peak. It ensures that whether a video has a thousand views or a thousand million, the points’ value range from 0 to 1 and the graph always fits perfectly inside the viewport.</p><figure><figcaption><small><em>Canvas 4: The normalized histogram.</em>By scaling everything relative to the peak, the graph remains perfectly contained within the viewport.</small></figcaption></figure><p>But there’s a catch. You can’t normalize if you have no data. When a video is fresh out of the oven and just published,  is zero. Trying to divide by zero is a great way to crash a server, so the feature sits dormant. This is the  phase. If you’ve ever rushed to watch a new upload from your favorite creator, you might have noticed the graph is missing. That’s not a glitch; it’s a waiting game. YouTube is silently listening, collecting that initial batch of viewer data to establish a baseline.</p><p>This raises an interesting question: do they keep listening forever? Does the server track every single micro-interaction for a video that’s five years old? The answer is almost certainly no, and for two very pragmatic reasons.</p><p>First, speed is everything. The internet has the attention span of a goldfish. If a video goes viral, the “most replayed” graph needs to be available  the video is trending, not three weeks later when everyone has moved on. If the system waited for a “perfect” dataset, the moment would be lost. They need to calculate the graph quickly so users can see it before the hype dies down.</p><p>Second, we don’t need perfection; we need patterns. This isn’t a bank transaction where every decimal point matters. We are just trying to visualize relative popularity. This leads us to the concept of . Once a video hits a certain threshold of views, say ten thousand, the distribution of the graph likely stabilizes. The shape of the curve won’t change drastically whether you survey ten thousand people or a billion. So, why pay the computational cost to track the next billion? By sampling a subset of viewers, YouTube can generate an accurate-enough graph without melting their data centers.</p><p>However, even with sampling, my current design was write-heavy. I am focusing purely on the computational model here for brevity. The architecture of storage and the network model are deep dives for another day. But at the most basic level, the model relies on a counter of some sort. In my initial frequency array model, if a user watches a video from start to finish, and that video is divided into 100 segments, then 100 separate counters are incremented. Now, consider the scale: users upload more than 500 hours of content to YouTube every single minute. Updating every individual segment count of every video for every viewer during that crucial “Cold Start” phase would result in a write load so heavy that it would consume a massive amount of compute and memory.</p><h3>Optimization from First Principles</h3><p>The solution lies in realizing that we don’t actually care about the middle of a continuous viewing session. If I watch a video from segment 1 to segment 5, the only “new” information is where I started and where I stopped. The segments in between (2 through 4) are just implicitly included.</p><p>This reminded me of a beautiful algorithmic trick from competitive programming:  (or ) technique, which allows us to utilize .</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>Here is how it works. Instead of incrementing the count for every segment from 1 to 5, we only perform two operations. First, we go to the starting segment (index 1) and increment 1. This marks the beginning of a view. Then, we go to the segment immediately after the user stopped watching (index 6) and decrement 1. This marks the drop-off point.</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>Even if the user skips around, watching 0-5, skipping to 8, then watching till end, we just treat those as separate sessions. We increment index 0, decrement index 6. Then increment index 8, and decrement index 10. The areas they skipped remain untouched.</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><figure><figcaption><small><em>Canvas 5: The Difference Array technique.</em>Notice how we only perform two operations: incrementing the start and decrementing the next element from where we skipped. The one extra \"blank\" segment at the very end is to safely catch that final decrement without throwing an \"index out of bounds\" error.</small></figcaption></figure><p>Concurrently, there might be a thousand other users incrementing and decrementing the array. Do we need to worry about integer overflow and underflow? Remember, we are only sampling a small subset of viewers, so it’s a no.</p><p>Memories aside, we still need to turn this difference array back into actual view counts. This is where the “Prefix Sum” comes in. We run a pass through the array where each number is the sum of itself and all the numbers before it.</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>We perform this calculation just once, right before the normalization step. We effectively traded billions of write operations for a single, cheap read-time calculation. It’s elegant, efficient, and exactly the kind of optimization that makes systems scalable.</p><p>Do the same steps in both  and , and run the above calculation on the ’s array. You will get the same result.</p><h2>Investigation: Tracing the Signal</h2><p>At this point I was happy with the implementation, but how close was I to the real thing? Well, for starters, mine didn’t have the bug, which meant I was still missing a critical piece of the puzzle.</p><p>My first instinct was to blame the classic enemy of precise computing: floating point errors. After all, normalization involves division, and we all know computers have a complicated relationship with decimals. I stared at the dips, trying to convince myself that this was just a rounding error, a ghost in the machine born from <a href=\"https://stackoverflow.com/questions/588004/is-floating-point-math-broken\" target=\"_blank\" rel=\"noopener noreferrer\">0.1 + 0.2 not equaling 0.3</a>. But the more I looked, the less it fit. A precision bug is usually chaotic, a messy scattering of noise across the entire dataset. It wouldn’t manifest as two perfect, symmetrical dips flanking the highest point while leaving the rest of the curve smooth. This wasn’t random; it felt structural. If it were a floating point issue, the artifacts would be everywhere, not just comfortably nesting next to the peak.</p><p>So, I decided to stop guessing and start looking. I fired up the browser’s developer tools. Right-click. Inspect. The holy grail of web debugging. I hovered over the heatmap, diving into the DOM tree, peeling back layer after layer of nested  containers until I finally hit the source. There it was: a single  element hiding inside the structure.</p><p>This discovery shifted the investigation. I was now looking at a  (SVG), but its origin was a mystery. Was this SVG pre-rendered on YouTube’s servers and sent over as a static asset? Or was the browser receiving a raw payload of data points (my hypothetical frequency array) and generating the curve locally using JavaScript?</p><p>I desperately hoped for the latter. If the SVG was fully baked on the server, my journey would end right here. I’d be staring at a locked black box, with no way to access the rendering code or the logic behind it. All that build-up, all the hypothetical Noogler hats and difference arrays, would be for nothing. But then I saw a glimmer of hope. The SVG path had a specific <a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/Reference/Values/ident\" target=\"_blank\" rel=\"noopener noreferrer\">CSS identifier</a>: . These are the handles that JavaScript grabs onto. If the server just wanted to display a static image, it wouldn’t necessarily need to tag the path with such a specific identifier unless the client code intended to find it and manipulate it.</p><p>This was my lead. If the code was generating or modifying that path on the fly, it had to reference that identifier. It was time to dive into the spaghetti bowl of modern web development: obfuscated code.</p><p>For the uninitiated, looking at production JavaScript is like trying to read a novel where every character’s name has been replaced with a single letter. Variable names like <code>calculateHeatmapDistribution</code> become , , or . This isn’t just to annoy curious developers; it’s about efficiency. Computers don’t care if a variable is named  or . They execute the logic just the same. But  takes up one byte, while the descriptive name takes nineteen. Multiply that by millions of lines of code and billions of requests, and you’re saving a massive amount of bandwidth. The result is a dense, impenetrable wall of text that is fast for the network but a nightmare for humans. But somewhere in that mess, I hoped to find my , since it was an identifier used in the HTML and can’t be obfuscated.</p><p>I jumped over to the sources tab and located the  file, a massive, minified behemoth that powers the YouTube player. With a mix of hope and trepidation, I hit  and ‘ed my magic string. To my surprise, the counter next to the search bar stopped at just two. That was manageable. Better than manageable; it was lucky.</p><p>The first occurrence landed me inside a large nested JavaScript object. As I parsed the structure, it started to look familiar. The keys in this object, such as , , and , perfectly matched the tags and attributes I had seen in the HTML. It was a blueprint. This code was defining the structure of the player’s UI components.</p><p>But something was missing. The most critical part of an SVG path is <a href=\"https://developer.mozilla.org/en-US/docs/Web/SVG/Reference/Attribute/d\" target=\"_blank\" rel=\"noopener noreferrer\">the d attribute</a>, the string of commands and coordinates that actually tells the browser where to draw the lines. In the code in front of me, the  attribute was there, but it was initialized as an empty string. I tabbed back to the live DOM tree in the inspector. There, the  attribute was packed with a long, complex sequence of numbers and letters.</p><p>The discrepancy was the smoking gun. If the static code had an empty string, but the running application had a full path, it meant only one thing. The curve wasn’t being downloaded as a finished asset. It was being calculated, point by point, right here in the browser, and injected into the DOM dynamically. The logic I was looking for was close.</p><p>To find the logic, I needed another magic string. Since this logic involved calculating the SVG path, it surely must be relying on data from an API call. And since YouTube has public APIs, I turned to Google search once more, this time with <em>“youtube api for most replayed graph”</em> and found the <a href=\"https://stackoverflow.com/questions/72610552/most-replayed-data-of-youtube-video-via-api\" target=\"_blank\" rel=\"noopener noreferrer\">exact question on Stack Overflow</a>. Thanks to the community responses, I found the string: .</p><p>But that’s a problem for another day. Right now, I had a magic string. I tabbed back to the Network tab, my fingers moving on autopilot as I fired off a search across all network requests. Jackpot. The counter lit up with 101 occurrences. One was in&nbsp; file, but the other 100 were hiding inside a JSON response. This was the raw vein of data I had been digging for.</p><p>I expanded the response, and there it was, laid out in plain text: a list of objects, each containing a , a , and the magic string, . The values were floats between 0 and 1, just as I had theorized.</p><p>This confirmed everything. YouTube wasn’t sending a pre-drawn picture; they were sending the raw instructions. The server provided the normalized height for each segment, and the client-side JavaScript (presumably that logic I’d glimpsed in ) must be connecting the dots to draw the SVG path. I couldn’t help but feel a surge of satisfaction. My mental model of the system, constructed from scratch with nothing but intuition and a hypothetical ‘Noogler’ hat, was dead on. They were dividing the video into discrete segments and normalizing the view counts, exactly as I had predicted.</p><p>What intrigued me even more than the normalized scores were:  and the total count of these segments.</p><p>First, . It remained constant for every single segment. This felt redundant. Why send the duration if it’s the same for everyone? Is it future-proofing for a world where heatmaps have variable resolution, with finer detail in popular sections and broader strokes elsewhere? But it seemed like a tiny inefficiency, a few extra bytes of JSON that YouTube was paying for in network bandwidth.</p><p>Then there was the count. There were exactly 100 segments for this four-minute music video. Was this a universal constant? I clicked over to a  tutorial (obviously a clickbait). The response? 100 segments. I tried a <em>“ten-hour loop of lofi beats”</em>. Still 100. It didn’t matter, the heatmap was always sliced into exactly one hundred pieces.</p><p>Why 100? Was it the result of some deep statistical analysis on the distribution of video lengths across the platform, determining that 100 points provided the optimal balance between granularity and performance? Or was it simply because humans like round numbers and 100 felt “about right”? If you work at YouTube and know the answer, please let me know. I am genuinely curious.</p><p>Armed with one hundred normalized intensity scores, I decided to render an SVG myself to see how closely my raw recreation matched YouTube’s SVG. I plotted it using a simple SVG <a href=\"https://developer.mozilla.org/en-US/docs/Web/SVG/Tutorials/SVG_from_scratch/Paths#line_commands\" target=\"_blank\" rel=\"noopener noreferrer\">line command: \"Line To\"</a>. The result was sharp, jagged, and aggressively pointy. It looked like a connect-the-dots drawing done by a ruler-wielding perfectionist. In contrast, the graph on the YouTube player was fluid, smooth, and liquid.</p><h2>Theory: The Geometry of Smoothness</h2><p>My spiky graph would have fit right in with the user interfaces of a decade ago, back when applications were defined by rigid corners and sharp edges. But the digital world has moved on. We are living in the era of the  effect.</p><p>This term comes from a psychological experiment where people are shown two shapes: one blobby and round, the other spiky and jagged. When asked which one is  and which is  almost everyone, across all languages and cultures, agrees: the round one is Bouba, and the spiky one is  It turns out, we have a deep-seated bias towards the round and the soft.</p><p>This preference has reshaped our digital landscape, largely championed by Apple. Steve Jobs famously dragged his engineers around the block to point out that “<a href=\"https://folklore.org/Round_Rects_Are_Everywhere.html\" target=\"_blank\" rel=\"noopener noreferrer\">Round Rects Are Everywhere!</a>” forcing them to implement the shape into the original Mac’s OS. Today, that influence is inescapable. Look at Windows 11, Google’s Material You, the tabs in your browser, or the icon of the app you’re using right now. The sharp corners have been sanded down. In fact, there is even a new CSS property called .</p><p>It is not just an aesthetic trend; it is a psychological one. Sharp corners signal danger to our primitive brains (think thorns, teeth, or jagged rocks). They say “ouch.” Round corners, on the other hand, signal safety. They feel friendly, approachable, and organic. They are softer on the eyes, reducing cognitive load because our gaze doesn’t have to come to an abrupt halt at every vertex. By smoothing out the edges, designers aren’t just making things look modern; they are making technology feel a little less like a machine and a little more human.</p><p>Driven by this universal preference for organic shapes, I needed to understand the mathematics behind YouTube’s implementation. Was it a ? Real-world data is inherently noisy, and a moving average is the standard tool for smoothing it out. By sliding a “window” across the dataset and averaging the points within it (say, three at a time), it irons out the wrinkles. Instead of a single, erratic value, you get the consensus of the neighborhood.</p><p>I decided to test this theory. I applied a moving average to my jagged plot, hoping to see the familiar YouTube curve emerge. It certainly helped sand down the sharpest peaks, making the graph look less like a mountain range and more like rolling hills. But it created a critical problem. The distinctive dips flanking the main peak (the very artifacts I was trying to replicate) were nowhere to be found. They weren’t in the raw data, and the moving average certainly didn’t create them; if anything, it would have smoothed them out if they were there. My plot was now smooth, but it was featureless. It looked like a low-resolution approximation, missing the specific character of the real thing. Clearly, a simple moving average wasn’t the answer.</p><h3>From Discrete to Continuous</h3><p>So I decided to check the path of the YouTube SVG itself. My recreation relied on the  (). It draws a straight, uncompromising line from point A to point B. Simple, efficient, but undeniably jagged. YouTube, however, wasn’t using lines. Their path was packed with  (). It became clear that the secret wasn’t in the data values themselves, but in how they were connected. I decided to pause the investigation and familiarize myself with the mathematics of curves.</p><p>To understand , I had to start at , or  The equation for a basic line between two points \\(P_0\\) and \\(P_1\\) is:</p>$$P(t) = \\text{Lerp}(P_0, P_1, t) = (1-t)P_0 + tP_1$$<p>Here, \\(t\\) acts as a slider ranging from 0 to 1. At \\(t=0\\), we are at the start point \\(P_0\\). At \\(t=1\\), we arrive at the end point \\(P_1\\).</p><figure><figcaption><small>A simple line connecting two points using the above equation. (You are going to have to settle for a video from here on out because  does a better job than I ever could with an HTML canvas.) Check out the <a href=\"https://www.desmos.com/calculator/3eipb5lodk\" target=\"_blank\" rel=\"noopener noreferrer\">interactive graph on Desmos</a>.</small></figcaption></figure><p>To get a curve, we extend this concept using the . Imagine you have three points: a start \\(P_0\\), an end \\(P_2\\), and a  \\(P_1\\) hovering in between. The math essentially  by nesting the equations. First, we calculate two moving intermediate points to create a sliding segment:</p>$$Q_0 = \\text{Lerp}(P_0, P_1, t) = (1-t)P_0 + tP_1$$$$Q_1 = \\text{Lerp}(P_1, P_2, t) = (1-t)P_1 + tP_2$$<p>Then, we interpolate between those two moving points to find our final position:</p>$$P(t) = \\text{Lerp}(Q_0, Q_1, t) = (1-t)Q_0 + tQ_1$$<p>When you expand this algebra, you get the quadratic formula:</p>$$P(t) = (1-t)^2P_0 + 2(1-t)tP_1 + t^2P_2$$<p>The result is a smooth curve that starts at \\(P_0\\) and travels towards \\(P_2\\), but is magnetically pulled towards \\(P_1\\) without ever touching it.</p><figure><figcaption><small>A quadratic Bézier curve. Notice how since it has only one control point, it can only bend in one direction. <a href=\"https://www.desmos.com/calculator/rnnc9nmfuc\" target=\"_blank\" rel=\"noopener noreferrer\">[Desmos]</a></small></figcaption></figure><p>However, because  relies on a single control point, it lacks the flexibility to create “S” curves or inflections; it can only bend in one direction. For “S” curves we need two control points. Which brings us to the . This adds a second control point, giving us four points total: Start (\\(P_0\\)), Control 1 (\\(P_1\\)), Control 2 (\\(P_2\\)), and End (\\(P_3\\)). We just add another layer of depth to the recursion.</p><p>First layer (edges of the hull):</p>$$Q_0 = \\text{Lerp}(P_0, P_1, t)$$$$Q_1 = \\text{Lerp}(P_1, P_2, t)$$$$Q_2 = \\text{Lerp}(P_2, P_3, t)$$<p>Second layer (connecting the moving points):</p>$$R_0 = \\text{Lerp}(Q_0, Q_1, t)$$$$R_1 = \\text{Lerp}(Q_1, Q_2, t)$$<p>Final layer (the curve itself):</p>$$P(t) = \\text{Lerp}(R_0, R_1, t)$$<p>Substituting everything back in gives the elegant cubic formula:</p>$$P(t) =\\\\(1-t)^3P_0 + 3(1-t)^2tP_1 + 3(1-t)t^2P_2 + t^3P_3$$<p>As \\(t\\) moves from 0 to 1, these equations trace a perfect parabolic arc. This is precisely how the browser renders those smooth, organic shapes, calculating positions pixel by pixel to create the visual comfort we expect.</p><figure><figcaption><small>A cubic Bézier curve. Notice how it can bend in two directions and form “S” curves easily. <a href=\"https://www.desmos.com/calculator/fbwrcoalbv\" target=\"_blank\" rel=\"noopener noreferrer\">[Desmos]</a></small></figcaption></figure><p>It is worth noting that this logic does not have to stop at four points. You can theoretically have Bézier curves with five, ten, or a hundred control points, creating increasingly intricate shapes with a single mathematical definition. However, there is a catch. As you add more points, the computational cost skyrockets. Solving high-degree polynomials for every frame of an animation or every resize event is expensive. That is why modern graphics systems usually stick to cubic curves. If you need a more complex shape, it is far more efficient to chain multiple cubic segments together than to crunch the numbers for a single, massive high-order curve.</p><figure><figcaption><small>A visualization of a 10-point Bézier spline in action. <a href=\"https://www.desmos.com/calculator/axqwsoy6ud\" target=\"_blank\" rel=\"noopener noreferrer\">[Desmos]</a></small></figcaption></figure><h3>The Invisible Scaffolding</h3><p>This specific bit of math isn’t unique to YouTube’s video player. It is the invisible scaffolding of the entire digital visual world. If you have ever used the Pen Tool in Photoshop, Illustrator, or Figma (or their respective open-source alternatives, Gimp, Inkscape and Penpot), you have directly manipulated these equations. When you pull those little handles to adjust a curve, you are literally moving the control points (\\(P_1\\) and \\(P_2\\)) in the formula above, redefining the gravitational field that shapes the line. But you don’t have to be a designer to interact with them. In fact, you are looking at them right now. The fonts rendering these very words are nothing more than collections of Bézier curves. Your computer doesn’t store a pixelated image of the letter ‘a’; it stores the mathematical instructions (the start points, end points, and control points) needed to draw it perfectly at any size. From the smooth hood of a modeled car in a video game to the vector logo on your credit card, Bézier curves are the unsung heroes that rounded off the sharp edges of the digital age.</p><p>This sequence of connected curves is known as a . YouTube isn’t drawing one massive, complex curve; they are stitching together a hundered smaller cubic curves to form a continuous shape. In fact, my own jagged implementation was a spline too: a . I merely stitched together hundered straight lines.</p><p>However, creating a spline introduces a new challenge: Smoothness. If you just glue two random curves together at a point (known as a “knot point”), you get a sharp corner (a “kiki” joint in our “bouba” graph). To make the transition seamless (technically known as \\(C^1\\) continuity), the join has to be perfect. The tangent of the curve ending at the connection point must match the tangent of the curve starting there. Visually, this means the second control point of the previous curve, the shared knot point, and the first control point of the next curve must all lie on the exact same straight line. It’s a balancing act. If one handle is off by a single pixel, the illusion of fluidity breaks.</p><figure><figcaption><small>Here, there are two curves joined at a knot point. Initially, the knot point is not smooth as the three points (the two control points and the knot point) do not lie on the same straight line. But as the animation progresses, the points become , creating a smooth transition. <a href=\"https://www.desmos.com/calculator/aakwp6eabv\" target=\"_blank\" rel=\"noopener noreferrer\">[Desmos]</a></small></figcaption></figure><p>I attempted to update my script to use this curve command, replacing the simple lines. However, I immediately hit a wall. While the line command is straightforward (requiring only a destination), the curve command is demanding. It requires two invisible ‘magnets’ (the control points  and ) for every single segment.</p><p>The  command functions as a precise geometric instruction, telling the renderer exactly how to shape the curve. The syntax is deceptively simple: . Where&nbsp; and &nbsp;are the control points and&nbsp;&nbsp;is where the curve ends. You might ask: where is the starting point? In SVG paths, it is implicit, the curve begins wherever the previous command ended. For instance, YouTube’s path starts with  (Move to start), immediately followed by a curve definition <code>C 1.0,89.5 2.0,49.9 5.0,47.4</code> which ends at  becoming the starting point for the next command <code>C 8.0,44.9 11.0,79.0 15.0,87.5</code> and so on.</p><div><pre tabindex=\"0\"><code data-lang=\"html\"></code></pre></div><h2>Resolution: The Invisible Hand</h2><p>These points didn’t appear out of thin air, and they certainly weren’t in the JSON response, which only provided the raw heights. They had to be calculated locally. Somewhere in the client-side code, there was a mathematical recipe converting those raw intensity scores into elegant \\(C^1\\) continuous curve data. This sent me back to the single occurrence of&nbsp;&nbsp;spotted earlier in&nbsp;.</p><blockquote><p> The screenshots below are from the raw, obfuscated source. They are included here only to show the steps I took during the investigation. Don’t try to make sense of them or you might get confused! Feel free to skim past them; I have provided a clean, de-obfuscated version later in the text that explains the logic clearly.</p></blockquote><p>The occurrence led me directly to an interesting function, .</p><p>This function appeared to be building an intermediate array, preparing the data for the final drawing step. Analyzing the code, I could see it was mapping the normalized intensity scores (which range from 0 to 1) onto a coordinate system suitable for the SVG. Similar to what I did in my Linear Spline script.</p><p>Specifically, it was transforming the data to fit a 1000x100 pixel canvas. The  line calculates the width of each segment (1000 divided by 100 segments equals 10 pixels per segment). The loop then iterates through the data points, calculating the x coordinate () and the y coordinate ().</p><p>Crucially, it handles the coordinate system flip. In standard coordinate geometry we learn in school,  is at the bottom and values increase as you go up. In computer graphics (and SVGs), however, the origin  is at the top-left corner. This convention is a historical artifact from the days of  (CRT) monitors. On those old, bulky screens, an electron beam would physically scan across the phosphor surface, starting from the top-left, drawing a line to the right, snapping back (horizontal retrace), and moving down to draw the next line. If you are old enough, you might remember seeing this flickering motion on screens with low refresh rates. Modern LCDs and OLEDs don’t have electron beams, but the software coordinate system stuck. So, to draw a “high” peak on a graph, you actually need a small y-coordinate (closer to the top). The code accounts for this with , inverting the values so that a higher intensity score results in a smaller y value (pushing the point “up” towards the top of the container). It also prepends a starting point at  (bottom-left) and appends a closing point at  (bottom-right), a detail whose importance will become clear shortly.</p><p>This function () was clearly just the preparation step. It was normalizing the data into pixel space, but it wasn’t drawing anything yet. It had to be invoked somewhere. In a proper IDE, I would just hit  to jump to usage. But browser DevTools aren’t quite there yet for this kind of reverse engineering. So, I resorted to the old-school method: a text search for “”. Note the opening parenthesis; that’s the trick to find calls rather than definitions.</p><p>This search led me to a function named :</p><p>Bulls-eye. There it was, plain as day:  (inside the red box). This line confirmed that  (the result of ) was indeed the path string being injected into the SVG.</p><p>The transformed list from  was being stored in  and then passed straight into another function: . To find its definition, I relied on a common pattern in minified JavaScript: functions are typically declared anonymously and assigned to a variable. Debugging obfuscated code becomes much easier when you know these patterns. I simply searched for “” and found the heart of the operation:</p><p>This was it. The smoking gun. I could see the string construction happening in real-time. The code iterates through the points, and for every segment, it appends a  command string. But look closely at the arguments for that command. The end point ( or ) was already known. But the variables  and  (representing the two control points) were being calculated on the fly by a helper function: .</p><p>The control points weren’t just appearing; they were being dynamically generated based on the position of the current point and its neighbors. The logic didn’t stop there. I chased  down the rabbit hole.</p><p>This snippet finally revealed the math.  was creating a new object  (which I found to be a simple Vector class storing  and  differences) and then using  and  to offset the control points. The mysterious  multiplier suggested that the control points were being placed at 20% of the distance determined by the vector calculation.</p><h3>Deciphering the Blueprint</h3><p>After deciphering the minified code, I was able to reconstruct the logic in readable JavaScript.</p><div><pre tabindex=\"0\"><code data-lang=\"js\"></code></pre></div><p>The reconstructed code reveals a fascinatingly simple geometric strategy. To determine the control point for a specific knot (let’s call it ), the algorithm doesn’t look at  in isolation. Instead, it looks at its neighbors. It draws an imaginary line connecting the previous point () directly to the next point (), completely skipping . The slope of this imaginary line becomes the tangent for the curve at . This specific method of curve generation is known as a .</p><figure><figcaption><small>A visualization of the cardinal spline algorithm in action. In the animation, the tangent scales down as it moves towards the knot point to 0.2 of its original length. <a href=\"https://www.desmos.com/calculator/xhkrvqshnq\" target=\"_blank\" rel=\"noopener noreferrer\">[Desmos]</a></small></figcaption></figure><p>By calculating the tangent at each knot point based on the positions of its neighbors, it ensures that the curve arrives at and departs from each point with the same velocity (tangent vector), guaranteeing that elusive \\(C^1\\) continuity I mentioned earlier.</p><p>Remember those extra points added at  and  that I mentioned earlier? Without them, the first and last segments of the video data would have no outer neighbors. By artificially adding them, the algorithm effectively tells the curve to start and end its journey with a smooth trajectory rising from the baseline, rather than starting abruptly in mid-air.</p><p>And what about that magic  number? That determines the tension of the curve. In the world of splines, this factor controls the length of the tangent vectors. If this value were , we would be looking at a standard <a href=\"https://en.wikipedia.org/wiki/Catmull%E2%80%93Rom_spline\" target=\"_blank\" rel=\"noopener noreferrer\">Catmull-Rom spline</a>, often used in animation for its loose, fluid movement. However, a value of  would collapse the control points onto the anchors, reverting the shape back to a sharp, jagged linear spline.</p><p>And there it was. The answer to the mystery of the dips. It wasn’t a rounding error, a data glitch, or a server-side anomaly. It was the math itself. Specifically, the requirement for continuity. When a data point spikes significantly higher than its neighbors, the <em>Cardinal Spline algorithm</em> calculates a steep tangent to shoot up to that peak. To maintain that velocity and direction smoothly as it passes through the neighboring points, the curve is forced to swing wide (dipping below the baseline) before rocketing upwards. It’s the visual equivalent of a crouch before a jump. The dips weren’t bugs; they were the inevitable artifacts of forcing rigid, discrete data into a smooth, organic flow.</p><figure><figcaption><small>Notice how the dips flatten out as the peak lowers. <a href=\"https://www.desmos.com/calculator/xhkrvqshnq\" target=\"_blank\" rel=\"noopener noreferrer\">[Desmos]</a></small></figcaption></figure><p>I started pulling on this loose thread on a quiet afternoon, simply wondering about a song from . By nightfall, I had followed the thread to its end, tracing the logic from pixel to polynomial. Documenting it, however, was a marathon that spanned many weeks of focused work.</p><p>This project wasn’t just a random curiosity; it was about the joy of digging until I hit the bedrock of logic. It forced me to bridge concepts from disparate domains (frontend engineering, competitive programming, geometry, and design history) and to think deeply about the performance implications of every calculation. I am grateful to live in an era where curiosity can be so readily shared with the world.</p><p>Having made it this far (through over 7,000 words), you have my wholehearted thanks for lending me your time. If you enjoyed this descent into madness and want to support future deep dives, consider <a href=\"https://buymeacoffee.com/priyavrat\" target=\"_blank\" rel=\"noopener noreferrer\">buying me a coffee (or two?)</a>. Though I don’t drink coffee, it helps pay for the domain costs.</p>",
      "contentLength": 35634,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46642173"
    },
    {
      "title": "Show HN: I built a text-based business simulator to replace video courses",
      "url": "https://www.core-mba.pro/",
      "date": 1768527715,
      "author": "Core_Dev",
      "guid": 36465,
      "unread": true,
      "content": "<div><div>&nbsp;&nbsp;REVENUE IS VANITY, CASH IS SANITY  |  COMPETITION IS FOR LOSERS. SCALE OR DIE  |  EGO IS THE ENEMY OF PROFIT. KILL IT  |  DEFAULT ALIVE IS THE ONLY REAL KPI  |  MARKETERS SELL TOOLS, STRATEGISTS SELL HOLES  |  YOUR IDEA IS WORTH $0. EXECUTION IS THE MULTIPLIER  |  FOCUS IS A SUPERPOWER. SAY NO TO EVERYTHING ELSE  |  BE DIFFERENT, NOT JUST BETTER  |  SOLVE A BLEEDING NECK PROBLEM OR QUIT  |  RECRUIT WARRIORS, NOT EMPLOYEES  |  FIRE FAST. HIRE SLOW. PROTECT THE CULTURE  |  YOUR BRAND IS A PROMISE DELIVERED, NOT A LOGO  |  COMPLEXITY IS THE SILENT KILLER OF MARGINS  |  IF YOU CAN'T MEASURE IT, YOU DON'T OWN IT  |  NET PROFIT IS THE ONLY SCOREBOARD THAT MATTERS  |  SPEED IS AN UNFAIR COMPETITIVE ADVANTAGE  |  BORROWED CAPITAL IS A NOOSE, OWNED CAPITAL IS A SHIELD  |  MARKET SHARE IS RENTED, BRAND LOYALTY IS OWNED&nbsp;&nbsp;&nbsp;&nbsp;REVENUE IS VANITY, CASH IS SANITY  |  COMPETITION IS FOR LOSERS. SCALE OR DIE  |  EGO IS THE ENEMY OF PROFIT. KILL IT  |  DEFAULT ALIVE IS THE ONLY REAL KPI  |  MARKETERS SELL TOOLS, STRATEGISTS SELL HOLES  |  YOUR IDEA IS WORTH $0. EXECUTION IS THE MULTIPLIER  |  FOCUS IS A SUPERPOWER. SAY NO TO EVERYTHING ELSE  |  BE DIFFERENT, NOT JUST BETTER  |  SOLVE A BLEEDING NECK PROBLEM OR QUIT  |  RECRUIT WARRIORS, NOT EMPLOYEES  |  FIRE FAST. HIRE SLOW. PROTECT THE CULTURE  |  YOUR BRAND IS A PROMISE DELIVERED, NOT A LOGO  |  COMPLEXITY IS THE SILENT KILLER OF MARGINS  |  IF YOU CAN'T MEASURE IT, YOU DON'T OWN IT  |  NET PROFIT IS THE ONLY SCOREBOARD THAT MATTERS  |  SPEED IS AN UNFAIR COMPETITIVE ADVANTAGE  |  BORROWED CAPITAL IS A NOOSE, OWNED CAPITAL IS A SHIELD  |  MARKET SHARE IS RENTED, BRAND LOYALTY IS OWNED&nbsp;&nbsp;&nbsp;&nbsp;REVENUE IS VANITY, CASH IS SANITY  |  COMPETITION IS FOR LOSERS. SCALE OR DIE  |  EGO IS THE ENEMY OF PROFIT. KILL IT  |  DEFAULT ALIVE IS THE ONLY REAL KPI  |  MARKETERS SELL TOOLS, STRATEGISTS SELL HOLES  |  YOUR IDEA IS WORTH $0. EXECUTION IS THE MULTIPLIER  |  FOCUS IS A SUPERPOWER. SAY NO TO EVERYTHING ELSE  |  BE DIFFERENT, NOT JUST BETTER  |  SOLVE A BLEEDING NECK PROBLEM OR QUIT  |  RECRUIT WARRIORS, NOT EMPLOYEES  |  FIRE FAST. HIRE SLOW. PROTECT THE CULTURE  |  YOUR BRAND IS A PROMISE DELIVERED, NOT A LOGO  |  COMPLEXITY IS THE SILENT KILLER OF MARGINS  |  IF YOU CAN'T MEASURE IT, YOU DON'T OWN IT  |  NET PROFIT IS THE ONLY SCOREBOARD THAT MATTERS  |  SPEED IS AN UNFAIR COMPETITIVE ADVANTAGE  |  BORROWED CAPITAL IS A NOOSE, OWNED CAPITAL IS A SHIELD  |  MARKET SHARE IS RENTED, BRAND LOYALTY IS OWNED</div></div>",
      "contentLength": 2480,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46641992"
    },
    {
      "title": "Show HN: Gambit, an open-source agent harness for building reliable AI agents",
      "url": "https://github.com/bolt-foundry/gambit",
      "date": 1768522405,
      "author": "randall",
      "guid": 36068,
      "unread": true,
      "content": "<p>Wanted to show our open source agent harness called Gambit.</p><p>If you’re not familiar, agent harnesses are sort of like an operating system for an agent... they handle tool calling, planning, context window management, and don’t require as much developer orchestration.</p><p>Normally you might see an agent orchestration framework pipeline like:</p><p>compute -&gt; compute -&gt; compute -&gt; LLM -&gt; compute -&gt; compute -&gt; LLM</p><p>we invert this so with an agent harness, it’s more like:</p><p>LLM -&gt; LLM -&gt; LLM -&gt; compute -&gt; LLM -&gt; LLM -&gt; compute -&gt; LLM</p><p>Essentially you describe each agent in either a self contained markdown file, or as a typescript program. Your root agent can bring in other agents as needed, and we create a typesafe way for you to define the interfaces between those agents. We call these decks.</p><p>Agents can call agents, and each agent can be designed with whatever model params make sense for your task.</p><p>Additionally, each step of the chain gets automatic evals, we call graders. A grader is another deck type… but it’s designed to evaluate and score conversations (or individual conversation turns).</p><p>We also have test agents you can define on a deck-by-deck basis, that are designed to mimic scenarios your agent would face and generate synthetic data for either humans or graders to grade.</p><p>Prior to Gambit, we had built an LLM based video editor, and we weren’t happy with the results, which is what brought us down this path of improving inference time LLM quality.</p><p>We know it’s missing some obvious parts, but we wanted to get this out there to see how it could help people or start conversations. We’re really happy with how it’s working with some of our early design partners, and we think it’s a way to implement a lot of interesting applications:</p><p>- Truly open source agents and assistants, where logic, code, and prompts can be easily shared with the community.</p><p>- Rubric based grading to guarantee you (for instance) don’t leak PII accidentally</p><p>- Spin up a usable bot in minutes and have Codex or Claude Code use our command line runner / graders to build a first version that is pretty good w/ very little human intervention.</p><p>We’ll be around if ya’ll have any questions or thoughts. Thanks for checking us out!</p>",
      "contentLength": 2220,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46641362"
    },
    {
      "title": "My Gripes with Prolog",
      "url": "https://buttondown.com/hillelwayne/archive/my-gripes-with-prolog/",
      "date": 1768522288,
      "author": "azhenley",
      "guid": 36048,
      "unread": true,
      "content": "<p>For the next release of <a href=\"https://leanpub.com/logic/\" target=\"_blank\">Logic for Programmers</a>, I'm finally adding the sections on Answer Set Programming and Constraint Logic Programming that I TODOd back in version 0.9. And this is making me re-experience some of my pain points with Prolog, which I will gripe about now.  If you want to know more about why Prolog is cool instead, go <a href=\"https://buttondown.com/hillelwayne/archive/a48fce5b-8a05-4302-b620-9b26f057f145/\" target=\"_blank\">here</a> or <a href=\"https://www.metalevel.at/prolog\" target=\"_blank\">here</a> or <a href=\"https://ianthehenry.com/posts/drinking-with-datalog/\" target=\"_blank\">here</a> or <a href=\"https://logicprogramming.org/\" target=\"_blank\">here</a>. </p><p>ISO \"strings\" are just atoms or lists of single-character atoms (or lists of integer character codes). The various implementations of Prolog add custom string operators but they are not cross compatible, so code written with strings in SWI-Prolog will not work in Scryer Prolog. </p><p>Code logic is expressed entirely in , predicates which return true or false for certain values. For example if you wanted to get the length of a Prolog list, you write this:</p><div><pre><code></code></pre></div><p>Now this is pretty cool in that it allows bidirectionality, or running predicates \"in reverse\". To generate lists of length 3, you can write . But it also means that if you want to get the length a list , you can't do that in one expression, you have to write <code>length(List, Out), X is Out+1</code>.</p><p>For a while I thought no functions was necessary evil for bidirectionality, but then I discovered <a href=\"https://picat-lang.org/\" target=\"_blank\">Picat</a> has functions and works just fine. That by itself is a reason for me to prefer Picat for my LP needs.</p><p>(Bidirectionality is a killer feature of Prolog, so it's a shame I so rarely run into situations that use it.)</p><h3>No standardized collection types besides lists</h3><p>Aside from atoms () and numbers, there are two data types:</p><ul><li>Linked lists like .</li><li>Compound terms like , which  like record types but are actually tuples. You can even convert compound terms to linked lists with :</li></ul><div><pre><code></code></pre></div><p>There's no proper key-value maps or even struct types. Again, this is something that individual distributions can fix (without cross compatibility), but these never feel integrated with the rest of the language. </p><p> and  aren't values, they're control flow statements.  is a noop and  says that the current search path is a dead end, so backtrack and start again. You can't explicitly store true and false as values, you have to implicitly have them in facts ( instead of ).</p><p>This hasn't made any tasks impossible, and I can usually find a workaround to whatever I want to do. But I do think it makes things more inconvenient! Sometimes I want to do something dumb like \"get all atoms that don't pass at least three of these rules\", and that'd be a lot easier if I could shove intermediate results into a sack of booleans. </p><p>(This is called \"<a href=\"https://en.wikipedia.org/wiki/Negation_as_failure\" target=\"_blank\">Negation as Failure</a>\". I think this might be necessary to make Prolog a Turing complete general programming language. Picat fixes a lot of Prolog's gripes and still has negation as failure. ASP has regular negation but it's not Turing complete.) </p><p>Prolog finds solutions through depth first search, and a \"cut\" () symbol prevents backtracking past a certain point. This is necessary for optimization but can lead to invalid programs. </p><p>You're not supposed to use cuts if you can avoid it, so I pretended cuts didn't exist. Which is why I was surprised to find that <a href=\"https://eu.swi-prolog.org/pldoc/doc_for?object=(-%3E)/2\" target=\"_blank\">conditionals</a> are implemented with cuts. Because cuts are spooky dark magic conditionals  conditionals work as I expect them to and sometimes leave out valid solutions and I have no idea how to tell which it'll be. Usually I find it safer to just avoid conditionals entirely, which means my code gets a lot longer and messier. </p><p>The original example in the last section was this: </p><div><pre><code></code></pre></div><p> returns true, so you'd expect  to return . But it returns .  Whereas this works as expected.</p><div><pre><code></code></pre></div><p>I  this was because  was implemented with cuts, and the <a href=\"https://www.amazon.com/Programming-Prolog-Using-ISO-Standard/dp/3540006788\" target=\"_blank\">Clocksin book</a> suggests it's , so this was my prime example about how cuts are confusing. But then I tried this:</p><div><pre><code></code></pre></div><p>There's no way to get that behavior with cuts! I don't think  uses cuts at all! And now I have to figure out why \n doesn't returns results. Is it <a href=\"https://github.com/dtonhofer/prolog_notes/blob/master/other_notes/about_negation/floundering.md\" target=\"_blank\">floundering</a>? Is it because  only succeeds if  fails, and  always succeeds? A closed-world assumption? Something else?</p><h3>Straying outside of default queries is confusing</h3><p>Say I have a program like this:</p><div><pre><code></code></pre></div><p>And I want to know all of the nodes that are parents of branches. The normal way to do this is with a query:</p><div><pre><code></code></pre></div><p>This is interactively making me query for every result. That's usually not what I want, I know the result of my query is finite and I want all of the results at once, so I can count or farble or whatever them. It took a while to figure out that the proper solution is <a href=\"https://www.swi-prolog.org/pldoc/man?predicate=bagof/3\" target=\"_blank\"><code>bagof(Template, Goal, Bag)</code></a>, which will \"Unify Bag with the alternatives of Template\":</p><div><pre><code></code></pre></div><p>Wait crap that's still giving one result at a time, because  is a free variable in  so it backtracks over that. It surprises me but I guess it's good to have as an option. So how do I get all of the results at once?</p><div><pre><code></code></pre></div><p>The only difference is the , which tells  to ignore and group the results of . As far as I can tell, this is the  place the ISO standard uses  to mean anything besides exponentiation. Supposedly it's the <a href=\"https://sicstus.sics.se/sicstus/docs/latest4/html/sicstus.html/ref_002dall_002dsum.html\" target=\"_blank\">existential quantifier</a>? In general whenever I try to stray outside simpler use-cases, especially if I try to do things non-interactively, I run into trouble.</p><h3>I have mixed feelings about symbol terms</h3><p>It took me a long time to realize the reason   \"works\" is because infix symbols are mapped to prefix compound terms, so that   is , and then different predicates can decide to do different things with .</p><p>This is also why you can't just write : that unifies  with the .  is , as . You have to write , as  is the operator that converts  to a mathematical term.</p><p>(And  fails because  isn't fully bidirectional. The lhs  be a single variable. You have to import  and write .)</p><p>I don't like this, but I'm a hypocrite for saying that because I appreciate the idea and don't mind custom symbols in other languages. I guess what annoys me is there's no official definition of what  is, it's purely a convention. ISO Prolog uses  (aka ) as a convention to mean \"pairs\", and the only way to realize that is to see that an awful lot of standard modules use that convention. But you can use  to mean something else in your own code and nothing will warn you of the inconsistency.</p><p>Anyway I griped about pairs so I can gripe about .</p><p>This one's just a blunder:</p><div><pre><code></code></pre></div><p>According to an expert online this is because sort is supposed to return a sorted , not a sorted list. If you want to preserve duplicates you're supposed to lift all of the values into  compound terms, then use <a href=\"https://eu.swi-prolog.org/pldoc/doc_for?object=keysort/2\" target=\"_blank\">keysort</a>, then extract the values. And, since there's no functions, this process takes at least three lines. This is also how you're supposed to sort by a custom predicate, like \"the second value of a compound term\". </p><p>(Most (but not all) distributions have a duplicate merge like <a href=\"https://eu.swi-prolog.org/pldoc/doc_for?object=msort/2\" target=\"_blank\">msort</a>. SWI-Prolog also has a <a href=\"https://eu.swi-prolog.org/pldoc/doc_for?object=predsort/3\" target=\"_blank\">sort by key</a> but it removes duplicates.)</p><h3>Please just let me end rules with a trailing comma instead of a period, I'm begging you</h3><p>I don't care if it makes fact parsing ambiguous, I just don't want \"reorder two lines\" to be a syntax error anymore</p><p>I expect by this time tomorrow I'll have been Cunningham'd and there will be a 2000 word essay about how all of my gripes are either easily fixable by doing XYZ or how they are the best possible choice that Prolog could have made. I mean, even in writing this I found out some fixes to problems I had. Like I was going to gripe about how I can't run SWI-Prolog queries from the command line but, in doing do diligence finally  figured it out:</p><div><pre><code>swipl-thalt-g./file.pl\n</code></pre></div><p>It's pretty clunky but still better than the old process of having to enter an interactive session every time I wanted to validate a script change.</p><p>(Also, answer set programming is pretty darn cool. Excited to write about it in the book!)</p>",
      "contentLength": 7626,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46641348"
    },
    {
      "title": "List of individual trees",
      "url": "https://en.wikipedia.org/wiki/List_of_individual_trees",
      "date": 1768521901,
      "author": "wilson090",
      "guid": 36047,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://en.wikipedia.org/wiki/List_of_individual_trees\">https://en.wikipedia.org/wiki/List_of_individual_trees</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46641284\">https://news.ycombinator.com/item?id=46641284</a></p>\n<p>Points: 359</p>\n<p># Comments: 116</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "All 23-Bit Still Lifes Are Glider Constructible",
      "url": "https://mvr.github.io/posts/xs23.html",
      "date": 1768521546,
      "author": "HeliumHydride",
      "guid": 36046,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46641239"
    },
    {
      "title": "Tldraw pauses external contributions due to AI slop",
      "url": "https://github.com/tldraw/tldraw/issues/7695",
      "date": 1768520262,
      "author": "pranav_rajs",
      "guid": 36639,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://github.com/tldraw/tldraw/issues/7695\">https://github.com/tldraw/tldraw/issues/7695</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46641042\">https://news.ycombinator.com/item?id=46641042</a></p>\n<p>Points: 181</p>\n<p># Comments: 104</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "From Nevada to Kansas by Glider",
      "url": "https://www.weglide.org/flight/978820",
      "date": 1768520200,
      "author": "sammelaugust",
      "guid": 37075,
      "unread": true,
      "content": "<!DOCTYPE html>",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46641036"
    },
    {
      "title": "Why senior engineers let bad projects fail",
      "url": "https://lalitm.com/post/why-senior-engineers-let-bad-projects-fail/",
      "date": 1768516438,
      "author": "SupremumLimit",
      "guid": 36453,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://lalitm.com/post/why-senior-engineers-let-bad-projects-fail/\">https://lalitm.com/post/why-senior-engineers-let-bad-projects-fail/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46640366\">https://news.ycombinator.com/item?id=46640366</a></p>\n<p>Points: 262</p>\n<p># Comments: 160</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Linux boxes via SSH: suspended when disconected",
      "url": "https://shellbox.dev/",
      "date": 1768508413,
      "author": "messh",
      "guid": 36022,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://shellbox.dev/\">https://shellbox.dev/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46638629\">https://news.ycombinator.com/item?id=46638629</a></p>\n<p>Points: 301</p>\n<p># Comments: 155</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Briar keeps Iran connected via Bluetooth and Wi-Fi when the internet goes dark",
      "url": "https://briarproject.org/manual/fa/",
      "date": 1768505882,
      "author": "us321",
      "guid": 36021,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://briarproject.org/manual/fa/\">https://briarproject.org/manual/fa/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46638013\">https://news.ycombinator.com/item?id=46638013</a></p>\n<p>Points: 582</p>\n<p># Comments: 357</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Show HN: 1Code – Open-source Cursor-like UI for Claude Code",
      "url": "https://github.com/21st-dev/1code",
      "date": 1768504848,
      "author": "Bunas",
      "guid": 36658,
      "unread": true,
      "content": "<p>Hi, we're Sergey and Serafim. We've been building dev tools at 21st.dev and recently open-sourced 1Code (<a href=\"https://1code.dev\" rel=\"nofollow\">https://1code.dev</a>), a local UI for Claude Code.</p><p>Claude Code has been our go-to for 4 months. When Opus 4.5 dropped, parallel agents stopped needing so much babysitting. We started trusting it with more: building features end to end, adding tests, refactors. Stuff you'd normally hand off to a developer. We started running 3-4 at once. Then the CLI became annoying: too many terminals, hard to track what's where, diffs scattered everywhere.</p><p>So we built 1Code.dev, an app to run your Claude Code agents in parallel that works on Mac and Web. On Mac: run locally, with or without worktrees. On Web: run in remote sandboxes with live previews of your app, mobile included, so you can check on agents from anywhere.\nRunning multiple Claude Codes in parallel dramatically sped up how we build features.</p><p>What’s next: Bug bot for identifying issues based on your changes; QA Agent, that checks that new features don't break anything; Adding OpenCode, Codex, other models and coding agents. \nAPI for starting Claude Codes in remote sandboxes.</p><p>Try it out! We're open-source, so you can just bun build it.\nIf you want something hosted, Pro ($20/mo) gives you web with live browser previews hosted on remote sandboxes. We’re also working on API access for running Claude Code sessions programmatically.</p><p>We'd love to hear your feedback!</p>",
      "contentLength": 1429,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46637723"
    },
    {
      "title": "Data is the only moat",
      "url": "https://frontierai.substack.com/p/data-is-your-only-moat",
      "date": 1768503272,
      "author": "cgwu",
      "guid": 36045,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://frontierai.substack.com/p/data-is-your-only-moat\">https://frontierai.substack.com/p/data-is-your-only-moat</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46637328\">https://news.ycombinator.com/item?id=46637328</a></p>\n<p>Points: 202</p>\n<p># Comments: 47</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "JuiceFS is a distributed POSIX file system built on top of Redis and S3",
      "url": "https://github.com/juicedata/juicefs",
      "date": 1768502718,
      "author": "tosh",
      "guid": 36044,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://github.com/juicedata/juicefs\">https://github.com/juicedata/juicefs</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46637165\">https://news.ycombinator.com/item?id=46637165</a></p>\n<p>Points: 179</p>\n<p># Comments: 107</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "‘ELITE’: The Palantir app ICE uses to find neighborhoods to raid",
      "url": "https://werd.io/elite-the-palantir-app-ice-uses-to-find-neighborhoods-to-raid/",
      "date": 1768502566,
      "author": "sdoering",
      "guid": 36020,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://werd.io/elite-the-palantir-app-ice-uses-to-find-neighborhoods-to-raid/\">https://werd.io/elite-the-palantir-app-ice-uses-to-find-neighborhoods-to-raid/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46637127\">https://news.ycombinator.com/item?id=46637127</a></p>\n<p>Points: 430</p>\n<p># Comments: 368</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "CVEs affecting the Svelte ecosystem",
      "url": "https://svelte.dev/blog/cves-affecting-the-svelte-ecosystem",
      "date": 1768499484,
      "author": "tobr",
      "guid": 36638,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://svelte.dev/blog/cves-affecting-the-svelte-ecosystem\">https://svelte.dev/blog/cves-affecting-the-svelte-ecosystem</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46636387\">https://news.ycombinator.com/item?id=46636387</a></p>\n<p>Points: 179</p>\n<p># Comments: 28</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Ask HN: How can we solve the loneliness epidemic?",
      "url": "https://news.ycombinator.com/item?id=46635345",
      "date": 1768495753,
      "author": "publicdebates",
      "guid": 36019,
      "unread": true,
      "content": "\n<p>Countless voiceless people sit alone every day and have no one to talk to, people of all ages, who don't feel that they can join any local groups. So they sit on social media all day when they're not at work or school. How can we solve this?</p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46635345\">https://news.ycombinator.com/item?id=46635345</a></p>\n<p>Points: 762</p>\n<p># Comments: 1195</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "LLM Structured Outputs Handbook",
      "url": "https://nanonets.com/cookbooks/structured-llm-outputs",
      "date": 1768495610,
      "author": "vitaelabitur",
      "guid": 36593,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://nanonets.com/cookbooks/structured-llm-outputs\">https://nanonets.com/cookbooks/structured-llm-outputs</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46635309\">https://news.ycombinator.com/item?id=46635309</a></p>\n<p>Points: 302</p>\n<p># Comments: 49</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Apple is fighting for TSMC capacity as Nvidia takes center stage",
      "url": "https://www.culpium.com/p/exclusiveapple-is-fighting-for-tsmc",
      "date": 1768489362,
      "author": "speckx",
      "guid": 36018,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.culpium.com/p/exclusiveapple-is-fighting-for-tsmc\">https://www.culpium.com/p/exclusiveapple-is-fighting-for-tsmc</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46633488\">https://news.ycombinator.com/item?id=46633488</a></p>\n<p>Points: 763</p>\n<p># Comments: 466</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "The Palantir app helping ICE raids in Minneapolis",
      "url": "https://www.404media.co/elite-the-palantir-app-ice-uses-to-find-neighborhoods-to-raid/",
      "date": 1768488891,
      "author": "fajmccain",
      "guid": 36017,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.404media.co/elite-the-palantir-app-ice-uses-to-find-neighborhoods-to-raid/\">https://www.404media.co/elite-the-palantir-app-ice-uses-to-find-neighborhoods-to-raid/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46633378\">https://news.ycombinator.com/item?id=46633378</a></p>\n<p>Points: 634</p>\n<p># Comments: 838</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Show HN: TinyCity – A tiny city SIM for MicroPython (Thumby micro console)",
      "url": "https://github.com/chrisdiana/TinyCity",
      "date": 1768486290,
      "author": "inflam52",
      "guid": 36067,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46632768"
    },
    {
      "title": "Conditions in the Intel 8087 floating-point chip's microcode",
      "url": "https://www.righto.com/2025/12/8087-microcode-conditions.html",
      "date": 1768483413,
      "author": "diogotozzi",
      "guid": 37074,
      "unread": true,
      "content": "<p>In the 1980s, if you wanted your computer to do floating-point calculations faster, you could buy\nthe Intel 8087 floating-point coprocessor chip.\nPlugging it into your IBM PC would make operations up to 100 times faster, a big boost for spreadsheets\nand other number-crunching applications.\nThe 8087 uses complicated algorithms to compute trigonometric, logarithmic, and exponential functions.\nThese algorithms are implemented inside the chip in microcode.\nI'm part of a group that is reverse-engineering this microcode.\nIn this post, I examine the 49 types of conditional tests that the 8087's microcode uses inside its algorithms.\nSome conditions are simple, such as checking if a number is zero or negative, while others are specialized,\nsuch as determining what direction to round a number.</p><p>To explore the 8087's circuitry, I opened up an 8087 chip and took numerous photos of the silicon die with a microscope.\nAround the edges of the die, you can see the hair-thin bond wires that connect the chip to its 40 external pins.\nThe complex patterns on the die are formed by its metal wiring, as well as the polysilicon and silicon underneath.\nThe bottom half of the chip is the \"datapath\", the circuitry that performs calculations on 80-bit floating point values. \nAt the left of the datapath, a <a href=\"https://www.righto.com/2020/05/extracting-rom-constants-from-8087-math.html\">constant ROM</a> holds important constants such as π.\nAt the right are the eight registers that the\nprogrammer uses to hold floating-point values; in an unusual design decision,\nthese registers are arranged as a <a href=\"https://www.righto.com/2025/12/8087-stack-circuitry.html\">stack</a>.</p><div>Die of the Intel 8087 floating point unit chip, with main functional blocks labeled. The die is 5mm×6mm.  Click for a larger image.</div><p>The chip's instructions are defined by the large <a href=\"https://www.righto.com/2018/09/two-bits-per-transistor-high-density.html\">microcode ROM</a> in the middle.\nTo execute a floating-point instruction, the 8087 decodes the instruction and the microcode engine starts executing\nthe appropriate micro-instructions from the microcode ROM.\nThe microcode decode circuitry to the right of the ROM generates the appropriate control signals from each micro-instruction.\nThe bus registers and control circuitry handle interactions with the main 8086 processor and the rest of the system.</p><p>Executing an 8087 instruction such as arctan requires hundreds of internal steps to compute the result.\nThese steps are implemented in microcode with micro-instructions specifying each step of the algorithm.\n(Keep in mind the difference between the assembly language instructions used by a programmer and the\nundocumented low-level micro-instructions used internally by the chip.)\nThe microcode ROM holds 1648 micro-instructions, implementing the 8087's instruction set.\nEach micro-instruction is 16 bits long and performs a simple operation such as moving data inside the chip, adding two values, or <a href=\"https://www.righto.com/2020/05/die-analysis-of-8087-math-coprocessors.html\">shifting</a> data.\nI'm working with the \"Opcode Collective\" to reverse engineer the micro-instructions and fully understand the microcode (<a href=\"https://github.com/a-mcego/granite/blob/main/tools/8087mc/bin/8087.md\">link</a>).</p><p>The microcode engine (below) controls the execution of micro-instructions, acting as the mini-CPU inside the 8087.\nSpecifically, it generates an 11-bit micro-address, the address of a micro-instruction in the ROM.\nThe microcode engine implements jumps, subroutine calls, and returns within the microcode.\nThese jumps, subroutine calls, and returns are all conditional; the microcode engine will either perform the\noperation or skip it, depending on the value of a specified condition.</p><div>The microcode engine. In this image, the metal is removed, showing the underlying silicon and polysilicon.</div><p>I'll write more about the microcode engine later, but I'll give an overview here.\nAt the top, the Instruction Decode PLA decodes an 8087 instruction to determine the starting address in\nmicrocode.\nBelow that, the Jump PLA holds microcode addresses for jumps and subroutine calls.\nBelow this, six 11-bit registers implement the microcode stack, allowing six levels of subroutine calls inside the\nmicrocode.\n(Note that this stack is completely different from the 8087's register stack that holds eight floating-point values.)\nThe stack registers have associated read/write circuitry.\nThe incrementer adds one to the micro-address to step through the code.\nThe engine also implements relative jumps, using an adder to add an offset to the current location.\nAt the bottom, the address latch and drivers boost the 11-bit address output\nand send it to the microcode ROM.</p><p>A micro-instruction can say \"jump ahead 5 micro-instructions if a register is zero\" and the\nmicrocode engine will either perform the jump or ignore it, based on the register value.\nIn the circuitry, the condition causes the microcode engine to either perform the jump or block the jump.\nBut how does the hardware select one condition out of the large set of conditions?</p><p>Six bits of the micro-instruction can specify one of 64 conditions.\nA circuit similar to the idealized diagram below selects the specified condition.\nThe key component is a multiplexer, represented by a trapezoid below.\nA multiplexer is a simple circuit that selects one of its four inputs.\nBy arranging multiplexers in a tree, one of the 64 conditions on the left is selected and becomes the output,\npassed to the microcode engine.</p><div>A tree of multiplexers selects one of the conditions. This diagram is simplified.</div><p>For example, if bits J and K of the microcode are 00, the rightmost multiplexer will select the first input.\nIf bits LM are 01, the middle multiplexer will select the second input, and if bits NO are 10, the left\nmultiplexer will select its third input. The result is that condition 06 will pass through the tree and become the output.\nBy changing the bits that control the multiplexers, any of the inputs can be used.\n(We've arbitrarily given the 16 microcode bits the letter names A through P.)</p><p>Physically, the conditions come from locations scattered across the die. For instance, conditions involving the opcode\ncome from the instruction decoding part of the chip, while conditions involving a register are evaluated\nnext to the register.\nIt would be inefficient to run 64 wires for all the conditions to the microcode engine.\nThe tree-based approach reduces the wiring since the \"leaf\" multiplexers can be located\nnear the associated condition circuitry. Thus, only one wire needs to travel a long distance rather than multiple wires.\nIn other words, the condition selection circuitry is distributed across the chip instead of being implemented as\na centralized module.</p><p>Because the conditions don't always fall into groups of four, the actual implementation is slightly different from\nthe idealized diagram above.\nIn particular, the top-level multiplexer has five inputs, rather than four.\nOther multiplexers don't use all four inputs.\nThis provides a better match between the physical locations of the condition circuits and the multiplexers.\nIn total, 49 of the possible 64 conditions are implemented in the 8087.</p><p>The circuit that selects one of the four conditions is called a multiplexer.\nIt is constructed from pass transistors, transistors that are configured to either pass a signal through\nor block it.\nTo operate the multiplexer, one of the select lines is energized, turning on the corresponding pass transistor.\nThis allows the selected input to pass through the transistor to the output, while the other inputs are blocked.</p><div>A 4-1 multiplexer, constructed from four pass transistors.</div><p>The diagram below shows how a multiplexer appears on the die. The pinkish regions are doped silicon. The white\nlines are polysilicon wires.\nWhen polysilicon crosses over doped silicon, a transistor is formed.\nOn the left is a four-way multiplexer, constructed from four pass transistors. It takes inputs (black) for four conditions,\nnumbered 38, 39, 3a, and 3b.\nThere are four control signals (red) corresponding to the four combinations of bits N and O.\nOne of the inputs will pass through a transistor to the output, selected by the active control signal.\nThe right half contains the logic (four NOR gates and two inverters) to generate the control signals from the\nmicrocode bits.\n(Metal lines run horizontally from the logic to the control signal contacts, but I dissolved the metal for this\nphoto.)\nEach multiplexer in the 8087 has a completely different layout,\nmanually optimized based on the location of the signals and surrounding circuitry.\nAlthough the circuit for a multiplexer is regular (four transistors in parallel), the physical layout looks\nsomewhat chaotic.</p><div>Multiplexers as they appear on the die. The metal layer has been removed to show the polysilicon and silicon. The \"tie-die\" patterns are due to thin-film effects where the oxide layer wasn't completely removed.</div><p>The 8087 uses pass transistors for many circuits, not just multiplexers.\nCircuits with pass transistors are different from regular logic gates\nbecause the pass transistors provide no amplification. Instead, signals get weaker as they go through pass\ntransistors.\nTo solve this problem, inverters or buffers are inserted into the condition tree to boost signals;\nthey are omitted from the diagram above.</p><p>Of the 8087's 49 different conditions, some are widely used in the microcode, while others are designed for\na specific purpose and are only used once.\nThe full set of conditions is described in a footnote but I'll give some highlights here.</p><p>Fifteen conditions examine the bits of the current instruction's opcode. This allows\none microcode routine to handle a group of similar instructions and then change behavior based on the specific\ninstruction. For example, conditions test if the instruction is multiplication, if the instruction is an FILD/FIST\n(integer load or store), or if the bottom bit of the opcode is set.</p><p>The 8087 has three temporary registers—tmpA, tmpB, and tmpC—that hold values during computation.\nVarious conditions examine the values in the tmpA and tmpB registers.\nIn particular, the 8087 uses an interesting way to store numbers internally: each 80-bit floating-point value also \nhas two \"tag\" bits.\nThese bits are mostly invisible to the programmer and can be thought of as metadata.\nThe tag bits indicate if a register is empty, contains zero, contains a \"normal\" number, or contains a special\nvalue such as NaN (Not a Number) or infinity.\nThe 8087 uses the tag bits to optimize operations.\nThe tags also detect stack overflow (storing to a non-empty stack register) or stack underflow (reading from\nan empty stack register).</p><p>Other conditions are highly specialized. For instance, one condition looks at the rounding mode setting and\nthe sign of the value to determine if the value should be rounded up or down.\nOther conditions deal with exceptions such as numbers that are too small (i.e. denormalized) or numbers that\nlose precision.\nAnother condition tests if two values have the same sign or not.\nYet another condition tests if two values have the same sign or not, but inverts the result if the current\ninstruction is subtraction.\nThe simplest condition is simply \"true\", allowing an unconditional branch.</p><p>For flexibility, conditions can be \"flipped\", either jumping if the condition is true or jumping if the condition is false.\nThis is controlled by bit P of the microcode.\nIn the circuitry, this is implemented by a gate that XORs the P bit with the condition. The result is that the\nstate of the condition is flipped if bit P is set.</p><p>For a concrete example of how conditions are used, consider the\n<a href=\"https://raw.githubusercontent.com/a-mcego/granite/refs/heads/main/tools/8087mc/bin/8087mc_out.txt#:~:text=%230896%09AB%20%20%20%20%20%20I%20%20L%20N%20%20%09c094%09%2Bjmp%2D%3E%230898%20cond%3D0x0a%20opcode%261\">microcode routine</a>\nthat implements  and , the\ninstructions to change the sign and compute the absolute value, respectively.\nThese operations are almost the same (toggling the sign bit versus clearing the sign bit), so the same\nmicrocode routine handles both instructions, with a jump instruction to handle the difference.\nThe  and  instructions were designed with identical opcodes,\nexcept that the bottom bit is set for .\nThus, the microcode routine uses a condition that tests the bottom bit, allowing the routine to branch and\nchange its behavior for  vs .</p><p>Looking at the relevant micro-instruction, it has the hex value\n, or in binary .\nThe first three bits (ABC=110) specify the relative jump operation (100 would jump to a fixed target and 101 would\nperform a subroutine call.)\nBits D through I () indicate the amount of the jump (+`). \nBits J through O (, hex 0a) specify the condition to test, in this case, the last bit of the instruction opcode.\nThe final bit (P) would toggle the condition if set, (i.e. jump if false).\nThus, for , the jump instruction will jump ahead one micro-instruction.\nThis has the effect of skipping the next micro-instruction, which sets the appropriate sign bit for\n.</p><p>The 8087 performs floating-point operations much faster than the 8086 by using\nspecial hardware, optimized for floating-point.\nThe condition code circuitry is one example of this: the 8087\ncan test a complicated condition in a single operation.\nHowever, these complicated conditions make it much harder to understand the microcode.\nBut by a combination of examining the circuitry and looking at the micocode, we're making progress.\nThanks to the members of the \"Opcode Collective\" for their hard work, especially Smartest Blob and Gloriouscow.</p>",
      "contentLength": 13076,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46632120"
    },
    {
      "title": "25 Years of Wikipedia",
      "url": "https://wikipedia25.org/",
      "date": 1768483027,
      "author": "easton",
      "guid": 36016,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://wikipedia25.org\">https://wikipedia25.org</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46632023\">https://news.ycombinator.com/item?id=46632023</a></p>\n<p>Points: 558</p>\n<p># Comments: 458</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "High-Level Is the Goal",
      "url": "https://bvisness.me/high-level/",
      "date": 1768474894,
      "author": "tobr",
      "guid": 36636,
      "unread": true,
      "content": "<p>\n      If you have heard of the Handmade community, you likely think we are about “low-level programming” in some way. After all, we are a community inspired by <a href=\"https://www.youtube.com/watch?v=A2dxjOjWHxQ\">Handmade Hero</a>, a series where you learn to make a game and engine from scratch.\n    </p><p>\n      We in the Handmade community often bemoan the state of the software industry. Modern software is slow and bloated beyond belief—our computers are literally ten times more powerful than a decade ago, yet they run  than they used to, purely because the software is so bad. The actual user experience has steadily declined over the years despite the insane power at our fingertips. Worst of all, people’s expectations have hit rock bottom, and everyone thinks this is normal.\n    </p><p>\n      The Handmade crowd seems to think that low-level programming is the key to building better software. But this doesn’t really make sense on the surface. How is this practical for the average programmer? Do we really expect everyone to make their own UI frameworks and memory allocators from scratch? Do we really think you should never use libraries? Even if the average programmer could actually work that way, would anything actually improve, or would the world of software just become more fragmented?\n    </p><p>\n      I do believe, with all my heart, that low-level programming is the path to a better future for the software industry. But the previous criticisms are valid, and should be a serious concern for the Handmade programmer. So what is the connection here? What role does “low-level” play in a better future for software?\n    </p><p>\n      In 2019, a maker and YouTuber named Simone Giertz unveiled <a href=\"https://youtu.be/jKv_N0IDS2A?si=jdvzynwWmbfc9fx-\">“Truckla”</a>.\n    </p><p>\n      Simone wanted an Tesla pickup truck, but the Cybertruck was still just a rumor, and she was feeling impatient. So she did what any reasonable person would do, and decided to convert a Tesla Model 3 into a pickup truck.\n    </p><p>\n      The results speak for themselves. Truckla looks amazing, drives perfectly, and still functions  as a modern EV. This is no small feat—obviously you cannot just cut the roof off a sedan and call it a pickup truck. She and her team had to ensure that the car was structurally sound, that it could still charge, and that the software still worked as intended. Truckla is an impressive feat of engineering that took genuine creativity and craftsmanship.\n    </p><p>\n      And yet, Truckla is still a pretty bad pickup truck! The bed size is small, it can’t haul much weight, and it’s likely much less efficient than a truck engineered from the ground up. If you were in the market for a pickup truck, you would not buy Truckla! (You probably wouldn't buy a Cybertruck either, but I digress.)\n    </p><p>\n      Truckla is an excellent execution of a flawed idea. If you want to build a good pickup truck, you have to start with the frame.\n    </p><p>\n      In the world of software, the equivalent of the \"frame\" is the tech stack. Software is shaped by programming languages, frameworks, libraries, and platforms in the same way that a car is shaped by its frame. If you convert a sedan into a truck, you will get a bad truck, and if you start with the wrong stack, you will get bad software. No engineering effort will be able to save you.\n    </p><p>\n      As an example, let’s look at a program that everyone has interacted with at some point.\n    </p><p>\n      This is New Reddit. It is a new frontend they rolled out roughly a decade ago, and it is...not well-loved. Because so many people hate it, Old Reddit is still online, and this gives us a unique opportunity to compare two functionally identical pieces of software made a decade apart.\n    </p><p>\n      Back in 2023, I was experiencing horrible lag on New Reddit. The comment editor was sluggish, UI was slow to expand and collapse, and even hovering over a tooltip would cause a full-page hitch—all typical of modern software. Old Reddit, on the other hand, was a breath of fresh air—everything responded instantly. Aside from outdated aesthetics, Old Reddit was better in every way.\n    </p><p>\n      So here’s a thought experiment: How much work should it take to collapse a single comment?\n    </p><p>\n      This is a pretty easy question. All that needs to happen—all that  happen—is to hide or remove a few DOM elements, and update some text to say “collapsed”. A well-written Reddit frontend should more or less do exactly this. But let’s see what New Reddit did:\n    </p><p>\n      Gross. Call stacks thirty functions deep, layout computation in the middle of rendering, some kind of event or animation framework, and…hold on, is that jQuery?\n    </p><p>\n      My mistake, that’s actually a profile of Old Reddit. Here’s New Reddit:\n    </p><p>\n      At the time, it took New Reddit  to collapse a single comment. That is 200 milliseconds of pure JavaScript, with hardly any DOM work in sight. If you care about quality software, your jaw should be on the floor. It is a staggering amount of waste for what should have been a few DOM calls. And you feel it as a user: an ugly, intense hitch.\n    </p><p>\n      Old Reddit, on the other hand, did its work in about 10 milliseconds. That could be improved, but 10 milliseconds is totally fine. It feels responsive and keeps the site running at 60 frames per second. So Old Reddit is the clear winner here, with a UI 20 times faster than New Reddit.\n    </p><p>\n      So, we must pick up our jaws off the floor and ask the question: How on earth did we get here? Were New Reddit’s devs just stupid, lazy JS fanboys who would rather build Rube Goldberg machines than do their jobs?\n    </p><p>\n      Maybe tbh. But laziness alone doesn’t tell the whole story. The real problem with New Reddit was the stack it was built on.\n    </p><p>\n      So what was the Reddit stack? Back in 2023, New Reddit was a React app with Redux for state management. (These days they seem to have rewritten it in Web Components.) React and Redux of course sit atop the web platform: HTML, CSS, and JavaScript. This platform is implemented by some browser engine, which then runs on some operating system, and finally on the user’s physical hardware (which is itself extremely complicated, but we have to stop somewhere).\n    </p><p>\n      At my last job, I worked on an application that used precisely this same stack. Our application was an employee scheduling program that allowed managers to create weekly schedules for hourly workers. In about 2016 we replaced our aging Backbone.js frontend with a new one written in React and Redux, presumably because it was a popular choice at the time.\n    </p><p>\n      As a result, I became intimately familiar with how a React+Redux app is constructed. I also spent a lot of time trying to improve the app’s abysmal performance. I lived inside the Chrome and React profilers, diligently tracking down slow functions and suppressing unnecessary React updates. We had a whole caching system for our Redux selectors, and I added logging to help us find selectors with a high cache miss rate. I built scripts to parse our source code and make graphs of our selector dependencies, so I could find places to split the app bundle into smaller pieces. Unfortunately, none of my work made much of a difference—performance continued to plummet as the app increased in complexity.\n    </p><p>\n      When you try to make a fast React+Redux app, you are constantly fighting the frameworks. These two libraries constantly do unnecessary work, and your job is to  that work until things run acceptably again. But sometimes the cure is worse than the poison: an expensive  versus an expensive React re-render. Everything wants to update all the time, and as the app grows larger, the frequency and complexity of updates increases until there's no salvaging it.\n    </p><p>\n      New Reddit exemplified this perfectly: collapsing a comment would dispatch a Redux action, which would update the global Redux store, which would cause  Redux-connected components on the page to update, which would cause all their children to update as well. In other words, <b>collapsing one comment triggered an update for nearly every React component on the page.</b> No amount of caching, DOM-diffing, or  can save you from this amount of waste.\n    </p><p>\n      At the end of the day, I had to conclude that it is simply not possible to build a fast app on this stack. I have since encountered many web applications that suffer in exactly the same way. Time and again, if it’s slow, it’s probably using React, and if it’s  slow, it’s probably using Redux. The stack is the problem. It’s the only reasonable conclusion.\n    </p><p>\n      Thankfully, React+Redux is not the only possible software stack. We can choose alternatives at every point:\n    </p><ul><li>\n        You could choose a different JavaScript framework. Perhaps you could use Vue, or Svelte, or SolidJS, since these have presumably had time to learn from React’s mistakes. Or, of course, you could ditch all the frameworks and just use the DOM APIs directly, especially if your application is mostly static like Reddit.\n      </li><li>\n        You could use other browser APIs instead of HTML, CSS, and JS. You could use an alternative framework like Flutter, or you could build a custom UI stack in WebGL and WebAssembly. Building it yourself might sound crazy, but it’s been done successfully many times before—for example, Figma famously <a href=\"https://www.figma.com/blog/building-a-professional-design-tool-on-the-web/\">built their app from scratch</a> in WASM and WebGL, and it runs shockingly well on very large projects. Google Docs and Google Sheets also use WebGL instead of HTML and CSS, and the apps themselves are written in Java and compiled to JS or WASM.\n      </li><li>\n        You could build a native app! You could use a cross-platform framework like Qt, a game engine like Unity, an OS abstraction layer like SDL, or again just use the native APIs directly and build the rest from scratch. This is obviously the right choice for performance-intensive applications, and a valid option in general for developers who are serious about delivering a high-quality experience.\n      </li></ul><p>\n      Together all these choices actually form a tree. Every node in this tree is a valid stack you could choose to build your software on. Most importantly, different choices in this tree will be better for different kinds of software, so being comfortable with many options allows you to make better choices for each problem you face.\n    </p><svg width=\"100%\" height=\"100%\" viewBox=\"0 0 1920 1080\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:space=\"preserve\" xmlns:serif=\"http://www.serif.com/\"></svg><p>\n      Unfortunately, this is how I imagine the developers of New Reddit saw the tree:\n    </p><svg width=\"100%\" height=\"100%\" viewBox=\"0 0 1920 455\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:space=\"preserve\" xmlns:serif=\"http://www.serif.com/\"></svg><p>\n      There are not a lot of choices here. Critically, the best choice for them (direct DOM manipulation, like Old Reddit) was not even on the table. For whatever reason, I think they just didn’t even consider it as an option. Ew, icky, we can't just keep doing what Old Reddit did! We can't use </p><p>\n      Their view of the world was too high-level. If all you know is React, you have no choices—you can only use React, or meta-frameworks on top of React. But the lower level you can go, the more the tree opens up to you. Going lower level allows you to access other choices, and to recognize when another choice would be a better fit.\n    </p><p>\n      The first reason, then, that we care about low-level is that it allows us to make better choices. We can make better software by  in the right place, with the right frame and the right stack. Low-level programming allows us to build trucks instead of Trucklas.\n    </p><p>\n      But…this isn’t really enough, right? The software industry will not be saved by a few programmers making better choices. It would help, to be sure, but it’s far from the answer.\n    </p><p>\n      This presents an uncomfortable question: What if there are no good options in this tree? What if none of these choices are actually good for the kind of software we want to make?\n    </p><p>\n      For example, what if your app wants direct access to the hardware, but you also want a cross-platform UI? What are your choices? You could use Qt, but it tends to feel very dated and has strong opinions about how you architect your software. Game engines would likewise be a strange fit for a lot of applications, offering plenty of rendering power but little for 2D UI. There are some relative newcomers like Flutter, but Flutter makes you buy into Dart, and we all know Dart is not the right tool for a performance-intensive application. So what do you do? There are no good choices on the market—you’ll have to build it yourself.\n    </p><p>\n      Our tree is top-heavy. If we survey the software development landscape today, we see an insane number of JavaScript libraries and frameworks, an ever-growing number of browser APIs, and very little development outside of browsers besides frameworks that are Web-compatible and therefore subject to the same constraints. If our tree was a real tree, it would look something like this—and this is not a healthy tree.\n    </p><p>\n      The analogy works even better, actually, when you consider how many branches are dead or dying. What is the lifespan of a JS framework these days? Two years? Five if you’re lucky? More likely, the developer will have vanished off the face of the earth within a month.\n    </p><p>\n      Do we really imagine that the future of the software industry is to grow this tree even taller? To build more on top? Frameworks on top of frameworks? Do we imagine that in the future we’ll still be using HTML and CSS for sophisticated applications, when they’ve clearly been the wrong choice for years? Do we imagine that we’ll continue to ship apps on top of browsers, on top of operating systems, when modern browsers are basically operating systems unto themselves?\n    </p><p>\n      If we keep building, this tree will collapse under its own weight. We need to prune it, and grow new branches from lower in the tree.\n    </p><p>\n      But who is going to do that? Who is going to build that future for the software industry?\n    </p><p>\n      It requires a particular type of person. They must have inherent drive and passion for innovation in software. But they also must have low-level knowledge. They need to be able to make different choices from those who came before, to explore parts of the tree that haven’t yet been explored.\n    </p><div><div><svg width=\"100%\" height=\"100%\" viewBox=\"0 0 1498 798\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:space=\"preserve\" xmlns:serif=\"http://www.serif.com/\"></svg></div></div><p>\n      The overlap between these circles is . There are so few people who fit into both categories that we are just not seeing much innovation in that space. In fact, this image is pretty generous when you consider how few low-level programmers there are in general.\n    </p><p>\n      On the other hand, there are actually lots of people in the software industry with a drive to innovate. The problem is, they’re all making JavaScript frameworks.\n    </p><svg width=\"100%\" height=\"100%\" viewBox=\"0 0 1920 1080\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:space=\"preserve\" xmlns:serif=\"http://www.serif.com/\"></svg><p>\n      They don’t possess the low-level knowledge required to actually make a significant difference. That’s just the reality: if you build from the top of the tree, all the important decisions have already been made for you. It's like painting a Truckla a different color—it will not make a difference!\n    </p><p>\n      So the second reason I believe low-level is critical to the future of the software industry is that it simply expands the circle. We can capture some of those people with the drive to innovate and equip them to actually innovate in meaningful ways. We need more people exploring this low-level space, and I know that for many people, low-level knowledge would open their eyes to possibilities they would never have dreamed of before.\n    </p><p>\n      Not everyone who makes their own text editor will have great ideas about the future of programming. Not everyone who makes their own compiler will have great ideas about programming languages. But  of them will. And it only takes a few of them to make a difference in the software industry.\n    </p><svg width=\"100%\" height=\"100%\" viewBox=\"0 0 1920 1080\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:space=\"preserve\" xmlns:serif=\"http://www.serif.com/\"></svg><p>\n      So, to recap: the first reason we care about low-level is because low-level knowledge leads to better engineering choices. The second reason we care about low-level is because, in the long term, low-level knowledge is the path to better tools and better ways of programming—it is a  for building the platforms of the future.\n    </p><p>\n      But there is still one big problem with all of this: low-level programming today is absolutely .\n    </p><p>\n      Low-level programming is so frustrating, and so difficult. The  of low-level programming does not hold a candle to the experience of using high-level tools today—the very tools we see as a problem.\n    </p><p>\n      If I want to make a React app, I can simply Google “how to build react app” and I will find a beautifully-crafted web page with demos, installation guides, documentation, and resources to get me on my way. It has commands I can run to get an app up and running in five minutes. If I change a line of code in my editor, it refreshes immediately in my browser, shortening that feedback loop and making learning fun. And there is a wealth of other resources online: dev tools, libraries, tutorials, and more, making it easy for anyone to get up and running.\n    </p><p>\n      This is simply not the case for the low-level space. If you’re lucky, you can maybe find an expensive book or course. But more likely, you’ll just get a gigantic manual that lists every property of the system in excruciating detail, which is totally worthless for learning and barely usable as reference. And that’s if you’re lucky—there’s a good chance that you’ll only get a wiki or a maze of man pages, which are impenetrable walls of jargon. In some cases the only documentation that exists is the Linux Kernel Mailing List, and you can only pray that the one guy who can answer your question hasn’t flamed out in the past decade.\n    </p><p>\n      This isn’t just bad for beginners, it’s bad for . If this is the state of low-level knowledge, how can we expect  to practice low-level programming, much less the wider industry?\n    </p><p>\n      And the story doesn’t end there, because low-level  are terrible too. In a browser, I can open up the dev tools, go to Performance, click “Record”, and I will get a complete timeline of everything my application did. Every JavaScript function, every network request, every frame rendered, all correlated on a timeline so you can understand how everything relates. It is a developer’s dream, and it is a single click away! But the low-level space just does not have tools like this. There are a few decent profilers, but in most cases you just have to run a command-line program with some bizarre set of flags, pipe it through other tools, and then squint at a PDF or whatever.\n    </p><p>\n      The crazy thing is: <b>there is no reason for this to be the case</b>. We could absolutely have the same kind of “dev tools” for native development that we do for the web. We could have profilers that are actually designed to highlight useful info. We could have GUIs that show us network and file I/O, or inter-process communication. We could have interactive documentation and live reloading. We could have editor plugins and language servers to help beginners along. The raw capabilities are there. We're just waiting for someone with high-level sensibilities to come along and build the tools of our dreams.\n    </p><p>\n      But until we build that, why should we expect  to learn low-level programming? How can we expect them to?\n    </p><p>\n      So now we come back to Handmade, and what made Handmade Hero so special. Most programmers look at game engines and think that only a super-genius could write one—and the idea of making a game without an engine is lunacy. But Handmade Hero just didn’t care. Casey just sat down, showed you how to compile C, showed you how to put pixels on the screen, and before too long, you had a game. Not the most sophisticated game in the world, but a game nonetheless.\n    </p><p>\n      Handmade Hero shattered the barrier between low-level and high-level. Casey made a game,  he made an engine. The mystique was stripped away and replaced by an actual understanding of how games are made. Many people have the same reaction when they finally go through Handmade Hero: “Hey, this is not as hard as I thought!” It turns out you  make your own engine, despite the naysayers online.\n    </p><p>\n      I personally have found this to be true of so many “low-level” disciplines. “Low-level” programming is not impossible; in fact, in many cases, it’s  than the high-level web dev work I used to do! Today’s “high-level” frameworks and tools are so complicated and so poorly designed that they are  to understand and work with than their low-level counterparts. But all the modern nonsense like Svelte, Symfony, Kubernetes—those tools have docs! They have dev tools! Because, for some reason, people are not afraid of them!\n    </p><p>\n      Low-level programming is  terrible. I really believe that. And I know that it doesn’t have to be this way.\n    </p><p>\n      So my final question about low-level programming is: why do we even call it “low-level”?\n    </p><p>\n      The intent of any “high-level” tool is to make it easier to express our intent as programmers. “High-level” tools abstract away difficult details so we can focus on what we really care about. And in many cases this has worked: we’ve seen it in the evolution of programming languages, in the proliferation of game engines, and yes, even in the development of the web.\n    </p><p>\n      But notice: this is not about where these tools are in the stack. It’s not about how many layers they’ve built on top of. “High-level” is about <b>the expression of the programmer’s intent.</b> The position in the stack is ultimately irrelevant if programmers can use it to achieve their goals.\n    </p><p>\n      What then does this mean for “low-level”? The conclusion is inevitable: the reason we call things “low-level” is  they are terrible to use. They are “low-level”  we do not use them directly!  we sweep them under the rug and build abstractions on top, they  this low level that we don’t want to touch anymore!\n    </p><p>\n      Why are things “low-level” today? <b>Because no one has made them high-level yet.</b></p><p>\n      When I imagine a better future for the software industry, I  imagine one where everyone is making their own text editors, their own debuggers, or their own UI frameworks. Instead, I imagine a future where we have new “high-level” tools, built from lower in the stack. I imagine new tools that give the same high-level benefits we expect today, and in fact do  than the tools we have today, because they are freed from the constraining decisions of the past. We can build new platforms, new tools, and new libraries that  from the past, but build on solid foundations instead of piling more on top.\n    </p><div><div><svg width=\"100%\" height=\"100%\" viewBox=\"0 0 1560 540\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:space=\"preserve\" xmlns:serif=\"http://www.serif.com/\"></svg></div></div><p>\n      For the developers who truly care about making high-quality software, tools built lower in the stack can be their superpower. These programmers can be equipped to fine-tune their software in ways the web could never allow. And for the lazy Reddit dev who would rather push some slop out the door for a paycheck? Hey, at least their slop can run on a simpler, smaller, more efficient platform. It’s still a net positive in the end.\n    </p><p>\n      The Handmade community is positioned right in the middle of that Venn diagram today. We have people with low-level expertise. We have people with a drive to make software better. Our job, then, is  to just write low-level code and feel smug for knowing how things work. Our job is to build a  high level for the rest of the software industry.\n    </p><p>\n      Low-level programming is  the goal unto itself. High-level programming—a  kind of high-level programming—is the goal, and low-level is how we get there.\n    </p><p>\n      This post is adapted from a talk I delivered to the Handmade community in 2023. The original talk can be viewed <a href=\"https://www.youtube.com/watch?v=AmrBpxAtPrI\">here</a>.\n    </p>",
      "contentLength": 23592,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46630816"
    },
    {
      "title": "Photos capture the breathtaking scale of China's wind and solar buildout",
      "url": "https://e360.yale.edu/digest/china-renewable-photo-essay",
      "date": 1768470850,
      "author": "mrtksn",
      "guid": 36015,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://e360.yale.edu/digest/china-renewable-photo-essay\">https://e360.yale.edu/digest/china-renewable-photo-essay</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46630369\">https://news.ycombinator.com/item?id=46630369</a></p>\n<p>Points: 734</p>\n<p># Comments: 552</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Raspberry Pi's New AI Hat Adds 8GB of RAM for Local LLMs",
      "url": "https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/",
      "date": 1768465382,
      "author": "ingve",
      "guid": 36014,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/\">https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46629682\">https://news.ycombinator.com/item?id=46629682</a></p>\n<p>Points: 249</p>\n<p># Comments: 205</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Have Taken Up Farming",
      "url": "https://dylan.gr/1768295794",
      "date": 1768464766,
      "author": "djnaraps",
      "guid": 36452,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://dylan.gr/1768295794\">https://dylan.gr/1768295794</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46629610\">https://news.ycombinator.com/item?id=46629610</a></p>\n<p>Points: 240</p>\n<p># Comments: 153</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "To those who fired or didn't hire tech writers because of AI",
      "url": "https://passo.uno/letter-those-who-fired-tech-writers-ai/",
      "date": 1768463903,
      "author": "theletterf",
      "guid": 36013,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://passo.uno/letter-those-who-fired-tech-writers-ai/\">https://passo.uno/letter-those-who-fired-tech-writers-ai/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46629474\">https://news.ycombinator.com/item?id=46629474</a></p>\n<p>Points: 341</p>\n<p># Comments: 258</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Handy – Free open source speech-to-text app",
      "url": "https://github.com/cjpais/Handy",
      "date": 1768454598,
      "author": "tin7in",
      "guid": 36012,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://github.com/cjpais/Handy\">https://github.com/cjpais/Handy</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46628397\">https://news.ycombinator.com/item?id=46628397</a></p>\n<p>Points: 238</p>\n<p># Comments: 104</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Pocket TTS: A high quality TTS that gives your CPU a voice",
      "url": "https://kyutai.org/blog/2026-01-13-pocket-tts",
      "date": 1768454048,
      "author": "pain_perdu",
      "guid": 36011,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://kyutai.org/blog/2026-01-13-pocket-tts\">https://kyutai.org/blog/2026-01-13-pocket-tts</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46628329\">https://news.ycombinator.com/item?id=46628329</a></p>\n<p>Points: 610</p>\n<p># Comments: 148</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "The URL shortener that makes your links look as suspicious as possible",
      "url": "https://creepylink.com/",
      "date": 1768447700,
      "author": "dreadsword",
      "guid": 36010,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://creepylink.com/\">https://creepylink.com/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46627652\">https://news.ycombinator.com/item?id=46627652</a></p>\n<p>Points: 797</p>\n<p># Comments: 147</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Nepal's Mountainside Teahouses Elevate the Experience for Trekkers",
      "url": "https://www.smithsonianmag.com/travel/nepal-mountainside-teahouses-elevate-experience-trekkers-heading-to-top-world-180987844/",
      "date": 1768445236,
      "author": "bookofjoe",
      "guid": 37011,
      "unread": true,
      "content": "<p>We were only three days into a&nbsp;27-day journey through the forests of eastern Nepal, pushing through low clouds and a steady downpour.&nbsp;</p><p>“I think so,” he said, rain dripping from the bill of his baseball cap. “I’m pretty sure.”&nbsp;</p><p>The mist was so dense we could see barely 100 feet ahead. It was an especially wet September, the end of monsoon season. As we descended a narrow, muddy gulch, we reached a rushing waterfall that cascaded down a rocky incline and directly blocked our path. To cross it, we slipped off our shoes and socks and waded barefoot through the icy torrent, which dropped off sharply to our right.</p><div><div><h4>Did You Know? How high can you go before needing serious gear?</h4>\n    In <a href=\"https://www.smithsonianmag.com/smart-news/the-nepali-army-is-removing-trash-and-bodies-from-mount-everest-ahead-of-this-years-mountaineering-season-180984162/\">Nepal</a>,&nbsp;some&nbsp;mountain peaks are accessible by trekking,&nbsp;but&nbsp;higher altitude apexes require actual climbing.&nbsp;At&nbsp;21,247 feet, Mera is considered Nepal’s highest trekking peak.&nbsp;Ascending&nbsp;its&nbsp;low-grade&nbsp;glacial slopes&nbsp;doesn’t&nbsp;require the technical rock or ice climbing that peaks&nbsp;such as <a href=\"https://www.smithsonianmag.com/blogs/smithsonian-books/2024/05/14/the-many-faces-of-mt-everest/\">Mount Everest</a> (at&nbsp;29,032 feet)&nbsp;demand.\n    </div></div><p>Rai crossed with ease and grace, lightly gliding from one slick river stone to the next. I clumsily grabbed on to rocks in the water to hold my balance, trying not to scrape my feet or tumble down the waterfall to my death. After we made our way to the other side, Rai lit a cigarette and said, “Now this is a real adventure!” We looked at each other through the downpour and both threw our heads back in laughter, briefly forgetting that we were drenched, cold and covered with mud.</p><p>That positive outlook is the way of the Nepalis—and especially the native Nepalis of the mountains. When things get hard, which in the Himalayas is most of the time, you make the best of it and keep walking. Still, only about 7 percent of Nepal’s population lives in these high-altitude regions. Life in the Solukhumbu district—home to many of the country’s tallest peaks, including Everest—is harsh and often unforgiving: freezing temperatures, steep cliffs, infertile soil and few drivable roads. Yet its&nbsp;remoteness and rugged beauty are exactly what draw tourists here. Between October 2023 and September 2024, around 56,000 tourists visited Solukhumbu, most of them foreigners.</p><p>After five hours of trudging through the rain, we finally saw it: a small hut on a hillside, smoke curling from its entrance. A teahouse. Reaching the doorway, I ducked beneath a low beam and found Pembadoma Sherpa, 14, and her sister, Mingmakanchhi, 7, tending the fire. Pembadoma smiled shyly as she stirred a pot of po cha, or Sherpa tea. In this region, the thick, salty brew is made of dzomo (a yak and cow hybrid) butter, black tea and roasted barley flour. As the rain drummed against the tin roof, she worked the churn, stirring the tea with a wooden dowel, her movements steady and practiced.</p><p>This modest teahouse is one of hundreds of simple mountain shelters that are the lifeblood of Himalayan trekking, serving as rest stops and social hubs for climbers, herders and travelers moving between villages. Tourists stop here for many reasons—to rest, stay overnight, eat and dry their clothes by the fire—but often what they’re seeking is connection. Conversations around teahouse stoves can stretch for hours, spanning languages and continents. Inside, the food is hearty and nourishing: steaming plates of dal bhat, Nepal’s national dish of lentil stew with rice, vegetables and pickles; momos, soft dumplings filled with meat or vegetables and served with spicy dipping sauce; roti, a thin unleavened flatbread; and thukpa, a thick noodle soup. These meals reflect a blend of Indian, Tibetan and Chinese influences, humble but built for endurance and warmth.</p><p>Some teahouses are operated by the wives of mountain guides who spend months away leading expeditions. Many others are family-run, some passed down through generations. For many women and families, the teahouse is more than a business; it’s a vital source of stability. It keeps families—many named for the Sherpa ethnic group—rooted in their ancestral villages, financially provides for their children’s education and creates a steady income in places where few other opportunities exist.</p><p>The teahouse in the next village, Ramailo Danda, belongs to Phurtemba Sherpa, a retired mountain guide. “My father built this place himself,” his son and lodge manager, Chhongba Sherpa, told me. “It was jungle before. He cut the hillside by hand to make flat ground. At first it was very small, only enough space for six people. Every year he added a little more, stone by stone.”</p><p>Phurtemba started guiding when he was around 20. He tried to summit Everest five times and made it twice. When he retired, he turned his focus to the teahouse—work that was no easier. “The hardest thing,” Phurtemba said, “was water. There’s no natural source here. We tried pipes from 3,000 meters [about two miles] up the valley, but it didn’t work. Finally, we brought water from 450 meters [about 1,500 feet] below, pumping it through the rock.” He paused, then smiled. “Now it works. Mostly.”</p><p>Phurtemba also owns another teahouse farther up the trail in Chhatra Khola—the Mera Riverside Lodge—perched on the edge of a bluff beside a roaring river and a waterfall that feeds the valley below. There, a young woman named Rachana Rai worked in the kitchen, stirring pots of lentils and kneading dough. “I like it here,” she told me. “It’s quiet, but not lonely. The mountains keep me company.”</p><p>Every major trekin Solukhumbu begins with a flight into Lukla, a small town perched on a cliff with a runway so short and steep it’s considered one of the most dangerous in the world. From there, trails wind northeast, toward Everest Base Camp, some 64 miles away on foot, or east for 40 miles toward Mera Peak (21,247 feet).&nbsp;</p><p>At Mera’s high camp, approximately 19,000 feet, stands what is said to be the highest teahouse in Nepal. Surrounded by icefall, cornices and jagged seracs, it’s run by Sherpa cooks who live there for weeks at a time in subfreezing cold. “We don’t make much—maybe 30,000 rupees [about $210] a month,” one told me through an interpreter. They rely on tips. “Tourists are kind. Even with little salary, we are happy to work. When they smile and say thank you, we feel rich.”</p><p>The Sherpa people, whose name comes from the Tibetan words  (“east”) and  (“people”), are celebrated for their endurance and grace at altitude. Descended from nomads who migrated south from Tibet, they settled in the Solukhumbu region centuries ago and became the backbone of Himalayan mountaineering. While climbers from around the world seek glory on these peaks, it’s the Sherpas who guide climbers, fix the ropes, break trails, carry the loads and, when necessary, recover the fallen—as they did during my own time on Mera, when a Korean climber died and a Sherpa rescue team found his body and brought him down the mountain.</p><p>Sherpa culture is steeped in Buddhism. Along the trails, the landscape is marked by mani stones, carved with prayers, mantras or religious images, and stupas, dome-shaped Buddhist shrines; strings of blue, white, red, green and yellow prayer flags flutter in the wind. Inside the teahouses, the rhythm of life slows: Fires crackle, tea simmers, and stories are told easily between strangers. For the teahouse staff, the work is relentless—cooking, cleaning, stoking fires in the thin air—but most owners I met spoke of pride, not exhaustion. “People come from everywhere,” said Lakpa Sherpa, owner of <a href=\"https://himalayanlodgemerapeak.com/\">Himalayan Lodge &amp; Cafe</a> in Khote. “We take care of them like family. That is our way.”</p><p>On the final morning of the ascent, we left high camp at 3 a.m., climbing slowly toward Mera’s summit as dawn broke, casting an alpenglow on pristine slopes. Our pace was steady. As I continued my way upward, the sun appeared over the mountaintops, and the full expanse of the Himalayas came into view: Everest to the north, Makalu to the northeast, a sweep of glistening peaks stretching in all directions, and the villages we’d passed through scattered far below.</p><p>By Day 21 of the journey, I’d visited nearly a dozen teahouses, had hundreds of conversations and sipped countless cups of Sherpa tea. Now, as we headed back to Lukla, it struck me how much this climb had depended on the families who opened their doors along the way and offered warmth in a place defined by cold. In Nepal, the rugged landscape is shaped by mountains, but it’s the Nepali people and the teahouses that make moving through it possible. &nbsp;</p><div><p>Explore great travel deals</p><div><p>\n        Smithsonian magazine participates in affiliate link advertising programs. If you purchase an item through these links, we receive a commission.\n      </p></div></div>",
      "contentLength": 8773,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46627410"
    },
    {
      "title": "Furiosa: 3.5x efficiency over H100s",
      "url": "https://furiosa.ai/blog/introducing-rngd-server-efficient-ai-inference-at-data-center-scale",
      "date": 1768438401,
      "author": "written-beyond",
      "guid": 36510,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://furiosa.ai/blog/introducing-rngd-server-efficient-ai-inference-at-data-center-scale\">https://furiosa.ai/blog/introducing-rngd-server-efficient-ai-inference-at-data-center-scale</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46626410\">https://news.ycombinator.com/item?id=46626410</a></p>\n<p>Points: 210</p>\n<p># Comments: 155</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Gas Town Decoded",
      "url": "https://www.alilleybrinker.com/mini/gas-town-decoded/",
      "date": 1768430373,
      "author": "alilleybrinker",
      "guid": 36906,
      "unread": true,
      "content": "<div><p>On January 1st, Steve Yegge published <a href=\"https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04\">“Welcome to Gas Town,”</a> an introduction to his new AI agent orchestration tool written in a loose and chaotic mode, and accompanied by AI-generated images depicting a fictional industrial city populated with weasels (yes, really).</p><p>Reactions were swift, mostly agog at the scale and hubris of such a (self-admittedly) wasteful and obscenely  system, and alternately confused or amazed at the amount of new,  tailor-made to describe it.</p><p>In the interest of making Gas Town intelligible (because, despite the prose, the idea of agent orchestration it describes will be important), I’d like to share a quick decoder for the many new terms Steve introduces. His article itself offers definitions, but those definitions reuse his insular terms, making by-hand decoding tedious. Here, I’ve done the work for you.</p><table><thead><tr></tr></thead><tbody><tr><td>Top-level folder containing your individual projects. The  binary manages projects under this folder.</td></tr><tr><td>A project. It’s a folder tracked by a unique Git repository within your workspace.</td></tr><tr><td>The user (you). You have an “inbox” to receive notifications from agents in your projects.</td></tr><tr><td>The managing agent for a project. Usually you send this agent messages, and it coordinates the work of other agents in the project.</td></tr><tr><td>Worker agent, taking commands from the mayor, doing some work, submitting a Merge Request, and then stopping.</td></tr><tr><td>Merge agent, who coordinates and makes decisions about merge requests coming from Worker Agents.</td></tr><tr><td>Fixer agent, that watches the worker agents and tries to fix any that are stuck.</td></tr><tr><td>Maintenance agent, runs a consistent workflow in a loop, unlike “worker agents” who do arbitrary tasks and then die.</td><td>Maintenance Manager Agent</td></tr><tr><td>Maintenance worker agents who do cleanup tasks, directed by the Maintenance Agent.</td><td>Maintenance Worker Agents</td></tr><tr><td>Maintenance Manager checker agent, just checks on the Maintenance Manager Agent periodically to see if it needs a reboot or anything else.</td><td>Maintenance Manager Checker Agent</td></tr><tr><td>Persistent Worker Agents, which you talk to directly (not through the Mayor), and which persist after their tasks are done, to be reused. These are per-Project.</td><td>Persistent Worker Agents.</td></tr><tr><td>System for tracking work history across the system.</td></tr><tr><td>Project-specific work, tracked in the Work Tracker.</td></tr><tr><td>Whole-workspace work, tracked in the Work Tracker.</td></tr></tbody></table><p>Even with these definitions and alternative terms, Gas Town is still a bit of a mess, with watchers-on-watchers at times (do we really need a Maintenance Manager Checker Agent?). That said, hopefully this decoder at least makes understanding  easier.</p></div>",
      "contentLength": 2552,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46624883"
    },
    {
      "title": "Crafting Interpreters",
      "url": "https://craftinginterpreters.com/",
      "date": 1768429577,
      "author": "tosh",
      "guid": 36009,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://craftinginterpreters.com/\">https://craftinginterpreters.com/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46624658\">https://news.ycombinator.com/item?id=46624658</a></p>\n<p>Points: 237</p>\n<p># Comments: 56</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Training my smartwatch to track intelligence",
      "url": "https://dmvaldman.github.io/rooklift/",
      "date": 1768429176,
      "author": "dmvaldman",
      "guid": 36464,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46624563"
    },
    {
      "title": "Scaling long-running autonomous coding",
      "url": "https://cursor.com/blog/scaling-agents",
      "date": 1768429084,
      "author": "samwillis",
      "guid": 36008,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://cursor.com/blog/scaling-agents\">https://cursor.com/blog/scaling-agents</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46624541\">https://news.ycombinator.com/item?id=46624541</a></p>\n<p>Points: 273</p>\n<p># Comments: 175</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "The State of OpenSSL for pyca/cryptography",
      "url": "https://cryptography.io/en/latest/statements/state-of-openssl/",
      "date": 1768428250,
      "author": "SGran",
      "guid": 36509,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://cryptography.io/en/latest/statements/state-of-openssl/\">https://cryptography.io/en/latest/statements/state-of-openssl/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46624352\">https://news.ycombinator.com/item?id=46624352</a></p>\n<p>Points: 208</p>\n<p># Comments: 57</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "The Influentists: AI hype without proof",
      "url": "https://carette.xyz/posts/influentists/",
      "date": 1768424079,
      "author": "LucidLynx",
      "guid": 36007,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://carette.xyz/posts/influentists/\">https://carette.xyz/posts/influentists/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46623195\">https://news.ycombinator.com/item?id=46623195</a></p>\n<p>Points: 259</p>\n<p># Comments: 171</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Claude Cowork exfiltrates files",
      "url": "https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files",
      "date": 1768421545,
      "author": "takira",
      "guid": 36006,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files\">https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46622328\">https://news.ycombinator.com/item?id=46622328</a></p>\n<p>Points: 858</p>\n<p># Comments: 393</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Every country should set 16 as the minimum age for social media accounts",
      "url": "https://www.afterbabel.com/p/why-every-country-should-set-16",
      "date": 1768420395,
      "author": "paulpauper",
      "guid": 36005,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.afterbabel.com/p/why-every-country-should-set-16\">https://www.afterbabel.com/p/why-every-country-should-set-16</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46621945\">https://news.ycombinator.com/item?id=46621945</a></p>\n<p>Points: 248</p>\n<p># Comments: 321</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Verizon outages reported across U.S.",
      "url": "https://www.firstcoastnews.com/article/news/nation-world/verizon-outage-reported/507-ef3cb3d0-f595-432f-9f84-d1690a5085a7",
      "date": 1768417096,
      "author": "Scubabear68",
      "guid": 36004,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.firstcoastnews.com/article/news/nation-world/verizon-outage-reported/507-ef3cb3d0-f595-432f-9f84-d1690a5085a7\">https://www.firstcoastnews.com/article/news/nation-world/verizon-outage-reported/507-ef3cb3d0-f595-432f-9f84-d1690a5085a7</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46620835\">https://news.ycombinator.com/item?id=46620835</a></p>\n<p>Points: 226</p>\n<p># Comments: 183</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Show HN: Tabstack – Browser infrastructure for AI agents (by Mozilla)",
      "url": "https://news.ycombinator.com/item?id=46620358",
      "date": 1768415636,
      "author": "MrTravisB",
      "guid": 36066,
      "unread": true,
      "content": "Hi HN,<p>Maintaining a complex infrastructure stack for web browsing is one of the biggest bottlenecks in building reliable agents. You start with a simple fetch, but quickly end up managing a complex stack of proxies, handling client-side hydration, and debugging brittle selectors. and writing custom parsing logic for every site.</p><p>Tabstack is an API that abstracts that infrastructure. You send a URL and an intent; we handle the rendering and return clean, structured data for the LLM.</p><p>How it works under the hood:</p><p>- Escalation Logic: We don't spin up a full browser instance for every request (which is slow and expensive). We attempt lightweight fetches first, escalating to full browser automation only when the site requires JS execution/hydration.</p><p>- Token Optimization: Raw HTML is noisy and burns context window tokens. We process the DOM to strip non-content elements and return a markdown-friendly structure that is optimized for LLM consumption.</p><p>- Infrastructure Stability: Scaling headless browsers is notoriously hard (zombie processes, memory leaks, crashing instances). We manage the fleet lifecycle and orchestration so you can run thousands of concurrent requests without maintaining the underlying grid.</p><p>On Ethics: Since we are backed by Mozilla, we are strict about how this interacts with the open web.</p><p>- We respect robots.txt rules.</p><p>- We identify our User Agent.</p><p>- We do not use requests/content to train models.</p><p>- Data is ephemeral and discarded after the task.</p><p>The linked post goes into more detail on the infrastructure and why we think browsing needs to be a distinct layer in the AI stack.</p><p>This is obviously a very new space and we're all learning together. There are plenty of known unknowns (and likely even more unknown unknowns) when it comes to agentic browsing, so we’d genuinely appreciate your feedback, questions, and tips.</p><p>Happy to answer questions about the stack, our architecture, or the challenges of building browser infrastructure.</p>",
      "contentLength": 1960,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46620358"
    },
    {
      "title": "Show HN: Sparrow-1 – Audio-native model for human-level turn-taking without ASR",
      "url": "https://www.tavus.io/post/sparrow-1-human-level-conversational-timing-in-real-time-voice",
      "date": 1768413683,
      "author": "code_brian",
      "guid": 36065,
      "unread": true,
      "content": "<div>Sparrow-1 is now live: A real-time conversational-flow model for AI. </div>",
      "contentLength": 69,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46619614"
    },
    {
      "title": "Ask HN: What did you find out or explore today?",
      "url": "https://news.ycombinator.com/item?id=46619464",
      "date": 1768413265,
      "author": "blahaj",
      "guid": 36508,
      "unread": true,
      "content": "\n<p>Doesn't matter what domain and how big or small.</p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46619464\">https://news.ycombinator.com/item?id=46619464</a></p>\n<p>Points: 210</p>\n<p># Comments: 390</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Xous Operating System",
      "url": "https://xous.dev/",
      "date": 1768411749,
      "author": "eustoria",
      "guid": 36748,
      "unread": true,
      "content": "<div><img src=\"https://xous.dev/img/author.jpg\" title=\"Xous Operating System\" width=\"200px\" alt=\"Author's picture or avatar or logo\"><ul><li>A microkernel operating system for embedded devices.</li></ul><p>Xous is a microkernel operating system designed for medium embedded systems with clear separation of processes. Nearly everything is implemented in userspace, where message passing forms the basic communications primitive.</p><p>This project is funded through the NGI0 PET Fund, a fund established by NLnet\nwith financial support from the European Commission's Next Generation Internet\nprogramme, under the aegis of DG Communications Networks, Content and Technology\nunder grant agreement No 825310.</p></div>",
      "contentLength": 544,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46619059"
    },
    {
      "title": "So, you’ve hit an age gate. What now?",
      "url": "https://www.eff.org/deeplinks/2026/01/so-youve-hit-age-gate-what-now",
      "date": 1768411642,
      "author": "hn_acker",
      "guid": 36003,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.eff.org/deeplinks/2026/01/so-youve-hit-age-gate-what-now\">https://www.eff.org/deeplinks/2026/01/so-youve-hit-age-gate-what-now</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46619030\">https://news.ycombinator.com/item?id=46619030</a></p>\n<p>Points: 361</p>\n<p># Comments: 281</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Ford F-150 Lightning outsold the Cybertruck and was then canceled for poor sales",
      "url": "https://electrek.co/2026/01/13/ford-f150-lightning-outsold-tesla-cybertruck-canceled-not-selling-enough/",
      "date": 1768411209,
      "author": "MBCook",
      "guid": 36002,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://electrek.co/2026/01/13/ford-f150-lightning-outsold-tesla-cybertruck-canceled-not-selling-enough/\">https://electrek.co/2026/01/13/ford-f150-lightning-outsold-tesla-cybertruck-canceled-not-selling-enough/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46618901\">https://news.ycombinator.com/item?id=46618901</a></p>\n<p>Points: 669</p>\n<p># Comments: 947</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Ask HN: Share your personal website",
      "url": "https://news.ycombinator.com/item?id=46618714",
      "date": 1768410462,
      "author": "susam",
      "guid": 36001,
      "unread": true,
      "content": "\n<p>Hello HN!  I am putting together a community-maintained directory of personal websites at <https://hnpwd.github.io/>.  More details about the project can be found in the README at <https://github.com/hnpwd/hnpwd.github.io#readme>.<p>As you can see, the directory currently has only a handful of entries.  I need your help to grow it.  If you have a personal website, I would be glad if you shared it here.  If your website is hosted on a web space where you have full control over its design and content, and if it has been well received in past HN discussions, I might add it to the directory.  Just drop a link in the comments.  Please let me know if you do not want your website to be included in the directory.<p>Also, I intend this to be a community maintained resource, so if you would like to join the GitHub project as a maintainer, please let me know either here or via the IRC link in the README.<p>By the way, see also 'Ask HN: Could you share your personal blog here?' - https://news.ycombinator.com/item?id=36575081 - July 2023 - (1014 points, 1940 comments).  In this post, the scope is not restricted to blogs though.  Any personal website is welcome, whether it is a blog, digital garden, personal wiki or something else entirely.<p>UPDATE: It is going to take a while to go through all the submissions and add them.  If you'd like to help with the process, please send a PR directly to this project: https://github.com/hnpwd/hnpwd.github.io</p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46618714\">https://news.ycombinator.com/item?id=46618714</a></p>\n<p>Points: 885</p>\n<p># Comments: 2270</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Claude is good at assembling blocks, but still falls apart at creating them",
      "url": "https://www.approachwithalacrity.com/claude-ne/",
      "date": 1768407971,
      "author": "bblcla",
      "guid": 36000,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.approachwithalacrity.com/claude-ne/\">https://www.approachwithalacrity.com/claude-ne/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46618042\">https://news.ycombinator.com/item?id=46618042</a></p>\n<p>Points: 299</p>\n<p># Comments: 213</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "GitHub should charge everyone $1 more per month to fund open source",
      "url": "https://blog.greg.technology/2025/11/27/github-should-charge-1-dollar-more-per-month.html",
      "date": 1768407907,
      "author": "evakhoury",
      "guid": 35999,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://blog.greg.technology/2025/11/27/github-should-charge-1-dollar-more-per-month.html\">https://blog.greg.technology/2025/11/27/github-should-charge-1-dollar-more-per-month.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46618027\">https://news.ycombinator.com/item?id=46618027</a></p>\n<p>Points: 310</p>\n<p># Comments: 335</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Show HN: A 10KiB kernel for cloud apps",
      "url": "https://github.com/ReturnInfinity/BareMetal-Cloud",
      "date": 1768406693,
      "author": "ianseyler",
      "guid": 36064,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46617705"
    },
    {
      "title": "Roam 50GB is now Roam 100GB",
      "url": "https://starlink.com/support/article/58c9c8b7-474e-246f-7e3c-06db3221d34d",
      "date": 1768406591,
      "author": "bahmboo",
      "guid": 35998,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://starlink.com/support/article/58c9c8b7-474e-246f-7e3c-06db3221d34d\">https://starlink.com/support/article/58c9c8b7-474e-246f-7e3c-06db3221d34d</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46617668\">https://news.ycombinator.com/item?id=46617668</a></p>\n<p>Points: 292</p>\n<p># Comments: 364</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Find a pub that needs you",
      "url": "https://www.ismypubfucked.com/",
      "date": 1768405462,
      "author": "thinkingemote",
      "guid": 35997,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.ismypubfucked.com/\">https://www.ismypubfucked.com/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46617360\">https://news.ycombinator.com/item?id=46617360</a></p>\n<p>Points: 359</p>\n<p># Comments: 349</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "FBI raids Washington Post reporter's home",
      "url": "https://www.theguardian.com/us-news/2026/jan/14/fbi-raid-washington-post-hannah-natanson",
      "date": 1768402650,
      "author": "echelon_musk",
      "guid": 35996,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.theguardian.com/us-news/2026/jan/14/fbi-raid-washington-post-hannah-natanson\">https://www.theguardian.com/us-news/2026/jan/14/fbi-raid-washington-post-hannah-natanson</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46616745\">https://news.ycombinator.com/item?id=46616745</a></p>\n<p>Points: 935</p>\n<p># Comments: 582</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Ask HN: How are you doing RAG locally?",
      "url": "https://news.ycombinator.com/item?id=46616529",
      "date": 1768401509,
      "author": "tmaly",
      "guid": 35995,
      "unread": true,
      "content": "\n<p>I am curious how people are doing RAG locally with minimal dependencies for internal code or complex documents?<p>Are you using a vector database, some type of semantic search, a knowledge graph, a hypergraph?</p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46616529\">https://news.ycombinator.com/item?id=46616529</a></p>\n<p>Points: 369</p>\n<p># Comments: 146</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "SparkFun Officially Dropping AdaFruit due to CoC Violation",
      "url": "https://www.sparkfun.com/official-response",
      "date": 1768401297,
      "author": "yaleman",
      "guid": 35994,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.sparkfun.com/official-response\">https://www.sparkfun.com/official-response</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46616488\">https://news.ycombinator.com/item?id=46616488</a></p>\n<p>Points: 502</p>\n<p># Comments: 530</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Show HN: Webctl – Browser automation for agents based on CLI instead of MCP",
      "url": "https://github.com/cosinusalpha/webctl",
      "date": 1768401280,
      "author": "cosinusalpha",
      "guid": 36063,
      "unread": true,
      "content": "<p>Hi HN, I built webctl because I was frustrated by the gap between curl and full browser automation frameworks like Playwright.</p><p>I initially built this to solve a personal headache: I wanted an AI agent to handle project management tasks on my company’s intranet. I needed it to persist cookies across sessions (to handle SSO) and then scrape a Kanban board.</p><p>Existing AI browser tools (like current MCP implementations) often force unsolicited data into the context window—dumping the full accessibility tree, console logs, and network errors whether you asked for them or not.</p><p>webctl is an attempt to solve this with a Unix-style CLI:</p><p>- Filter before context: You pipe the output to standard tools. webctl snapshot --interactive-only | head -n 20 means the LLM only sees exactly what I want it to see.</p><p>- Daemon Architecture: It runs a persistent background process. The goal is to keep the browser state (cookies/session) alive while you run discrete, stateless CLI commands.</p><p>- Semantic targeting: It uses ARIA roles (e.g., role=button name~=\"Submit\") rather than fragile CSS selectors.</p><p>Disclaimer: The daemon logic for state persistence is still a bit experimental, but the architecture feels like the right direction for building local, token-efficient agents.</p><p>It’s basically \"Playwright for the terminal.\"</p>",
      "contentLength": 1305,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46616481"
    },
    {
      "title": "Show HN: Tiny FOSS Compass and Navigation App (<2MB)",
      "url": "https://github.com/CompassMB/MBCompass",
      "date": 1768388975,
      "author": "nativeforks",
      "guid": 36062,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46614688"
    },
    {
      "title": "More sustainable epoxy thanks to phosphorus",
      "url": "https://www.empa.ch/web/s604/flamm-hemmendes-epoxidharz-nachhaltiger-machen",
      "date": 1768387411,
      "author": "JeanKage",
      "guid": 36888,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.empa.ch/web/s604/flamm-hemmendes-epoxidharz-nachhaltiger-machen\">https://www.empa.ch/web/s604/flamm-hemmendes-epoxidharz-nachhaltiger-machen</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46614487\">https://news.ycombinator.com/item?id=46614487</a></p>\n<p>Points: 80</p>\n<p># Comments: 36</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Inside The Internet Archive's Infrastructure",
      "url": "https://hackernoon.com/the-long-now-of-the-web-inside-the-internet-archives-fight-against-forgetting",
      "date": 1768375612,
      "author": "dvrp",
      "guid": 36043,
      "unread": true,
      "content": "\n<p><a href=\"https://github.com/internetarchive/heritrix3\" rel=\"nofollow\">https://github.com/internetarchive/heritrix3</a></p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46613324\">https://news.ycombinator.com/item?id=46613324</a></p>\n<p>Points: 419</p>\n<p># Comments: 98</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "First impressions of Claude Cowork",
      "url": "https://simonw.substack.com/p/first-impressions-of-claude-cowork",
      "date": 1768371266,
      "author": "stosssik",
      "guid": 36042,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://simonw.substack.com/p/first-impressions-of-claude-cowork\">https://simonw.substack.com/p/first-impressions-of-claude-cowork</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46612919\">https://news.ycombinator.com/item?id=46612919</a></p>\n<p>Points: 210</p>\n<p># Comments: 117</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Show HN: OpenWork – An open-source alternative to Claude Cowork",
      "url": "https://github.com/different-ai/openwork",
      "date": 1768366504,
      "author": "ben_talent",
      "guid": 36041,
      "unread": true,
      "content": "<p>i built openwork, an open-source, local-first system inspired by claude cowork.</p><p>it’s a native desktop app that runs on top of opencode (opencode.ai).\nit’s basically an alternative gui for opencode, which (at least until now) has been more focused on technical folks.</p><p>the original seed for openwork was simple: i have a home server, and i wanted my wife and i to be able to run privileged workflows. things like controlling home assistant, or deploying custom web apps (e.g. our customs recipe app recipes.benjaminshafii.com), legal torrents, without living in a terminal.</p><p>our initial setup was running the opencode web server directly and sharing credentials to it. that worked, but i found the web ui unreliable and very unfriendly for non-technical users.</p><p>the goal with openwork is to bring the kind of workflows i’m used to running in the cli into a gui, while keeping a very deep extensibility mindset. ideally this grows into something closer to an obsidian-style ecosystem, but for agentic work.</p><p>some core principles i had in mind:</p><p>- open by design: no black boxes, no hosted lock-in. everything runs locally or on your own servers. (models don’t run locally yet, but both opencode and openwork are built with that future in mind.)\n- hyper extensible: skills are installable modules via a skill/package manager, using the native opencode plugin ecosystem.\n- non-technical by default: plans, progress, permissions, and artifacts are surfaced in the ui, not buried in logs.</p><p>you can already try it:\n- there’s an unsigned dmg\n- or you can clone the repo, install deps, and if you already have opencode running it should work right away</p><p>it’s very alpha, lots of rough edges. i’d love feedback on what feels the roughest or most confusing.</p><p>happy to answer questions.</p>",
      "contentLength": 1771,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46612494"
    },
    {
      "title": "Wine 11.0",
      "url": "https://gitlab.winehq.org/wine/wine/-/releases/wine-11.0",
      "date": 1768360090,
      "author": "zdw",
      "guid": 36887,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://gitlab.winehq.org/wine/wine/-/releases/wine-11.0\">https://gitlab.winehq.org/wine/wine/-/releases/wine-11.0</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46611816\">https://news.ycombinator.com/item?id=46611816</a></p>\n<p>Points: 248</p>\n<p># Comments: 50</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Show HN: OSS AI agent that indexes and searches the Epstein files",
      "url": "https://epstein.trynia.ai/",
      "date": 1768355812,
      "author": "jellyotsiro",
      "guid": 36061,
      "unread": true,
      "content": "<div><div><div><div><p>Indexed emails, messages, flight logs, court documents, and other records from the Epstein archive.</p></div></div></div><p>Search the Epstein archive — emails, messages, and documents. Powered by<a href=\"https://trynia.ai\" target=\"_blank\" rel=\"noopener noreferrer\">Nia</a>.</p></div>",
      "contentLength": 177,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46611348"
    },
    {
      "title": "AVX-512: First Impressions on Performance and Programmability",
      "url": "https://shihab-shahriar.github.io//blog/2026/AVX-512-First-Impressions-on-Performance-and-Programmability/",
      "date": 1768351416,
      "author": "shihab",
      "guid": 36957,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://shihab-shahriar.github.io//blog/2026/AVX-512-First-Impressions-on-Performance-and-Programmability/\">https://shihab-shahriar.github.io//blog/2026/AVX-512-First-Impressions-on-Performance-and-Programmability/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46610800\">https://news.ycombinator.com/item?id=46610800</a></p>\n<p>Points: 105</p>\n<p># Comments: 38</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "The Cathedral, the Megachurch, and the Bazaar",
      "url": "https://opensourcesecurity.io/2026/01-cathedral-megachurch-bazaar/",
      "date": 1768339619,
      "author": "todsacerdoti",
      "guid": 36845,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://opensourcesecurity.io/2026/01-cathedral-megachurch-bazaar/\">https://opensourcesecurity.io/2026/01-cathedral-megachurch-bazaar/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46608342\">https://news.ycombinator.com/item?id=46608342</a></p>\n<p>Points: 161</p>\n<p># Comments: 127</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Show HN: Nogic – VS Code extension that visualizes your codebase as a graph",
      "url": "https://marketplace.visualstudio.com/items?itemName=Nogic.nogic",
      "date": 1768329808,
      "author": "davelradindra",
      "guid": 36060,
      "unread": true,
      "content": "<div><div><div><table role=\"presentation\"><tbody><tr><td><div><div><p><strong>Turn your codebase into an interactive map to help you navigate and understand complex architecture within it</strong></p><p>More languages and frameworks coming soon! 🎉</p><ol><li>Open the Command Palette ( / )</li><li>Run </li><li>Right-click files or folders in the Explorer and select </li></ol><p>Your codebase is automatically indexed when you open the visualizer, if given permission.</p><ul><li>🌲  — Browse files, classes, and functions in an interactive hierarchical graph</li><li>📋  — Create custom boards to organize and focus on specific parts of your codebase</li><li>🎯  — View class relationships, inheritance, and method structures</li><li>🔄  — Trace function calls and dependencies across your codebase</li><li>🔍  — Find elements instantly with </li><li>⚡  — Changes to your code are automatically reflected in the visualization</li></ul><p>Your code never leaves your machine. All parsing, indexing, and visualization happens  within your IDE. No data is sent to any external server.</p><table><tbody><tr><td>Open the interactive visualizer</td></tr><tr></tr><tr><td>Add a file/folder to a board (right-click menu)</td></tr></tbody></table><ul><li>📋 Create multiple boards to organize different areas of your codebase</li><li>🖱️ Double click nodes on the canvas to zoom into focus view</li><li>📂 Right-click file nodes and select \"Add Connected Files\" to automatically add imported/dependent files</li><li>🖐️ Drag to pan, scroll to zoom</li></ul></div></div></td></tr></tbody></table></div></div></div>",
      "contentLength": 1258,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46605675"
    },
    {
      "title": "Using proxies to hide secrets from Claude Code",
      "url": "https://www.joinformal.com/blog/using-proxies-to-hide-secrets-from-claude-code/",
      "date": 1768327929,
      "author": "drewgregory",
      "guid": 36919,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.joinformal.com/blog/using-proxies-to-hide-secrets-from-claude-code/\">https://www.joinformal.com/blog/using-proxies-to-hide-secrets-from-claude-code/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46605155\">https://news.ycombinator.com/item?id=46605155</a></p>\n<p>Points: 119</p>\n<p># Comments: 39</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Show HN: The Tsonic Programming Language",
      "url": "https://tsonic.org/",
      "date": 1768325182,
      "author": "jeswin",
      "guid": 36059,
      "unread": true,
      "content": "<p>Tsonic is a TypeScript to C# compiler that produces native executables via .NET NativeAOT. Write TypeScript, get fast native binaries. Opt into  (JavaScript runtime APIs) and  (Node-style APIs) when you want them.</p><p>Tsonic lets TypeScript/JavaScript developers build fast native binaries for x64 and ARM64:</p><ul><li> (no JS runtime).</li><li>: use the .NET runtime + BCL (files, networking, crypto, concurrency, etc.).</li><li><strong>Optional JS/Node APIs when you want them</strong>:  (JavaScript runtime APIs) and  (Node-style APIs).</li><li>: your code still typechecks with . Tsonic also adds CLR-style numeric types like , , , etc. via .</li><li>: you build on a widely used runtime and standard library with regular updates.</li></ul><p>Tsonic targets the .NET BCL (not Node’s built-in modules). If you want JavaScript-style APIs, opt into . If you want Node-like APIs, opt into .</p><p>Tsonic compiles TypeScript to C#, then uses the standard CLR NativeAOT pipeline () to produce native binaries.</p><p>TypeScript maps well to C#/.NET:</p><ul><li><strong>Classes, interfaces, generics</strong>: translate naturally to CLR types.</li><li>: TS  maps cleanly to /.</li><li>: map to C# iterator patterns.</li><li>: map to / without inventing a new runtime ABI.</li></ul><p>NativeAOT produces <strong>single-file, self-contained native executables</strong>.</p><p>Details live in the docs:  and <code>/tsonic/architecture/pipeline/</code>.</p><ul><li>: Compile TypeScript directly to native executables</li><li><strong>Optional JS/Node compatibility</strong>:  (JS runtime APIs) and  (Node-style APIs)</li><li>: Full access to .NET BCL with native performance</li><li>: Single-file, self-contained executables</li><li>: Import and use any .NET library</li><li>: Standard ES modules with  import specifiers</li></ul><pre><code>mkdir my-app &amp;&amp; cd my-app\n\n# Basic workspace + default project\ntsonic init\n\n# Or: include JavaScript runtime APIs (console, JSON, timers, etc.)\ntsonic init --js\n\n# Or: include Node-style APIs (fs, path, crypto, http, etc.)\ntsonic init --nodejs\n</code></pre><ul><li> - Workspace config (dependencies live here)</li><li> - Workspace-scoped DLLs</li><li><code>packages/my-app/tsonic.json</code> - Project config</li><li><code>packages/my-app/src/App.ts</code> - Entry point</li><li> - NPM workspaces + scripts</li></ul><pre><code>npm run build    # Build native executable\n./packages/my-app/out/my-app  # Run it\n\n# Or build and run in one step\nnpm run dev\n</code></pre><pre><code>// packages/my-app/src/App.ts\nimport { Console } from \"@tsonic/dotnet/System.js\";\n\nexport function main(): void {\n  const message = \"Hello from Tsonic!\";\n  Console.WriteLine(message);\n\n  const numbers = [1, 2, 3, 4, 5];\n  Console.WriteLine(`Numbers: ${numbers.length}`);\n}\n</code></pre><pre><code>import { Console } from \"@tsonic/dotnet/System.js\";\nimport { File } from \"@tsonic/dotnet/System.IO.js\";\nimport { List } from \"@tsonic/dotnet/System.Collections.Generic.js\";\n\nexport function main(): void {\n  // File I/O\n  const content = File.ReadAllText(\"./README.md\");\n  Console.WriteLine(content);\n\n  // .NET collections\n  const list = new List&lt;number&gt;();\n  list.Add(1);\n  list.Add(2);\n  list.Add(3);\n  Console.WriteLine(`Count: ${list.Count}`);\n}\n</code></pre><h3>LINQ extension methods (, )</h3><pre><code>import { List } from \"@tsonic/dotnet/System.Collections.Generic.js\";\nimport type { ExtensionMethods as Linq } from \"@tsonic/dotnet/System.Linq.js\";\n\ntype LinqList&lt;T&gt; = Linq&lt;List&lt;T&gt;&gt;;\n\nconst xs = new List&lt;number&gt;() as unknown as LinqList&lt;number&gt;;\nxs.Add(1);\nxs.Add(2);\nxs.Add(3);\n\nconst doubled = xs.Where((x) =&gt; x % 2 === 0).Select((x) =&gt; x * 2).ToList();\nvoid doubled;\n</code></pre><h3>JSON with the .NET BCL ()</h3><pre><code>import { Console } from \"@tsonic/dotnet/System.js\";\nimport { JsonSerializer } from \"@tsonic/dotnet/System.Text.Json.js\";\n\ntype User = { id: number; name: string };\n\nconst user: User = { id: 1, name: \"Alice\" };\nconst json = JsonSerializer.Serialize(user);\nConsole.WriteLine(json);\n\nconst parsed = JsonSerializer.Deserialize&lt;User&gt;(json);\nif (parsed !== undefined) {\n  Console.WriteLine(parsed.name);\n}\n</code></pre><h3>JavaScript runtime APIs ()</h3><p>First, enable JSRuntime APIs:</p><pre><code># New project\ntsonic init --js\n\n# Existing project\ntsonic add js\n</code></pre><pre><code>import { console, JSON } from \"@tsonic/js/index.js\";\n\nexport function main(): void {\n  const value = JSON.parse&lt;{ x: number }&gt;('{\"x\": 1}');\n  console.log(JSON.stringify(value));\n}\n</code></pre><h3>Node-style APIs ()</h3><p>First, enable Node-style APIs:</p><pre><code># New project\ntsonic init --nodejs\n\n# Existing project\ntsonic add nodejs\n</code></pre><pre><code>import { console, path } from \"@tsonic/nodejs/index.js\";\n\nexport function main(): void {\n  console.log(path.join(\"a\", \"b\", \"c\"));\n}\n</code></pre><p>First, add the shared framework + bindings:</p><pre><code>tsonic add framework Microsoft.AspNetCore.App @tsonic/aspnetcore\n</code></pre><pre><code>import { WebApplication } from \"@tsonic/aspnetcore/Microsoft.AspNetCore.Builder.js\";\n\nexport function main(): void {\n  const builder = WebApplication.CreateBuilder([]);\n  const app = builder.Build();\n\n  app.MapGet(\"/\", () =&gt; \"Hello from Tsonic + ASP.NET Core!\");\n  app.Run();\n}\n</code></pre><h2>tsbindgen (CLR Bindings Generator)</h2><p>Tsonic doesn’t “guess” CLR types from strings. It relies on  generated by :</p><ul><li>Given a  (or a directory of assemblies), tsbindgen produces:\n<ul><li>ESM namespace facades () + TypeScript types ()</li><li> (namespace → CLR mapping)</li><li> (full type declarations)</li></ul></li><li>Tsonic uses these artifacts to resolve imports like:\n<ul><li><code>import { Console } from \"@tsonic/dotnet/System.js\"</code></li></ul></li></ul><p>Tsonic can run tsbindgen for you:</p><pre><code># Add a local DLL (auto-generates bindings if you omit the types package)\ntsonic add package ./path/to/MyLib.dll\n\n# Add a NuGet package (auto-generates bindings for the full transitive closure)\ntsonic add nuget Newtonsoft.Json 13.0.3\n\n# Or use published bindings packages (no auto-generation)\ntsonic add nuget Microsoft.EntityFrameworkCore 10.0.1 @tsonic/efcore\n</code></pre><table><tbody><tr><td>Initialize workspace + default project</td></tr><tr></tr><tr></tr><tr></tr><tr><td>Add  + JSRuntime DLLs</td></tr><tr><td>Add  + NodeJS DLLs</td></tr><tr><td><code>tsonic add package &lt;dll&gt; [types]</code></td><td>Add a local DLL + bindings</td></tr><tr><td><code>tsonic add nuget &lt;id&gt; &lt;ver&gt; [types]</code></td><td>Add a NuGet package + bindings</td></tr><tr><td><code>tsonic add framework &lt;ref&gt; [types]</code></td><td>Add a FrameworkReference + bindings</td></tr><tr></tr><tr></tr></tbody></table><table><tbody><tr><td>Workspace config path (default: auto-detect )</td></tr><tr><td>Output name (binary/assembly)</td></tr><tr><td>Runtime identifier (e.g., linux-x64)</td></tr><tr><td>Optimization: size or speed</td></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>Tsonic uses  config files:</p><ul><li> (workspace root) — shared settings and <strong>all external dependencies</strong></li><li><code>packages/&lt;project&gt;/tsonic.json</code> — per-project compilation settings</li></ul><p>See  for the full reference.</p><pre><code>my-app/\n├── tsonic.workspace.json\n├── libs/\n├── packages/\n│   └── my-app/\n│       ├── tsonic.json\n│       └── src/App.ts\n└── package.json         # npm workspaces + scripts\n</code></pre><h2>Npm Workspaces (Multi-Assembly Repos)</h2><p>Tsonic workspaces are plain npm workspaces, so you can build multi-assembly repos (e.g.  + ).</p><ul><li>Each workspace package has its own <code>packages/&lt;name&gt;/tsonic.json</code> and produces its own output ( for libraries,  for executables).</li><li>For library packages, you can generate  CLR bindings under  and expose them via npm ; Tsonic resolves imports using Node resolution (including ) and locates the nearest .</li></ul><p>See  for the recommended  +  layout.</p><table><tbody><tr><td>Base types (Array, String, iterators, Promise)</td></tr><tr><td>Core types (int, float, etc.)</td></tr><tr><td>.NET BCL type declarations</td></tr><tr><td>JavaScript runtime APIs (JS semantics on .NET)</td></tr><tr><td>Node-style APIs implemented in .NET</td></tr></tbody></table>",
      "contentLength": 6802,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46604308"
    },
    {
      "title": "Show HN: Self-host Reddit – 2.38B posts, works offline, yours forever",
      "url": "https://github.com/19-84/redd-archiver",
      "date": 1768318509,
      "author": "19-84",
      "guid": 36058,
      "unread": true,
      "content": "<p>Reddit's API is effectively dead for archival. Third-party apps are gone. Reddit has threatened to cut off access to the Pushshift dataset multiple times. But 3.28TB of Reddit history exists as a torrent right now, and I built a tool to turn it into something you can browse on your own hardware.</p><p>The key point: This doesn't touch Reddit's servers. Ever. Download the Pushshift dataset, run my tool locally, get a fully browsable archive. Works on an air-gapped machine. Works on a Raspberry Pi serving your LAN. Works on a USB drive you hand to someone.</p><p>What it does: Takes compressed data dumps from Reddit (.zst), Voat (SQL), and Ruqqus (.7z) and generates static HTML. No JavaScript, no external requests, no tracking. Open index.html and browse. Want search? Run the optional Docker stack with PostgreSQL – still entirely on your machine.</p><p>API &amp; AI Integration: Full REST API with 30+ endpoints – posts, comments, users, subreddits, full-text search, aggregations. Also ships with an MCP server (29 tools) so you can query your archive directly from AI tools.</p><p>Self-hosting options:\n - USB drive / local folder (just open the HTML files)\n - Home server on your LAN\n - Tor hidden service (2 commands, no port forwarding needed)\n - VPS with HTTPS\n - GitHub Pages for small archives</p><p>Why this matters: Once you have the data, you own it. No API keys, no rate limits, no ToS changes can take it away.</p><p>Scale: Tens of millions of posts per instance. PostgreSQL backend keeps memory constant regardless of dataset size. For the full 2.38B post dataset, run multiple instances by topic.</p><p>How I built it: Python, PostgreSQL, Jinja2 templates, Docker. Used Claude Code throughout as an experiment in AI-assisted development. Learned that the workflow is \"trust but verify\" – it accelerates the boring parts but you still own the architecture.</p>",
      "contentLength": 1832,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46602324"
    },
    {
      "title": "Show HN: SnackBase – Open-source, GxP-compliant back end for Python teams",
      "url": "https://snackbase.dev/",
      "date": 1768307273,
      "author": "lalitgehani",
      "guid": 36057,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46600092"
    },
    {
      "title": "Show HN: HyTags – HTML as a Programming Language",
      "url": "https://hytags.org/",
      "date": 1768301822,
      "author": "lassejansen",
      "guid": 36056,
      "unread": true,
      "content": "<p>Modern web frameworks usually split applications into a backend and a frontend, connected through an API. While flexible, this architecture introduces extra complexity: duplicated routing, API contracts, and data transfer layers whose main purpose is to bridge an often unnecessary divide.</p><p>The goal of hyTags is to make it possible to combine backend and frontend into a single application, with UI behavior and markup defined together in small, composable UI components, without compromising the user experience.</p>",
      "contentLength": 512,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46599403"
    },
    {
      "title": "Show HN: An iOS budget app I've been maintaining since 2011",
      "url": "https://primoco.me/en/",
      "date": 1768301079,
      "author": "Priotecs",
      "guid": 36055,
      "unread": true,
      "content": "<p>One , several Devices, different Platforms, ! Always there,  and as .</p>",
      "contentLength": 69,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46599341"
    },
    {
      "title": "How London cracked mobile phone coverage on the Underground",
      "url": "https://www.ianvisits.co.uk/articles/how-london-finally-cracked-mobile-phone-coverage-on-the-underground-86784/",
      "date": 1768289867,
      "author": "beardyw",
      "guid": 36758,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.ianvisits.co.uk/articles/how-london-finally-cracked-mobile-phone-coverage-on-the-underground-86784/\">https://www.ianvisits.co.uk/articles/how-london-finally-cracked-mobile-phone-coverage-on-the-underground-86784/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46598244\">https://news.ycombinator.com/item?id=46598244</a></p>\n<p>Points: 135</p>\n<p># Comments: 144</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Show HN: Fall asleep by watching JavaScript load",
      "url": "https://github.com/sarusso/bedtime",
      "date": 1768242972,
      "author": "sarusso",
      "guid": 36054,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46592376"
    },
    {
      "title": "Show HN: Yolobox – Run AI coding agents with full sudo without nuking home dir",
      "url": "https://github.com/finbarr/yolobox",
      "date": 1768242842,
      "author": "Finbarr",
      "guid": 36053,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46592344"
    },
    {
      "title": "Show HN: AI in SolidWorks",
      "url": "https://www.trylad.com/",
      "date": 1768236977,
      "author": "WillNickols",
      "guid": 36052,
      "unread": true,
      "content": "<p>Design from Documentation and Images</p><p>Provide documentation files, images, or examples of previous parts and assemblies, and LAD will intelligently read and use them.</p>",
      "contentLength": 164,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46591100"
    },
    {
      "title": "We put Claude Code in Rollercoaster Tycoon",
      "url": "https://labs.ramp.com/rct",
      "date": 1768228097,
      "author": "iamwil",
      "guid": 36694,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://labs.ramp.com/rct\">https://labs.ramp.com/rct</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46588972\">https://news.ycombinator.com/item?id=46588972</a></p>\n<p>Points: 512</p>\n<p># Comments: 277</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Show HN: Agent-of-empires: OpenCode and Claude Code session manager",
      "url": "https://github.com/njbrake/agent-of-empires",
      "date": 1768227787,
      "author": "river_otter",
      "guid": 36051,
      "unread": true,
      "content": "<p>Hi! I’m Nathan: an ML Engineer at Mozilla.ai: I built agent-of-empires (aoe): a CLI application to help you manage all of your running Claude Code/Opencode sessions and know when they are waiting for you.</p><p>- Written in rust and relies on tmux for security and reliability\n- Monitors state of cli sessions to tell you when an agent is running vs idle vs waiting for your input\n- Manage sessions by naming them, grouping them, configuring profiles for various settings</p><p>I'm passionate about getting self-hosted open-weight LLMs to be valid options to compete with proprietary closed models. One roadblock for me is that although tools like opencode allow you to connect to Local LLMs (Ollama, lm studio, etc), they generally run muuuuuch slower than models hosted by Anthropic and OpenAI. I would start a coding agent on a task, but then while I was sitting waiting for that task to complete, I would start opening new terminal windows to start multitasking. Pretty soon, I was spending a lot of time toggling between terminal windows to see which one needed me: like help in adding a clarification, approving a new command, or giving it a new task.</p><p>That’s why I build agent-of-empires (“aoe”). With aoe, I can launch a bunch of opencode and Claude Code sessions and quickly see their status or toggle between them, which helps me avoid having a lot of terminal windows open, or having to manually attach and detach from tmux sessions myself. It’s helping me give local LLMs a fair try, because them being slower is now much less of a bottleneck.</p><p>You can give it an install with</p><p>Or brew install njbrake/aoe/aoe</p><p>And then launch by simply entering the command `aoe`.</p><p>I’m interested in what you think as well as what features you think would be useful to add!</p><p>I am planning to add some further features around sandboxing (with docker) as well as support for intuitive git worktrees and am curious if there are any opinions about what should or shouldn’t be in it.</p><p>I decided against MCP management or generic terminal usage, to help keep the tool focused on parts of agentic coding that I haven’t found a usable solution for.</p><p>I hit the character limit on this post which prevented me from including a view of the output, but the readme on the github link has a screenshot showing what it looks like.</p>",
      "contentLength": 2297,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": "https://news.ycombinator.com/item?id=46588905"
    },
    {
      "title": "Lock-Picking Robot",
      "url": "https://github.com/etinaude/Lock-Picking-Robot",
      "date": 1768225362,
      "author": "p44v9n",
      "guid": 36463,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://github.com/etinaude/Lock-Picking-Robot\">https://github.com/etinaude/Lock-Picking-Robot</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46588379\">https://news.ycombinator.com/item?id=46588379</a></p>\n<p>Points: 290</p>\n<p># Comments: 127</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Go-legacy-winxp: Compile Golang 1.24 code for Windows XP",
      "url": "https://github.com/syncguy/go-legacy-winxp/tree/winxp-compat",
      "date": 1768224762,
      "author": "Oxodao",
      "guid": 36040,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://github.com/syncguy/go-legacy-winxp/tree/winxp-compat\">https://github.com/syncguy/go-legacy-winxp/tree/winxp-compat</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46588243\">https://news.ycombinator.com/item?id=46588243</a></p>\n<p>Points: 119</p>\n<p># Comments: 60</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Milk-V Titan: A $329 8-Core 64-bit RISC-V mini-ITX board with PCIe Gen4x16",
      "url": "https://www.cnx-software.com/2026/01/12/milk-v-titan-a-329-octa-core-64-bit-risc-v-mini-itx-motherboard-with-a-pcie-gen4-x16-slot/",
      "date": 1768224322,
      "author": "fork-bomber",
      "guid": 36808,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.cnx-software.com/2026/01/12/milk-v-titan-a-329-octa-core-64-bit-risc-v-mini-itx-motherboard-with-a-pcie-gen4-x16-slot/\">https://www.cnx-software.com/2026/01/12/milk-v-titan-a-329-octa-core-64-bit-risc-v-mini-itx-motherboard-with-a-pcie-gen4-x16-slot/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46588159\">https://news.ycombinator.com/item?id=46588159</a></p>\n<p>Points: 108</p>\n<p># Comments: 57</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Elasticsearch was never a database",
      "url": "https://www.paradedb.com/blog/elasticsearch-was-never-a-database",
      "date": 1768164887,
      "author": "jamesgresql",
      "guid": 36592,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.paradedb.com/blog/elasticsearch-was-never-a-database\">https://www.paradedb.com/blog/elasticsearch-was-never-a-database</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46579954\">https://news.ycombinator.com/item?id=46579954</a></p>\n<p>Points: 118</p>\n<p># Comments: 84</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "M8SBC-486 (Homebrew 486 computer)",
      "url": "https://maniek86.xyz/projects/m8sbc_486.php",
      "date": 1768157899,
      "author": "rasz",
      "guid": 36726,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://maniek86.xyz/projects/m8sbc_486.php\">https://maniek86.xyz/projects/m8sbc_486.php</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46578601\">https://news.ycombinator.com/item?id=46578601</a></p>\n<p>Points: 88</p>\n<p># Comments: 8</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "The Olivetti Company",
      "url": "https://www.abortretry.fail/p/the-olivetti-company",
      "date": 1768145584,
      "author": "rbanffy",
      "guid": 36725,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.abortretry.fail/p/the-olivetti-company\">https://www.abortretry.fail/p/the-olivetti-company</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46576581\">https://news.ycombinator.com/item?id=46576581</a></p>\n<p>Points: 199</p>\n<p># Comments: 41</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Below the Surface: Archeological Finds from the Amsterdam Noord/Zuid Metro Line",
      "url": "https://belowthesurface.amsterdam/en/vondsten",
      "date": 1768142173,
      "author": "stefanvdw1",
      "guid": 36747,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://belowthesurface.amsterdam/en/vondsten\">https://belowthesurface.amsterdam/en/vondsten</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46576091\">https://news.ycombinator.com/item?id=46576091</a></p>\n<p>Points: 78</p>\n<p># Comments: 10</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "I Built a 1 Petabyte Server from Scratch [video]",
      "url": "https://www.youtube.com/watch?v=vVI7atoAeoo",
      "date": 1768103753,
      "author": "zdw",
      "guid": 36039,
      "unread": true,
      "content": "\n<p>Article URL: <a href=\"https://www.youtube.com/watch?v=vVI7atoAeoo\">https://www.youtube.com/watch?v=vVI7atoAeoo</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46572589\">https://news.ycombinator.com/item?id=46572589</a></p>\n<p>Points: 98</p>\n<p># Comments: 30</p>\n",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    }
  ],
  "tags": [
    "hn"
  ]
}
{
  "id": "KWcVEiwEzuiDsTqLn6LojC9X2yBX45N36DRtYHsFby6oiM4t597YhKBSdxr5wqMdj8kvSgDuiUuV6yvogw",
  "title": "GitHub All Languages Daily Trending",
  "displayTitle": "Github Trending",
  "url": "https://mshibanami.github.io/GitHubTrendingRSS/daily/all.xml",
  "feedLink": "http://mshibanami.github.io/GitHubTrendingRSS",
  "isQuery": false,
  "isEmpty": false,
  "isHidden": false,
  "itemCount": 50,
  "items": [
    {
      "title": "hiddify/hiddify-app",
      "url": "https://github.com/hiddify/hiddify-app",
      "date": 1771729583,
      "author": "",
      "guid": 47261,
      "unread": true,
      "content": "<p>Multi-platform auto-proxy client, supporting Sing-box, X-ray, TUIC, Hysteria, Reality, Trojan, SSH etc. It‚Äôs an open-source, secure and ad-free.</p><p dir=\"ltr\">A multi-platform proxy client based on <a href=\"https://github.com/SagerNet/sing-box\">Sing-box</a> universal proxy tool-chain. Hiddify offers a wide range of capabilities, like automatic node selection, TUN mode, remote profiles etc. Hiddify is ad-free and open-source. With support for a wide range of protocols, it provides a secure and private way for accessing free internet.</p><p>‚úàÔ∏è Multi-platform: Android, iOS, Windows, macOS and Linux</p><p>‚≠ê Intuitive and accessible UI</p><p>üîç Delay based node selection</p><p>üü° Wide range of protocols: Vless, Vmess, Reality, TUIC, Hysteria, Wireguard, SSH etc.</p><p>üü° Subscription link and configuration formats: Sing-box, V2ray, Clash, Clash meta</p><p>üîÑ Automatic subscription update</p><p>üîé Display profile information including remaining days and traffic usage</p><p>üõ° Open source, secure and community driven</p><p>‚öô Compatible with all proxy management panels</p><p>‚≠ê Appropriate configuration for Iran, China, Russia and other countries</p><p>üì± Available on official stores</p><h2>‚öôÔ∏è Installation and tutorials</h2><p><strong>Find tutorial information on our wiki page by clicking on image below.</strong></p><p>Improve existing languages or add new ones by manually editing the JSON files located in  or by using the <a href=\"https://fink.inlang.com/github.com/hiddify/hiddify-next\">Inlang online editor</a>.</p><p>We would like to express our sincere appreciation to the contributors of the following projects, whose robust foundation and innovative features have significantly enhanced the success and functionality of this project.</p><p>The easiest way to support us is to click on the star (‚≠ê) at the top of this page.</p><a href=\"https://next.ossinsight.io/widgets/official/analyze-repo-stars-history?repo_id=643504282\" target=\"_blank\" align=\"center\"></a><p>We also need financial support for our services. All of our activities are done voluntarily and financial support will be spent on the development of the project. You can view our support addresses <a href=\"https://github.com/hiddify/hiddify-server/wiki/support\">here</a>.</p><h2>üë©‚Äçüè´ Collaboration and Contact Information</h2><p>Hiddify is a community driven project. If you're interested in contributing, please read the <a href=\"https://raw.githubusercontent.com/hiddify/hiddify-app/main/CONTRIBUTING.md\">contribution guidelines</a>. We would specially appreciate any help we can get in these areas: <strong>Flutter, Go, iOS development (Swift), Android development (Kotlin).</strong></p><p align=\"center\"> We appreciate all people who are participating in this project. Some people here and many many more outside of Github. It means a lot to us. ‚ô• </p>",
      "contentLength": 2258,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Stremio/stremio-web",
      "url": "https://github.com/Stremio/stremio-web",
      "date": 1771729583,
      "author": "",
      "guid": 47262,
      "unread": true,
      "content": "<p>Stremio - Freedom to Stream</p><p>Stremio is a modern media center that's a one-stop solution for your video entertainment. You discover, watch and organize video content from easy to install addons.</p><ul></ul><pre><code>docker build -t stremio-web .\ndocker run -p 8080:8080 stremio-web\n</code></pre><p>Stremio is copyright 2017-2023 Smart code and available under GPLv2 license. See the <a href=\"https://raw.githubusercontent.com/Stremio/stremio-web/development/LICENSE.md\">LICENSE</a> file in the project for more information.</p>",
      "contentLength": 392,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ggml-org/ggml",
      "url": "https://github.com/ggml-org/ggml",
      "date": 1771729583,
      "author": "",
      "guid": 47263,
      "unread": true,
      "content": "<p>Tensor library for machine learning</p><p>Tensor library for machine learning</p><p><em><strong>Note that this project is under active development.  Some of the development is currently happening in the <a href=\"https://github.com/ggerganov/llama.cpp\">llama.cpp</a> and <a href=\"https://github.com/ggerganov/whisper.cpp\">whisper.cpp</a> repos</strong></em></p><ul><li>Low-level cross-platform implementation</li><li>Integer quantization support</li><li>Automatic differentiation</li><li>ADAM and L-BFGS optimizers</li><li>No third-party dependencies</li><li>Zero memory allocations during runtime</li></ul><pre><code>git clone https://github.com/ggml-org/ggml\ncd ggml\n\n# install python dependencies in a virtual environment\npython3.10 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n\n# build the examples\nmkdir build &amp;&amp; cd build\ncmake ..\ncmake --build . --config Release -j 8\n</code></pre><pre><code># run the GPT-2 small 117M model\n../examples/gpt-2/download-ggml-model.sh 117M\n./bin/gpt-2-backend -m models/gpt-2-117M/ggml-model.bin -p \"This is an example\"\n</code></pre><p>For more information, checkout the corresponding programs in the <a href=\"https://raw.githubusercontent.com/ggml-org/ggml/master/examples\">examples</a> folder.</p><pre><code># fix the path to point to your CUDA compiler\ncmake -DGGML_CUDA=ON -DCMAKE_CUDA_COMPILER=/usr/local/cuda-12.1/bin/nvcc ..\n</code></pre><pre><code>cmake -DCMAKE_C_COMPILER=\"$(hipconfig -l)/clang\" -DCMAKE_CXX_COMPILER=\"$(hipconfig -l)/clang++\" -DGGML_HIP=ON\n</code></pre><pre><code># linux\nsource /opt/intel/oneapi/setvars.sh\ncmake -G \"Ninja\" -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL=ON ..\n\n# windows\n\"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat\"\ncmake -G \"Ninja\" -DCMAKE_C_COMPILER=cl -DCMAKE_CXX_COMPILER=icx -DGGML_SYCL=ON ..\n</code></pre><p>Download and unzip the NDK from this download <a href=\"https://developer.android.com/ndk/downloads\">page</a>. Set the NDK_ROOT_PATH environment variable or provide the absolute path to the CMAKE_ANDROID_NDK in the command below.</p><pre><code>cmake .. \\\n   -DCMAKE_SYSTEM_NAME=Android \\\n   -DCMAKE_SYSTEM_VERSION=33 \\\n   -DCMAKE_ANDROID_ARCH_ABI=arm64-v8a \\\n   -DCMAKE_ANDROID_NDK=$NDK_ROOT_PATH\n   -DCMAKE_ANDROID_STL_TYPE=c++_shared\n</code></pre><pre><code># create directories\nadb shell 'mkdir /data/local/tmp/bin'\nadb shell 'mkdir /data/local/tmp/models'\n\n# push the compiled binaries to the folder\nadb push bin/* /data/local/tmp/bin/\n\n# push the ggml library\nadb push src/libggml.so /data/local/tmp/\n\n# push model files\nadb push models/gpt-2-117M/ggml-model.bin /data/local/tmp/models/\n\nadb shell\ncd /data/local/tmp\nexport LD_LIBRARY_PATH=/data/local/tmp\n./bin/gpt-2-backend -m models/ggml-model.bin -p \"this is an example\"\n</code></pre>",
      "contentLength": 2249,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "abhigyanpatwari/GitNexus",
      "url": "https://github.com/abhigyanpatwari/GitNexus",
      "date": 1771729583,
      "author": "",
      "guid": 47264,
      "unread": true,
      "content": "<p>GitNexus: The Zero-Server Code Intelligence Engine - GitNexus is a client-side knowledge graph creator that runs entirely in your browser. Drop in a GitHub repo or ZIP file, and get an interactive knowledge graph wit a built in Graph RAG Agent. Perfect for code exploration</p><p><strong>Building git for agent context.</strong></p><p>Indexes any codebase into a knowledge graph ‚Äî every dependency, call chain, cluster, and execution flow ‚Äî then exposes it through smart tools so AI agents never miss code.</p><blockquote><p><em>Like DeepWiki, but deeper.</em> DeepWiki helps you  code. GitNexus lets you  it ‚Äî because a knowledge graph tracks every relationship, not just descriptions.</p></blockquote><p> The  is a quick way to chat with any repo. The  is how you make your AI agent actually reliable ‚Äî it gives Cursor, Claude Code, and friends a deep architectural view of your codebase so they stop missing dependencies, breaking call chains, and shipping blind edits. Even smaller models get full architectural clarity, making it compete with goliath models.</p><table><tbody><tr><td>Index repos locally, connect AI agents via MCP</td><td>Visual graph explorer + AI chat in browser</td></tr><tr><td>Daily development with Cursor, Claude Code, Windsurf, OpenCode</td><td>Quick exploration, demos, one-off analysis</td></tr><tr><td>Limited by browser memory (~5k files)</td></tr><tr><td>KuzuDB native (fast, persistent)</td><td>KuzuDB WASM (in-memory, per session)</td></tr><tr><td>Tree-sitter native bindings</td></tr><tr><td>Everything local, no network</td><td>Everything in-browser, no server</td></tr></tbody></table><p>The CLI indexes your repository and runs an MCP server that gives AI agents deep codebase awareness.</p><pre><code># Index your repo (run from repo root)\nnpx gitnexus analyze\n</code></pre><p>That's it. This indexes the codebase, installs agent skills, registers Claude Code hooks, and creates  /  context files ‚Äî all in one command.</p><p>To configure MCP for your editor, run  once ‚Äî or set it up manually below.</p><p> auto-detects your editors and writes the correct global MCP config. You only need to run it once.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><blockquote><p> gets the deepest integration: MCP tools + agent skills + PreToolUse hooks that automatically enrich grep/glob/bash calls with knowledge graph context.</p></blockquote><p>If you prefer manual configuration:</p><p> (full support ‚Äî MCP + skills + hooks):</p><pre><code>claude mcp add gitnexus -- npx -y gitnexus@latest mcp\n</code></pre><p> ( ‚Äî global, works for all projects):</p><pre><code>{\n  \"mcpServers\": {\n    \"gitnexus\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"gitnexus@latest\", \"mcp\"]\n    }\n  }\n}\n</code></pre><p> (<code>~/.config/opencode/config.json</code>):</p><pre><code>{\n  \"mcp\": {\n    \"gitnexus\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"gitnexus@latest\", \"mcp\"]\n    }\n  }\n}\n</code></pre><pre><code>gitnexus setup                    # Configure MCP for your editors (one-time)\ngitnexus analyze [path]           # Index a repository (or update stale index)\ngitnexus analyze --force          # Force full re-index\ngitnexus analyze --skip-embeddings  # Skip embedding generation (faster)\ngitnexus mcp                     # Start MCP server (stdio) ‚Äî serves all indexed repos\ngitnexus serve                   # Start HTTP server for web UI connection\ngitnexus list                    # List all indexed repositories\ngitnexus status                  # Show index status for current repo\ngitnexus clean                   # Delete index for current repo\ngitnexus clean --all --force     # Delete all indexes\ngitnexus wiki [path]             # Generate repository wiki from knowledge graph\ngitnexus wiki --model &lt;model&gt;    # Wiki with custom LLM model (default: gpt-4o-mini)\ngitnexus wiki --base-url &lt;url&gt;   # Wiki with custom LLM API base URL\n</code></pre><table><thead><tr></tr></thead><tbody><tr><td>Discover all indexed repositories</td></tr><tr><td>Process-grouped hybrid search (BM25 + semantic + RRF)</td></tr><tr><td>360-degree symbol view ‚Äî categorized refs, process participation</td></tr><tr><td>Blast radius analysis with depth grouping and confidence</td></tr><tr><td>Git-diff impact ‚Äî maps changed lines to affected processes</td></tr><tr><td>Multi-file coordinated rename with graph + text search</td></tr><tr></tr></tbody></table><blockquote><p>When only one repo is indexed, the  parameter is optional. With multiple repos, specify which one: <code>query({query: \"auth\", repo: \"my-app\"})</code>.</p></blockquote><p> for instant context:</p><table><tbody><tr><td>List all indexed repositories (read this first)</td></tr><tr><td><code>gitnexus://repo/{name}/context</code></td><td>Codebase stats, staleness check, and available tools</td></tr><tr><td><code>gitnexus://repo/{name}/clusters</code></td><td>All functional clusters with cohesion scores</td></tr><tr><td><code>gitnexus://repo/{name}/cluster/{name}</code></td><td>Cluster members and details</td></tr><tr><td><code>gitnexus://repo/{name}/processes</code></td></tr><tr><td><code>gitnexus://repo/{name}/process/{name}</code></td><td>Full process trace with steps</td></tr><tr><td><code>gitnexus://repo/{name}/schema</code></td><td>Graph schema for Cypher queries</td></tr></tbody></table><p> for guided workflows:</p><table><tbody><tr><td>Pre-commit change analysis ‚Äî scope, affected processes, risk level</td></tr><tr><td>Architecture documentation from the knowledge graph with mermaid diagrams</td></tr></tbody></table><p> installed to  automatically:</p><ul><li> ‚Äî Navigate unfamiliar code using the knowledge graph</li><li> ‚Äî Trace bugs through call chains</li><li> ‚Äî Analyze blast radius before changes</li><li> ‚Äî Plan safe refactors using dependency mapping</li></ul><h2>Multi-Repo MCP Architecture</h2><p>GitNexus uses a  so one MCP server can serve multiple indexed repos. No per-project MCP config needed ‚Äî set it up once and it works everywhere.</p><pre><code>flowchart TD\n    subgraph CLI [CLI Commands]\n        Setup[\"gitnexus setup\"]\n        Analyze[\"gitnexus analyze\"]\n        Clean[\"gitnexus clean\"]\n        List[\"gitnexus list\"]\n    end\n\n    subgraph Registry [\"~/.gitnexus/\"]\n        RegFile[\"registry.json\"]\n    end\n\n    subgraph Repos [Project Repos]\n        RepoA[\".gitnexus/ in repo A\"]\n        RepoB[\".gitnexus/ in repo B\"]\n    end\n\n    subgraph MCP [MCP Server]\n        Server[\"server.ts\"]\n        Backend[\"LocalBackend\"]\n        Pool[\"Connection Pool\"]\n        ConnA[\"KuzuDB conn A\"]\n        ConnB[\"KuzuDB conn B\"]\n    end\n\n    Setup --&gt;|\"writes global MCP config\"| CursorConfig[\"~/.cursor/mcp.json\"]\n    Analyze --&gt;|\"registers repo\"| RegFile\n    Analyze --&gt;|\"stores index\"| RepoA\n    Clean --&gt;|\"unregisters repo\"| RegFile\n    List --&gt;|\"reads\"| RegFile\n    Server --&gt;|\"reads registry\"| RegFile\n    Server --&gt; Backend\n    Backend --&gt; Pool\n    Pool --&gt;|\"lazy open\"| ConnA\n    Pool --&gt;|\"lazy open\"| ConnB\n    ConnA --&gt;|\"queries\"| RepoA\n    ConnB --&gt;|\"queries\"| RepoB\n</code></pre><p> Each  stores the index in  inside the repo (portable, gitignored) and registers a pointer in <code>~/.gitnexus/registry.json</code>. When an AI agent starts, the MCP server reads the registry and can serve any indexed repo. KuzuDB connections are opened lazily on first query and evicted after 5 minutes of inactivity (max 5 concurrent). If only one repo is indexed, the  parameter is optional on all tools ‚Äî agents don't need to change anything.</p><p>A fully client-side graph explorer and AI chat. No server, no install ‚Äî your code never leaves the browser.</p><img width=\"2550\" height=\"1343\" alt=\"gitnexus_img\" src=\"https://github.com/user-attachments/assets/cc5d637d-e0e5-48e6-93ff-5bcfdb929285\"><pre><code>git clone https://github.com/abhigyanpatwari/gitnexus.git\ncd gitnexus/gitnexus-web\nnpm install\nnpm run dev\n</code></pre><p>The web UI uses the same indexing pipeline as the CLI but runs entirely in WebAssembly (Tree-sitter WASM, KuzuDB WASM, in-browser embeddings). It's great for quick exploration but limited by browser memory for larger repos.</p><h2>The Problem GitNexus Solves</h2><p>Tools like , , , , and  are powerful ‚Äî but they don't truly know your codebase structure.</p><ol><li>AI edits </li><li>Doesn't know 47 functions depend on its return type</li></ol><h3>Traditional Graph RAG vs GitNexus</h3><p>Traditional approaches give the LLM raw graph edges and hope it explores enough. GitNexus <strong>precomputes structure at index time</strong> ‚Äî clustering, tracing, scoring ‚Äî so tools return complete context in one call:</p><pre><code>flowchart TB\n    subgraph Traditional[\"Traditional Graph RAG\"]\n        direction TB\n        U1[\"User: What depends on UserService?\"]\n        U1 --&gt; LLM1[\"LLM receives raw graph\"]\n        LLM1 --&gt; Q1[\"Query 1: Find callers\"]\n        Q1 --&gt; Q2[\"Query 2: What files?\"]\n        Q2 --&gt; Q3[\"Query 3: Filter tests?\"]\n        Q3 --&gt; Q4[\"Query 4: High-risk?\"]\n        Q4 --&gt; OUT1[\"Answer after 4+ queries\"]\n    end\n\n    subgraph GN[\"GitNexus Smart Tools\"]\n        direction TB\n        U2[\"User: What depends on UserService?\"]\n        U2 --&gt; TOOL[\"impact UserService upstream\"]\n        TOOL --&gt; PRECOMP[\"Pre-structured response:\n        8 callers, 3 clusters, all 90%+ confidence\"]\n        PRECOMP --&gt; OUT2[\"Complete answer, 1 query\"]\n    end\n</code></pre><p><strong>Core innovation: Precomputed Relational Intelligence</strong></p><ul><li> ‚Äî LLM can't miss context, it's already in the tool response</li><li> ‚Äî No 10-query chains to understand one function</li><li> ‚Äî Smaller LLMs work because tools do the heavy lifting</li></ul><p>GitNexus builds a complete knowledge graph of your codebase through a multi-phase indexing pipeline:</p><ol><li> ‚Äî Walks the file tree and maps folder/file relationships</li><li> ‚Äî Extracts functions, classes, methods, and interfaces using Tree-sitter ASTs</li><li> ‚Äî Resolves imports and function calls across files with language-aware logic</li><li> ‚Äî Groups related symbols into functional communities</li><li> ‚Äî Traces execution flows from entry points through call chains</li><li> ‚Äî Builds hybrid search indexes for fast retrieval</li></ol><p>TypeScript, JavaScript, Python, Java, C, C++, C#, Go, Rust</p><pre><code>impact({target: \"UserService\", direction: \"upstream\", minConfidence: 0.8})\n\nTARGET: Class UserService (src/services/user.ts)\n\nUPSTREAM (what depends on this):\n  Depth 1 (WILL BREAK):\n    handleLogin [CALLS 90%] -&gt; src/api/auth.ts:45\n    handleRegister [CALLS 90%] -&gt; src/api/auth.ts:78\n    UserController [CALLS 85%] -&gt; src/controllers/user.ts:12\n  Depth 2 (LIKELY AFFECTED):\n    authRouter [IMPORTS] -&gt; src/routes/auth.ts\n</code></pre><p>Options: , ,  (, , , ), </p><pre><code>query({query: \"authentication middleware\"})\n\nprocesses:\n  - summary: \"LoginFlow\"\n    priority: 0.042\n    symbol_count: 4\n    process_type: cross_community\n    step_count: 7\n\nprocess_symbols:\n  - name: validateUser\n    type: Function\n    filePath: src/auth/validate.ts\n    process_id: proc_login\n    step_index: 2\n\ndefinitions:\n  - name: AuthConfig\n    type: Interface\n    filePath: src/types/auth.ts\n</code></pre><h3>Context (360-degree Symbol View)</h3><pre><code>context({name: \"validateUser\"})\n\nsymbol:\n  uid: \"Function:validateUser\"\n  kind: Function\n  filePath: src/auth/validate.ts\n  startLine: 15\n\nincoming:\n  calls: [handleLogin, handleRegister, UserController]\n  imports: [authRouter]\n\noutgoing:\n  calls: [checkPassword, createSession]\n\nprocesses:\n  - name: LoginFlow (step 2/7)\n  - name: RegistrationFlow (step 3/5)\n</code></pre><h3>Detect Changes (Pre-Commit)</h3><pre><code>detect_changes({scope: \"all\"})\n\nsummary:\n  changed_count: 12\n  affected_count: 3\n  changed_files: 4\n  risk_level: medium\n\nchanged_symbols: [validateUser, AuthService, ...]\naffected_processes: [LoginFlow, RegistrationFlow, ...]\n</code></pre><pre><code>rename({symbol_name: \"validateUser\", new_name: \"verifyUser\", dry_run: true})\n\nstatus: success\nfiles_affected: 5\ntotal_edits: 8\ngraph_edits: 6     (high confidence)\ntext_search_edits: 2  (review carefully)\nchanges: [...]\n</code></pre><pre><code>-- Find what calls auth functions with high confidence\nMATCH (c:Community {heuristicLabel: 'Authentication'})&lt;-[:CodeRelation {type: 'MEMBER_OF'}]-(fn)\nMATCH (caller)-[r:CodeRelation {type: 'CALLS'}]-&gt;(fn)\nWHERE r.confidence &gt; 0.8\nRETURN caller.name, fn.name, r.confidence\nORDER BY r.confidence DESC\n</code></pre><p>Generate LLM-powered documentation from your knowledge graph:</p><pre><code># Requires an LLM API key (OPENAI_API_KEY, etc.)\ngitnexus wiki\n\n# Use a custom model or provider\ngitnexus wiki --model gpt-4o\ngitnexus wiki --base-url https://api.anthropic.com/v1\n\n# Force full regeneration\ngitnexus wiki --force\n</code></pre><p>The wiki generator reads the indexed graph structure, groups files into modules via LLM, generates per-module documentation pages, and creates an overview page ‚Äî all with cross-references to the knowledge graph.</p><table><tbody><tr></tr><tr><td>Tree-sitter native bindings</td></tr><tr></tr><tr><td>HuggingFace transformers.js (GPU/CPU)</td><td>transformers.js (WebGPU/WASM)</td></tr><tr></tr><tr></tr><tr><td>Sigma.js + Graphology (WebGL)</td></tr><tr><td>React 18, TypeScript, Vite, Tailwind v4</td></tr><tr></tr><tr></tr></tbody></table><ul><li>: Everything runs locally on your machine. No network calls. Index stored in  (gitignored). Global registry at  stores only paths and metadata.</li><li>: Everything runs in your browser. No code uploaded to any server. API keys stored in localStorage only.</li><li>Open source ‚Äî audit the code yourself.</li></ul>",
      "contentLength": 11609,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "cloudflare/agents",
      "url": "https://github.com/cloudflare/agents",
      "date": 1771729583,
      "author": "",
      "guid": 47265,
      "unread": true,
      "content": "<p>Build and deploy AI Agents on Cloudflare</p><p>Agents are persistent, stateful execution environments for agentic workloads, powered by Cloudflare <a href=\"https://developers.cloudflare.com/durable-objects/\">Durable Objects</a>. Each agent has its own state, storage, and lifecycle ‚Äî with built-in support for real-time communication, scheduling, AI model calls, MCP, workflows, and more.</p><p>Agents hibernate when idle and wake on demand. You can run millions of them ‚Äî one per user, per session, per game room ‚Äî each costs nothing when inactive.</p><pre><code>npm create cloudflare@latest -- --template cloudflare/agents-starter\n</code></pre><p>Or add to an existing project:</p><p>A counter agent with persistent state, callable methods, and real-time sync to a React frontend:</p><pre><code>// server.ts\nimport { Agent, routeAgentRequest, callable } from \"agents\";\n\nexport type CounterState = { count: number };\n\nexport class CounterAgent extends Agent&lt;Env, CounterState&gt; {\n  initialState = { count: 0 };\n\n  @callable()\n  increment() {\n    this.setState({ count: this.state.count + 1 });\n    return this.state.count;\n  }\n\n  @callable()\n  decrement() {\n    this.setState({ count: this.state.count - 1 });\n    return this.state.count;\n  }\n}\n\nexport default {\n  async fetch(request: Request, env: Env, ctx: ExecutionContext) {\n    return (\n      (await routeAgentRequest(request, env)) ??\n      new Response(\"Not found\", { status: 404 })\n    );\n  }\n};\n</code></pre><pre><code>// client.tsx\nimport { useAgent } from \"agents/react\";\nimport { useState } from \"react\";\nimport type { CounterAgent, CounterState } from \"./server\";\n\nfunction Counter() {\n  const [count, setCount] = useState(0);\n\n  const agent = useAgent&lt;CounterAgent, CounterState&gt;({\n    agent: \"CounterAgent\",\n    onStateUpdate: (state) =&gt; setCount(state.count)\n  });\n\n  return (\n    &lt;div&gt;\n      &lt;span&gt;{count}&lt;/span&gt;\n      &lt;button onClick={() =&gt; agent.stub.increment()}&gt;+&lt;/button&gt;\n      &lt;button onClick={() =&gt; agent.stub.decrement()}&gt;-&lt;/button&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre><p>State changes sync to all connected clients automatically. Call methods like they're local functions.</p><table><tbody><tr><td>Syncs to all connected clients, survives restarts</td></tr><tr><td>Type-safe RPC via the  decorator</td></tr><tr><td>One-time, recurring, and cron-based tasks</td></tr><tr><td>Real-time bidirectional communication with lifecycle hooks</td></tr><tr><td>Message persistence, resumable streaming, server/client tool execution</td></tr><tr><td>Act as MCP servers or connect as MCP clients</td></tr><tr><td>Durable multi-step tasks with human-in-the-loop approval</td></tr><tr><td>Receive and respond via Cloudflare Email Routing</td></tr><tr><td>LLMs generate executable TypeScript instead of individual tool calls</td></tr><tr><td>Direct SQLite queries via Durable Objects</td></tr><tr><td> and  for frontend integration</td></tr><tr><td> for non-React environments</td></tr></tbody></table><p> Realtime voice agents, web browsing (headless browser), sandboxed code execution, and multi-channel communication (SMS, messengers).</p><table><tbody><tr><td>Core SDK ‚Äî Agent class, routing, state, scheduling, MCP, email, workflows</td></tr><tr><td>Higher-level AI chat ‚Äî persistent messages, resumable streaming, tool execution</td></tr><tr><td>Hono middleware for adding agents to Hono apps</td></tr></tbody></table><p>The <a href=\"https://raw.githubusercontent.com/cloudflare/agents/main/examples\"></a> directory has self-contained demos covering most SDK features ‚Äî MCP servers/clients, workflows, email agents, webhooks, tic-tac-toe, resumable streaming, and more. The <a href=\"https://raw.githubusercontent.com/cloudflare/agents/main/examples/playground\"></a> is the kitchen-sink showcase with everything in one UI.</p><pre><code>cd examples/playground\nnpm run dev\n</code></pre><p>Node 24+ required. Uses npm workspaces.</p><pre><code>npm install          # install all workspaces\nnpm run build        # build all packages\nnpm run check        # full CI check (format, lint, typecheck, exports)\nCI=true npm test     # run tests (vitest + vitest-pool-workers)\n</code></pre><p>Changes to  need a changeset:</p>",
      "contentLength": 3435,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "anthropics/claude-code",
      "url": "https://github.com/anthropics/claude-code",
      "date": 1771729583,
      "author": "",
      "guid": 47266,
      "unread": true,
      "content": "<p>Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.</p><p>Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows -- all through natural language commands. Use it in your terminal, IDE, or tag @claude on Github.</p><img src=\"https://raw.githubusercontent.com/anthropics/claude-code/main/demo.gif\"><blockquote><p>[!NOTE] Installation via npm is deprecated. Use one of the recommended methods below.</p></blockquote><p>For more installation options, uninstall steps, and troubleshooting, see the <a href=\"https://code.claude.com/docs/en/setup\">setup documentation</a>.</p><ol><li><p><strong>MacOS/Linux (Recommended):</strong></p><pre><code>curl -fsSL https://claude.ai/install.sh | bash\n</code></pre><pre><code>brew install --cask claude-code\n</code></pre><pre><code>irm https://claude.ai/install.ps1 | iex\n</code></pre><pre><code>winget install Anthropic.ClaudeCode\n</code></pre><pre><code>npm install -g @anthropic-ai/claude-code\n</code></pre></li><li><p>Navigate to your project directory and run .</p></li></ol><p>This repository includes several Claude Code plugins that extend functionality with custom commands and agents. See the <a href=\"https://raw.githubusercontent.com/anthropics/claude-code/main/plugins/README.md\">plugins directory</a> for detailed documentation on available plugins.</p><p>We welcome your feedback. Use the  command to report issues directly within Claude Code, or file a <a href=\"https://github.com/anthropics/claude-code/issues\">GitHub issue</a>.</p><p>Join the <a href=\"https://anthropic.com/discord\">Claude Developers Discord</a> to connect with other developers using Claude Code. Get help, share feedback, and discuss your projects with the community.</p><h2>Data collection, usage, and retention</h2><p>When you use Claude Code, we collect feedback, which includes usage data (such as code acceptance or rejections), associated conversation data, and user feedback submitted via the  command.</p><p>We have implemented several safeguards to protect your data, including limited retention periods for sensitive information, restricted access to user session data, and clear policies against using feedback for model training.</p>",
      "contentLength": 1892,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "stan-smith/FossFLOW",
      "url": "https://github.com/stan-smith/FossFLOW",
      "date": 1771729583,
      "author": "",
      "guid": 47267,
      "unread": true,
      "content": "<p>Make beautiful isometric infrastructure diagrams</p><p> Stan here, if you've used FossFLOW and it's helped you, <b>I'd really appreciate if you could donate something small :)</b> I work full time, and finding the time to work on this project is challenging enough. If you've had a feature that I've implemented for you, or fixed a bug it'd be great if you could :) if not, that's not a problem, this software will always remain free!</p><p> If you haven't yet, please check out the underlying library this is built on by <a href=\"https://github.com/markmanx/isoflow\">@markmanx</a> I truly stand on the shoulders of a giant here ü´°</p><p align=\"center\"> Go to <b> --&gt; https://stan-smith.github.io/FossFLOW/ &lt;-- </b></p><p>FossFLOW is a powerful, open-source Progressive Web App (PWA) for creating beautiful isometric diagrams. Built with React and the <a href=\"https://github.com/markmanx/isoflow\">Isoflow</a> (Now forked and published to NPM as fossflow) library, it runs entirely in your browser with offline support.</p><h2>üê≥ Quick Deploy with Docker</h2><pre><code># Using Docker Compose (recommended - includes persistent storage)\ndocker compose up\n\n# Or run directly from Docker Hub with persistent storage\ndocker run -p 80:80 -v $(pwd)/diagrams:/data/diagrams stnsmith/fossflow:latest\n</code></pre><p>Server storage is enabled by default in Docker. Your diagrams will be saved to  on the host.</p><p>To disable server storage, set <code>ENABLE_SERVER_STORAGE=false</code>:</p><pre><code>docker run -p 80:80 -e ENABLE_SERVER_STORAGE=false stnsmith/fossflow:latest\n</code></pre><h2>Quick Start (Local Development)</h2><pre><code># Clone the repository\ngit clone https://github.com/stan-smith/FossFLOW\ncd FossFLOW\n\n# Install dependencies\nnpm install\n\n# Build the library (required first time)\nnpm run build:lib\n\n# Start development server\nnpm run dev\n</code></pre><p>This is a monorepo containing two packages:</p><ul><li> - React component library for drawing network diagrams (built with Webpack)</li><li> - Progressive Web App which wraps the lib and presents it (built with RSBuild)</li></ul><pre><code># Development\nnpm run dev          # Start app development server\nnpm run dev:lib      # Watch mode for library development\n\n# Building\nnpm run build        # Build both library and app\nnpm run build:lib    # Build library only\nnpm run build:app    # Build app only\n\n# Testing &amp; Linting\nnpm test             # Run unit tests\nnpm run lint         # Check for linting errors\n\n# E2E Tests (Selenium)\ncd e2e-tests\n./run-tests.sh       # Run end-to-end tests (requires Docker &amp; Python)\n\n# Publishing\nnpm run publish:lib  # Publish library to npm\n</code></pre><ol><li><ul><li>Press the \"+\" button on the top right menu, the library of components will appear on the left</li><li>Drag and drop components from the library onto the canvas</li><li>Or right-click on the grid and select \"Add node\"</li></ul></li><li><ul><li>Select the Connector tool (press 'C' or click connector icon)</li><li> (default): Click first node, then click second node</li><li> (optional): Click and drag from first to second node</li><li>Switch modes in Settings ‚Üí Connectors tab</li></ul></li><li><ul><li> - Saves to browser session</li><li> - Download as JSON file</li><li> - Load from JSON file</li></ul></li></ol><ul><li>: Temporary saves cleared when browser closes</li><li>: Permanent storage as JSON files</li><li>: Automatically saves changes every 5 seconds to session</li></ul>",
      "contentLength": 2948,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "PowerShell/PowerShell",
      "url": "https://github.com/PowerShell/PowerShell",
      "date": 1771729583,
      "author": "",
      "guid": 47268,
      "unread": true,
      "content": "<p>PowerShell for every system!</p><p>Welcome to the PowerShell GitHub Community! <a href=\"https://learn.microsoft.com/powershell/scripting/overview\">PowerShell</a> is a cross-platform (Windows, Linux, and macOS) automation and configuration tool/framework that works well with your existing tools and is optimized for dealing with structured data (e.g. JSON, CSV, XML, etc.), REST APIs, and object models. It includes a command-line shell, an associated scripting language, and a framework for processing cmdlets.</p><h2>Windows PowerShell vs. PowerShell 7+</h2><p>Although this repository started as a fork of the Windows PowerShell codebase, changes made in this repository are not ported back to Windows PowerShell 5.1. This also means that <a href=\"https://github.com/PowerShell/PowerShell/issues\">issues tracked here</a> are only for PowerShell 7.x and higher. Windows PowerShell specific issues should be reported with the <a href=\"https://support.microsoft.com/windows/send-feedback-to-microsoft-with-the-feedback-hub-app-f59187f8-8739-22d6-ba93-f66612949332\">Feedback Hub app</a>, by choosing \"Apps &gt; PowerShell\" in the category.</p><p>If you are new to PowerShell and want to learn more, we recommend reviewing the <a href=\"https://learn.microsoft.com/powershell/scripting/learn/more-powershell-learning\">getting started</a> documentation.</p><p>PowerShell is supported on Windows, macOS, and a variety of Linux platforms. For more information, see <a href=\"https://learn.microsoft.com/powershell/scripting/install/installing-powershell\">Installing PowerShell</a>.</p><p>For best results when upgrading, you should use the same install method you used when you first installed PowerShell. The update method is different for each platform and install method.</p><p><a href=\"https://aka.ms/PSPublicDashboard\">Dashboard</a> with visualizations for community contributions and project status using PowerShell, Azure, and PowerBI.</p><p>For more information on how and why we built this dashboard, check out this <a href=\"https://devblogs.microsoft.com/powershell/powershell-open-source-community-dashboard/\">blog post</a>.</p><p><a href=\"https://docs.github.com/discussions/quickstart\">GitHub Discussions</a> is a feature to enable free and open discussions within the community for topics that are not related to code, unlike issues.</p><p>This is an experiment we are trying in our repositories, to see if it helps move discussions out of issues so that issues remain actionable by the team or members of the community. There should be no expectation that PowerShell team members are regular participants in these discussions. Individual PowerShell team members may choose to participate in discussions, but the expectation is that community members help drive discussions so that team members can focus on issues.</p><p>Want to chat with other members of the PowerShell community?</p><p>There are dozens of topic-specific channels on our community-driven PowerShell Virtual User Group, which you can join on:</p><h2>Developing and Contributing</h2><p>Want to contribute to PowerShell? Please start with the <a href=\"https://raw.githubusercontent.com/PowerShell/PowerShell/master/.github/CONTRIBUTING.md\">Contribution Guide</a> to learn how to develop and contribute.</p><p>If you are developing .NET Core C# applications targeting PowerShell Core, <a href=\"https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/FAQ.md#where-do-i-get-the-powershell-core-sdk-package\">check out our FAQ</a> to learn more about the PowerShell SDK NuGet package.</p><p>Also, make sure to check out our <a href=\"https://github.com/powershell/powershell-rfc\">PowerShell-RFC repository</a> for request-for-comments (RFC) documents to submit and give comments on proposed and future designs.</p><p>If you have any problems building PowerShell, please start by consulting the developer <a href=\"https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/FAQ.md\">FAQ</a>.</p><h2>Downloading the Source Code</h2><p>You can clone the repository:</p><pre><code>git clone https://github.com/PowerShell/PowerShell.git\n</code></pre><blockquote><p>[!Important] The PowerShell container images are now <a href=\"https://github.com/PowerShell/Announcements/issues/75\">maintained by the .NET team</a>. The containers at <code>mcr.microsoft.com/powershell</code> are currently not maintained.</p></blockquote><p>License: By requesting and using the Container OS Image for Windows containers, you acknowledge, understand, and consent to the Supplemental License Terms available on <a href=\"https://mcr.microsoft.com/en-us/product/powershell/tags\">Microsoft Artifact Registry</a>.</p><p>Please visit our <a href=\"https://learn.microsoft.com/powershell/module/microsoft.powershell.core/about/about_telemetry\">about_Telemetry</a> topic to read details about telemetry gathered by PowerShell.</p>",
      "contentLength": 3353,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "databricks-solutions/ai-dev-kit",
      "url": "https://github.com/databricks-solutions/ai-dev-kit",
      "date": 1771642723,
      "author": "",
      "guid": 47065,
      "unread": true,
      "content": "<p>Databricks Toolkit for Coding Agents provided by Field Engineering</p>",
      "contentLength": 66,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "eslint/eslint",
      "url": "https://github.com/eslint/eslint",
      "date": 1771642723,
      "author": "",
      "guid": 47066,
      "unread": true,
      "content": "<p>Find and fix problems in your JavaScript code.</p><p>ESLint is a tool for identifying and reporting on patterns found in ECMAScript/JavaScript code. In many ways, it is similar to JSLint and JSHint with a few exceptions:</p><ul><li>ESLint uses <a href=\"https://github.com/eslint/js/tree/main/packages/espree\">Espree</a> for JavaScript parsing.</li><li>ESLint uses an AST to evaluate patterns in code.</li><li>ESLint is completely pluggable, every single rule is a plugin and you can add more at runtime.</li></ul><p>Prerequisites: <a href=\"https://nodejs.org/\">Node.js</a> (, , or ) built with SSL support. (If you are using an official Node.js distribution, SSL is always built in.)</p><p>You can install and configure ESLint using this command:</p><pre><code>npm init @eslint/config@latest\n</code></pre><p>After that, you can run ESLint on any file or directory like this:</p><p>To use ESLint with pnpm, we recommend setting up a  file with at least the following settings:</p><pre><code>auto-install-peers=true\nnode-linker=hoisted\n</code></pre><p>This ensures that pnpm installs dependencies in a way that is more compatible with npm and is less likely to produce errors.</p><p>You can configure rules in your  files as in this example:</p><pre><code>import { defineConfig } from \"eslint/config\";\n\nexport default defineConfig([\n\t{\n\t\tfiles: [\"**/*.js\", \"**/*.cjs\", \"**/*.mjs\"],\n\t\trules: {\n\t\t\t\"prefer-const\": \"warn\",\n\t\t\t\"no-constant-binary-expression\": \"error\",\n\t\t},\n\t},\n]);\n</code></pre><p>The names  and <code>\"no-constant-binary-expression\"</code> are the names of <a href=\"https://eslint.org/docs/rules\">rules</a> in ESLint. The first value is the error level of the rule and can be one of these values:</p><ul><li> or  - turn the rule off</li><li> or  - turn the rule on as a warning (doesn't affect exit code)</li><li> or  - turn the rule on as an error (exit code will be 1)</li></ul><p>The three error levels allow you fine-grained control over how ESLint applies rules (for more configuration options and details, see the <a href=\"https://eslint.org/docs/latest/use/configure\">configuration docs</a>).</p><p>The ESLint team provides ongoing support for the current version and six months of limited support for the previous version. Limited support includes critical bug fixes, security issues, and compatibility issues only.</p><p>ESLint offers commercial support for both current and previous versions through our partners, <a href=\"https://tidelift.com/funding/github/npm/eslint\">Tidelift</a> and <a href=\"https://www.herodevs.com/support/eslint-nes?utm_source=ESLintWebsite&amp;utm_medium=ESLintWebsite&amp;utm_campaign=ESLintNES&amp;utm_id=ESLintNES\">HeroDevs</a>.</p><p>Before filing an issue, please be sure to read the guidelines for what you're reporting:</p><h2>Frequently Asked Questions</h2><p>Yes, ESLint natively supports parsing JSX syntax (this must be enabled in <a href=\"https://eslint.org/docs/latest/use/configure\">configuration</a>). Please note that supporting JSX syntax  the same as supporting React. React applies specific semantics to JSX syntax that ESLint doesn't recognize. We recommend using <a href=\"https://www.npmjs.com/package/eslint-plugin-react\">eslint-plugin-react</a> if you are using React and want React semantics.</p><h3>Does Prettier replace ESLint?</h3><p>No, ESLint and Prettier have different jobs: ESLint is a linter (looking for problematic patterns) and Prettier is a code formatter. Using both tools is common, refer to <a href=\"https://prettier.io/docs/en/install#eslint-and-other-linters\">Prettier's documentation</a> to learn how to configure them to work well with each other.</p><h3>What ECMAScript versions does ESLint support?</h3><p>ESLint has full support for ECMAScript 3, 5, and every year from 2015 up until the most recent stage 4 specification (the default). You can set your desired ECMAScript syntax and other settings (like global variables) through <a href=\"https://eslint.org/docs/latest/use/configure\">configuration</a>.</p><h3>What about experimental features?</h3><p>ESLint's parser only officially supports the latest final ECMAScript standard. We will make changes to core rules in order to avoid crashes on stage 3 ECMAScript syntax proposals (as long as they are implemented using the correct experimental ESTree syntax). We may make changes to core rules to better work with language extensions (such as JSX, Flow, and TypeScript) on a case-by-case basis.</p><p>In other cases (including if rules need to warn on more or fewer cases due to new syntax, rather than just not crashing), we recommend you use other parsers and/or rule plugins. If you are using Babel, you can use <a href=\"https://www.npmjs.com/package/@babel/eslint-parser\">@babel/eslint-parser</a> and <a href=\"https://www.npmjs.com/package/@babel/eslint-plugin\">@babel/eslint-plugin</a> to use any option available in Babel.</p><p>Once a language feature has been adopted into the ECMAScript standard (stage 4 according to the <a href=\"https://tc39.github.io/process-document/\">TC39 process</a>), we will accept issues and pull requests related to the new feature, subject to our <a href=\"https://eslint.org/docs/latest/contribute\">contributing guidelines</a>. Until then, please use the appropriate parser and plugin(s) for your experimental feature.</p><h3>Which Node.js versions does ESLint support?</h3><p>ESLint updates the supported Node.js versions with each major release of ESLint. At that time, ESLint's supported Node.js versions are updated to be:</p><ol><li>The most recent maintenance release of Node.js</li><li>The lowest minor version of the Node.js LTS release that includes the features the ESLint team wants to use.</li><li>The Node.js Current release</li></ol><p>ESLint is also expected to work with Node.js versions released after the Node.js Current release.</p><p>Refer to the <a href=\"https://eslint.org/docs/latest/use/getting-started#prerequisites\">Quick Start Guide</a> for the officially supported Node.js versions for a given ESLint release.</p><h3>Why doesn't ESLint lock dependency versions?</h3><p>Lock files like  are helpful for deployed applications. They ensure that dependencies are consistent between environments and across deployments.</p><p>Packages like  that get published to the npm registry do not include lock files.  as a user will respect version constraints in ESLint's . ESLint and its dependencies will be included in the user's lock file if one exists, but ESLint's own lock file would not be used.</p><p>We intentionally don't lock dependency versions so that we have the latest compatible dependency versions in development and CI that our users get when installing ESLint in a project.</p><p>We have scheduled releases every two weeks on Friday or Saturday. You can follow a <a href=\"https://github.com/eslint/eslint/issues?q=is%3Aopen+is%3Aissue+label%3Arelease\">release issue</a> for updates about the scheduling of any particular release.</p><p>ESLint takes security seriously. We work hard to ensure that ESLint is safe for everyone and that security issues are addressed quickly and responsibly. Read the full <a href=\"https://github.com/eslint/.github/raw/master/SECURITY.md\">security policy</a>.</p><h2>Semantic Versioning Policy</h2><p>ESLint follows <a href=\"https://semver.org\">semantic versioning</a>. However, due to the nature of ESLint as a code quality tool, it's not always clear when a minor or major version bump occurs. To help clarify this for everyone, we've defined the following semantic versioning policy for ESLint:</p><ul><li>Patch release (intended to not break your lint build) \n  <ul><li>A bug fix in a rule that results in ESLint reporting fewer linting errors.</li><li>A bug fix to the CLI or core (including formatters).</li><li>Improvements to documentation.</li><li>Non-user-facing changes such as refactoring code, adding, deleting, or modifying tests, and increasing test coverage.</li><li>Re-releasing after a failed release (i.e., publishing a release that doesn't work for anyone).</li></ul></li><li>Minor release (might break your lint build) \n  <ul><li>A bug fix in a rule that results in ESLint reporting more linting errors.</li><li>A new option to an existing rule that does not result in ESLint reporting more linting errors by default.</li><li>A new addition to an existing rule to support a newly-added language feature (within the last 12 months) that will result in ESLint reporting more linting errors by default.</li><li>An existing rule is deprecated.</li><li>A new CLI capability is created.</li><li>New capabilities to the public API are added (new classes, new methods, new arguments to existing methods, etc.).</li><li>A new formatter is created.</li><li> is updated and will result in strictly fewer linting errors (e.g., rule removals).</li></ul></li><li>Major release (likely to break your lint build) \n  <ul><li> is updated and may result in new linting errors (e.g., rule additions, most rule option updates).</li><li>A new option to an existing rule that results in ESLint reporting more linting errors by default.</li><li>An existing formatter is removed.</li><li>Part of the public API is removed or changed in an incompatible way. The public API includes: \n    <ul><li>Rule, formatter, parser, plugin APIs</li></ul></li></ul></li></ul><p>According to our policy, any minor update may report more linting errors than the previous release (ex: from a bug fix). As such, we recommend using the tilde () in  e.g.  to guarantee the results of your builds.</p><p>Since ESLint is a CommonJS package, there are restrictions on which ESM-only packages can be used as dependencies.</p><p>Packages that are controlled by the ESLint team and have no external dependencies can be safely loaded synchronously using <a href=\"https://nodejs.org/api/modules.html#loading-ecmascript-modules-using-require\"></a> and therefore used in any contexts.</p><p>For external packages, we don't use  because a package could add a top-level  and thus break ESLint. We can use an external ESM-only package only in case it is needed only in asynchronous code, in which case it can be loaded using dynamic .</p><p>Copyright OpenJS Foundation and other contributors, &lt;www.openjsf.org&gt;</p><p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p><p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p><p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p><p>These folks keep the project moving and are resources for help.</p><h3>Technical Steering Committee (TSC)</h3><p>The people who manage releases, review feature requests, and meet regularly to ensure ESLint is properly maintained.</p><p>The people who review and implement new features.</p><p>The people who review and fix bugs and help triage issues.</p><p>Team members who focus specifically on eslint.org</p><p>The following companies, organizations, and individuals support ESLint's ongoing maintenance and development. <a href=\"https://eslint.org/donate\">Become a Sponsor</a> to get your logo on our READMEs and <a href=\"https://eslint.org/sponsors\">website</a>.</p> Technology sponsors allow us to use their products and services for free as part of a contribution to the open source ecosystem and our work. \n",
      "contentLength": 9918,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "PostHog/posthog",
      "url": "https://github.com/PostHog/posthog",
      "date": 1771642723,
      "author": "",
      "guid": 47067,
      "unread": true,
      "content": "<p>ü¶î PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack.</p><h2>PostHog is an all-in-one, open source platform for building successful products</h2><p><a href=\"https://posthog.com/\">PostHog</a> provides every tool you need to build a successful product including:</p><ul><li><a href=\"https://posthog.com/product-analytics\">Product Analytics</a>: Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL.</li><li><a href=\"https://posthog.com/web-analytics\">Web Analytics</a>: Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue.</li><li><a href=\"https://posthog.com/session-replay\">Session Replays</a>: Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior.</li><li><a href=\"https://posthog.com/feature-flags\">Feature Flags</a>: Safely roll out features to select users or cohorts with feature flags.</li><li><a href=\"https://posthog.com/experiments\">Experiments</a>: Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too.</li><li><a href=\"https://posthog.com/error-tracking\">Error Tracking</a>: Track errors, get alerts, and resolve issues to improve your product.</li><li><a href=\"https://posthog.com/surveys\">Surveys</a>: Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder.</li><li><a href=\"https://posthog.com/data-warehouse\">Data warehouse</a>: Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data.</li><li><a href=\"https://posthog.com/cdp\">Data pipelines</a>: Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse.</li><li><a href=\"https://posthog.com/docs/llm-analytics\">LLM analytics</a>: Capture traces, generations, latency, and cost for your LLM-powered app.</li><li><a href=\"https://posthog.com/docs/workflows\">Workflows</a>: Create workflows that automate actions or send messages to your users.</li></ul><h2>Getting started with PostHog</h2><h3>PostHog Cloud (Recommended)</h3><p>The fastest and most reliable way to get started with PostHog is signing up for free to&nbsp;<a href=\"https://us.posthog.com/signup\">PostHog Cloud</a> or <a href=\"https://eu.posthog.com/signup\">PostHog Cloud EU</a>. Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage.</p><h3>Self-hosting the open-source hobby deploy (Advanced)</h3><p>If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker (recommended 4GB memory):</p><pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)\"\n</code></pre><p>We have SDKs and libraries for popular languages and frameworks like:</p><h2>Learning more about PostHog</h2><p>We &lt;3 contributions big and small:</p><p>Need ? Check out our <a href=\"https://github.com/PostHog/posthog-foss\">posthog-foss</a> repository, which is purged of all proprietary code and features.</p><p>The pricing for our paid plan is completely transparent and available on <a href=\"https://posthog.com/pricing\">our pricing page</a>.</p><img src=\"https://res.cloudinary.com/dmukukwp6/image/upload/v1/posthog.com/src/components/Home/images/mission-control-hog\" alt=\"Hedgehog working on a Mission Control Center\" width=\"350px\"><p>Hey! If you're reading this, you've proven yourself as a dedicated README reader.</p>",
      "contentLength": 2826,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "roboflow/trackers",
      "url": "https://github.com/roboflow/trackers",
      "date": 1771642723,
      "author": "",
      "guid": 47068,
      "unread": true,
      "content": "<p>Trackers gives you clean, modular re-implementations of leading multi-object tracking algorithms released under the permissive Apache 2.0 license. You combine them with any detection model you already use.</p><p>Point at a video, webcam, RTSP stream, or image directory. Get tracked output.</p><pre><code>trackers track \\\n    --source video.mp4 \\\n    --output output.mp4 \\\n    --model rfdetr-medium \\\n    --tracker bytetrack \\\n    --show-labels \\\n    --show-trajectories\n</code></pre><p>Plug trackers into your existing detection pipeline. Works with any detector.</p><pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\nfrom trackers import ByteTrackTracker\n\nmodel = get_model(model_id=\"rfdetr-medium\")\ntracker = ByteTrackTracker()\n\nlabel_annotator = sv.LabelAnnotator()\ntrajectory_annotator = sv.TrajectoryAnnotator()\n\ncap = cv2.VideoCapture(\"video.mp4\")\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    tracked = tracker.update(detections)\n\n    frame = label_annotator.annotate(frame, tracked)\n    frame = trajectory_annotator.annotate(frame, tracked)\n</code></pre><p>Benchmark your tracker against ground truth with standard MOT metrics.</p><pre><code>trackers eval \\\n    --gt-dir data/gt \\\n    --tracker-dir data/trackers \\\n    --metrics CLEAR HOTA Identity\n</code></pre><pre><code>Sequence                        MOTA    HOTA    IDF1  IDSW\n----------------------------------------------------------\nMOT17-02-FRCNN                75.600  62.300  72.100    42\nMOT17-04-FRCNN                78.200  65.100  74.800    31\n----------------------------------------------------------\nCOMBINED                      75.033  62.400  72.033    73\n</code></pre><p>Clean, modular implementations of leading trackers. See the <a href=\"https://trackers.roboflow.com/develop/trackers/comparison/\">tracker comparison</a> for detailed benchmarks.</p>",
      "contentLength": 1787,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "blackboardsh/electrobun",
      "url": "https://github.com/blackboardsh/electrobun",
      "date": 1771642723,
      "author": "",
      "guid": 47069,
      "unread": true,
      "content": "<p>Build ultra fast, tiny, and cross-platform desktop apps with Typescript.</p><div align=\"center\">\n  Get started with a template \n </div><p>Electrobun aims to be a complete  for building, updating, and shipping ultra fast, tiny, and cross-platform desktop applications written in Typescript. Under the hood it uses <a href=\"https://bun.sh\">bun</a> to execute the main process and to bundle webview typescript, and has native bindings written in <a href=\"https://ziglang.org/\">zig</a>.</p><ul><li>Write typescript for the main process and webviews without having to think about it.</li><li>Isolation between main and webview processes with fast, typed, easy to implement RPC between them.</li><li>Small self-extracting app bundles ~12MB (when using system webview, most of this is the bun runtime)</li><li>Even smaller app updates as small as 14KB (using bsdiff it only downloads tiny patches between versions)</li><li>Provide everything you need in one tightly integrated workflow to start writing code in 5 minutes and distribute in 10.</li></ul><h2>Apps Built with Electrobun</h2><ul><li><a href=\"https://github.com/blackboardsh/audio-tts\">Audio TTS</a> - desktop text-to-speech app using Qwen3-TTS for voice design, cloning, and generation</li><li><a href=\"https://blackboard.sh/colab/\">Co(lab)</a> - a hybrid web browser + code editor for deep work</li></ul><ul><li>Create and participate in Github issues and discussions</li><li>Let me know what you're building with Electrobun</li></ul><p>Building apps with Electrobun is as easy as updating your package.json dependencies with  or try one of our templates via .</p><p><strong>This section is for building Electrobun from source locally in order to contribute fixes to it.</strong></p><ul><li>cmake (install via homebrew: )</li></ul><ul><li>Visual Studio Build Tools or Visual Studio with C++ development tools</li></ul><ul><li>webkit2gtk and GTK development packages</li></ul><p>On Ubuntu/Debian based distros: <code>sudo apt install build-essential cmake pkg-config libgtk-3-dev libwebkit2gtk-4.1-dev libayatana-appindicator3-dev librsvg2-dev</code></p><pre><code>git clone --recurse-submodules https://github.com/blackboardsh/electrobun.git\ncd electrobun/package\nbun install\nbun dev:clean\n</code></pre><pre><code># All commands are run from the /package directory\ncd electrobun/package\n\n# After making changes to source code\nbun dev\n\n# If you only changed kitchen sink code (not electrobun source)\nbun dev:rerun\n\n# If you need a completely fresh start\nbun dev:clean\n</code></pre><p>All commands are run from the  directory:</p><ul><li> - Build and run kitchen sink in canary mode</li><li> - Build electrobun in development mode</li><li> - Build electrobun in release mode</li></ul><p> Use <code>lldb &lt;path-to-bundle&gt;/Contents/MacOS/launcher</code> and then  to debug release builds</p><table><tbody><tr><td>Other Linux distros (gtk3, webkit2gtk-4.1)</td></tr></tbody></table>",
      "contentLength": 2351,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Effect-TS/effect-smol",
      "url": "https://github.com/Effect-TS/effect-smol",
      "date": 1771642723,
      "author": "",
      "guid": 47070,
      "unread": true,
      "content": "<p>Core libraries and experimental work for Effect v4</p>",
      "contentLength": 50,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "google-research/timesfm",
      "url": "https://github.com/google-research/timesfm",
      "date": 1771642723,
      "author": "",
      "guid": 47071,
      "unread": true,
      "content": "<p>TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.</p><p>TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.</p><p>This open version is not an officially supported Google product.</p><p> TimesFM 2.5</p><ul><li>1.0 and 2.0: relevant code archived in the sub directory . You can <code>pip install timesfm==1.3.0</code> to install an older version of this package to load them.</li></ul><p>Added back the covariate support through XReg for TimesFM 2.5.</p><p>Comparing to TimesFM 2.0, this new 2.5 model:</p><ul><li>uses 200M parameters, down from 500M.</li><li>supports up to 16k context length, up from 2048.</li><li>supports continuous quantile forecast up to 1k horizon via an optional 30M quantile head.</li><li>gets rid of the  indicator.</li><li>has a couple of new forecasting flags.</li></ul><p>Along with the model upgrade we have also upgraded the inference API. This repo will be under construction over the next few weeks to</p><ol><li>add support for an upcoming Flax version of the model (faster inference).</li><li>add back covariate support.</li><li>populate more docstrings, docs and notebook.</li></ol><ol><li><pre><code>git clone https://github.com/google-research/timesfm.git\ncd timesfm\n</code></pre></li><li><p>Create a virtual environment and install dependencies using :</p><pre><code># Create a virtual environment\nuv venv\n\n# Activate the environment\nsource .venv/bin/activate\n\n# Install the package in editable mode with torch\nuv pip install -e .[torch]\n# Or with flax\nuv pip install -e .[flax]\n# Or XReg is needed\nuv pip install -e .[xreg]\n</code></pre></li><li><p>[Optional] Install your preferred  /  backend based on your OS and accelerators (CPU, GPU, TPU or Apple Silicon).:</p></li></ol><pre><code>import torch\nimport numpy as np\nimport timesfm\n\ntorch.set_float32_matmul_precision(\"high\")\n\nmodel = timesfm.TimesFM_2p5_200M_torch.from_pretrained(\"google/timesfm-2.5-200m-pytorch\")\n\nmodel.compile(\n    timesfm.ForecastConfig(\n        max_context=1024,\n        max_horizon=256,\n        normalize_inputs=True,\n        use_continuous_quantile_head=True,\n        force_flip_invariance=True,\n        infer_is_positive=True,\n        fix_quantile_crossing=True,\n    )\n)\npoint_forecast, quantile_forecast = model.forecast(\n    horizon=12,\n    inputs=[\n        np.linspace(0, 1, 100),\n        np.sin(np.linspace(0, 20, 67)),\n    ],  # Two dummy inputs\n)\npoint_forecast.shape  # (2, 12)\nquantile_forecast.shape  # (2, 12, 10): mean, then 10th to 90th quantiles.\n</code></pre>",
      "contentLength": 2385,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "huggingface/skills",
      "url": "https://github.com/huggingface/skills",
      "date": 1771642723,
      "author": "",
      "guid": 47072,
      "unread": true,
      "content": "<p>Hugging Face Skills are definitions for AI/ML tasks like dataset creation, model training, and evaluation. They are interoperable with all major coding agent tools like OpenAI Codex, Anthropic's Claude Code, Google DeepMind's Gemini CLI, and Cursor.</p><p>The Skills in this repository follow the standardized format <a href=\"https://agentskills.io/home\">Agent Skill</a> format.</p><p>In practice, skills are self-contained folders that package instructions, scripts, and resources together for an AI agent to use on a specific use case. Each folder includes a  file with YAML frontmatter (name and description) followed by the guidance your coding agent follows while the skill is active.</p><blockquote><p>[!NOTE] 'Skills' is actually an Anthropic term used within Claude AI and Claude Code and not adopted by other agent tools, but we love it! OpenAI Codex uses an  file to define the instructions for your coding agent. Google Gemini uses 'extensions' to define the instructions for your coding agent in a  file. <strong>This repo is compatible with all of them, and more!</strong></p></blockquote><blockquote><p>[!TIP] If your agent doesn't support skills, you can use <a href=\"https://raw.githubusercontent.com/huggingface/skills/main/agents/AGENTS.md\"></a> directly as a fallback.</p></blockquote><p>Hugging Face skills are compatible with Claude Code, Codex, Gemini CLI, and Cursor.</p><ol><li>Register the repository as a plugin marketplace:</li></ol><pre><code>/plugin marketplace add huggingface/skills\n</code></pre><pre><code>/plugin install &lt;skill-name&gt;@huggingface/skills\n</code></pre><pre><code>/plugin install hugging-face-cli@huggingface/skills\n</code></pre><ol><li>Codex will identify the skills via the  file. You can verify the instructions are loaded with:</li></ol><pre><code>codex --ask-for-approval never \"Summarize the current instructions.\"\n</code></pre><ol><li><p>This repo includes  to integrate with the Gemini CLI.</p></li></ol><pre><code>gemini extensions install . --consent\n</code></pre><pre><code>gemini extensions install https://github.com/huggingface/skills.git --consent\n</code></pre><p>This repository includes Cursor plugin manifests:</p><ul><li><code>.cursor-plugin/plugin.json</code></li><li> (configured with the Hugging Face MCP server URL)</li></ul><p>Install from repository URL (or local checkout) via the Cursor plugin flow.</p><p>For contributors, regenerate manifests with:</p><p>This repository contains a few skills to get you started. You can also contribute your own skills to the repository.</p><table><thead><tr></tr></thead><tbody><tr><td>Execute Hugging Face Hub operations using the hf CLI. Download models/datasets, upload files, manage repos, and run cloud compute jobs.</td></tr><tr><td>Create and manage datasets on Hugging Face Hub. Supports initializing repos, defining configs/system prompts, streaming row updates, and SQL-based dataset querying/transformation.</td></tr><tr><td>Add and manage evaluation results in Hugging Face model cards. Supports extracting eval tables from README content, importing scores from Artificial Analysis API, and running custom evaluations with vLLM/lighteval.</td></tr><tr><td>Run compute jobs on Hugging Face infrastructure. Execute Python scripts, manage scheduled jobs, and monitor job status.</td></tr><tr><td><code>hugging-face-model-trainer</code></td><td>Train or fine-tune language models using TRL on Hugging Face Jobs infrastructure. Covers SFT, DPO, GRPO and reward modeling training methods, plus GGUF conversion for local deployment. Includes hardware selection, cost estimation, Trackio monitoring, and Hub persistence.</td></tr><tr><td><code>hugging-face-paper-publisher</code></td><td>Publish and manage research papers on Hugging Face Hub. Supports creating paper pages, linking papers to models/datasets, claiming authorship, and generating professional markdown-based research articles.</td></tr><tr><td><code>hugging-face-tool-builder</code></td><td>Build reusable scripts for Hugging Face API operations. Useful for chaining API calls or automating repeated tasks.</td></tr><tr><td>Track and visualize ML training experiments with Trackio. Log metrics via Python API and retrieve them via CLI. Supports real-time dashboards synced to HF Spaces.</td></tr></tbody></table><h3>Using skills in your coding agent</h3><p>Once a skill is installed, mention it directly while giving your coding agent instructions:</p><ul><li>\"Use the HF LLM trainer skill to estimate the GPU memory needed for a 70B model run.\"</li><li>\"Use the HF model evaluation skill to launch  on the latest checkpoint.\"</li><li>\"Use the HF dataset creator skill to draft new few-shot classification templates.\"</li><li>\"Use the HF paper publisher skill to index my arXiv paper and link it to my model.\"</li></ul><p>Your coding agent automatically loads the corresponding  instructions and helper scripts while it completes the task.</p><h3>Contribute or customize a skill</h3><ol><li>Copy one of the existing skill folders (for example, ) and rename it.</li><li>Update the new folder's  frontmatter: <pre><code>---\nname: my-skill-name\ndescription: Describe what the skill does and when to use it\n---\n\n# Skill Title\nGuidance + examples + guardrails\n</code></pre></li><li>Add or edit supporting scripts, templates, and documents referenced by your instructions.</li><li>Add an entry to <code>.claude-plugin/marketplace.json</code> with a concise, human-readable description.</li><li>Run:  to regenerate and validate all generated metadata.</li><li>Reinstall or reload the skill bundle in your coding agent so the updated folder is available.</li></ol><p>The <code>.claude-plugin/marketplace.json</code> file lists skills with human-readable descriptions for the plugin marketplace. The CI validates that skill names and paths match between  files and , but descriptions are maintained separately:  descriptions guide when Claude activates the skill, while marketplace descriptions are written for humans browsing available skills.</p><ul><li>Review Hugging Face documentation for the specific libraries or workflows you reference inside each skill.</li></ul>",
      "contentLength": 5154,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "vxcontrol/pentagi",
      "url": "https://github.com/vxcontrol/pentagi",
      "date": 1771642723,
      "author": "",
      "guid": 47073,
      "unread": true,
      "content": "<p>‚ú® Fully autonomous AI Agents system capable of performing complex penetration testing tasks</p><div align=\"center\">enetration testing \n rtificial \n eneral \n ntelligence \n</div><div align=\"center\"><blockquote><p>üöÄ  Connect with security researchers, AI enthusiasts, and fellow ethical hackers. Get support, share insights, and stay updated with the latest PentAGI developments.</p></blockquote></div><p>PentAGI is an innovative tool for automated security testing that leverages cutting-edge artificial intelligence technologies. The project is designed for information security professionals, researchers, and enthusiasts who need a powerful and flexible solution for conducting penetration tests.</p><p>You can watch the video : <a href=\"https://youtu.be/R70x5Ddzs1o\"><img src=\"https://github.com/user-attachments/assets/0828dc3e-15f1-4a1d-858e-9696a146e478\" alt=\"PentAGI Overview Video\"></a></p><ul><li>üõ°Ô∏è Secure &amp; Isolated. All operations are performed in a sandboxed Docker environment with complete isolation.</li><li>ü§ñ Fully Autonomous. AI-powered agent that automatically determines and executes penetration testing steps.</li><li>üî¨ Professional Pentesting Tools. Built-in suite of 20+ professional security tools including nmap, metasploit, sqlmap, and more.</li><li>üß† Smart Memory System. Long-term storage of research results and successful approaches for future use.</li><li>üìö Knowledge Graph Integration. Graphiti-powered knowledge graph using Neo4j for semantic relationship tracking and advanced context understanding.</li><li>üîç Web Intelligence. Built-in browser via <a href=\"https://hub.docker.com/r/vxcontrol/scraper\">scraper</a> for gathering latest information from web sources.</li><li>üë• Team of Specialists. Delegation system with specialized AI agents for research, development, and infrastructure tasks.</li><li>üìä Comprehensive Monitoring. Detailed logging and integration with Grafana/Prometheus for real-time system observation.</li><li>üìù Detailed Reporting. Generation of thorough vulnerability reports with exploitation guides.</li><li>üì¶ Smart Container Management. Automatic Docker image selection based on specific task requirements.</li><li>üì± Modern Interface. Clean and intuitive web UI for system management and monitoring.</li><li>üîå Comprehensive APIs. Full-featured REST and GraphQL APIs with Bearer token authentication for automation and integration.</li><li>üíæ Persistent Storage. All commands and outputs are stored in PostgreSQL with <a href=\"https://hub.docker.com/r/vxcontrol/pgvector\">pgvector</a> extension.</li><li>üéØ Scalable Architecture. Microservices-based design supporting horizontal scaling.</li><li>üè† Self-Hosted Solution. Complete control over your deployment and data.</li><li>üîê API Token Authentication. Secure Bearer token system for programmatic access to REST and GraphQL APIs.</li><li>‚ö° Quick Deployment. Easy setup through <a href=\"https://docs.docker.com/compose/\">Docker Compose</a> with comprehensive environment configuration.</li></ul><pre><code>flowchart TB\n    classDef person fill:#08427B,stroke:#073B6F,color:#fff\n    classDef system fill:#1168BD,stroke:#0B4884,color:#fff\n    classDef external fill:#666666,stroke:#0B4884,color:#fff\n\n    pentester[\"üë§ Security Engineer\n    (User of the system)\"]\n\n    pentagi[\"‚ú® PentAGI\n    (Autonomous penetration testing system)\"]\n\n    target[\"üéØ target-system\n    (System under test)\"]\n    llm[\"üß† llm-provider\n    (OpenAI/Anthropic/Ollama/Bedrock/Gemini/Custom)\"]\n    search[\"üîç search-systems\n    (Google/DuckDuckGo/Tavily/Traversaal/Perplexity/Searxng)\"]\n    langfuse[\"üìä langfuse-ui\n    (LLM Observability Dashboard)\"]\n    grafana[\"üìà grafana\n    (System Monitoring Dashboard)\"]\n\n    pentester --&gt; |Uses HTTPS| pentagi\n    pentester --&gt; |Monitors AI HTTPS| langfuse\n    pentester --&gt; |Monitors System HTTPS| grafana\n    pentagi --&gt; |Tests Various protocols| target\n    pentagi --&gt; |Queries HTTPS| llm\n    pentagi --&gt; |Searches HTTPS| search\n    pentagi --&gt; |Reports HTTPS| langfuse\n    pentagi --&gt; |Reports HTTPS| grafana\n\n    class pentester person\n    class pentagi system\n    class target,llm,search,langfuse,grafana external\n\n    linkStyle default stroke:#ffffff,color:#ffffff\n</code></pre><p>The architecture of PentAGI is designed to be modular, scalable, and secure. Here are the key components:</p><ol><li><ul><li>Frontend UI: React-based web interface with TypeScript for type safety</li><li>Backend API: Go-based REST and GraphQL APIs with Bearer token authentication for programmatic access</li><li>Vector Store: PostgreSQL with pgvector for semantic search and memory storage</li><li>Task Queue: Async task processing system for reliable operation</li><li>AI Agent: Multi-agent system with specialized roles for efficient testing</li></ul></li><li><ul><li>Graphiti: Knowledge graph API for semantic relationship tracking and contextual understanding</li><li>Neo4j: Graph database for storing and querying relationships between entities, actions, and outcomes</li><li>Automatic capturing of agent responses and tool executions for building comprehensive knowledge base</li></ul></li><li><ul><li>OpenTelemetry: Unified observability data collection and correlation</li><li>Grafana: Real-time visualization and alerting dashboards</li><li>VictoriaMetrics: High-performance time-series metrics storage</li><li>Jaeger: End-to-end distributed tracing for debugging</li><li>Loki: Scalable log aggregation and analysis</li></ul></li><li><ul><li>Langfuse: Advanced LLM observability and performance analytics</li><li>ClickHouse: Column-oriented analytics data warehouse</li><li>Redis: High-speed caching and rate limiting</li><li>MinIO: S3-compatible object storage for artifacts</li></ul></li><li><ul><li>Web Scraper: Isolated browser environment for safe web interaction</li><li>Pentesting Tools: Comprehensive suite of 20+ professional security tools</li><li>Sandboxed Execution: All operations run in isolated containers</li></ul></li><li><ul><li>Long-term Memory: Persistent storage of knowledge and experiences</li><li>Working Memory: Active context and goals for current operations</li><li>Episodic Memory: Historical actions and success patterns</li><li>Knowledge Base: Structured domain expertise and tool capabilities</li><li>Context Management: Intelligently manages growing LLM context windows using chain summarization</li></ul></li></ol><p>The system uses Docker containers for isolation and easy deployment, with separate networks for core services, monitoring, and analytics to ensure proper security boundaries. Each component is designed to scale horizontally and can be configured for high availability in production environments.</p><ul><li>Docker and Docker Compose</li><li>Internet access for downloading images and updates</li></ul><h3>Using Installer (Recommended)</h3><p>PentAGI provides an interactive installer with a terminal-based UI for streamlined configuration and deployment. The installer guides you through system checks, LLM provider setup, search engine configuration, and security hardening.</p><p><strong>Quick Installation (Linux amd64):</strong></p><pre><code># Create installation directory\nmkdir -p pentagi &amp;&amp; cd pentagi\n\n# Download installer\nwget -O installer.zip https://pentagi.com/downloads/linux/amd64/installer-latest.zip\n\n# Extract\nunzip installer.zip\n\n# Run interactive installer\n./installer\n</code></pre><p><strong>Prerequisites &amp; Permissions:</strong></p><p>The installer requires appropriate privileges to interact with the Docker API for proper operation. By default, it uses the Docker socket () which requires either:</p><ul><li><p><strong>Option 1 (Recommended for production):</strong> Run the installer as root:</p></li><li><p><strong>Option 2 (Development environments):</strong> Grant your user access to the Docker socket by adding them to the  group:</p><pre><code># Add your user to the docker group\nsudo usermod -aG docker $USER\n\n# Log out and log back in, or activate the group immediately\nnewgrp docker\n\n# Verify Docker access (should run without sudo)\ndocker ps\n</code></pre><p>‚ö†Ô∏è  Adding a user to the  group grants root-equivalent privileges. Only do this for trusted users in controlled environments. For production deployments, consider using rootless Docker mode or running the installer with sudo.</p></li></ul><ol><li>: Verify Docker, network connectivity, and system requirements</li><li>: Create and configure  file with optimal defaults</li><li>: Set up LLM providers (OpenAI, Anthropic, Gemini, Bedrock, Ollama, Custom)</li><li>: Configure DuckDuckGo, Google, Tavily, Traversaal, Perplexity, Searxng</li><li>: Generate secure credentials and configure SSL certificates</li><li>: Start PentAGI with docker-compose</li></ol><p><strong>For Production &amp; Enhanced Security:</strong></p><p>For production deployments or security-sensitive environments, we  using a distributed two-node architecture where worker operations are isolated on a separate server. This prevents untrusted code execution and network access issues on your main system.</p><p>The two-node setup provides:</p><ul><li>: Worker containers run on dedicated hardware</li><li>: Separate network boundaries for penetration testing</li><li>: Docker-in-Docker with TLS authentication</li><li>: Dedicated port ranges for out-of-band techniques</li></ul><ol><li>Create a working directory or clone the repository:</li></ol><pre><code>mkdir pentagi &amp;&amp; cd pentagi\n</code></pre><ol start=\"2\"><li>Copy  to  or download it:</li></ol><pre><code>curl -o .env https://raw.githubusercontent.com/vxcontrol/pentagi/master/.env.example\n</code></pre><ol start=\"3\"><li>Touch examples files (<code>example.custom.provider.yml</code>, <code>example.ollama.provider.yml</code>) or download it:</li></ol><pre><code>curl -o example.custom.provider.yml https://raw.githubusercontent.com/vxcontrol/pentagi/master/examples/configs/custom-openai.provider.yml\ncurl -o example.ollama.provider.yml https://raw.githubusercontent.com/vxcontrol/pentagi/master/examples/configs/ollama-llama318b.provider.yml\n</code></pre><ol start=\"4\"><li>Fill in the required API keys in  file.</li></ol><pre><code># Required: At least one of these LLM providers\nOPEN_AI_KEY=your_openai_key\nANTHROPIC_API_KEY=your_anthropic_key\nGEMINI_API_KEY=your_gemini_key\n\n# Optional: AWS Bedrock provider (enterprise-grade models)\nBEDROCK_REGION=us-east-1\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key\n\n# Optional: Local LLM provider (zero-cost inference)\nOLLAMA_SERVER_URL=http://localhost:11434\nOLLAMA_SERVER_MODEL=your_model_name\n\n# Optional: Additional search capabilities\nDUCKDUCKGO_ENABLED=true\nGOOGLE_API_KEY=your_google_key\nGOOGLE_CX_KEY=your_google_cx\nTAVILY_API_KEY=your_tavily_key\nTRAVERSAAL_API_KEY=your_traversaal_key\nPERPLEXITY_API_KEY=your_perplexity_key\nPERPLEXITY_MODEL=sonar-pro\nPERPLEXITY_CONTEXT_SIZE=medium\n\n# Searxng meta search engine (aggregates results from multiple sources)\nSEARXNG_URL=http://your-searxng-instance:8080\nSEARXNG_CATEGORIES=general\nSEARXNG_LANGUAGE=\nSEARXNG_SAFESEARCH=0\nSEARXNG_TIME_RANGE=\n\n## Graphiti knowledge graph settings\nGRAPHITI_ENABLED=true\nGRAPHITI_TIMEOUT=30\nGRAPHITI_URL=http://graphiti:8000\nGRAPHITI_MODEL_NAME=gpt-5-mini\n\n# Neo4j settings (used by Graphiti stack)\nNEO4J_USER=neo4j\nNEO4J_DATABASE=neo4j\nNEO4J_PASSWORD=devpassword\nNEO4J_URI=bolt://neo4j:7687\n\n# Assistant configuration\nASSISTANT_USE_AGENTS=false         # Default value for agent usage when creating new assistants\n</code></pre><ol start=\"5\"><li>Change all security related environment variables in  file to improve security.</li></ol><ol start=\"6\"><li>Remove all inline comments from  file if you want to use it in VSCode or other IDEs as a envFile option:</li></ol><pre><code>perl -i -pe 's/\\s+#.*$//' .env\n</code></pre><pre><code>curl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose.yml\ndocker compose up -d\n</code></pre><p>Visit <a href=\"https://localhost:8443\">localhost:8443</a> to access PentAGI Web UI (default is  / )</p><blockquote><p>[!NOTE] If you caught an error about  or  or  you need to run  firstly to create these networks and after that run <code>docker-compose-langfuse.yml</code>, <code>docker-compose-graphiti.yml</code>, and <code>docker-compose-observability.yml</code> to use Langfuse, Graphiti, and Observability services.</p><p>You have to set at least one Language Model provider (OpenAI, Anthropic, Gemini, AWS Bedrock, or Ollama) to use PentAGI. AWS Bedrock provides enterprise-grade access to multiple foundation models from leading AI companies, while Ollama provides zero-cost local inference if you have sufficient computational resources. Additional API keys for search engines are optional but recommended for better results.</p><p> environment variables are experimental feature and will be changed in the future. Right now you can use them to specify custom LLM server URL and one model for all agent types.</p><p> is a global proxy URL for all LLM providers and external search systems. You can use it for isolation from external networks.</p><p>The  file runs the PentAGI service as root user because it needs access to docker.sock for container management. If you're using TCP/IP network connection to Docker instead of socket file, you can remove root privileges and use the default  user for better security.</p></blockquote><p>PentAGI allows you to configure default behavior for assistants:</p><table><thead><tr></tr></thead><tbody><tr><td>Controls the default value for agent usage when creating new assistants</td></tr></tbody></table><p>The  setting affects the initial state of the \"Use Agents\" toggle when creating a new assistant in the UI:</p><ul><li> (default): New assistants are created with agent delegation disabled by default</li><li>: New assistants are created with agent delegation enabled by default</li></ul><p>Note that users can always override this setting by toggling the \"Use Agents\" button in the UI when creating or editing an assistant. This environment variable only controls the initial default state.</p><p>PentAGI provides comprehensive programmatic access through both REST and GraphQL APIs, allowing you to integrate penetration testing workflows into your automation pipelines, CI/CD processes, and custom applications.</p><p>API tokens are managed through the PentAGI web interface:</p><ol><li>Navigate to  ‚Üí  in the web UI</li><li>Click  to generate a new API token</li><li>Configure token properties: \n  <ul><li> (optional): A descriptive name for the token</li><li>: When the token will expire (minimum 1 minute, maximum 3 years)</li></ul></li><li>Click  and <strong>copy the token immediately</strong> - it will only be shown once for security reasons</li><li>Use the token as a Bearer token in your API requests</li></ol><p>Each token is associated with your user account and inherits your role's permissions.</p><p>Include the API token in the  header of your HTTP requests:</p><pre><code># GraphQL API example\ncurl -X POST https://your-pentagi-instance:8443/api/v1/graphql \\\n  -H \"Authorization: Bearer YOUR_API_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"{ flows { id title status } }\"}'\n\n# REST API example\ncurl https://your-pentagi-instance:8443/api/v1/flows \\\n  -H \"Authorization: Bearer YOUR_API_TOKEN\"\n</code></pre><h3>API Exploration and Testing</h3><p>PentAGI provides interactive documentation for exploring and testing API endpoints:</p><p>Access the GraphQL Playground at <code>https://your-pentagi-instance:8443/api/v1/graphql/playground</code></p><ol><li>Click the  tab at the bottom</li><li>Add your authorization header: <pre><code>{\n  \"Authorization\": \"Bearer YOUR_API_TOKEN\"\n}\n</code></pre></li><li>Explore the schema, run queries, and test mutations interactively</li></ol><p>Access the REST API documentation at <code>https://your-pentagi-instance:8443/api/v1/swagger/index.html</code></p><ol><li>Enter your token in the format: </li><li>Test endpoints directly from the Swagger UI</li></ol><p>You can generate type-safe API clients for your preferred programming language using the schema files included with PentAGI:</p><p>The GraphQL schema is available at:</p><ul><li>: Navigate to Settings to download </li><li>: <code>backend/pkg/graph/schema.graphqls</code> in the repository</li></ul><p>Generate clients using tools like:</p><p>The OpenAPI specification is available at:</p><ul><li>: <code>https://your-pentagi-instance:8443/api/v1/swagger/doc.json</code></li><li>: Available in <code>backend/pkg/server/docs/swagger.yaml</code></li></ul><p>When working with API tokens:</p><ul><li><strong>Never commit tokens to version control</strong> - use environment variables or secrets management</li><li> - set appropriate expiration dates and create new tokens periodically</li><li><strong>Use separate tokens for different applications</strong> - makes it easier to revoke access if needed</li><li> - review API token activity in the Settings page</li><li> - disable or delete tokens that are no longer needed</li><li> - never send API tokens over unencrypted connections</li></ul><ul><li>: See all your active tokens in Settings ‚Üí API Tokens</li><li>: Update token names or revoke tokens</li><li>: Permanently remove tokens (this action cannot be undone)</li><li>: Each token has a unique ID that can be copied for reference</li></ul><ul><li>Token ID (unique identifier)</li><li>Status (active/revoked/expired)</li></ul><h3>Custom LLM Provider Configuration</h3><p>When using custom LLM providers with the  variables, you can fine-tune the reasoning format used in requests:</p><table><thead><tr></tr></thead><tbody><tr><td>Base URL for the custom LLM API endpoint</td></tr><tr><td>API key for the custom LLM provider</td></tr><tr><td>Default model to use (can be overridden in provider config)</td></tr><tr><td>Path to the YAML configuration file for agent-specific models</td></tr><tr><td>Provider name prefix for model names (e.g., ,  for LiteLLM proxy)</td></tr><tr><td><code>LLM_SERVER_LEGACY_REASONING</code></td><td>Controls reasoning format in API requests</td></tr><tr><td><code>LLM_SERVER_PRESERVE_REASONING</code></td><td>Preserve reasoning content in multi-turn conversations (required by some providers)</td></tr></tbody></table><p>The  setting is particularly useful when using , which adds a provider prefix to model names. For example, when connecting to Moonshot API through LiteLLM, models like  become . By setting <code>LLM_SERVER_PROVIDER=moonshot</code>, you can use the same provider configuration file for both direct API access and LiteLLM proxy access without modifications.</p><p>The <code>LLM_SERVER_LEGACY_REASONING</code> setting affects how reasoning parameters are sent to the LLM:</p><ul><li> (default): Uses modern format where reasoning is sent as a structured object with  parameter</li><li>: Uses legacy format with string-based  parameter</li></ul><p>This setting is important when working with different LLM providers as they may expect different reasoning formats in their API requests. If you encounter reasoning-related errors with custom providers, try changing this setting.</p><p>The <code>LLM_SERVER_PRESERVE_REASONING</code> setting controls whether reasoning content is preserved in multi-turn conversations:</p><ul><li> (default): Reasoning content is not preserved in conversation history</li><li>: Reasoning content is preserved and sent in subsequent API calls</li></ul><p>This setting is required by some LLM providers (e.g., Moonshot) that return errors like \"thinking is enabled but reasoning_content is missing in assistant tool call message\" when reasoning content is not included in multi-turn conversations. Enable this setting if your provider requires reasoning content to be preserved.</p><h3>Local LLM Provider Configuration</h3><p>PentAGI supports Ollama for local LLM inference, providing zero-cost operation and enhanced privacy:</p><table><thead><tr></tr></thead><tbody><tr><td>URL of your Ollama server</td></tr><tr><td><code>llama3.1:8b-instruct-q8_0</code></td><td>Default model for inference</td></tr><tr><td><code>OLLAMA_SERVER_CONFIG_PATH</code></td><td>Path to custom agent configuration file</td></tr><tr><td><code>OLLAMA_SERVER_PULL_MODELS_TIMEOUT</code></td><td>Timeout for model downloads (seconds)</td></tr><tr><td><code>OLLAMA_SERVER_PULL_MODELS_ENABLED</code></td><td>Auto-download models on startup</td></tr><tr><td><code>OLLAMA_SERVER_LOAD_MODELS_ENABLED</code></td><td>Query server for available models</td></tr></tbody></table><pre><code># Basic Ollama setup with default model\nOLLAMA_SERVER_URL=http://localhost:11434\nOLLAMA_SERVER_MODEL=llama3.1:8b-instruct-q8_0\n\n# Production setup with auto-pull and model discovery\nOLLAMA_SERVER_URL=http://ollama-server:11434\nOLLAMA_SERVER_PULL_MODELS_ENABLED=true\nOLLAMA_SERVER_PULL_MODELS_TIMEOUT=900\nOLLAMA_SERVER_LOAD_MODELS_ENABLED=true\n\n# Custom configuration with agent-specific models\nOLLAMA_SERVER_CONFIG_PATH=/path/to/ollama-config.yml\n\n# Default configuration file inside docker container\nOLLAMA_SERVER_CONFIG_PATH=/opt/pentagi/conf/ollama-llama318b.provider.yml\n</code></pre><p><strong>Performance Considerations:</strong></p><ul><li> (<code>OLLAMA_SERVER_LOAD_MODELS_ENABLED=true</code>): Adds 1-2s startup latency querying Ollama API</li><li> (<code>OLLAMA_SERVER_PULL_MODELS_ENABLED=true</code>): First startup may take several minutes downloading models</li><li> (<code>OLLAMA_SERVER_PULL_MODELS_TIMEOUT=900</code>): 15 minutes in seconds</li><li>: Disable both flags and specify models in config file for fastest startup</li></ul><h4>Creating Custom Ollama Models with Extended Context</h4><p>PentAGI requires models with larger context windows than the default Ollama configurations. You need to create custom models with increased  parameter through Modelfiles. While typical agent workflows consume around 64K tokens, PentAGI uses 110K context size for safety margin and handling complex penetration testing scenarios.</p><p>: The  parameter can only be set during model creation via Modelfile - it cannot be changed after model creation or overridden at runtime.</p><h5>Example: Qwen3 32B FP16 with Extended Context</h5><p>Create a Modelfile named <code>Modelfile_qwen3_32b_fp16_tc</code>:</p><pre><code>FROM qwen3:32b-fp16\nPARAMETER num_ctx 110000\nPARAMETER temperature 0.3\nPARAMETER top_p 0.8\nPARAMETER min_p 0.0\nPARAMETER top_k 20\nPARAMETER repeat_penalty 1.1\n</code></pre><pre><code>ollama create qwen3:32b-fp16-tc -f Modelfile_qwen3_32b_fp16_tc\n</code></pre><h5>Example: QwQ 32B FP16 with Extended Context</h5><p>Create a Modelfile named <code>Modelfile_qwq_32b_fp16_tc</code>:</p><pre><code>FROM qwq:32b-fp16\nPARAMETER num_ctx 110000\nPARAMETER temperature 0.2\nPARAMETER top_p 0.7\nPARAMETER min_p 0.0\nPARAMETER top_k 40\nPARAMETER repeat_penalty 1.2\n</code></pre><pre><code>ollama create qwq:32b-fp16-tc -f Modelfile_qwq_32b_fp16_tc\n</code></pre><blockquote><p>: The QwQ 32B FP16 model requires approximately  for inference. Ensure your system has sufficient GPU memory before attempting to use this model.</p></blockquote><p>These custom models are referenced in the pre-built provider configuration files (<code>ollama-qwen332b-fp16-tc.provider.yml</code> and <code>ollama-qwq32b-fp16-tc.provider.yml</code>) that are included in the Docker image at .</p><h3>OpenAI Provider Configuration</h3><p>PentAGI supports OpenAI's advanced language models, including the latest reasoning-capable o-series models designed for complex analytical tasks:</p><table><thead><tr></tr></thead><tbody><tr><td>API key for OpenAI services</td></tr><tr><td><code>https://api.openai.com/v1</code></td></tr></tbody></table><pre><code># Basic OpenAI setup\nOPEN_AI_KEY=your_openai_api_key\nOPEN_AI_SERVER_URL=https://api.openai.com/v1\n\n# Using with proxy for enhanced security\nOPEN_AI_KEY=your_openai_api_key\nPROXY_URL=http://your-proxy:8080\n</code></pre><p>The OpenAI provider offers cutting-edge capabilities including:</p><ul><li>: Advanced o-series models (o1, o3, o4-mini) with step-by-step analytical thinking</li><li>: Flagship models optimized for complex security research and exploit development</li><li>: From nano models for high-volume scanning to powerful reasoning models for deep analysis</li><li>: Fast, intelligent models perfect for multi-step security analysis and penetration testing</li><li>: Industry-leading models with consistent performance across diverse security scenarios</li></ul><p>The system automatically selects appropriate OpenAI models based on task complexity, optimizing for both performance and cost-effectiveness.</p><h3>Anthropic Provider Configuration</h3><p>PentAGI integrates with Anthropic's Claude models, known for their exceptional safety, reasoning capabilities, and sophisticated understanding of complex security contexts:</p><table><thead><tr></tr></thead><tbody><tr><td>API key for Anthropic services</td></tr><tr><td><code>https://api.anthropic.com/v1</code></td></tr></tbody></table><pre><code># Basic Anthropic setup\nANTHROPIC_API_KEY=your_anthropic_api_key\nANTHROPIC_SERVER_URL=https://api.anthropic.com/v1\n\n# Using with proxy for secure environments\nANTHROPIC_API_KEY=your_anthropic_api_key\nPROXY_URL=http://your-proxy:8080\n</code></pre><p>The Anthropic provider delivers superior capabilities including:</p><ul><li>: Claude 4 series with exceptional reasoning for sophisticated penetration testing</li><li>: Claude 3.7 with step-by-step thinking capabilities for methodical security research</li><li>: Claude 3.5 Haiku for blazing-fast vulnerability scans and real-time monitoring</li><li>: Claude Sonnet models for complex security analysis and threat hunting</li><li>: Built-in safety mechanisms ensuring responsible security testing practices</li></ul><p>The system leverages Claude's advanced understanding of security contexts to provide thorough and responsible penetration testing guidance.</p><h3>Google AI (Gemini) Provider Configuration</h3><p>PentAGI supports Google's Gemini models through the Google AI API, offering state-of-the-art reasoning capabilities and multimodal features:</p><table><thead><tr></tr></thead><tbody><tr><td>API key for Google AI services</td></tr><tr><td><code>https://generativelanguage.googleapis.com</code></td></tr></tbody></table><pre><code># Basic Gemini setup\nGEMINI_API_KEY=your_gemini_api_key\nGEMINI_SERVER_URL=https://generativelanguage.googleapis.com\n\n# Using with proxy\nGEMINI_API_KEY=your_gemini_api_key\nPROXY_URL=http://your-proxy:8080\n</code></pre><p>The Gemini provider offers advanced features including:</p><ul><li>: Advanced reasoning models (Gemini 2.5 series) with step-by-step analysis</li><li>: Text and image processing for comprehensive security assessments</li><li>: Up to 2M tokens for analyzing extensive codebases and documentation</li><li>: From high-performance pro models to economical flash variants</li><li>: Specialized configurations optimized for penetration testing workflows</li></ul><p>The system automatically selects appropriate Gemini models based on agent requirements, balancing performance, capabilities, and cost-effectiveness.</p><h3>AWS Bedrock Provider Configuration</h3><p>PentAGI integrates with Amazon Bedrock, offering access to a wide range of foundation models from leading AI companies including Anthropic, AI21, Cohere, Meta, and Amazon's own models:</p><table><thead><tr></tr></thead><tbody><tr><td>AWS region for Bedrock service</td></tr><tr><td>AWS access key ID for authentication</td></tr><tr><td><code>BEDROCK_SECRET_ACCESS_KEY</code></td><td>AWS secret access key for authentication</td></tr><tr><td>AWS session token as alternative way for authentication</td></tr><tr><td>Optional custom Bedrock endpoint URL</td></tr></tbody></table><pre><code># Basic AWS Bedrock setup with credentials\nBEDROCK_REGION=us-east-1\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key\n\n# Using with proxy for enhanced security\nBEDROCK_REGION=us-east-1\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key\nPROXY_URL=http://your-proxy:8080\n\n# Using custom endpoint (for VPC endpoints or testing)\nBEDROCK_REGION=us-east-1\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key\nBEDROCK_SERVER_URL=https://bedrock-runtime.us-east-1.amazonaws.com\n</code></pre><blockquote><p>[!IMPORTANT] <strong>AWS Bedrock Rate Limits Warning</strong></p><p>The default PentAGI configuration for AWS Bedrock uses two primary models:</p><ul><li><code>us.anthropic.claude-sonnet-4-20250514-v1:0</code> (for most agents) -  for new AWS accounts</li><li><code>us.anthropic.claude-3-5-haiku-20241022-v1:0</code> (for simple tasks) -  for new AWS accounts</li></ul><p>These default rate limits are  for comfortable penetration testing scenarios and will significantly impact your workflow. We :</p><ol><li> for your AWS Bedrock models through the AWS Service Quotas console</li><li><strong>Use provisioned throughput models</strong> with hourly billing for higher throughput requirements</li><li><strong>Switch to alternative models</strong> with higher default quotas (e.g., Amazon Nova series, Meta Llama models)</li><li><strong>Consider using a different LLM provider</strong> (OpenAI, Anthropic, Gemini) if you need immediate high-throughput access</li></ol><p>Without adequate rate limits, you may experience frequent delays, timeouts, and degraded testing performance.</p></blockquote><p>The AWS Bedrock provider delivers comprehensive capabilities including:</p><ul><li>: Access to models from Anthropic (Claude), AI21 (Jamba), Cohere (Command), Meta (Llama), Amazon (Nova, Titan), and DeepSeek (R1) through a single interface</li><li>: Support for Claude 4 and other reasoning-capable models with step-by-step thinking</li><li>: Amazon Nova series supporting text, image, and video processing for comprehensive security analysis</li><li>: AWS-native security controls, VPC integration, and compliance certifications</li><li>: Wide range of model sizes and capabilities for cost-effective penetration testing</li><li>: Deploy models in your preferred AWS region for data residency and performance</li><li>: Low-latency inference through AWS's global infrastructure</li></ul><p>The system automatically selects appropriate Bedrock models based on task complexity and requirements, leveraging the full spectrum of available foundation models for optimal security testing results.</p><blockquote><p>[!WARNING] <strong>Converse API Requirements</strong></p><p>PentAGI uses the <strong>Amazon Bedrock Converse API</strong> for model interactions, which requires models to support the following features:</p><ul><li>‚úÖ  - Basic conversation API support</li><li>‚úÖ  - Streaming response support</li><li>‚úÖ  - Function calling capabilities for penetration testing tools</li><li>‚úÖ  - Real-time tool execution feedback</li></ul><p>‚ö†Ô∏è : Some models like AI21 Jurassic-2 and Cohere Command (Text) have  and may not work properly with PentAGI's multi-turn conversation workflows.</p></blockquote><blockquote><p>: AWS credentials can also be provided through IAM roles, environment variables, or AWS credential files following standard AWS SDK authentication patterns. Ensure your AWS account has appropriate permissions for Amazon Bedrock service access.</p></blockquote><p>For advanced configuration options and detailed setup instructions, please visit our <a href=\"https://docs.pentagi.com\">documentation</a>.</p><p>Langfuse provides advanced capabilities for monitoring and analyzing AI agent operations.</p><ol><li>Configure Langfuse environment variables in existing  file.</li></ol><ol start=\"2\"><li>Enable integration with Langfuse for PentAGI service in  file.</li></ol><pre><code>LANGFUSE_BASE_URL=http://langfuse-web:3000\nLANGFUSE_PROJECT_ID= # default: value from ${LANGFUSE_INIT_PROJECT_ID}\nLANGFUSE_PUBLIC_KEY= # default: value from ${LANGFUSE_INIT_PROJECT_PUBLIC_KEY}\nLANGFUSE_SECRET_KEY= # default: value from ${LANGFUSE_INIT_PROJECT_SECRET_KEY}\n</code></pre><pre><code>curl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose-langfuse.yml\ndocker compose -f docker-compose.yml -f docker-compose-langfuse.yml up -d\n</code></pre><p>Visit <a href=\"http://localhost:4000\">localhost:4000</a> to access Langfuse Web UI with credentials from  file:</p><ul><li> - Admin email</li><li><code>LANGFUSE_INIT_USER_PASSWORD</code> - Admin password</li></ul><h3>Monitoring and Observability</h3><p>For detailed system operation tracking, integration with monitoring tools is available.</p><ol><li>Enable integration with OpenTelemetry and all observability services for PentAGI in  file.</li></ol><ol start=\"2\"><li>Run the observability stack:</li></ol><pre><code>curl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose-observability.yml\ndocker compose -f docker-compose.yml -f docker-compose-observability.yml up -d\n</code></pre><blockquote><p>[!NOTE] If you want to use Observability stack with Langfuse, you need to enable integration in  file to set <code>LANGFUSE_OTEL_EXPORTER_OTLP_ENDPOINT</code> to .</p><p>To run all available stacks together (Langfuse, Graphiti, and Observability):</p><pre><code>docker compose -f docker-compose.yml -f docker-compose-langfuse.yml -f docker-compose-graphiti.yml -f docker-compose-observability.yml up -d\n</code></pre><p>You can also register aliases for these commands in your shell to run it faster:</p><pre><code>alias pentagi=\"docker compose -f docker-compose.yml -f docker-compose-langfuse.yml -f docker-compose-graphiti.yml -f docker-compose-observability.yml\"\nalias pentagi-up=\"docker compose -f docker-compose.yml -f docker-compose-langfuse.yml -f docker-compose-graphiti.yml -f docker-compose-observability.yml up -d\"\nalias pentagi-down=\"docker compose -f docker-compose.yml -f docker-compose-langfuse.yml -f docker-compose-graphiti.yml -f docker-compose-observability.yml down\"\n</code></pre></blockquote><h3>Knowledge Graph Integration (Graphiti)</h3><p>PentAGI integrates with <a href=\"https://github.com/vxcontrol/pentagi-graphiti\">Graphiti</a>, a temporal knowledge graph system powered by Neo4j, to provide advanced semantic understanding and relationship tracking for AI agent operations. The vxcontrol fork provides custom entity and edge types that are specific to pentesting purposes.</p><p>Graphiti automatically extracts and stores structured knowledge from agent interactions, building a graph of entities, relationships, and temporal context. This enables:</p><ul><li>: Store and recall relationships between tools, targets, vulnerabilities, and techniques</li><li>: Track how different pentesting actions relate to each other over time</li><li>: Learn from past penetration tests and apply insights to new assessments</li><li>: Search for complex patterns like \"What tools were effective against similar targets?\"</li></ul><p>The Graphiti knowledge graph is  and disabled by default. To enable it:</p><ol><li>Configure Graphiti environment variables in  file:</li></ol><pre><code>## Graphiti knowledge graph settings\nGRAPHITI_ENABLED=true\nGRAPHITI_TIMEOUT=30\nGRAPHITI_URL=http://graphiti:8000\nGRAPHITI_MODEL_NAME=gpt-5-mini\n\n# Neo4j settings (used by Graphiti stack)\nNEO4J_USER=neo4j\nNEO4J_DATABASE=neo4j\nNEO4J_PASSWORD=devpassword\nNEO4J_URI=bolt://neo4j:7687\n\n# OpenAI API key (required by Graphiti for entity extraction)\nOPEN_AI_KEY=your_openai_api_key\n</code></pre><ol start=\"2\"><li>Run the Graphiti stack along with the main PentAGI services:</li></ol><pre><code># Download the Graphiti compose file if needed\ncurl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose-graphiti.yml\n\n# Start PentAGI with Graphiti\ndocker compose -f docker-compose.yml -f docker-compose-graphiti.yml up -d\n</code></pre><ol start=\"3\"><li>Verify Graphiti is running:</li></ol><pre><code># Check service health\ndocker compose -f docker-compose.yml -f docker-compose-graphiti.yml ps graphiti neo4j\n\n# View Graphiti logs\ndocker compose -f docker-compose.yml -f docker-compose-graphiti.yml logs -f graphiti\n\n# Access Neo4j Browser (optional)\n# Visit http://localhost:7474 and login with NEO4J_USER/NEO4J_PASSWORD\n\n# Access Graphiti API (optional, for debugging)\n# Visit http://localhost:8000/docs for Swagger API documentation\n</code></pre><blockquote><p>[!NOTE] The Graphiti service is defined in <code>docker-compose-graphiti.yml</code> as a separate stack. You must run both compose files together to enable the knowledge graph functionality. The pre-built Docker image <code>vxcontrol/graphiti:latest</code> is used by default.</p></blockquote><p>When enabled, PentAGI automatically captures:</p><ul><li>: All agent reasoning, analysis, and decisions</li><li>: Commands executed, tools used, and their results</li><li>: Flow, task, and subtask hierarchy</li></ul><h3>GitHub and Google OAuth Integration</h3><p>OAuth integration with GitHub and Google allows users to authenticate using their existing accounts on these platforms. This provides several benefits:</p><ul><li>Simplified login process without need to create separate credentials</li><li>Enhanced security through trusted identity providers</li><li>Access to user profile information from GitHub/Google accounts</li><li>Seamless integration with existing development workflows</li></ul><p>For using GitHub OAuth you need to create a new OAuth application in your GitHub account and set the  and  in  file.</p><p>For using Google OAuth you need to create a new OAuth application in your Google account and set the  and  in  file.</p><h3>Docker Image Configuration</h3><p>PentAGI allows you to configure Docker image selection for executing various tasks. The system automatically chooses the most appropriate image based on the task type, but you can constrain this selection by specifying your preferred images:</p><table><thead><tr></tr></thead><tbody><tr><td>Default Docker image for general tasks and ambiguous cases</td></tr><tr><td><code>DOCKER_DEFAULT_IMAGE_FOR_PENTEST</code></td><td>Default Docker image for security/penetration testing tasks</td></tr></tbody></table><p>When these environment variables are set, AI agents will be limited to the image choices you specify. This is particularly useful for:</p><ul><li>: Restricting usage to only verified and trusted images</li><li><strong>Environment Standardization</strong>: Using corporate or customized images across all operations</li><li>: Utilizing pre-built images with necessary tools already installed</li></ul><pre><code># Using a custom image for general tasks\nDOCKER_DEFAULT_IMAGE=mycompany/custom-debian:latest\n\n# Using a specialized image for penetration testing\nDOCKER_DEFAULT_IMAGE_FOR_PENTEST=mycompany/pentest-tools:v2.0\n</code></pre><blockquote><p>[!NOTE] If a user explicitly specifies a particular Docker image in their task, the system will try to use that exact image, ignoring these settings. These variables only affect the system's automatic image selection process.</p></blockquote><ul></ul><p>Run once <code>cd backend &amp;&amp; go mod download</code> to install needed packages.</p><p>For generating swagger files have to run</p><pre><code>swag init -g ../../pkg/server/router.go -o pkg/server/docs/ --parseDependency --parseInternal --parseDepth 2 -d cmd/pentagi\n</code></pre><p>before installing  package via</p><pre><code>go install github.com/swaggo/swag/cmd/swag@v1.8.7\n</code></pre><p>For generating graphql resolver files have to run</p><pre><code>go run github.com/99designs/gqlgen --config ./gqlgen/gqlgen.yml\n</code></pre><p>after that you can see the generated files in  folder.</p><p>For generating ORM methods (database package) from sqlc configuration</p><pre><code>docker run --rm -v $(pwd):/src -w /src --network pentagi-network -e DATABASE_URL=\"{URL}\" sqlc/sqlc generate -f sqlc/sqlc.yml\n</code></pre><p>For generating Langfuse SDK from OpenAPI specification</p><p>For running tests <code>cd backend &amp;&amp; go test -v ./...</code></p><p>Run once <code>cd frontend &amp;&amp; npm install</code> to install needed packages.</p><p>For generating graphql files have to run  which using  file.</p><p>Be sure that you have  installed globally:</p><pre><code>npm install -g graphql-codegen\n</code></pre><ul><li> to check if your code is formatted correctly</li><li> to fix it</li><li> to check if your code is linted correctly</li><li> to fix it</li></ul><p>For generating SSL certificates you need to run  which using  file or it will be generated automatically when you run .</p><p>Edit the configuration for  in  file:</p><ul><li> - PostgreSQL database URL (eg. <code>postgres://postgres:postgres@localhost:5432/pentagidb?sslmode=disable</code>)</li><li> - Docker SDK API (eg. for macOS <code>DOCKER_HOST=unix:///Users/&lt;my-user&gt;/Library/Containers/com.docker.docker/Data/docker.raw.sock</code>) <a href=\"https://stackoverflow.com/a/62757128/5922857\">more info</a></li></ul><ul><li> - Port to run the server (default: )</li><li> - Enable SSL for the server (default: )</li></ul><p>Edit the configuration for  in  file:</p><ul><li> - Backend API URL.  the URL scheme (e.g., )</li><li> - Enable SSL for the server (default: )</li><li> - Port to run the server (default: )</li><li> - Host to run the server (default: )</li></ul><p>Run the command(s) in  folder:</p><ul><li>Use  file to set environment variables like a </li><li>Run <code>go run cmd/pentagi/main.go</code> to start the server</li></ul><blockquote><p>[!NOTE] The first run can take a while as dependencies and docker images need to be downloaded to setup the backend environment.</p></blockquote><p>Run the command(s) in  folder:</p><ul><li>Run  to install the dependencies</li><li>Run  to run the web app</li><li>Run  to build the web app</li></ul><p>Open your browser and visit the web app URL.</p><p>PentAGI includes a powerful utility called  for testing and validating LLM agent capabilities. This tool helps ensure your LLM provider configurations work correctly with different agent types, allowing you to optimize model selection for each specific agent role.</p><p>The utility features parallel testing of multiple agents, detailed reporting, and flexible configuration options.</p><ul><li>: Tests multiple agents simultaneously for faster results</li><li>: Evaluates basic completion, JSON responses, function calling, and penetration testing knowledge</li><li>: Generates markdown reports with success rates and performance metrics</li><li>: Test specific agents or test groups as needed</li><li>: Includes domain-specific tests for cybersecurity and penetration testing scenarios</li></ul><h4>For Developers (with local Go environment)</h4><p>If you've cloned the repository and have Go installed:</p><pre><code># Default configuration with .env file\ncd backend\ngo run cmd/ctester/*.go -verbose\n\n# Custom provider configuration\ngo run cmd/ctester/*.go -config ../examples/configs/openrouter.provider.yml -verbose\n\n# Generate a report file\ngo run cmd/ctester/*.go -config ../examples/configs/deepinfra.provider.yml -report ../test-report.md\n\n# Test specific agent types only\ngo run cmd/ctester/*.go -agents simple,simple_json,primary_agent -verbose\n\n# Test specific test groups only\ngo run cmd/ctester/*.go -groups basic,advanced -verbose\n</code></pre><h4>For Users (using Docker image)</h4><p>If you prefer to use the pre-built Docker image without setting up a development environment:</p><pre><code># Using Docker to test with default environment\ndocker run --rm -v $(pwd)/.env:/opt/pentagi/.env vxcontrol/pentagi /opt/pentagi/bin/ctester -verbose\n\n# Test with your custom provider configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  -v $(pwd)/my-config.yml:/opt/pentagi/config.yml \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/config.yml -agents simple,primary_agent,coder -verbose\n\n# Generate a detailed report\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  -v $(pwd):/opt/pentagi/output \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -report /opt/pentagi/output/report.md\n</code></pre><h4>Using Pre-configured Providers</h4><p>The Docker image comes with built-in support for major providers (OpenAI, Anthropic, Gemini, Ollama) and pre-configured provider files for additional services (OpenRouter, DeepInfra, DeepSeek, Moonshot):</p><pre><code># Test with OpenRouter configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/openrouter.provider.yml\n\n# Test with DeepInfra configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/deepinfra.provider.yml\n\n# Test with DeepSeek configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/deepseek.provider.yml\n\n# Test with Moonshot configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/moonshot.provider.yml\n\n# Test with OpenAI configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -type openai\n\n# Test with Anthropic configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -type anthropic\n\n# Test with Gemini configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -type gemini\n\n# Test with AWS Bedrock configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -type bedrock\n\n# Test with Custom OpenAI configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/custom-openai.provider.yml\n\n# Test with Ollama configuration (local inference)\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/ollama-llama318b.provider.yml\n\n# Test with Ollama Qwen3 32B configuration (requires custom model creation)\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/ollama-qwen332b-fp16-tc.provider.yml\n\n# Test with Ollama QwQ 32B configuration (requires custom model creation and 71.3GB VRAM)\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/ollama-qwq32b-fp16-tc.provider.yml\n</code></pre><p>To use these configurations, your  file only needs to contain:</p><pre><code>LLM_SERVER_URL=https://openrouter.ai/api/v1      # or https://api.deepinfra.com/v1/openai or https://api.deepseek.com or https://api.openai.com/v1 or https://api.moonshot.ai/v1\nLLM_SERVER_KEY=your_api_key\nLLM_SERVER_MODEL=                                # Leave empty, as models are specified in the config\nLLM_SERVER_CONFIG_PATH=/opt/pentagi/conf/openrouter.provider.yml  # or deepinfra.provider.yml or deepseek.provider.yml or custom-openai.provider.yml or moonshot.provider.yml\nLLM_SERVER_PROVIDER=                             # Provider name for LiteLLM proxy (e.g., openrouter, deepseek, moonshot)\nLLM_SERVER_LEGACY_REASONING=false                # Controls reasoning format, for OpenAI must be true (default: false)\nLLM_SERVER_PRESERVE_REASONING=false              # Preserve reasoning content in multi-turn conversations (required by Moonshot, default: false)\n\n# For OpenAI (official API)\nOPEN_AI_KEY=your_openai_api_key                  # Your OpenAI API key\nOPEN_AI_SERVER_URL=https://api.openai.com/v1     # OpenAI API endpoint\n\n# For Anthropic (Claude models)\nANTHROPIC_API_KEY=your_anthropic_api_key         # Your Anthropic API key\nANTHROPIC_SERVER_URL=https://api.anthropic.com/v1  # Anthropic API endpoint\n\n# For Gemini (Google AI)\nGEMINI_API_KEY=your_gemini_api_key               # Your Google AI API key\nGEMINI_SERVER_URL=https://generativelanguage.googleapis.com  # Google AI API endpoint\n\n# For AWS Bedrock (enterprise foundation models)\nBEDROCK_REGION=us-east-1                         # AWS region for Bedrock service\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key        # AWS access key ID\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key    # AWS secret access key\nBEDROCK_SESSION_TOKEN=your_aws_session_token     # AWS session token (alternative auth method)\nBEDROCK_SERVER_URL=                              # Optional custom Bedrock endpoint\n\n# For Ollama (local inference)\nOLLAMA_SERVER_URL=http://localhost:11434\nOLLAMA_SERVER_MODEL=llama3.1:8b-instruct-q8_0\nOLLAMA_SERVER_CONFIG_PATH=/opt/pentagi/conf/ollama-llama318b.provider.yml\nOLLAMA_SERVER_PULL_MODELS_ENABLED=false\nOLLAMA_SERVER_LOAD_MODELS_ENABLED=false\n</code></pre><h4>Using OpenAI with Unverified Organizations</h4><p>For OpenAI accounts with unverified organizations that don't have access to the latest reasoning models (o1, o3, o4-mini), you need to use a custom configuration.</p><p>To use OpenAI with unverified organization accounts, configure your  file as follows:</p><pre><code>LLM_SERVER_URL=https://api.openai.com/v1\nLLM_SERVER_KEY=your_openai_api_key\nLLM_SERVER_MODEL=                                # Leave empty, models are specified in config\nLLM_SERVER_CONFIG_PATH=/opt/pentagi/conf/custom-openai.provider.yml\nLLM_SERVER_LEGACY_REASONING=true                 # Required for OpenAI reasoning format\n</code></pre><p>This configuration uses the pre-built <code>custom-openai.provider.yml</code> file that maps all agent types to models available for unverified organizations, using  instead of models like , , and .</p><p>You can test this configuration using:</p><pre><code># Test with custom OpenAI configuration for unverified accounts\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/custom-openai.provider.yml\n</code></pre><blockquote><p>[!NOTE] The <code>LLM_SERVER_LEGACY_REASONING=true</code> setting is crucial for OpenAI compatibility as it ensures reasoning parameters are sent in the format expected by OpenAI's API.</p></blockquote><p>When using LiteLLM proxy to access various LLM providers, model names are prefixed with the provider name (e.g.,  instead of ). To use the same provider configuration files with both direct API access and LiteLLM proxy, set the  variable:</p><pre><code># Direct access to Moonshot API\nLLM_SERVER_URL=https://api.moonshot.ai/v1\nLLM_SERVER_KEY=your_moonshot_api_key\nLLM_SERVER_CONFIG_PATH=/opt/pentagi/conf/moonshot.provider.yml\nLLM_SERVER_PROVIDER=                             # Empty for direct access\n\n# Access via LiteLLM proxy\nLLM_SERVER_URL=http://litellm-proxy:4000\nLLM_SERVER_KEY=your_litellm_api_key\nLLM_SERVER_CONFIG_PATH=/opt/pentagi/conf/moonshot.provider.yml\nLLM_SERVER_PROVIDER=moonshot                     # Provider prefix for LiteLLM\n</code></pre><p>With <code>LLM_SERVER_PROVIDER=moonshot</code>, the system automatically prefixes all model names from the configuration file with , making them compatible with LiteLLM's model naming convention.</p><p><strong>Supported provider names for LiteLLM:</strong></p><ul><li> - for OpenAI models via LiteLLM</li><li> - for Anthropic/Claude models via LiteLLM</li><li> - for Google Gemini models via LiteLLM</li><li> - for OpenRouter aggregator</li><li> - for DeepSeek models</li><li> - for DeepInfra hosting</li><li> - for Moonshot AI (Kimi)</li><li>Any other provider name configured in your LiteLLM instance</li></ul><p>This approach allows you to:</p><ul><li>Use the same configuration files for both direct and proxied access</li><li>Switch between providers without modifying configuration files</li><li>Easily test different routing strategies with LiteLLM</li></ul><h4>Running Tests in a Production Environment</h4><p>If you already have a running PentAGI container and want to test the current configuration:</p><pre><code># Run ctester in an existing container using current environment variables\ndocker exec -it pentagi /opt/pentagi/bin/ctester -verbose\n\n# Test specific agent types with deterministic ordering\ndocker exec -it pentagi /opt/pentagi/bin/ctester -agents simple,primary_agent,pentester -groups basic,knowledge -verbose\n\n# Generate a report file inside the container\ndocker exec -it pentagi /opt/pentagi/bin/ctester -report /opt/pentagi/data/agent-test-report.md\n\n# Access the report from the host\ndocker cp pentagi:/opt/pentagi/data/agent-test-report.md ./\n</code></pre><p>The utility accepts several options:</p><ul><li> - Path to environment file (default: )</li><li> - Provider type: , , , , ,  (default: )</li><li> - Path to custom provider config (default: from  env variable)</li><li> - Path to custom tests YAML file (optional)</li><li> - Path to write the report file (optional)</li><li> - Comma-separated list of agent types to test (default: )</li><li> - Comma-separated list of test groups to run (default: )</li><li> - Enable verbose output with detailed test results for each agent</li></ul><p>Agents are tested in the following deterministic order:</p><ol><li> - Basic completion tasks</li><li> - JSON-structured responses</li><li> - Main reasoning agent</li><li> - Interactive assistant mode</li><li> - Content generation</li><li> - Content refinement and improvement</li><li> - Expert advice and consultation</li><li> - Self-reflection and analysis</li><li> - Information gathering and search</li><li> - Data enrichment and expansion</li><li> - Code generation and analysis</li><li> - Installation and setup tasks</li><li> - Penetration testing and security assessment</li></ol><ul><li> - Fundamental completion and prompt response tests</li><li> - Complex reasoning and function calling tests</li><li> - JSON format validation and structure tests (specifically designed for  agent)</li><li> - Domain-specific cybersecurity and penetration testing knowledge tests</li></ul><blockquote><p>: The  test group is specifically designed for the  agent type, while all other agents are tested with , , and  groups. This specialization ensures optimal testing coverage for each agent's intended purpose.</p></blockquote><h3>Example Provider Configuration</h3><p>Provider configuration defines which models to use for different agent types:</p><pre><code>simple:\n  model: \"provider/model-name\"\n  temperature: 0.7\n  top_p: 0.95\n  n: 1\n  max_tokens: 4000\n\nsimple_json:\n  model: \"provider/model-name\"\n  temperature: 0.7\n  top_p: 1.0\n  n: 1\n  max_tokens: 4000\n  json: true\n\n# ... other agent types ...\n</code></pre><ol><li>: Run tests with default configuration to establish benchmark performance</li><li><strong>Analyze agent-specific performance</strong>: Review the deterministic agent ordering to identify underperforming agents</li><li><strong>Test specialized configurations</strong>: Experiment with different models for each agent type using provider-specific configs</li><li><strong>Focus on domain knowledge</strong>: Pay special attention to knowledge group tests for cybersecurity expertise</li><li><strong>Validate function calling</strong>: Ensure tool-based tests pass consistently for critical agent types</li><li>: Look for the best success rate and performance across all test groups</li><li><strong>Deploy optimal configuration</strong>: Use in production with your optimized setup</li></ol><p>This tool helps ensure your AI agents are using the most effective models for their specific tasks, improving reliability while optimizing costs.</p><h2>üßÆ Embedding Configuration and Testing</h2><p>PentAGI uses vector embeddings for semantic search, knowledge storage, and memory management. The system supports multiple embedding providers that can be configured according to your needs and preferences.</p><h3>Supported Embedding Providers</h3><p>PentAGI supports the following embedding providers:</p><ul><li> (default): Uses OpenAI's text embedding models</li><li>: Local embedding model through Ollama</li><li>: Mistral AI's embedding models</li><li>: Jina AI's embedding service</li><li>: Models from HuggingFace</li><li>: Google's embedding models</li><li>: VoyageAI's embedding models</li></ul><h3>Embedding Tester Utility (etester)</h3><p>PentAGI includes a specialized  utility for testing, managing, and debugging embedding functionality. This tool is essential for diagnosing and resolving issues related to vector embeddings and knowledge storage.</p><h2>üîç Function Testing with ftester</h2><p>PentAGI includes a versatile utility called  for debugging, testing, and developing specific functions and AI agent behaviors. While  focuses on testing LLM model capabilities,  allows you to directly invoke individual system functions and AI agent components with precise control over execution context.</p><ul><li>: Test individual functions without running the entire system</li><li>: Test functions without a live PentAGI deployment using built-in mocks</li><li>: Fill function arguments interactively for exploratory testing</li><li>: Color-coded terminal output with formatted responses and errors</li><li>: Debug AI agents within the context of specific flows, tasks, and subtasks</li><li><strong>Observability Integration</strong>: All function calls are logged to Langfuse and Observability stack</li></ul><p>Run ftester with specific function and arguments directly from the command line:</p><pre><code># Basic usage with mock mode\ncd backend\ngo run cmd/ftester/main.go [function_name] -[arg1] [value1] -[arg2] [value2]\n\n# Example: Test terminal command in mock mode\ngo run cmd/ftester/main.go terminal -command \"ls -la\" -message \"List files\"\n\n# Using a real flow context\ngo run cmd/ftester/main.go -flow 123 terminal -command \"whoami\" -message \"Check user\"\n\n# Testing AI agent in specific task/subtask context\ngo run cmd/ftester/main.go -flow 123 -task 456 -subtask 789 pentester -message \"Find vulnerabilities\"\n</code></pre><p>Run ftester without arguments for a guided interactive experience:</p><pre><code># Start interactive mode\ngo run cmd/ftester/main.go [function_name]\n\n# For example, to interactively fill browser tool arguments\ngo run cmd/ftester/main.go browser\n</code></pre><p>The main utility accepts several options:</p><ul><li> - Path to environment file (optional, default: )</li><li> - Provider type to use (default: , options: , , , , , )</li><li> - Flow ID for testing (0 means using mocks, default: )</li><li> - Task ID for agent context (optional)</li><li> - Subtask ID for agent context (optional)</li></ul><p>Function-specific arguments are passed after the function name using  format.</p><pre><code>docker build -t local/pentagi:latest .\n</code></pre><blockquote><p>[!NOTE] You can use  to build the image for different platforms like a <code>docker buildx build --platform linux/amd64 -t local/pentagi:latest .</code></p><p>You need to change image name in docker-compose.yml file to  and run  to start the server or use  key option in <a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose.yml\">docker-compose.yml</a> file.</p></blockquote><p>This project is made possible thanks to the following research and developments:</p><p>: Licensed under <a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/LICENSE\">MIT License</a> Copyright (c) 2025 PentAGI Development Team</p><h3>VXControl Cloud SDK Integration</h3><p><strong>VXControl Cloud SDK Integration</strong>: This repository integrates <a href=\"https://github.com/vxcontrol/cloud\">VXControl Cloud SDK</a> under a <strong>special licensing exception</strong> that applies  to the official PentAGI project.</p><h4>‚úÖ Official PentAGI Project</h4><ul><li>This official repository: <code>https://github.com/vxcontrol/pentagi</code></li><li>Official releases distributed by VXControl LLC</li><li>Code used under direct authorization from VXControl LLC</li></ul><h4>‚ö†Ô∏è Important for Forks and Third-Party Use</h4><p>If you fork this project or create derivative works, the VXControl SDK components are subject to  license terms. You must either:</p><ol><li><strong>Remove VXControl SDK integration</strong></li><li><strong>Open source your entire application</strong> (comply with AGPL-3.0 copyleft terms)</li><li><strong>Obtain a commercial license</strong> from VXControl LLC</li></ol><p>For commercial use of VXControl Cloud SDK in proprietary applications, contact:</p>",
      "contentLength": 51867,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "openclaw/openclaw",
      "url": "https://github.com/openclaw/openclaw",
      "date": 1771556616,
      "author": "",
      "guid": 46699,
      "unread": true,
      "content": "<p>Your own personal AI assistant. Any OS. Any Platform. The lobster way. ü¶û</p><p> is a  you run on your own devices. It answers you on the channels you already use (WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, Microsoft Teams, WebChat), plus extension channels like BlueBubbles, Matrix, Zalo, and Zalo Personal. It can speak and listen on macOS/iOS/Android, and can render a live Canvas you control. The Gateway is just the control plane ‚Äî the product is the assistant.</p><p>If you want a personal, single-user assistant that feels local, fast, and always-on, this is it.</p><p>Preferred setup: run the onboarding wizard () in your terminal. The wizard guides you step by step through setting up the gateway, workspace, channels, and skills. The CLI wizard is the recommended path and works on <strong>macOS, Linux, and Windows (via WSL2; strongly recommended)</strong>. Works with npm, pnpm, or bun. New install? Start here: <a href=\"https://docs.openclaw.ai/start/getting-started\">Getting started</a></p><p>Model note: while any model is supported, I strongly recommend <strong>Anthropic Pro/Max (100/200) + Opus 4.6</strong> for long‚Äëcontext strength and better prompt‚Äëinjection resistance. See <a href=\"https://docs.openclaw.ai/start/onboarding\">Onboarding</a>.</p><h2>Models (selection + auth)</h2><pre><code>npm install -g openclaw@latest\n# or: pnpm add -g openclaw@latest\n\nopenclaw onboard --install-daemon\n</code></pre><p>The wizard installs the Gateway daemon (launchd/systemd user service) so it stays running.</p><pre><code>openclaw onboard --install-daemon\n\nopenclaw gateway --port 18789 --verbose\n\n# Send a message\nopenclaw message send --to +1234567890 --message \"Hello from OpenClaw\"\n\n# Talk to the assistant (optionally deliver back to any connected channel: WhatsApp/Telegram/Slack/Discord/Google Chat/Signal/iMessage/BlueBubbles/Microsoft Teams/Matrix/Zalo/Zalo Personal/WebChat)\nopenclaw agent --message \"Ship checklist\" --thinking high\n</code></pre><ul><li>: tagged releases ( or ), npm dist-tag .</li><li>: prerelease tags (), npm dist-tag  (macOS app may be missing).</li><li>: moving head of , npm dist-tag  (when published).</li></ul><p>Switch channels (git + npm): <code>openclaw update --channel stable|beta|dev</code>. Details: <a href=\"https://docs.openclaw.ai/install/development-channels\">Development channels</a>.</p><h2>From source (development)</h2><p>Prefer  for builds from source. Bun is optional for running TypeScript directly.</p><pre><code>git clone https://github.com/openclaw/openclaw.git\ncd openclaw\n\npnpm install\npnpm ui:build # auto-installs UI deps on first run\npnpm build\n\npnpm openclaw onboard --install-daemon\n\n# Dev loop (auto-reload on TS changes)\npnpm gateway:watch\n</code></pre><p>Note:  runs TypeScript directly (via ).  produces  for running via Node / the packaged  binary.</p><h2>Security defaults (DM access)</h2><p>OpenClaw connects to real messaging surfaces. Treat inbound DMs as .</p><p>Default behavior on Telegram/WhatsApp/Signal/iMessage/Microsoft Teams/Discord/Google Chat/Slack:</p><ul><li> ( / <code>channels.discord.dmPolicy=\"pairing\"</code> / <code>channels.slack.dmPolicy=\"pairing\"</code>; legacy: <code>channels.discord.dm.policy</code>, ): unknown senders receive a short pairing code and the bot does not process their message.</li><li>Approve with: <code>openclaw pairing approve &lt;channel&gt; &lt;code&gt;</code> (then the sender is added to a local allowlist store).</li><li>Public inbound DMs require an explicit opt-in: set  and include  in the channel allowlist ( / <code>channels.discord.allowFrom</code> / ; legacy: <code>channels.discord.dm.allowFrom</code>, <code>channels.slack.dm.allowFrom</code>).</li></ul><p>Run  to surface risky/misconfigured DM policies.</p><ul><li> ‚Äî WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, BlueBubbles (iMessage), iMessage (legacy), Microsoft Teams, Matrix, Zalo, Zalo Personal, WebChat, macOS, iOS/Android.</li><li> ‚Äî route inbound channels/accounts/peers to isolated agents (workspaces + per-agent sessions).</li><li> ‚Äî browser, canvas, nodes, cron, sessions, and Discord/Slack actions.</li></ul><h2>Everything we built so far</h2><pre><code>WhatsApp / Telegram / Slack / Discord / Google Chat / Signal / iMessage / BlueBubbles / Microsoft Teams / Matrix / Zalo / Zalo Personal / WebChat\n               ‚îÇ\n               ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ            Gateway            ‚îÇ\n‚îÇ       (control plane)         ‚îÇ\n‚îÇ     ws://127.0.0.1:18789      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n               ‚îÇ\n               ‚îú‚îÄ Pi agent (RPC)\n               ‚îú‚îÄ CLI (openclaw ‚Ä¶)\n               ‚îú‚îÄ WebChat UI\n               ‚îú‚îÄ macOS app\n               ‚îî‚îÄ iOS / Android nodes\n</code></pre><h2>Tailscale access (Gateway dashboard)</h2><p>OpenClaw can auto-configure Tailscale  (tailnet-only) or  (public) while the Gateway stays bound to loopback. Configure :</p><ul><li>: no Tailscale automation (default).</li><li>: tailnet-only HTTPS via  (uses Tailscale identity headers by default).</li><li>: public HTTPS via  (requires shared password auth).</li></ul><ul><li> must stay  when Serve/Funnel is enabled (OpenClaw enforces this).</li><li>Serve can be forced to require a password by setting <code>gateway.auth.mode: \"password\"</code> or <code>gateway.auth.allowTailscale: false</code>.</li><li>Funnel refuses to start unless <code>gateway.auth.mode: \"password\"</code> is set.</li><li>Optional: <code>gateway.tailscale.resetOnExit</code> to undo Serve/Funnel on shutdown.</li></ul><h2>Remote Gateway (Linux is great)</h2><p>It‚Äôs perfectly fine to run the Gateway on a small Linux instance. Clients (macOS app, CLI, WebChat) can connect over  or , and you can still pair device nodes (macOS/iOS/Android) to execute device‚Äëlocal actions when needed.</p><ul><li> runs the exec tool and channel connections by default.</li><li> run device‚Äëlocal actions (, camera, screen recording, notifications) via . In short: exec runs where the Gateway lives; device actions run where the device lives.</li></ul><h2>macOS permissions via the Gateway protocol</h2><p>The macOS app can run in  and advertises its capabilities + permission map over the Gateway WebSocket ( / ). Clients can then execute local actions via :</p><ul><li> runs a local command and returns stdout/stderr/exit code; set <code>needsScreenRecording: true</code> to require screen-recording permission (otherwise you‚Äôll get ).</li><li> posts a user notification and fails if notifications are denied.</li><li>, , , and  are also routed via  and follow TCC permission status.</li></ul><p>Elevated bash (host permissions) is separate from macOS TCC:</p><ul><li>Use  to toggle per‚Äësession elevated access when enabled + allowlisted.</li><li>Gateway persists the per‚Äësession toggle via  (WS method) alongside , , , , and .</li></ul><h2>Agent to Agent (sessions_* tools)</h2><ul><li>Use these to coordinate work across sessions without jumping between chat surfaces.</li><li> ‚Äî discover active sessions (agents) and their metadata.</li><li> ‚Äî fetch transcript logs for a session.</li><li> ‚Äî message another session; optional reply‚Äëback ping‚Äëpong + announce step (, ).</li></ul><h2>Skills registry (ClawHub)</h2><p>ClawHub is a minimal skill registry. With ClawHub enabled, the agent can search for skills automatically and pull in new ones as needed.</p><p>Send these in WhatsApp/Telegram/Slack/Google Chat/Microsoft Teams/WebChat (group commands are owner-only):</p><ul><li> ‚Äî compact session status (model + tokens, cost when available)</li><li> or  ‚Äî reset the session</li><li> ‚Äî compact session context (summary)</li><li> ‚Äî off|minimal|low|medium|high|xhigh (GPT-5.2 + Codex models only)</li><li> ‚Äî per-response usage footer</li><li> ‚Äî restart the gateway (owner-only in groups)</li><li><code>/activation mention|always</code> ‚Äî group activation toggle (groups only)</li></ul><p>The Gateway alone delivers a great experience. All apps are optional and add extra features.</p><p>If you plan to build/run companion apps, follow the platform runbooks below.</p><h3>macOS (OpenClaw.app) (optional)</h3><ul><li>Menu bar control for the Gateway and health.</li><li>Voice Wake + push-to-talk overlay.</li><li>Remote gateway control over SSH.</li></ul><p>Note: signed builds required for macOS permissions to stick across rebuilds (see ).</p><ul><li>Pairs as a node via the Bridge.</li><li>Voice trigger forwarding + Canvas surface.</li><li>Controlled via .</li></ul><ul><li>Pairs via the same Bridge + pairing flow as iOS.</li><li>Exposes Canvas, Camera, and Screen capture commands.</li></ul><ul><li>Workspace root:  (configurable via <code>agents.defaults.workspace</code>).</li><li>Injected prompt files: , , .</li><li>Skills: <code>~/.openclaw/workspace/skills/&lt;skill&gt;/SKILL.md</code>.</li></ul><p>Minimal <code>~/.openclaw/openclaw.json</code> (model + defaults):</p><pre><code>{\n  agent: {\n    model: \"anthropic/claude-opus-4-6\",\n  },\n}\n</code></pre><h2>Security model (important)</h2><ul><li> tools run on the host for the  session, so the agent has full access when it‚Äôs just you.</li><li> set <code>agents.defaults.sandbox.mode: \"non-main\"</code> to run  (groups/channels) inside per‚Äësession Docker sandboxes; bash then runs in Docker for those sessions.</li><li> allowlist , , , , , , , , ; denylist , , , , , .</li></ul><ul><li>Link the device: <code>pnpm openclaw channels login</code> (stores creds in ).</li><li>Allowlist who can talk to the assistant via <code>channels.whatsapp.allowFrom</code>.</li><li>If  is set, it becomes a group allowlist; include  to allow all.</li></ul><ul><li>Set  or <code>channels.telegram.botToken</code> (env wins).</li><li>Optional: set  (with <code>channels.telegram.groups.\"*\".requireMention</code>); when set, it is a group allowlist (include  to allow all). Also <code>channels.telegram.allowFrom</code> or <code>channels.telegram.webhookUrl</code> + <code>channels.telegram.webhookSecret</code> as needed.</li></ul><pre><code>{\n  channels: {\n    telegram: {\n      botToken: \"123456:ABCDEF\",\n    },\n  },\n}\n</code></pre><ul><li>Set  +  (or  + ).</li></ul><ul><li>Set  or  (env wins).</li><li>Optional: set , , or , plus <code>channels.discord.allowFrom</code>, , or <code>channels.discord.mediaMaxMb</code> as needed.</li></ul><pre><code>{\n  channels: {\n    discord: {\n      token: \"1234abcd\",\n    },\n  },\n}\n</code></pre><ul><li>Requires  and a  config section.</li></ul><ul><li> iMessage integration.</li><li>Configure <code>channels.bluebubbles.serverUrl</code> + <code>channels.bluebubbles.password</code> and a webhook (<code>channels.bluebubbles.webhookPath</code>).</li><li>The BlueBubbles server runs on macOS; the Gateway can run on macOS or elsewhere.</li></ul><ul><li>Legacy macOS-only integration via  (Messages must be signed in).</li><li>If  is set, it becomes a group allowlist; include  to allow all.</li></ul><ul><li>Configure a Teams app + Bot Framework, then add a  config section.</li><li>Allowlist who can talk via ; group access via  or <code>msteams.groupPolicy: \"open\"</code>.</li></ul><ul><li>Uses the Gateway WebSocket; no separate WebChat port/config.</li></ul><p>Browser control (optional):</p><pre><code>{\n  browser: {\n    enabled: true,\n    color: \"#FF4500\",\n  },\n}\n</code></pre><p>Use these when you‚Äôre past the onboarding flow and want the deeper reference.</p><h2>Advanced docs (discovery + control)</h2><h2>Operations &amp; troubleshooting</h2><p>OpenClaw was built for , a space lobster AI assistant. ü¶û by Peter Steinberger and the community.</p><p>See <a href=\"https://raw.githubusercontent.com/openclaw/openclaw/main/CONTRIBUTING.md\">CONTRIBUTING.md</a> for guidelines, maintainers, and how to submit PRs. AI/vibe-coded PRs welcome! ü§ñ</p><p>Special thanks to <a href=\"https://mariozechner.at/\">Mario Zechner</a> for his support and for <a href=\"https://github.com/badlogic/pi-mono\">pi-mono</a>. Special thanks to Adam Doppelt for lobster.bot.</p><p>Thanks to all clawtributors:</p>",
      "contentLength": 10081,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "open-mercato/open-mercato",
      "url": "https://github.com/open-mercato/open-mercato",
      "date": 1771556616,
      "author": "",
      "guid": 46700,
      "unread": true,
      "content": "<p>AI‚Äësupportive CRM / ERP foundation framework ‚Äî built to power R&amp;D, new processes, operations, and growth. It‚Äôs modular, extensible, and designed for teams that want strong defaults with room to customize everything. Better than Django, Retool and other alternatives - and Enterprise Grade!</p><p>Open Mercato is a new‚Äëera, AI‚Äësupportive platform for shipping enterprise‚Äëgrade CRMs, ERPs, and commerce backends. It‚Äôs modular, extensible, and designed so teams can mix their own modules, entities, and workflows while keeping the guardrails of a production-ready stack.</p><p> Now, you can have best of both. Use  enterprise ready business features like CRM, Sales, OMS, Encryption and build the remaining  that really makes the difference for your business.</p><ul><li>üíº  ‚Äì model customers, opportunities, and bespoke workflows with infinitely flexible data definitions.</li><li>üè≠  ‚Äì manage orders, production, and service delivery while tailoring modules to match your operational reality.</li><li>üõí  ‚Äì launch CPQ flows, B2B ordering portals, or full commerce backends with reusable modules.</li><li>ü§ù  ‚Äì spin up customer or partner portals with configurable forms, guided flows, and granular permissions.</li><li>üîÑ  ‚Äì orchestrate custom data lifecycles and document workflows per tenant or team.</li><li>üßµ  ‚Äì coordinate production management with modular entities, automation hooks, and reporting.</li><li>üåê  ‚Äì expose rich, well-typed APIs for mobile and web apps using the same extensible data model.</li></ul><ul><li>üß©  ‚Äì drop in your own modules, pages, APIs, and entities with auto-discovery and overlay overrides.</li><li>üß¨ <strong>Custom entities &amp; dynamic forms</strong> ‚Äì declare fields, validators, and UI widgets per module and manage them live from the admin.</li><li>üè¢  ‚Äì SaaS-ready tenancy with strict organization/tenant scoping for every entity and API.</li><li>üèõÔ∏è <strong>Multi-hierarchical organizations</strong> ‚Äì built-in organization trees with role- and user-level visibility controls.</li><li>üõ°Ô∏è  ‚Äì combine per-role and per-user feature flags with organization scoping to gate any page or API.</li><li>‚ö°  ‚Äì hybrid JSONB indexing and smart caching for blazing-fast queries across base and custom fields.</li><li>üîî <strong>Event subscribers &amp; workflows</strong> ‚Äì publish domain events and process them via persistent subscribers (local or Redis).</li><li>‚úÖ  ‚Äì expanding unit and integration tests ensure modules stay reliable as you extend them.</li><li>üß†  ‚Äì structured for assistive workflows, automation, and conversational interfaces.</li><li>‚öôÔ∏è  ‚Äì Next.js App Router, TypeScript, zod, Awilix DI, MikroORM, and bcryptjs out of the box.</li></ul><ul><li>üß© Modules: Each feature lives under  with auto‚Äëdiscovered frontend/backend pages, APIs, CLI, i18n, and DB entities.</li><li>üóÉÔ∏è Database: MikroORM with per‚Äëmodule entities and migrations; no global schema. Migrations are generated and applied per module.</li><li>üß∞ Dependency Injection: Awilix container constructed per request. Modules can register and override services/components via .</li><li>üè¢ Multi‚Äëtenant: Core  module defines  and . Most entities carry  + .</li><li>üîê Security: RBAC roles, zod validation, bcryptjs hashing, JWT sessions, role‚Äëbased access in routes and APIs.</li></ul><p>Open Mercato includes a built-in AI Assistant that can discover and interact with your data model and APIs. The assistant uses MCP (Model Context Protocol) to expose tools for schema discovery and API execution.</p><ul><li>üîç  ‚Äì Query database entity schemas including fields, types, and relationships</li><li>üîó  ‚Äì Search for API endpoints using natural language queries</li><li>‚ö°  ‚Äì Execute API calls with automatic tenant context and authentication</li><li>üß†  ‚Äì Uses Meilisearch for fast fulltext + vector search across schemas and endpoints</li></ul><table><tbody><tr><td>Search entity schemas by name or keyword</td></tr><tr><td>Find API endpoints by natural language query</td></tr><tr><td>Execute API calls with tenant context</td></tr><tr><td>Get current authentication context</td></tr></tbody></table><ul><li> () ‚Äì For Claude Code and local development with API key auth</li><li> () ‚Äì For web AI chat with session tokens</li></ul><p>Open Mercato ships with tenant-scoped, field-level data encryption so PII and sensitive business data stay protected while you keep the flexibility of custom entities and fields. Encryption maps live in the admin UI/database, letting you pick which system and custom columns are encrypted; MikroORM hooks automatically encrypt on write and decrypt on read while keeping deterministic hashes (e.g., ) for lookups.</p><p>Architecture in two lines: Vault/KMS (or a derived-key fallback) issues per-tenant DEKs and caches them so performance stays snappy; AES-GCM wrappers sit in the ORM lifecycle, storing ciphertext at rest while CRUD and APIs keep working with plaintext. Read the docs to dive deeper: <a href=\"https://docs.openmercato.com/user-guide/encryption\">docs.openmercato.com/user-guide/encryption</a>.</p><p>We have migrated Open Mercato to a monorepo structure. If you're upgrading from a previous version, please note the following changes:</p><p>The codebase is now organized into:</p><ul><li> - Shared libraries and modules (, , , , , , , , , )</li><li> - Applications (main app in , docs in )</li></ul><p><strong>Important note on storage:</strong> The storage folder has been moved to the  folder as well. If you instance has got any attachments uploaded, please make sure you run:</p><pre><code>mv storage apps/mercato/storage\n</code></pre><p>... from the root Open Mercato folder.</p><p>Import aliases have changed from path-based to package-based imports:</p><ul><li>, , </li><li><code>@open-mercato/shared/lib/...</code>, <code>@open-mercato/ui/components/...</code>, <code>@open-mercato/core/modules/...</code>, etc.</li></ul><p>The  file now must live in  instead of the project root. The fastest way to start is to copy the example file:</p><pre><code>cp apps/mercato/.env.example apps/mercato/.env\n</code></pre><p>At minimum, set , , and  (or ) before bootstrapping.</p><p>Yarn 4 is now required. Ensure you have Yarn 4+ installed before proceeding.</p><p>This is a quickest way to get Open Mercato up and running on your localhost / server - ready for testing / demoing or for !</p><pre><code># macOS (Homebrew)\nbrew install node@24\n\n# Windows (Chocolatey)\nchoco install nodejs --version=24.x\n\n# Or use nvm (any platform)\nnvm install 24\nnvm use 24\n</code></pre><pre><code>git clone https://github.com/open-mercato/open-mercato.git\ncd open-mercato\ngit checkout develop\nyarn install\n\ncp apps/mercato/.env.example apps/mercato/.env # EDIT this file to set up your specific files\n#At minimum, set `DATABASE_URL`, `JWT_SECRET`, and `REDIS_URL` (or `EVENTS_REDIS_URL`) before bootstrapping.\n\nyarn generate\nyarn initialize # or yarn reinstall\nyarn dev\n</code></pre><p>For a fresh greenfield boot (build packages, generate registries, reinstall modules, then start dev), run:</p><p>Navigate to <code>http://localhost:3000/backend</code> and sign in with the default credentials printed by .</p><p>Open Mercato offers two Docker Compose configurations ‚Äî one for  (with hot reload) and one for . Both run the full stack (app + PostgreSQL + Redis + Meilisearch) in containers. The dev mode is the <strong>recommended setup for Windows</strong> users.</p><p>Run the entire stack with source code mounted from the host. File changes trigger automatic rebuilds ‚Äî no local Node.js or Yarn required.</p><pre><code>git clone https://github.com/open-mercato/open-mercato.git\ncd open-mercato\ngit checkout develop\ndocker compose -f docker-compose.fullapp.dev.yml up --build\n</code></pre><p> Ensure WSL 2 backend is enabled in Docker Desktop and clone with <code>git config --global core.autocrlf input</code> to avoid line-ending issues.</p><pre><code>docker compose -f docker-compose.fullapp.yml up --build\n</code></pre><ul><li>Start: <code>docker compose -f docker-compose.fullapp.yml up -d</code></li><li>Logs: <code>docker compose -f docker-compose.fullapp.yml logs -f app</code></li><li>Stop: <code>docker compose -f docker-compose.fullapp.yml down</code></li><li>Rebuild: <code>docker compose -f docker-compose.fullapp.yml up --build</code></li></ul><p>Navigate to <code>http://localhost:3000/backend</code> and sign in with the default credentials (<a href=\"mailto:admin@example.com\">admin@example.com</a>).</p><h3>Docker Environment Variables</h3><p>Before starting, you may want to configure the following environment variables. Create a  file in the project root or export them in your shell:</p><table><thead><tr></tr></thead><tbody><tr><td>Secret key for JWT token signing. <strong>Use a strong, unique value in production.</strong></td></tr><tr><td>PostgreSQL database password. <strong>Use a strong password in production.</strong></td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>Meilisearch API key. <strong>Use a strong key in production.</strong></td></tr><tr></tr><tr><td>OpenAI API key (enables AI features)</td></tr><tr><td>Anthropic API key (for opencode service)</td></tr><tr><td>Opencode service exposed port</td></tr></tbody></table><p>Example  file for production:</p><pre><code>JWT_SECRET=your-strong-secret-key-here\nPOSTGRES_PASSWORD=your-strong-db-password\nMEILISEARCH_MASTER_KEY=your-strong-meilisearch-key\nOPENAI_API_KEY=sk-...  # Optional, for AI features\n</code></pre><p>For production deployments, ensure strong , secure database credentials, and consider managed database services. See the <a href=\"https://docs.openmercato.com/installation/setup#docker-deployment-full-stack\">full Docker deployment guide</a> for detailed configuration and production tips.</p><h2>Standalone App &amp; Customization</h2><p>The <strong>recommended way to build on Open Mercato</strong> without modifying the core is to create a standalone app. This gives you a self-contained project that pulls Open Mercato packages from npm ‚Äî your own modules, overrides, and customizations live in your repo while core stays untouched and upgradeable.</p><pre><code>npx create-mercato-app my-store\ncd my-store\ncp .env.example .env   # configure DATABASE_URL, JWT_SECRET, REDIS_URL\ndocker compose up -d   # start PostgreSQL, Redis, Meilisearch\nyarn install\nyarn initialize\nyarn dev\n</code></pre><p>Navigate to <code>http://localhost:3000/backend</code> and sign in with the credentials printed by .</p><p>Drop your own modules into  and register them in  with :</p><pre><code>export const enabledModules: ModuleEntry[] = [\n  // ... core modules\n  { id: 'inventory', from: '@app' },\n]\n</code></pre><p>Run  and  ‚Äî your module's pages, APIs, and entities are auto-discovered.</p><h3>Eject core modules for deep customization</h3><p>When you need to change the internals of a core module (entities, business logic, UI),  it. The  command copies the module source into your  directory and switches it to local, so you can modify it freely while all other modules keep receiving package updates.</p><pre><code># See which modules support ejection\nyarn mercato eject --list\n\n# Eject a module (e.g., currencies)\nyarn mercato eject currencies\nyarn mercato generate all\nyarn dev\n</code></pre><p>Currently ejectable: , , , , , , , , .</p><p>Open Mercato follows a <strong>spec-first development approach</strong>. Before implementing new features or making significant changes, we document the design in the  folder.</p><ul><li>: Specs ensure everyone understands the feature before coding starts</li><li>: Design decisions are documented and can be referenced by humans and AI agents</li><li>: Each spec maintains a changelog tracking the evolution of the feature</li></ul><ol><li>: Check if a spec exists in  (named <code>SPEC-###-YYYY-MM-DD-title.md</code>)</li><li>: Create or update the spec with your design before implementation</li><li>: Update the spec's changelog with a dated summary</li></ol><p>: Specs use the format <code>SPEC-{number}-{date}-{title}.md</code> (e.g., <code>SPEC-007-2026-01-26-sidebar-reorganization.md</code>)</p><p>We welcome contributions of all sizes‚Äîfrom fixes and docs updates to new modules. Start by reading <a href=\"https://raw.githubusercontent.com/open-mercato/open-mercato/main/CONTRIBUTING.md\">CONTRIBUTING.md</a> for branching conventions (, , ), release flow, and the full PR checklist. Then check the open issues or propose an idea in a discussion, and:</p><ol><li>Fork the repository and create a branch that reflects your change.</li><li>Install dependencies with  and bootstrap via  (add  to skip demo CRM content;  for thousands of synthetic contacts, companies, deals, and timeline interactions; or  for high-volume contacts without the heavier extras).</li><li>Develop and validate your changes (, , or the relevant module scripts).</li><li>Open a pull request referencing any related issues and outlining the testing you performed.</li></ol><p>Refer to <a href=\"https://raw.githubusercontent.com/open-mercato/open-mercato/main/AGENTS.md\">AGENTS.md</a> for deeper guidance on architecture and conventions when extending modules.</p><p>Open Mercato let the module developers to expose the custom CLI commands for variouse maintenance tasks. Read more on the <a href=\"https://docs.openmercato.com/cli/overview\">CLI documentation</a></p><ul><li>MIT ‚Äî see  for details.</li></ul>",
      "contentLength": 11363,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "freemocap/freemocap",
      "url": "https://github.com/freemocap/freemocap",
      "date": 1771556616,
      "author": "",
      "guid": 46701,
      "unread": true,
      "content": "<p>Free Motion Capture for Everyone üíÄ‚ú®</p><h4 align=\"center\"> A free-and-open-source, hardware-and-software-agnostic, minimal-cost, research-grade, motion capture system and platform for decentralized scientific research, education, and training</h4><h4>0. Create a a Python 3.10 through 3.12 environment (python3.12 recommended)</h4><h4>1. Install software via <a href=\"https://pypi.org/project/freemocap/#description\">pip</a>:</h4><h4>2. Launch the GUI by entering the command:</h4><h4>3. A GUI should pop up that looks like this:</h4><img width=\"1457\" alt=\"image\" src=\"https://github.com/freemocap/freemocap/assets/15314521/90ef7e7b-48f3-4f46-8d4a-5b5bcc3254b3\"><h2>Install/run from source code (i.e. the code in this repo)</h2><ol><li>Create a  environment (Recommended version is )</li></ol><pre><code>conda create -n freemocap-env python=3.11\n</code></pre><ol start=\"2\"><li>Activate that newly created environment</li></ol><pre><code>conda activate freemocap-env\n</code></pre><pre><code>git clone https://github.com/freemocap/freemocap\n</code></pre><ol start=\"4\"><li>Navigate into the newly cloned/downloaded  folder</li></ol><ol start=\"5\"><li>Install the package via the  file</li></ol><ol start=\"6\"><li>Launch the GUI (via the  entry point)</li></ol><p>This project is licensed under the APGL License - see the <a href=\"https://raw.githubusercontent.com/freemocap/freemocap/main/LICENSE\">LICENSE</a> file for details.</p><p>If the AGPL does not work for your needs, we are happy to discuss terms to license this software to you with a different agreement at a price point that increases exponentially as you move <a href=\"https://www.gnu.org/philosophy/open-source-misses-the-point.en.html\">spiritually</a> away from the </p>",
      "contentLength": 1093,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "RichardAtCT/claude-code-telegram",
      "url": "https://github.com/RichardAtCT/claude-code-telegram",
      "date": 1771556616,
      "author": "",
      "guid": 46702,
      "unread": true,
      "content": "<p>A powerful Telegram bot that provides remote access to Claude Code, enabling developers to interact with their projects from anywhere with full AI assistance and session persistence.</p><p>A Telegram bot that gives you remote access to <a href=\"https://claude.ai/code\">Claude Code</a>. Chat naturally with Claude about your projects from anywhere -- no terminal commands needed.</p><p>This bot connects Telegram to Claude Code, providing a conversational AI interface for your codebase:</p><ul><li> -- ask Claude to analyze, edit, or explain your code in plain language</li><li> across conversations with automatic session persistence per project</li><li> from any device with Telegram</li><li><strong>Receive proactive notifications</strong> from webhooks, scheduled jobs, and CI/CD events</li><li> with built-in authentication, directory sandboxing, and audit logging</li></ul><pre><code>You: Can you help me add error handling to src/api.py?\n\nBot: I'll analyze src/api.py and add error handling...\n     [Claude reads your code, suggests improvements, and can apply changes directly]\n\nYou: Looks good. Now run the tests to make sure nothing broke.\n\nBot: Running pytest...\n     All 47 tests passed. The error handling changes are working correctly.\n</code></pre><p>Choose your preferred method:</p><h4>Option A: Install from a release tag (Recommended)</h4><pre><code># Using uv (recommended ‚Äî installs in an isolated environment)\nuv tool install git+https://github.com/RichardAtCT/claude-code-telegram@v1.3.0\n\n# Or using pip\npip install git+https://github.com/RichardAtCT/claude-code-telegram@v1.3.0\n\n# Track the latest stable release\npip install git+https://github.com/RichardAtCT/claude-code-telegram@latest\n</code></pre><h4>Option B: From source (for development)</h4><pre><code>git clone https://github.com/RichardAtCT/claude-code-telegram.git\ncd claude-code-telegram\nmake dev  # requires Poetry\n</code></pre><blockquote><p> Always install from a tagged release (not ) for stability. See <a href=\"https://github.com/RichardAtCT/claude-code-telegram/releases\">Releases</a> for available versions.</p></blockquote><pre><code>cp .env.example .env\n# Edit .env with your settings:\n</code></pre><pre><code>TELEGRAM_BOT_TOKEN=1234567890:ABC-DEF1234ghIkl-zyx57W2v1u123ew11\nTELEGRAM_BOT_USERNAME=my_claude_bot\nAPPROVED_DIRECTORY=/Users/yourname/projects\nALLOWED_USERS=123456789  # Your Telegram user ID\n</code></pre><pre><code>make run          # Production\nmake run-debug    # With debug logging\n</code></pre><p>Message your bot on Telegram to get started.</p><blockquote><p> See <a href=\"https://raw.githubusercontent.com/RichardAtCT/claude-code-telegram/main/docs/setup.md\">docs/setup.md</a> for Claude authentication options and troubleshooting.</p></blockquote><p>The bot supports two interaction modes:</p><p>The default conversational mode. Just talk to Claude naturally -- no special commands required.</p><p>, , , ,  If <code>ENABLE_PROJECT_THREADS=true</code>: </p><pre><code>You: What files are in this project?\nBot: Working... (3s)\n     üìñ Read\n     üìÇ LS\n     üí¨ Let me describe the project structure\nBot: [Claude describes the project structure]\n\nYou: Add a retry decorator to the HTTP client\nBot: Working... (8s)\n     üìñ Read: http_client.py\n     üí¨ I'll add a retry decorator with exponential backoff\n     ‚úèÔ∏è Edit: http_client.py\n     üíª Bash: poetry run pytest tests/ -v\nBot: [Claude shows the changes and test results]\n\nYou: /verbose 0\nBot: Verbosity set to 0 (quiet)\n</code></pre><p>Use  to control how much background activity is shown:</p><table><tbody><tr><td>Final response only (typing indicator stays active)</td></tr><tr><td>Tool names + reasoning snippets in real-time</td></tr><tr><td>Tool names with inputs + longer reasoning text</td></tr></tbody></table><p>Claude Code already knows how to use  CLI and . Authenticate on your server with , then work with repos conversationally:</p><pre><code>You: List my repos related to monitoring\nBot: [Claude runs gh repo list, shows results]\n\nYou: Clone the uptime one\nBot: [Claude runs gh repo clone, clones into workspace]\n\nYou: /repo\nBot: üì¶ uptime-monitor/  ‚óÄ\n     üìÅ other-project/\n\nYou: Show me the open issues\nBot: [Claude runs gh issue list]\n\nYou: Create a fix branch and push it\nBot: [Claude creates branch, commits, pushes]\n</code></pre><p>Use  to list cloned repos in your workspace, or  to switch directories (sessions auto-resume).</p><p>Set  to enable the full 13-command terminal-like interface with directory navigation, inline keyboards, quick actions, git integration, and session export.</p><p>, , , , , , , , , , , ,  If <code>ENABLE_PROJECT_THREADS=true</code>: </p><pre><code>You: /cd my-web-app\nBot: Directory changed to my-web-app/\n\nYou: /ls\nBot: src/  tests/  package.json  README.md\n\nYou: /actions\nBot: [Run Tests] [Install Deps] [Format Code] [Run Linter]\n</code></pre><p>Beyond direct chat, the bot can respond to external triggers:</p><ul><li> -- Receive GitHub events (push, PR, issues) and route them through Claude for automated summaries or code review</li><li> -- Run recurring Claude tasks on a cron schedule (e.g., daily code health checks)</li><li> -- Deliver agent responses to configured Telegram chats</li></ul><p>Enable with  and . See <a href=\"https://raw.githubusercontent.com/RichardAtCT/claude-code-telegram/main/docs/setup.md\">docs/setup.md</a> for configuration.</p><ul><li><p>Conversational agentic mode (default) with natural language interaction</p></li><li><p>Classic terminal-like mode with 13 commands and inline keyboards</p></li><li><p>Full Claude Code integration with SDK (primary) and CLI (fallback)</p></li><li><p>Automatic session persistence per user/project directory</p></li><li><p>Multi-layer authentication (whitelist + optional token-based)</p></li><li><p>Rate limiting with token bucket algorithm</p></li><li><p>Directory sandboxing with path traversal prevention</p></li><li><p>File upload handling with archive extraction</p></li><li><p>Image/screenshot upload with analysis</p></li><li><p>Git integration with safe repository operations</p></li><li><p>Quick actions system with context-aware buttons</p></li><li><p>Session export in Markdown, HTML, and JSON formats</p></li><li><p>SQLite persistence with migrations</p></li><li><p>Audit logging and security event tracking</p></li><li><p>Event bus for decoupled message routing</p></li><li><p>Webhook API server (GitHub HMAC-SHA256, generic Bearer token auth)</p></li><li><p>Job scheduler with cron expressions and persistent storage</p></li><li><p>Notification service with per-chat rate limiting</p></li><li><p>Tunable verbose output showing Claude's tool usage and reasoning in real-time</p></li><li><p>Persistent typing indicator so users always know the bot is working</p></li><li><p>16 configurable tools with allowlist/disallowlist control (see <a href=\"https://raw.githubusercontent.com/RichardAtCT/claude-code-telegram/main/docs/tools.md\">docs/tools.md</a>)</p></li></ul><ul><li>Plugin system for third-party extensions</li></ul><pre><code>TELEGRAM_BOT_TOKEN=...           # From @BotFather\nTELEGRAM_BOT_USERNAME=...        # Your bot's username\nAPPROVED_DIRECTORY=...           # Base directory for project access\nALLOWED_USERS=123456789          # Comma-separated Telegram user IDs\n</code></pre><pre><code># Claude\nANTHROPIC_API_KEY=sk-ant-...     # API key (optional if using CLI auth)\nCLAUDE_MAX_COST_PER_USER=10.0    # Spending limit per user (USD)\nCLAUDE_TIMEOUT_SECONDS=300       # Operation timeout\n\n# Mode\nAGENTIC_MODE=true                # Agentic (default) or classic mode\nVERBOSE_LEVEL=1                  # 0=quiet, 1=normal (default), 2=detailed\n\n# Rate Limiting\nRATE_LIMIT_REQUESTS=10           # Requests per window\nRATE_LIMIT_WINDOW=60             # Window in seconds\n\n# Features (classic mode)\nENABLE_GIT_INTEGRATION=true\nENABLE_FILE_UPLOADS=true\nENABLE_QUICK_ACTIONS=true\n</code></pre><pre><code># Webhook API Server\nENABLE_API_SERVER=false          # Enable FastAPI webhook server\nAPI_SERVER_PORT=8080             # Server port\n\n# Webhook Authentication\nGITHUB_WEBHOOK_SECRET=...        # GitHub HMAC-SHA256 secret\nWEBHOOK_API_SECRET=...           # Bearer token for generic providers\n\n# Scheduler\nENABLE_SCHEDULER=false           # Enable cron job scheduler\n\n# Notifications\nNOTIFICATION_CHAT_IDS=123,456    # Default chat IDs for proactive notifications\n</code></pre><pre><code># Enable strict topic routing by project\nENABLE_PROJECT_THREADS=true\n\n# Mode: private (default) or group\nPROJECT_THREADS_MODE=private\n\n# YAML registry file (see config/projects.example.yaml)\nPROJECTS_CONFIG_PATH=config/projects.yaml\n\n# Required only when PROJECT_THREADS_MODE=group\nPROJECT_THREADS_CHAT_ID=-1001234567890\n\n# Minimum delay (seconds) between Telegram API calls during topic sync\n# Set 0 to disable pacing\nPROJECT_THREADS_SYNC_ACTION_INTERVAL_SECONDS=1.1\n</code></pre><p>In strict mode, only  and  work outside mapped project topics. In private mode,  auto-syncs project topics for your private bot chat. To use topics with your bot, enable them in BotFather: <code>Bot Settings -&gt; Threaded mode</code>.</p><h3>Finding Your Telegram User ID</h3><p>Message <a href=\"https://t.me/userinfobot\">@userinfobot</a> on Telegram -- it will reply with your user ID number.</p><ul><li>Check your  is correct</li><li>Verify your user ID is in </li><li>Ensure Claude Code CLI is installed and accessible</li><li>Check bot logs with </li></ul><p><strong>Claude integration not working:</strong></p><ul><li>SDK mode (default): Check  or verify </li><li>CLI mode: Verify  and </li><li>Check  includes necessary tools (see <a href=\"https://raw.githubusercontent.com/RichardAtCT/claude-code-telegram/main/docs/tools.md\">docs/tools.md</a> for the full reference)</li></ul><ul><li>Adjust  to set spending limits</li><li>Monitor usage with </li><li>Use shorter, more focused requests</li></ul><p>This bot implements defense-in-depth security:</p><ul><li> -- Whitelist-based user authentication</li><li> -- Sandboxing to approved directories</li><li> -- Request and cost-based limits</li><li> -- Injection and path traversal protection</li><li> -- GitHub HMAC-SHA256 and Bearer token verification</li><li> -- Complete tracking of all user actions</li></ul><pre><code>make dev           # Install all dependencies\nmake test          # Run tests with coverage\nmake lint          # Black + isort + flake8 + mypy\nmake format        # Auto-format code\nmake run-debug     # Run with debug logging\n</code></pre><p>The version is defined once in  and read at runtime via . To cut a release:</p><pre><code>make bump-patch    # 1.2.0 -&gt; 1.2.1 (bug fixes)\nmake bump-minor    # 1.2.0 -&gt; 1.3.0 (new features)\nmake bump-major    # 1.2.0 -&gt; 2.0.0 (breaking changes)\n</code></pre><p>Each command commits, tags, and pushes automatically, triggering CI tests and a GitHub Release with auto-generated notes.</p><ol><li>Create a feature branch: <code>git checkout -b feature/amazing-feature</code></li><li>Make changes with tests: </li></ol><p> Python 3.11+, Black formatting (88 chars), type hints required, pytest with &gt;85% coverage.</p>",
      "contentLength": 9146,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "p2r3/convert",
      "url": "https://github.com/p2r3/convert",
      "date": 1771470321,
      "author": "",
      "guid": 46303,
      "unread": true,
      "content": "<p>Truly universal online file converter</p><p><strong>Truly universal online file converter.</strong></p><p>Many online file conversion tools are  and . They only allow conversion between two formats in the same medium (images to images, videos to videos, etc.), and they require that you <em>upload your files to some server</em>.</p><p>This is not just terrible for privacy, it's also incredibly lame. What if you  need to convert an AVI video to a PDF document? Try to find an online tool for that, I dare you.</p><p><a href=\"https://convert.to.it/\">Convert.to.it</a> aims to be a tool that \"just works\". You're almost  to get an output - perhaps not always the one you expected, but it'll try its best to not leave you hanging.</p><ol><li>Click the big blue box to add your file (or just drag it on to the window).</li><li>An input format should have been automatically selected. If it wasn't, yikes! Try searching for it, or if it's really not there, see the \"Issues\" section below.</li><li>Select an output format from the second list. If you're on desktop, that's the one on the right side. If you're on mobile, it'll be somewhere lower down.</li><li>Hopefully, after a bit (or a lot) of thinking, the program will spit out the file you wanted. If not, see the \"Issues\" section below.</li></ol><p>Ever since the YouTube video released, we've been getting spammed with issues suggesting the addition of all kinds of niche file formats. To keep things organized, I've decided to specify what counts as a valid issue and what doesn't.</p><blockquote><p>[!IMPORTANT] <strong>SIMPLY ASKING FOR A FILE FORMAT TO BE ADDED IS NOT A MEANINGFUL ISSUE!</strong></p></blockquote><p>There are thousands of file formats out there. It can take hours to add support for just one. The math is simple - we can't possibly support every single file. As such, simply listing your favorite file formats is not helpful. We already know that there are formats we don't support, we don't need tickets to tell us that.</p><p>When suggesting a file format, you must :</p><ul><li>Make sure that there isn't already an issue about the same thing, and that we don't already support the format.</li><li>Explain what you expect the conversion to be like (what medium is it converting to/from). It's important to note here that simply parsing the underlying data is . Imagine if we only treated SVG images as raw XML data and didn't support converting them to raster images - that would defeat the point.</li><li>Provide links to existing browser-based solutions if possible, or at the very least a reference for implementing the format, and make sure the license is compatible with GPL-2.0.</li></ul><p>If this seems like a lot, please remember - a developer will have to do 100x more work to actually implement the format. Doing a bit of research not only saves them precious time, it also weeds out \"unserious\" proposals that would only bloat our to-do list.</p><p><strong>If you're submitting a bug report,</strong> you only need to do step 1 - check if the problem isn't already reported by someone else. Bug reports are generally quite important otherwise.</p><p>Though please note, \"converting X to Y doesn't work\" is  a bug report. However, \"converting X to Y works but not how I expected\" likely  a bug report.</p><h3>Local development (Bun + Vite)</h3><ol><li>Clone this repository . You can use <code>git clone --recursive https://github.com/p2r3/convert</code> for that. Omitting submodules will leave you missing a few dependencies.</li><li>Run  to install dependencies.</li><li>Run  to start the development server.</li></ol><p><em>The following steps are optional, but recommended for performance:</em></p><p>When you first open the page, it'll take a while to generate the list of supported formats for each tool. If you open the console, you'll see it complaining a bunch about missing caches.</p><p>After this is done (indicated by a <code>Built initial format list</code> message in the console), use <code>printSupportedFormatCache()</code> to get a JSON string with the cache data. You can then save this string to  to skip that loading screen on startup.</p><p>Docker compose files live in the  directory, so run compose with  from the repository root:</p><pre><code>docker compose -f docker/docker-compose.yml up -d\n</code></pre><p>Alternatively download the  separately and start it by executing  in the same directory.</p><p>This runs the container on <code>http://localhost:8080/convert/</code>.</p><h3>Docker (local build for development)</h3><p>Use the override file to build the image locally:</p><pre><code>docker compose -f docker/docker-compose.yml -f docker/docker-compose.override.yml up --build -d\n</code></pre><p>The first Docker build is expected to be slow because Chromium and related system packages are installed in the build stage (needed for puppeteer in ). Later builds are usually much faster due to Docker layer caching.</p><p>The best way to contribute is by adding support for new file formats (duh). Here's how that works:</p><p>Each \"tool\" used for conversion has to be normalized to a standard form - effectively a \"wrapper\" that abstracts away the internal processes. These wrappers are available in <a href=\"https://raw.githubusercontent.com/p2r3/convert/master/src/handlers/\">src/handlers</a>.</p><p>Below is a super barebones handler that does absolutely nothing. You can use this as a starting point for adding a new format:</p><pre><code>// file: dummy.ts\n\nimport type { FileData, FileFormat, FormatHandler } from \"../FormatHandler.ts\";\nimport CommonFormats from \"src/CommonFormats.ts\";\n\nclass dummyHandler implements FormatHandler {\n\n  public name: string = \"dummy\";\n  public supportedFormats?: FileFormat[];\n  public ready: boolean = false;\n\n  async init () {\n    this.supportedFormats = [\n      // Example PNG format, with both input and output disabled\n      CommonFormats.PNG.builder(\"png\")\n        .markLossless()\n        // .allowFrom()\n        // .allowTo()\n    ];\n    this.ready = true;\n  }\n\n  async doConvert (\n    inputFiles: FileData[],\n    inputFormat: FileFormat,\n    outputFormat: FileFormat\n  ): Promise&lt;FileData[]&gt; {\n    const outputFiles: FileData[] = [];\n    return outputFiles;\n  }\n\n}\n\nexport default dummyHandler;\n</code></pre><p>For more details on how all of these components work, refer to the doc comments in <a href=\"https://raw.githubusercontent.com/p2r3/convert/master/src/FormatHandler.ts\">src/FormatHandler.ts</a>. You can also take a look at existing handlers to get a more practical example.</p><p>There are a few additional things that I want to point out in particular:</p><ul><li>Pay attention to the naming system. If your tool is called , then the class should be called , and the file should be called .</li><li>The handler is responsible for setting the output file's name. This is done to allow for flexibility in rare cases where the  file name matters. Of course, in most cases, you'll only have to swap the file extension.</li><li>The handler is also responsible for ensuring that any byte buffers that enter or exit the handler . If necessary, clone the buffer by wrapping it in .</li><li>When handling MIME types, run them through <a href=\"https://raw.githubusercontent.com/p2r3/convert/master/src/normalizeMimeType.ts\">normalizeMimeType</a> first. One file can have multiple valid MIME types, which isn't great when you're trying to match them algorithmically.</li><li>When implementing a new file format, please treat the file as the media that it represents, not the data that it contains. For example, if you were making an SVG handler, you should treat the file as an , not as XML.</li></ul><p>If your tool requires an external dependency (which it likely does), there are currently two well-established ways of going about this:</p><ul><li>If it's an  package, just install it to the project like you normally would.</li><li>If it's a Git repository, add it as a submodule to <a href=\"https://raw.githubusercontent.com/p2r3/convert/master/src/handlers\">src/handlers</a>.</li></ul><p><strong>Please try to avoid CDNs (Content Delivery Networks).</strong> They're really cool on paper, but they don't work well with TypeScript, and each one introduces a tiny bit of instability. For a project that leans heavily on external dependencies, those bits of instability can add up fast.</p><ul><li>If you need to load a WebAssembly binary (or similar), add its path to <a href=\"https://raw.githubusercontent.com/p2r3/convert/master/vite.config.js\">vite.config.js</a> and target it under . <strong>Do not link to node_modules</strong>.</li></ul>",
      "contentLength": 7479,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "harvard-edge/cs249r_book",
      "url": "https://github.com/harvard-edge/cs249r_book",
      "date": 1771470321,
      "author": "",
      "guid": 46304,
      "unread": true,
      "content": "<p>Introduction to Machine Learning Systems</p><p><em>Principles and Practices of Engineering Artificially Intelligent Systems</em></p><p><strong>The world is rushing to build AI systems. It is not engineering them.</strong></p><p>That gap is what we mean by AI engineering.</p><p><strong>AI engineering is the discipline of building efficient, reliable, safe, and robust intelligent systems that operate in the real world, not just models in isolation.</strong></p><p> Establish AI engineering as a foundational discipline, alongside software engineering and computer engineering, by teaching how to design, build, and evaluate end to end intelligent systems. The long term impact of AI will be shaped by engineers who can turn ideas into working, dependable systems.</p><p>This repository is the open learning stack for AI systems engineering.</p><p>It includes the textbook source, TinyTorch, hardware kits, and upcoming co-labs that connect principles to runnable code and real devices.</p><p>Choose a path based on your goal.</p><p> Start TinyTorch with the <a href=\"https://mlsysbook.ai/tinytorch/getting-started.html\">getting started guide</a>. Begin with Module 01 and work up from CNNs to transformers and the MLPerf benchmarks.</p><p> Pick a <a href=\"https://mlsysbook.ai/kits\">hardware kit</a> and run the labs on Arduino, Raspberry Pi, and other edge devices.</p><p> Say hello in <a href=\"https://github.com/harvard-edge/cs249r_book/discussions\">Discussions</a>. We will do our best to reply.</p><p>The learning stack below shows how the textbook connects to hands on work and deployment. Read the textbook, then pick your path:</p><pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                                                                               ‚îÇ\n‚îÇ                           MACHINE LEARNING SYSTEMS                            ‚îÇ\n‚îÇ                              Read the Textbook                                ‚îÇ\n‚îÇ                                                                               ‚îÇ\n‚îÇ                    Theory ‚Ä¢ Concepts ‚Ä¢ Best Practices                         ‚îÇ\n‚îÇ                                                                               ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                        ‚îÇ\n                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                          ‚îÇ             ‚îÇ             ‚îÇ\n                          ‚ñº             ‚ñº             ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                            HANDS-ON ACTIVITIES                                ‚îÇ\n‚îÇ                           (pick one or all)                                   ‚îÇ\n‚îÇ                                                                               ‚îÇ\n‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ\n‚îÇ     ‚îÇ    SOFTWARE     ‚îÇ      ‚îÇ    TINYTORCH    ‚îÇ      ‚îÇ    HARDWARE     ‚îÇ     ‚îÇ\n‚îÇ     ‚îÇ    CO-LABS      ‚îÇ      ‚îÇ    FRAMEWORK    ‚îÇ      ‚îÇ      LABS       ‚îÇ     ‚îÇ\n‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ\n‚îÇ     ‚îÇ EXPLORE         ‚îÇ      ‚îÇ BUILD           ‚îÇ      ‚îÇ DEPLOY          ‚îÇ     ‚îÇ\n‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ\n‚îÇ     ‚îÇ Run controlled  ‚îÇ      ‚îÇ Understand      ‚îÇ      ‚îÇ Engineer under  ‚îÇ     ‚îÇ\n‚îÇ     ‚îÇ experiments on  ‚îÇ      ‚îÇ frameworks by   ‚îÇ      ‚îÇ real constraints‚îÇ     ‚îÇ\n‚îÇ     ‚îÇ latency, memory,‚îÇ      ‚îÇ implementing    ‚îÇ      ‚îÇ memory, power,  ‚îÇ     ‚îÇ\n‚îÇ     ‚îÇ energy, cost    ‚îÇ      ‚îÇ them            ‚îÇ      ‚îÇ timing, safety  ‚îÇ     ‚îÇ\n‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ\n‚îÇ     ‚îÇ (coming 2026)   ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ Arduino, Pi     ‚îÇ     ‚îÇ\n‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n‚îÇ                                                                               ‚îÇ\n‚îÇ           EXPLORE                  BUILD                   DEPLOY             ‚îÇ\n‚îÇ                                                                               ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                        ‚îÇ\n                                        ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                                                                               ‚îÇ\n‚îÇ                                  AI OLYMPICS                                  ‚îÇ\n‚îÇ                                 Prove Mastery                                 ‚îÇ\n‚îÇ                                                                               ‚îÇ\n‚îÇ       Compete across all tracks ‚Ä¢ University teams ‚Ä¢ Public leaderboards      ‚îÇ\n‚îÇ                                                                               ‚îÇ\n‚îÇ                                (coming 2026)                                  ‚îÇ\n‚îÇ                                                                               ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</code></pre><table><tbody><tr><td>Run controlled experiments on latency, memory, energy, cost</td></tr><tr><td>Compete and benchmark across all tracks</td></tr></tbody></table><ul><li> teaches  ‚Äî Understand tradeoffs. Change batch sizes, precision, model architectures and see how latency, memory, and accuracy shift.</li><li> teaches  ‚Äî Understand internals. Implement autograd, optimizers, and attention from scratch to see how TensorFlow and PyTorch actually work.</li><li> teaches  ‚Äî Understand constraints. Face real memory limits, power budgets, and latency requirements on actual hardware.</li></ul><p>This textbook teaches you to think at the intersection of machine learning and systems engineering. Each chapter bridges algorithmic concepts with the infrastructure that makes them work in practice.</p><table><thead><tr></tr></thead><tbody><tr><td>How to fit large models on resource-limited devices</td></tr><tr><td>How GPUs, TPUs, and accelerators execute neural networks</td></tr><tr><td>How mixed-precision and optimization techniques reduce cost</td></tr><tr><td>How to compress models while preserving performance</td></tr><tr><td>How to build efficient data loading and preprocessing</td></tr><tr><td>How to monitor, version, and update models in production</td></tr><tr><td>How to train and adapt models without sending data to the cloud</td></tr></tbody></table><table><tbody><tr><td>Introduction, ML Systems, DL Primer, Architectures</td></tr><tr><td>Workflow, Data Engineering, Frameworks, Training</td></tr><tr><td>Efficient AI, Optimizations, HW Acceleration, Benchmarking</td></tr><tr><td>MLOps, On-device Learning, Privacy, Robustness</td></tr><tr><td>Responsible AI, Sustainable AI, AI for Good</td></tr><tr><td>Emerging trends and future directions</td></tr></tbody></table><h2>What Makes This Different</h2><p>This is a living textbook. We keep it updated as the field grows, with community input along the way.</p><p>AI may feel like it is moving at lightning speed, but the engineering building blocks that make it work do not change as quickly as the headlines. This project is built around those stable foundations.</p><p>Think of it like LEGO. New sets arrive all the time, but the bricks themselves stay the same. Once you learn how the bricks fit together, you can build anything. Here, those \"AI bricks\" are the solid systems principles that make AI work.</p><p>Whether you are reading a chapter, running a lab, or sharing feedback, you are helping make these ideas more accessible to the next learner.</p><h3>Research to Teaching Loop</h3><p>We use the same loop for research and teaching: define the system problem, build a reference implementation, benchmark it, then turn it into curriculum and tooling so others can reproduce and extend it.</p><table><thead><tr></tr></thead><tbody><tr><td>Benchmarks, suites, metrics</td><td>Benchmarking chapter, assignments</td></tr><tr><td>Reference systems, compilers, runtimes</td><td>TinyTorch modules, co-labs</td></tr><tr><td>Hardware targets, constraints, reliability</td></tr></tbody></table><p>We are working toward <strong>1 million learners by 2030</strong> so that AI engineering becomes a shared, teachable discipline, not a collection of isolated practices. Every star, share, and contribution helps move this effort forward.</p><div align=\"center\"><p><em>What gets measured gets improved.</em></p><p>Each star is a learner, educator, or supporter who believes AI systems should be engineered with rigor and real world constraints in mind.</p><p>1 learner ‚Üí 10 learners ‚Üí 100 learners ‚Üí 1,000 learners ‚Üí  ‚Üí 100,000 learners ‚Üí </p></div><p>Stars are not the goal. They are a signal.</p><p>A visible, growing community makes it easier for universities, foundations, and industry partners to adopt this material, donate hardware, and fund workshops. That momentum lowers the barrier for the next institution, the next classroom, and the next cohort of learners.</p><p>Support raised through this signal flows into <a href=\"https://opencollective.com/mlsysbook\">Open Collective</a> and funds concrete outcomes such as TinyML4D workshops, hardware kits for underserved classrooms, and the infrastructure required to keep this resource free and open.</p><p>One click can unlock the next classroom, the next contributor, and the next generation of AI engineers.</p><div align=\"center\"><p>All contributions go to <a href=\"https://opencollective.com/mlsysbook\">Open Collective</a>, a transparent fund that supports educational outreach.</p></div><p>We welcome contributions to the book, TinyTorch, and hardware kits!</p><pre><code>@inproceedings{reddi2024mlsysbook,\n  title        = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering},\n  author       = {Reddi, Vijay Janapa},\n  booktitle    = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)},\n  pages        = {41--42},\n  year         = {2024},\n  organization = {IEEE},\n  url          = {https://mlsysbook.org}\n}\n</code></pre><p>This project uses a dual-license structure:</p><table><thead><tr></tr></thead><tbody><tr><td>Share freely with attribution; no commercial use; no derivatives</td></tr><tr><td>Use, modify, and distribute freely; includes patent protection</td></tr></tbody></table><p>The textbook content (chapters, figures, explanations) is educational material that should circulate with attribution and without commercial exploitation. The software framework is a tool designed to be easy for anyone to use, modify, or integrate into their own projects.</p><p>Thanks goes to these wonderful people who have contributed to making this resource better for everyone!</p><p> ü™≤ Bug Hunter ¬∑ üßë‚Äçüíª Code Contributor ¬∑ ‚úçÔ∏è Doc Wizard ¬∑ üé® Design Artist ¬∑ üß† Idea Spark ¬∑ üîé Code Reviewer ¬∑ üß™ Test Tinkerer ¬∑ üõ†Ô∏è Tool Builder</p><h3>üõ†Ô∏è Hardware Kits Contributors</h3>",
      "contentLength": 11389,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "NirDiamant/RAG_Techniques",
      "url": "https://github.com/NirDiamant/RAG_Techniques",
      "date": 1771470321,
      "author": "",
      "guid": 46305,
      "unread": true,
      "content": "<p>This repository showcases various advanced techniques for Retrieval-Augmented Generation (RAG) systems. RAG systems combine information retrieval with generative models to provide accurate and contextually rich responses.</p><blockquote><p>üåü  Your sponsorship fuels innovation in RAG technologies.  to help maintain and expand this valuable resource!</p></blockquote><p>We gratefully acknowledge the organizations and individuals who have made significant contributions to this project.</p><p>Welcome to one of the most comprehensive and dynamic collections of Retrieval-Augmented Generation (RAG) tutorials available today. This repository serves as a hub for cutting-edge techniques aimed at enhancing the accuracy, efficiency, and contextual richness of RAG systems.</p><div align=\"center\"><table><tbody><tr></tr></tbody></table><p><em>Join over 50,000 AI enthusiasts getting unique cutting-edge insights and free tutorials!</em><em><strong>Plus, subscribers get exclusive early access and special 33% discounts to my book and the upcoming RAG Techniques course!</strong></em></p></div><p>Retrieval-Augmented Generation (RAG) is revolutionizing the way we combine information retrieval with generative AI. This repository showcases a curated collection of advanced techniques designed to supercharge your RAG systems, enabling them to deliver more accurate, contextually relevant, and comprehensive responses.</p><p>Our goal is to provide a valuable resource for researchers and practitioners looking to push the boundaries of what's possible with RAG. By fostering a collaborative environment, we aim to accelerate innovation in this exciting field.</p><p>üöÄ Level up with my  repository. It delivers horizontal, code-first tutorials that cover every tool and step in the lifecycle of building production-grade GenAI agents, guiding you from spark to scale with proven patterns and reusable blueprints for real-world launches, making it the smartest place to start if you're serious about shipping agents to production.</p><p>ü§ñ Explore my  to discover a variety of AI agent implementations and tutorials, showcasing how different AI technologies can be combined to create powerful, interactive systems.</p><p>üñãÔ∏è Check out my  for a comprehensive collection of prompting strategies, from basic concepts to advanced techniques, enhancing your ability to interact effectively with AI language models.</p><h2>A Community-Driven Knowledge Hub</h2><p><strong>This repository grows stronger with your contributions!</strong> Join our vibrant communities - the central hubs for shaping and advancing this project together ü§ù</p><p>Whether you're an expert or just starting out, your insights can shape the future of RAG. Join us to propose ideas, get feedback, and collaborate on innovative techniques. For contribution guidelines, please refer to our  file. Let's advance RAG technology together!</p><p>üîó For discussions on GenAI, RAG, or custom agents, or to explore knowledge-sharing opportunities, feel free to .</p><ul><li>üß† State-of-the-art RAG enhancements</li><li>üìö Comprehensive documentation for each technique</li><li>üõ†Ô∏è Practical implementation guidelines</li><li>üåü Regular updates with the latest advancements</li></ul><p>Explore our extensive list of cutting-edge RAG techniques:</p><h3>üå± Foundational RAG Techniques</h3><ol><li><p>Enhances the Simple RAG by adding validation and refinement to ensure the accuracy and relevance of retrieved information.</p><p>Check for retrieved document relevancy and highlight the segment of docs used for answering.</p></li><li><p>Selecting an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.</p><p>Experiment with different chunk sizes to find the optimal balance between preserving context and maintaining retrieval speed for your specific use case.</p></li><li><p>Breaking down the text into concise, complete, meaningful sentences allowing for better control and handling of specific queries (especially extracting knowledge).</p><ul><li>üí™  The LLM is used in conjunction with a custom prompt to generate factual statements from the document chunks.</li><li>‚úÖ  The generated propositions are passed through a grading system that evaluates accuracy, clarity, completeness, and conciseness.</li></ul></li></ol><ol start=\"6\"><li><p>Modifying and expanding queries to improve retrieval effectiveness.</p><ul><li>‚úçÔ∏è  Reformulate queries to improve retrieval.</li><li>üîô  Generate broader queries for better context retrieval.</li><li>üß©  Break complex queries into simpler sub-queries.</li></ul></li><li><p>Hypothetical Questions (HyDE Approach) ‚ùì</p><p>Generating hypothetical questions to improve alignment between queries and data.</p><p>Create hypothetical questions that point to relevant locations in the data, enhancing query-data matching.</p></li></ol><h3>üìö Context and Content Enrichment</h3><ol start=\"8\"><li><p>Hypothetical Prompt Embeddings (HyPE) ‚ùìüöÄ</p><p>HyPE (Hypothetical Prompt Embeddings) is an enhancement to traditional RAG retrieval that <strong>precomputes hypothetical prompts at the indexing stage</strong>, but inseting the chunk in their place. This transforms retrieval into a <strong>question-question matching task</strong>. This avoids the need for runtime synthetic answer generation, reducing inference-time computational overhead while <strong>improving retrieval alignment</strong>.</p><ul><li>üìñ  Instead of embedding document chunks, HyPE <strong>generates multiple hypothetical queries per chunk</strong> at indexing time.</li><li>üîç <strong>Question-Question Matching:</strong> User queries are matched against stored hypothetical questions, leading to <strong>better retrieval alignment</strong>.</li><li>‚ö°  Unlike HyDE, HyPE does <strong>not require LLM calls at query time</strong>, making retrieval .</li><li>üìà <strong>Higher Precision &amp; Recall:</strong> Improves retrieval <strong>context precision by up to 42 percentage points</strong> and <strong>claim recall by up to 45 percentage points</strong>.</li></ul></li><li><p>Contextual chunk headers (CCH) is a method of creating document-level and section-level context, and prepending those chunk headers to the chunks prior to embedding them.</p><p>Create a chunk header that includes context about the document and/or section of the document, and prepend that to each chunk in order to improve the retrieval accuracy.</p><p>: open-source retrieval engine that implements this technique (and a few other advanced RAG techniques)</p></li><li><p>Relevant segment extraction (RSE) is a method of dynamically constructing multi-chunk segments of text that are relevant to a given query.</p><p>Perform a retrieval post-processing step that analyzes the most relevant chunks and identifies longer multi-chunk segments to provide more complete context to the LLM.</p></li><li><p>Context Enrichment Techniques üìù</p></li></ol><p>Enhancing retrieval accuracy by embedding individual sentences and extending context to neighboring sentences.</p><p>Retrieve the most relevant sentence while also accessing the sentences before and after it in the original text.</p><p>Dividing documents based on semantic coherence rather than fixed sizes.</p><p>Use NLP techniques to identify topic boundaries or coherent sections within documents for more meaningful retrieval units.</p><ol start=\"13\"><li>Contextual Compression üóúÔ∏è</li></ol><p>Compressing retrieved information while preserving query-relevant content.</p><p>Use an LLM to compress or summarize retrieved chunks, preserving key information relevant to the query.</p><ol start=\"14\"><li>Document Augmentation through Question Generation for Enhanced Retrieval</li></ol><p>This implementation demonstrates a text augmentation technique that leverages additional question generation to improve document retrieval within a vector database. By generating and incorporating various questions related to each text fragment, the system enhances the standard retrieval process, thus increasing the likelihood of finding relevant documents that can be utilized as context for generative question answering.</p><p>Use an LLM to augment text dataset with all possible questions that can be asked to each document.</p><h3>üöÄ Advanced Retrieval Methods</h3><ol start=\"15\"><li><p>Multi-faceted Filtering üîç</p><p>Applying various filtering techniques to refine and improve the quality of retrieved results.</p><ul><li>üè∑Ô∏è  Apply filters based on attributes like date, source, author, or document type.</li><li>üìä  Set thresholds for relevance scores to keep only the most pertinent results.</li><li>üìÑ  Remove results that don't match specific content criteria or essential keywords.</li><li>üåà  Ensure result diversity by filtering out near-duplicate entries.</li></ul></li><li><p>Creating a multi-tiered system for efficient information navigation and retrieval.</p><p>Implement a two-tiered system for document summaries and detailed chunks, both containing metadata pointing to the same location in the data.</p></li><li><p>Combining multiple retrieval models or techniques for more robust and accurate results.</p><p>Apply different embedding models or retrieval algorithms and use voting or weighting mechanisms to determine the final set of retrieved documents.</p></li><li><p>Optimizing over Relevant Information Gain in Retrieval</p><ul><li>Combine both relevance and diversity into a single scoring function and directly optimize for it.</li><li>POC showing plain simple RAG underperforming when the database is dense, and the dartboard retrieval outperforming it.</li></ul></li></ol><h3>üîÅ Iterative and Adaptive Techniques</h3><ol start=\"22\"><li><p>Retrieval with Feedback Loops üîÅ</p><p>Implementing mechanisms to learn from user interactions and improve future retrievals.</p><p>Collect and utilize user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.</p></li><li><p>Dynamically adjusting retrieval strategies based on query types and user contexts.</p><p>Classify queries into different categories and use tailored retrieval strategies for each, considering user context and preferences.</p></li><li><p>Performing multiple rounds of retrieval to refine and enhance result quality.</p><p>Use the LLM to analyze initial results and generate follow-up queries to fill in gaps or clarify information.</p></li></ol><ol start=\"25\"><li><p>: <a href=\"https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_deep_eval.ipynb\"><img src=\"https://img.shields.io/badge/GitHub-View-blue\" height=\"20\"></a><a href=\"https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_deep_eval.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg?sanitize=true\" height=\"20\"></a> | Comprehensive RAG system evaluation |</p><p>Performing evaluations Retrieval-Augmented Generation systems, by covering several metrics and creating test cases.</p><p>Use the  library to conduct test cases on correctness, faithfulness and contextual relevancy of RAG systems.</p></li><li><p>: <a href=\"https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_grouse.ipynb\"><img src=\"https://img.shields.io/badge/GitHub-View-blue\" height=\"20\"></a><a href=\"https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_grouse.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg?sanitize=true\" height=\"20\"></a> | Contextually-grounded LLM evaluation |</p><p>Evaluate the final stage of Retrieval-Augmented Generation using metrics of the GroUSE framework and meta-evaluate your custom LLM judge on GroUSE unit tests.</p><p>Use the  package to evaluate contextually-grounded LLM generations with GPT-4 on the 6 metrics of the GroUSE framework and use unit tests to evaluate a custom Llama 3.1 405B evaluator.</p></li></ol><h3>üî¨ Explainability and Transparency</h3><ol start=\"27\"><li><p>Providing transparency in the retrieval process to enhance user trust and system refinement.</p><p>Explain why certain pieces of information were retrieved and how they relate to the query.</p></li></ol><h3>üèóÔ∏è Advanced Architectures</h3><ol start=\"28\"><li><p>Agentic RAG with Contextual AI ü§ñ</p><p>Building production-ready agentic RAG pipelines for financial document analysis with Contextual AI's managed platform. This comprehensive tutorial demonstrates how to leverage agentic RAG to solve complex queries through intelligent query reformulation, document parsing, reranking, and grounded language models.</p><ul><li>: Enterprise-grade parsing with vision models for complex tables, charts, and multi-page documents</li><li><strong>Instruction-Following Reranker</strong>: SOTA reranker with instruction-following capabilities for handling conflicting information</li><li><strong>Grounded Language Model (GLM)</strong>: World's most grounded LLM specifically engineered to minimize hallucinations for RAG use cases</li><li>: Natural language unit testing framework for evaluating and optimizing RAG system performance</li></ul></li><li><p>Graph RAG with Milvus Vector Database üîç</p><p>A simple yet powerful approach to implement Graph RAG using Milvus vector databases. This technique significantly improves performance on complex multi-hop questions by combining relationship-based retrieval with vector search and reranking.</p><ul><li>Store both text passages and relationship triplets (subject-predicate-object) in separate Milvus collections</li><li>Perform multi-way retrieval by querying both collections</li><li>Use an LLM to rerank retrieved relationships based on their relevance to the query</li><li>Retrieve the final passages based on the most relevant relationships</li></ul></li><li><p>Knowledge Graph Integration (Graph RAG) üï∏Ô∏è</p><p>Incorporating structured data from knowledge graphs to enrich context and improve retrieval.</p><p>Retrieve entities and their relationships from a knowledge graph relevant to the query, combining this structured data with unstructured text for more informative responses.</p></li><li><p>Microsoft GraphRAG (Open Source) is an advanced RAG system that integrates knowledge graphs to improve the performance of LLMs</p><p>‚Ä¢ Analyze an input corpus by extracting entities, relationships from text units. generates summaries of each community and its constituents from the bottom-up.</p></li><li><p>RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval üå≥</p><p>Implementing a recursive approach to process and organize retrieved information in a tree structure.</p><p>Use abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.</p></li><li><p>A dynamic approach that combines retrieval-based and generation-based methods, adaptively deciding whether to use retrieved information and how to best utilize it in generating responses.</p><p>‚Ä¢ Implement a multi-step process including retrieval decision, document retrieval, relevance evaluation, response generation, support assessment, and utility evaluation to produce accurate, relevant, and useful outputs.</p></li><li><p>A sophisticated RAG approach that dynamically evaluates and corrects the retrieval process, combining vector databases, web search, and language models for highly accurate and context-aware responses.</p><p>‚Ä¢ Integrate Retrieval Evaluator, Knowledge Refinement, Web Search Query Rewriter, and Response Generator components to create a system that adapts its information sourcing strategy based on relevance scores and combines multiple sources when necessary.</p></li></ol><h2>üåü Special Advanced Technique üåü</h2><ol start=\"35\"><li><p>An advanced RAG solution designed to tackle complex questions that simple semantic similarity-based retrieval cannot solve. This approach uses a sophisticated deterministic graph as the \"brain\" üß† of a highly controllable autonomous agent, capable of answering non-trivial questions from your own data.</p><p>‚Ä¢ Implement a multi-step process involving question anonymization, high-level planning, task breakdown, adaptive information retrieval and question answering, continuous re-planning, and rigorous answer verification to ensure grounded and accurate responses.</p></li></ol><p>To begin implementing these advanced RAG techniques in your projects:</p><ol><li>Clone this repository: <pre><code>git clone https://github.com/NirDiamant/RAG_Techniques.git\n</code></pre></li><li>Navigate to the technique you're interested in: <pre><code>cd all_rag_techniques/technique-name\n</code></pre></li><li>Follow the detailed implementation guide in each technique's directory.</li></ol><p>We welcome contributions from the community! If you have a new technique or improvement to suggest:</p><ol><li>Create your feature branch: <code>git checkout -b feature/AmazingFeature</code></li><li>Commit your changes: <code>git commit -m 'Add some AmazingFeature'</code></li><li>Push to the branch: <code>git push origin feature/AmazingFeature</code></li></ol><p>This project is licensed under a custom non-commercial license - see the <a href=\"https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/LICENSE\">LICENSE</a> file for details.</p><p>‚≠êÔ∏è If you find this repository helpful, please consider giving it a star!</p><p>Keywords: RAG, Retrieval-Augmented Generation, NLP, AI, Machine Learning, Information Retrieval, Natural Language Processing, LLM, Embeddings, Semantic Search</p>",
      "contentLength": 14872,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ComposioHQ/composio",
      "url": "https://github.com/ComposioHQ/composio",
      "date": 1771470321,
      "author": "",
      "guid": 46306,
      "unread": true,
      "content": "<p>Composio powers 1000+ toolkits, tool search, context management, authentication, and a sandboxed workbench to help you build AI agents that turn intent into action.</p><p>This repository contains the official Software Development Kits (SDKs) for Composio, providing seamless integration capabilities for Python and Typescript Agentic Frameworks and Libraries.</p><h3>TypeScript SDK Installation</h3><pre><code># Using npm\nnpm install @composio/core\n\n# Using yarn\nyarn add @composio/core\n\n# Using pnpm\npnpm add @composio/core\n</code></pre><pre><code>import { Composio } from '@composio/core';\n// Initialize the SDK\nconst composio = new Composio({\n  // apiKey: 'your-api-key',\n});\n</code></pre><h4>Simple Agent with OpenAI Agents</h4><pre><code>npm install @composio/openai-agents @openai/agents\n</code></pre><pre><code>import { Composio } from '@composio/core';\nimport { OpenAIAgentsProvider } from '@composio/openai-agents';\nimport { Agent, run } from '@openai/agents';\n\nconst composio = new Composio({\n  provider: new OpenAIAgentsProvider(),\n});\n\nconst userId = 'user@acme.org';\n\nconst tools = await composio.tools.get(userId, {\n  toolkits: ['HACKERNEWS'],\n});\n\nconst agent = new Agent({\n  name: 'Hackernews assistant',\n  tools: tools,\n});\n\nconst result = await run(agent, 'What is the latest hackernews post about?');\n\nconsole.log(JSON.stringify(result.finalOutput, null, 2));\n// will return the response from the agent with data from HACKERNEWS API.\n</code></pre><pre><code># Using pip\npip install composio\n\n# Using poetry\npoetry add composio\n</code></pre><pre><code>from composio import Composio\n\ncomposio = Composio(\n  # api_key=\"your-api-key\",\n)\n</code></pre><h4>Simple Agent with OpenAI Agents</h4><pre><code>pip install composio_openai_agents openai-agents\n</code></pre><pre><code>import asyncio\nfrom agents import Agent, Runner\nfrom composio import Composio\nfrom composio_openai_agents import OpenAIAgentsProvider\n\n# Initialize Composio client with OpenAI Agents Provider\ncomposio = Composio(provider=OpenAIAgentsProvider())\n\nuser_id = \"user@acme.org\"\ntools = composio.tools.get(user_id=user_id, toolkits=[\"HACKERNEWS\"])\n\n# Create an agent with the tools\nagent = Agent(\n    name=\"Hackernews Agent\",\n    instructions=\"You are a helpful assistant.\",\n    tools=tools,\n)\n\n# Run the agent\nasync def main():\n    result = await Runner.run(\n        starting_agent=agent,\n        input=\"What's the latest Hackernews post about?\",\n    )\n    print(result.final_output)\n\nasyncio.run(main())\n# will return the response from the agent with data from HACKERNEWS API.\n</code></pre><p>For more detailed usage instructions and examples, please refer to each SDK's specific documentation.</p><p>To update the OpenAPI specifications used for generating SDK documentation:</p><pre><code># Pull the latest API specifications from the backend\npnpm api:pull\n</code></pre><p>This command pulls the OpenAPI specification from <code>https://backend.composio.dev/api/v3/openapi.json</code> and updates the local API documentation files.</p><p>This is pulled automatically with build step.</p><p>The TypeScript SDK provides a modern, type-safe way to interact with Composio's services. It's designed for both Node.js and browser environments, offering full TypeScript support with comprehensive type definitions.</p><p>The Python SDK offers a Pythonic interface to Composio's services, making it easy to integrate Composio into your Python applications. It supports Python 3.10+ and follows modern Python development practices.</p><p>The following table shows which AI frameworks and platforms are supported in each SDK:</p><table><tbody></tbody></table><p>* <em>LangGraph in TypeScript is supported via the  package.</em></p><p><em>if you are looking for the older sdk, you can find them <a href=\"https://github.com/ComposioHQ/composio/tree/master\">here</a></em></p><p><a href=\"https://rube.app\">Rube</a> is a Model Context Protocol (MCP) server built with Composio. It connects your AI tools to 500+ apps like Gmail, Slack, GitHub, and Notion. Simply install it in your AI client, authenticate once with your apps, and start asking your AI to perform real actions like \"Send an email\" or \"Create a task.\"</p><p>It integrates with major AI clients like Cursor, Claude Desktop, VS Code, Claude Code and any custom MCP‚Äëcompatible client. You can switch between these clients and your integrations follow you.</p><p>This project is licensed under the MIT License - see the LICENSE file for details.</p><p>If you encounter any issues or have questions about the SDKs:</p>",
      "contentLength": 4048,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "QwenLM/qwen-code",
      "url": "https://github.com/QwenLM/qwen-code",
      "date": 1771470321,
      "author": "",
      "guid": 46307,
      "unread": true,
      "content": "<p>An open-source AI agent that lives in your terminal.</p><blockquote><p>üéâ : Qwen3.5-Plus is now live! Sign in via Qwen OAuth to use it directly, or get an API key from <a href=\"https://modelstudio.console.alibabacloud.com?tab=doc#/doc/?type=model&amp;url=2840914_2&amp;modelId=group-qwen3.5-plus\">Alibaba Cloud ModelStudio</a> to access it through the OpenAI-compatible API.</p></blockquote><p>Qwen Code is an open-source AI agent for the terminal, optimized for <a href=\"https://github.com/QwenLM/Qwen3-Coder\">Qwen3-Coder</a>. It helps you understand large codebases, automate tedious work, and ship faster.</p><ul><li><strong>Multi-protocol, OAuth free tier</strong>: use OpenAI / Anthropic / Gemini-compatible APIs, or sign in with Qwen OAuth for 1,000 free requests/day.</li><li>: both the framework and the Qwen3-Coder model are open-source‚Äîand they ship and evolve together.</li><li><strong>Agentic workflow, feature-rich</strong>: rich built-in tools (Skills, SubAgents) for a full agentic workflow and a Claude Code-like experience.</li><li><strong>Terminal-first, IDE-friendly</strong>: built for developers who live in the command line, with optional integration for VS Code, Zed, and JetBrains IDEs.</li></ul><h3>Quick Install (Recommended)</h3><pre><code>curl -fsSL https://qwen-code-assets.oss-cn-hangzhou.aliyuncs.com/installation/install-qwen.sh | bash\n</code></pre><h4>Windows (Run as Administrator CMD)</h4><pre><code>curl -fsSL -o %TEMP%\\install-qwen.bat https://qwen-code-assets.oss-cn-hangzhou.aliyuncs.com/installation/install-qwen.bat &amp;&amp; %TEMP%\\install-qwen.bat\n</code></pre><blockquote><p>: It's recommended to restart your terminal after installation to ensure environment variables take effect.</p></blockquote><p>Make sure you have Node.js 20 or later installed. Download it from <a href=\"https://nodejs.org/en/download\">nodejs.org</a>.</p><pre><code>npm install -g @qwen-code/qwen-code@latest\n</code></pre><pre><code># Start Qwen Code (interactive)\nqwen\n\n# Then, in the session:\n/help\n/auth\n</code></pre><p>On first use, you'll be prompted to sign in. You can run  anytime to switch authentication methods.</p><pre><code>What does this project do?\nExplain the codebase structure.\nHelp me refactor this function.\nGenerate unit tests for this module.\n</code></pre><p>Qwen Code supports two authentication methods:</p><ul><li><strong>Qwen OAuth (recommended &amp; free)</strong>: sign in with your  account in a browser.</li><li>: use an API key to connect to any supported provider (OpenAI, Anthropic, Google GenAI, Alibaba Cloud Bailian, and other compatible endpoints).</li></ul><p>Choose  and complete the browser flow. Your credentials are cached locally so you usually won't need to log in again.</p><blockquote><p> In non-interactive or headless environments (e.g., CI, SSH, containers), you typically  complete the OAuth browser login flow. In these cases, please use the API-KEY authentication method.</p></blockquote><p>Use this if you want more flexibility over which provider and model to use. Supports multiple protocols:</p><ul><li>: Alibaba Cloud Bailian, ModelScope, OpenAI, OpenRouter, and other OpenAI-compatible providers</li><li>: Gemini models</li></ul><p>The  way to configure models and providers is by editing  (create it if it doesn't exist). This file lets you define all available models, API keys, and default settings in one place.</p><p> Create or edit </p><p>Here is a complete example:</p><pre><code>{\n  \"modelProviders\": {\n    \"openai\": [\n      {\n        \"id\": \"qwen3-coder-plus\",\n        \"name\": \"qwen3-coder-plus\",\n        \"baseUrl\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n        \"description\": \"Qwen3-Coder via Dashscope\",\n        \"envKey\": \"DASHSCOPE_API_KEY\"\n      }\n    ]\n  },\n  \"env\": {\n    \"DASHSCOPE_API_KEY\": \"sk-xxxxxxxxxxxxx\"\n  },\n  \"security\": {\n    \"auth\": {\n      \"selectedType\": \"openai\"\n    }\n  },\n  \"model\": {\n    \"name\": \"qwen3-coder-plus\"\n  }\n}\n</code></pre><p> Understand each field</p><table><tbody><tr><td>Declares which models are available and how to connect to them. Keys like , ,  represent the API protocol.</td></tr><tr><td>The model ID sent to the API (e.g. , ).</td></tr><tr><td>The name of the environment variable that holds your API key.</td></tr><tr><td>The API endpoint URL (required for non-default endpoints).</td></tr><tr><td>A fallback place to store API keys (lowest priority; prefer  files or  for sensitive keys).</td></tr><tr><td><code>security.auth.selectedType</code></td><td>The protocol to use on startup (, , , ).</td></tr><tr><td>The default model to use when Qwen Code starts.</td></tr></tbody></table><p> Start Qwen Code ‚Äî your configuration takes effect automatically:</p><p>Use the  command at any time to switch between all configured models.</p><blockquote><p> You can also set API keys via  in your shell or  files, which take higher priority than  ‚Üí . See the <a href=\"https://qwenlm.github.io/qwen-code-docs/en/users/configuration/auth/\">authentication guide</a> for full details.</p></blockquote><blockquote><p> Never commit API keys to version control. The  file is in your home directory and should stay private.</p></blockquote><p>As an open-source terminal agent, you can use Qwen Code in four primary ways:</p><ol><li>Interactive mode (terminal UI)</li><li>Headless mode (scripts, CI)</li><li>IDE integration (VS Code, Zed)</li></ol><p>Run  in your project folder to launch the interactive terminal UI. Use  to reference local files (for example ).</p><pre><code>cd your-project/\nqwen -p \"your question\"\n</code></pre><p>Use  to run Qwen Code without the interactive UI‚Äîideal for scripts, automation, and CI/CD. Learn more: <a href=\"https://qwenlm.github.io/qwen-code-docs/en/users/features/headless\">Headless mode</a>.</p><p>Use Qwen Code inside your editor (VS Code, Zed, and JetBrains IDEs):</p><p>Build on top of Qwen Code with the TypeScript SDK:</p><ul><li> - Display available commands</li><li> - Clear conversation history</li><li> - Compress history to save tokens</li><li> - Show current session information</li><li> - Submit a bug report</li><li> or  - Exit Qwen Code</li></ul><ul><li> - Cancel current operation</li><li> - Exit (on empty line)</li><li> - Navigate command history</li></ul><blockquote><p>: In YOLO mode (), vision switching happens automatically without prompts when images are detected. Learn more about <a href=\"https://qwenlm.github.io/qwen-code-docs/en/users/features/approval-mode/\">Approval Mode</a></p></blockquote><p>Qwen Code can be configured via , environment variables, and CLI flags.</p><table><tbody><tr><td>Applies to all your Qwen Code sessions. <strong>Recommended for  and .</strong></td></tr><tr><td>Applies only when running Qwen Code in this project. Overrides user settings.</td></tr></tbody></table><p>The most commonly used top-level fields in :</p><table><tbody><tr><td>Define available models per protocol (, , , ).</td></tr><tr><td>Fallback environment variables (e.g. API keys). Lower priority than shell  and  files.</td></tr><tr><td><code>security.auth.selectedType</code></td><td>The protocol to use on startup (e.g. ).</td></tr><tr><td>The default model to use when Qwen Code starts.</td></tr></tbody></table><h3>Terminal-Bench Performance</h3><table><tbody><tr></tr><tr></tr></tbody></table><p>Looking for a graphical interface?</p><ul><li><a href=\"https://github.com/iOfficeAI/AionUi\"></a> A modern GUI for command-line AI tools including Qwen Code</li></ul><p>To report a bug from within the CLI, run  and include a short title and repro steps.</p><p>This project is based on <a href=\"https://github.com/google-gemini/gemini-cli\">Google Gemini CLI</a>. We acknowledge and appreciate the excellent work of the Gemini CLI team. Our main contribution focuses on parser-level adaptations to better support Qwen-Coder models.</p>",
      "contentLength": 5942,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "HailToDodongo/pyrite64",
      "url": "https://github.com/HailToDodongo/pyrite64",
      "date": 1771470321,
      "author": "",
      "guid": 46308,
      "unread": true,
      "content": "<p>N64 Game-Engine and Editor using libdragon &amp; tiny3d</p><blockquote><p>Note: This project does NOT use any proprietary N64 SDKs or libraries.</p></blockquote><p>Pyrite64 is a visual editor + runtime-engine to create 3D games that can run on a real N64 console or accurate emulators. Besides the usual editor, some extra features include:</p><ul><li>Automatic toolchain installation on Windows</li><li>3D-Model import (GLTF) from blender with <a href=\"https://github.com/Fast-64/fast64\">fast64</a> material support.</li><li>Runtime engine handling scene-management, rendering, collision, audio and more.</li><li>Global asset management with automatic memory cleanup</li><li>Node-Graph editor to script basic control flow</li></ul><p>Note that this project focuses on real hardware, so accurate emulation is required to run/test games on PC. Emulators that are accurate enough include <a href=\"https://ares-emu.net/\">Ares (v147 or newer)</a> and <a href=\"https://github.com/gopher64/gopher64\">gopher64</a>.</p><blockquote><p>[!WARNING] This project is still in early development, so features are going to be missing. Documentation is also still a work in progress, and breaking API changes are to be expected.</p></blockquote><p>Before starting, please read the <a href=\"https://raw.githubusercontent.com/HailToDodongo/pyrite64/main/docs/faq.md\">FAQ</a>!</p><p align=\"center\"><a href=\"https://www.youtube.com/watch?v=zz_wByA_k6E\" target=\"_blank\"><img src=\"https://img.youtube.com/vi/zz_wByA_k6E/0.jpg\" width=\"250\"></a><a href=\"https://www.youtube.com/watch?v=4BCmKnN5eGA\" target=\"_blank\"><img src=\"https://img.youtube.com/vi/4BCmKnN5eGA/0.jpg\" width=\"250\"></a> Cathode Quest 64 (YouTube) &nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp; Pyrite64 Release Video &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </p><p>¬© 2025-2026 - Max Beb√∂k (HailToDodongo)</p><p>Pyrite64 is licensed under the MIT License, see the <a href=\"https://raw.githubusercontent.com/HailToDodongo/pyrite64/main/LICENSE\">LICENSE</a> file for more information. Licenses for external libraries used in the editor can be found in their respective directory under </p><p>Pyrite64 does NOT force any restrictions or licenses on games made with it. Pyrite64 does NOT claim any copyright or force licenses for assets / source-code generated by the editor.</p><p>While not required, please consider crediting Pyrite64 with a logo and/or name in your credits and/or boot logo sequence.</p>",
      "contentLength": 1603,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "steipete/summarize",
      "url": "https://github.com/steipete/summarize",
      "date": 1771383966,
      "author": "",
      "guid": 45958,
      "unread": true,
      "content": "<p>Point at any URL/YouTube/Podcast or file. Get the gist. CLI and Chrome Extension.</p><p>Fast summaries from URLs, files, and media. Works in the terminal, a Chrome Side Panel and Firefox Sidebar.</p><p><strong>0.11.0 preview (unreleased):</strong> this README reflects the upcoming release.</p><h2>0.11.0 preview highlights (most interesting first)</h2><ul><li>Chrome Side Panel  (streaming agent + history) inside the sidebar.</li><li>: screenshots + OCR + transcript cards, timestamped seek, OCR/Transcript toggle.</li><li>Media-aware summaries: auto‚Äëdetect video/audio vs page content.</li><li>Streaming Markdown + metrics + cache‚Äëaware status.</li><li>CLI supports URLs, files, podcasts, YouTube, audio/video, PDFs.</li></ul><ul><li>URLs, files, and media: web pages, PDFs, images, audio/video, YouTube, podcasts, RSS.</li><li>Slide extraction for video sources (YouTube/direct media) with OCR + timestamped cards.</li><li>Transcript-first media flow: published transcripts when available, Whisper fallback when not.</li><li>Streaming output with Markdown rendering, metrics, and cache-aware status.</li><li>Local, paid, and free models: OpenAI‚Äëcompatible local endpoints, paid providers, plus an OpenRouter free preset.</li><li>Output modes: Markdown/text, JSON diagnostics, extract-only, metrics, timing, and cost estimates.</li><li>Smart default: if content is shorter than the requested length, we return it as-is (use  to override).</li></ul><h2>Get the extension (recommended)</h2><p>One‚Äëclick summarizer for the current tab. Chrome Side Panel + Firefox Sidebar + local daemon for streaming Markdown.</p><p>YouTube slide screenshots (from the browser):</p><h3>Beginner quickstart (extension)</h3><ol><li>Install the CLI (choose one): \n  <ul><li> (cross‚Äëplatform): <code>npm i -g @steipete/summarize</code></li><li> (macOS arm64): <code>brew install steipete/tap/summarize</code></li></ul></li><li>Install the extension (Chrome Web Store link above) and open the Side Panel.</li><li>The panel shows a token + install command. Run it in Terminal: \n  <ul><li><code>summarize daemon install --token &lt;TOKEN&gt;</code></li></ul></li></ol><ul><li>The extension can‚Äôt run heavy extraction inside the browser. It talks to a local background service on  for fast streaming and media tools (yt‚Äëdlp, ffmpeg, OCR, transcription).</li><li>The service autostarts (launchd/systemd/Scheduled Task) so the Side Panel is always ready.</li></ul><p>If you only want the , you can skip the daemon install entirely.</p><ul><li>Summarization only runs when the Side Panel is open.</li><li>Auto mode summarizes on navigation (incl. SPAs); otherwise use the button.</li><li>Daemon is localhost-only and requires a shared token.</li><li>Autostart: macOS (launchd), Linux (systemd user), Windows (Scheduled Task).</li><li>Tip: configure  via  (needs ). Add  to set model=.</li></ul><ul><li>Select  in the Summarize picker.</li><li>Slides render at the top; expand to full‚Äëwidth cards with timestamps.</li><li>Click a slide to seek the video; toggle  when OCR is significant.</li><li>Requirements:  +  for extraction;  for OCR. Missing tools show an in‚Äëpanel notice.</li></ul><h3>Advanced (unpacked / dev)</h3><ol><li>Build + load the extension (unpacked): \n  <ul><li>Chrome: <code>pnpm -C apps/chrome-extension build</code><ul><li> ‚Üí Developer mode ‚Üí Load unpacked</li><li>Pick: <code>apps/chrome-extension/.output/chrome-mv3</code></li></ul></li><li>Firefox: <code>pnpm -C apps/chrome-extension build:firefox</code><ul><li><code>about:debugging#/runtime/this-firefox</code> ‚Üí Load Temporary Add-on</li><li>Pick: <code>apps/chrome-extension/.output/firefox-mv3/manifest.json</code></li></ul></li></ul></li><li>Open Side Panel/Sidebar ‚Üí copy token.</li><li>Install daemon in dev mode: \n  <ul><li><code>pnpm summarize daemon install --token &lt;TOKEN&gt; --dev</code></li></ul></li></ol><pre><code>npx -y @steipete/summarize \"https://example.com\"\n</code></pre><pre><code>npm i -g @steipete/summarize\n</code></pre><ul><li>npm (library / minimal deps):</li></ul><pre><code>npm i @steipete/summarize-core\n</code></pre><pre><code>import { createLinkPreviewClient } from \"@steipete/summarize-core/content\";\n</code></pre><pre><code>brew install steipete/tap/summarize\n</code></pre><p>Apple Silicon only (arm64).</p><ul><li> just install via npm/Homebrew and run  (no daemon needed).</li><li><strong>Chrome/Firefox extension:</strong> install the CLI  run <code>summarize daemon install --token &lt;TOKEN&gt;</code> so the Side Panel can stream results and use local tools.</li></ul><pre><code>summarize \"https://example.com\"\n</code></pre><pre><code>summarize \"/path/to/file.pdf\" --model google/gemini-3-flash-preview\nsummarize \"https://example.com/report.pdf\" --model google/gemini-3-flash-preview\nsummarize \"/path/to/audio.mp3\"\nsummarize \"/path/to/video.mp4\"\n</code></pre><p>Stdin (pipe content using ):</p><pre><code>echo \"content\" | summarize -\npbpaste | summarize -\n# binary stdin also works (PDF/image/audio/video bytes)\ncat /path/to/file.pdf | summarize -\n</code></pre><ul><li>Stdin has a 50MB size limit</li><li>The  argument tells summarize to read from standard input</li><li>Text stdin is treated as UTF-8 text (whitespace-only input is rejected as empty)</li><li>Binary stdin is preserved as raw bytes and file type is auto-detected when possible</li><li>Useful for piping clipboard content or command output</li></ul><p>YouTube (supports  and ):</p><pre><code>summarize \"https://youtu.be/dQw4w9WgXcQ\" --youtube auto\n</code></pre><p>Podcast RSS (transcribes latest enclosure):</p><pre><code>summarize \"https://feeds.npr.org/500005/podcast.xml\"\n</code></pre><p>Apple Podcasts episode page:</p><pre><code>summarize \"https://podcasts.apple.com/us/podcast/2424-jelly-roll/id360084272?i=1000740717432\"\n</code></pre><p>Spotify episode page (best-effort; may fail for exclusives):</p><pre><code>summarize \"https://open.spotify.com/episode/5auotqWAXhhKyb9ymCuBJY\"\n</code></pre><p> controls how much output we ask for (guideline), not a hard cap.</p><pre><code>summarize \"https://example.com\" --length long\nsummarize \"https://example.com\" --length 20k\n</code></pre><ul><li>Presets: </li><li>Character targets: , , </li><li>Optional hard cap: <code>--max-output-tokens &lt;count&gt;</code> (e.g. , ) \n  <ul><li>Provider/model APIs still enforce their own maximum output limits.</li><li>If omitted, no max token parameter is sent (provider default).</li><li>Prefer  unless you need a hard cap.</li></ul></li><li>Short content: when extracted content is shorter than the requested length, the CLI returns the content as-is. \n  <ul><li>Override with  to always run the LLM.</li></ul></li><li>Minimums:  numeric values must be &gt;= 50 chars;  must be &gt;= 16.</li><li>Preset targets (source of truth: <code>packages/core/src/prompts/summary-lengths.ts</code>): \n  <ul><li>short: target ~900 chars (range 600-1,200)</li><li>medium: target ~1,800 chars (range 1,200-2,500)</li><li>long: target ~4,200 chars (range 2,500-6,000)</li><li>xl: target ~9,000 chars (range 6,000-14,000)</li><li>xxl: target ~17,000 chars (range 14,000-22,000)</li></ul></li></ul><p>Best effort and provider-dependent. These usually work well:</p><ul><li> and common structured text (, , , , , ...) \n  <ul><li>Text-like files are inlined into the prompt for better provider compatibility.</li></ul></li><li>PDFs:  (provider support varies; Google is the most reliable here)</li><li>Images: , , , </li><li>Audio/Video: ,  (local audio/video files MP3/WAV/M4A/OGG/FLAC/MP4/MOV/WEBM automatically transcribed, when supported by the model)</li></ul><ul><li>If a provider rejects a media type, the CLI fails fast with a friendly message.</li><li>xAI models do not support attaching generic files (like PDFs) via the AI SDK; use Google/OpenAI/Anthropic for those.</li></ul><p>Use gateway-style ids: .</p><ul><li><code>anthropic/claude-sonnet-4-5</code></li><li><code>xai/grok-4-fast-non-reasoning</code></li><li><code>google/gemini-3-flash-preview</code></li><li><code>openrouter/openai/gpt-5-mini</code> (force OpenRouter)</li></ul><p>Note: some models/providers do not support streaming or certain file media types. When that happens, the CLI prints a friendly error (or auto-disables streaming for that model when supported by the provider).</p><ul><li>Text inputs over 10 MB are rejected before tokenization.</li><li>Text prompts are preflighted against the model input limit (LiteLLM catalog), using a GPT tokenizer.</li></ul><pre><code>summarize &lt;input&gt; [flags]\n</code></pre><p>Use  or  for the full help text.</p><ul><li>: which model to use (defaults to )</li><li>: automatic model selection + fallback (default)</li><li>: use a config-defined model (see Configuration)</li><li>: , ,  (default )</li><li>: LLM retry attempts on timeout (default )</li><li><code>--length short|medium|long|xl|xxl|s|m|l|&lt;chars&gt;</code></li><li><code>--language, --lang &lt;language&gt;</code>: output language ( = match source)</li><li><code>--max-output-tokens &lt;count&gt;</code>: hard cap for LLM output tokens</li><li>: use a CLI provider (). Supports , , , . If omitted, uses auto selection with CLI enabled.</li><li>: stream LLM output ( = TTY only; disabled in  mode)</li><li>: keep raw output (no ANSI/OSC Markdown rendering)</li><li>: disable ANSI colors</li><li>: CLI theme (, , , )</li><li>: website/file content format (default )</li><li><code>--markdown-mode off|auto|llm|readability</code>: HTML -&gt; Markdown mode (default )</li><li><code>--preprocess off|auto|always</code>: controls  usage (default ) \n  </li><li>: print extracted content and exit (URLs only; stdin  is not supported) \n  <ul><li>Deprecated alias: </li></ul></li><li>: extract slides for YouTube/direct video URLs and render them inline in the summary narrative (auto-renders inline in supported terminals)</li><li>: run OCR on extracted slides (requires )</li><li>: base output dir for slide images (default )</li><li><code>--slides-scene-threshold &lt;value&gt;</code>: scene detection threshold (0.1-1.0)</li><li>: maximum slides to extract (default )</li><li><code>--slides-min-duration &lt;seconds&gt;</code>: minimum seconds between slides</li><li>: machine-readable output with diagnostics, prompt, , and optional summary</li><li>: debug/diagnostics on stderr</li><li><code>--metrics off|on|detailed</code>: metrics output (default )</li></ul><h3>Coding CLIs (Codex, Claude, Gemini, Agent)</h3><p>Summarize can use common coding CLIs as local model backends:</p><ul><li> -&gt;  / <code>--model cli/codex/&lt;model&gt;</code></li><li> -&gt;  / <code>--model cli/claude/&lt;model&gt;</code></li><li> -&gt;  / <code>--model cli/gemini/&lt;model&gt;</code></li><li> (Cursor Agent CLI) -&gt;  / <code>--model cli/agent/&lt;model&gt;</code></li></ul><ul><li>Binary installed and on  (or set , , , )</li><li>Provider authenticated (, ,  login flow,  or )</li></ul><pre><code>printf \"Summarize CLI smoke input.\\nOne short paragraph. Reply can be brief.\\n\" &gt;/tmp/summarize-cli-smoke.txt\n\nsummarize --cli codex --plain --timeout 2m /tmp/summarize-cli-smoke.txt\nsummarize --cli claude --plain --timeout 2m /tmp/summarize-cli-smoke.txt\nsummarize --cli gemini --plain --timeout 2m /tmp/summarize-cli-smoke.txt\nsummarize --cli agent --plain --timeout 2m /tmp/summarize-cli-smoke.txt\n</code></pre><p>Set explicit CLI allowlist/order:</p><pre><code>{\n  \"cli\": { \"enabled\": [\"codex\", \"claude\", \"gemini\", \"agent\"] }\n}\n</code></pre><p>Configure implicit auto CLI fallback:</p><pre><code>{\n  \"cli\": {\n    \"autoFallback\": {\n      \"enabled\": true,\n      \"onlyWhenNoApiKeys\": true,\n      \"order\": [\"claude\", \"gemini\", \"codex\", \"agent\"]\n    }\n  }\n}\n</code></pre><p> builds candidate attempts from built-in rules (or your  overrides). CLI attempts are prepended when:</p><ul><li> is set (explicit allowlist/order), or</li><li>implicit auto selection is active and  is enabled.</li></ul><p>Default fallback behavior: only when no API keys are configured, order <code>claude, gemini, codex, agent</code>, and remember/prioritize last successful provider (<code>~/.summarize/cli-state.json</code>).</p><p>Set explicit CLI attempts:</p><pre><code>{\n  \"cli\": { \"enabled\": [\"gemini\"] }\n}\n</code></pre><p>Disable implicit auto CLI fallback:</p><pre><code>{\n  \"cli\": { \"autoFallback\": { \"enabled\": false } }\n}\n</code></pre><p>Note: explicit  does not trigger implicit auto CLI fallback unless  is set.</p><h3>Website extraction (Firecrawl + Markdown)</h3><p>Non-YouTube URLs go through a fetch -&gt; extract pipeline. When direct fetch/extraction is blocked or too thin,  can fall back to Firecrawl (if configured).</p><ul><li><code>--firecrawl off|auto|always</code> (default )</li><li><code>--extract --format md|text</code> (default ; if  is omitted,  defaults to  for non-YouTube URLs)</li><li><code>--markdown-mode off|auto|llm|readability</code> (default ) \n  <ul><li>: use an LLM converter when configured; may fall back to </li><li>: force LLM conversion (requires a configured model key)</li><li>: disable LLM conversion (still may return Firecrawl Markdown when configured)</li></ul></li><li>Plain-text mode: use .</li></ul><p> tries best-effort web transcript endpoints first. When captions are not available, it falls back to:</p><ol><li>Apify (if  is set): uses a scraping actor ()</li><li>yt-dlp + Whisper (if  is available): downloads audio, then transcribes with local  when installed (preferred), otherwise falls back to OpenAI () or FAL ()</li></ol><p>Environment variables for yt-dlp mode:</p><ul><li> - optional path to yt-dlp binary (otherwise  is resolved via )</li><li><code>SUMMARIZE_WHISPER_CPP_MODEL_PATH</code> - optional override for the local  model file</li><li><code>SUMMARIZE_WHISPER_CPP_BINARY</code> - optional override for the local binary (default: )</li><li><code>SUMMARIZE_DISABLE_LOCAL_WHISPER_CPP=1</code> - disable local whisper.cpp (force remote)</li><li> - OpenAI Whisper transcription</li><li> - optional OpenAI-compatible Whisper endpoint override</li><li> - FAL AI Whisper fallback</li></ul><p>Apify costs money but tends to be more reliable when captions exist.</p><h3>Slide extraction (YouTube + direct video URLs)</h3><p>Extract slide screenshots (scene detection via ) and optional OCR:</p><pre><code>summarize \"https://www.youtube.com/watch?v=...\" --slides\nsummarize \"https://www.youtube.com/watch?v=...\" --slides --slides-ocr\n</code></pre><p>Outputs are written under  (or ). OCR results are included in JSON output () and stored in  inside the slide directory. When scene detection is too sparse, the extractor also samples at a fixed interval to improve coverage. When using , supported terminals (kitty/iTerm/Konsole) render inline thumbnails automatically inside the summary narrative (the model inserts  markers). Timestamp links are clickable when the terminal supports OSC-8 (YouTube/Vimeo/Loom/Dropbox). If inline images are unsupported, Summarize prints a note with the on-disk slide directory.</p><p>Use  to print the full timed transcript and insert slide images inline at matching timestamps.</p><p>Format the extracted transcript as Markdown (headings + paragraphs) via an LLM:</p><pre><code>summarize \"https://www.youtube.com/watch?v=...\" --extract --format md --markdown-mode llm\n</code></pre><h3>Media transcription (Whisper)</h3><p>Local audio/video files are transcribed first, then summarized.  forces direct media URLs (and embedded media) through Whisper first. Prefers local  when available; otherwise requires  or .</p><h3>Local ONNX transcription (Parakeet/Canary)</h3><p>Summarize can use NVIDIA Parakeet/Canary ONNX models via a local CLI you provide. Auto selection (default) prefers ONNX when configured.</p><ul><li>Setup helper: <code>summarize transcriber setup</code></li><li>Install  from upstream binaries/build (Homebrew may not have a formula)</li><li>Auto selection: set <code>SUMMARIZE_ONNX_PARAKEET_CMD</code> or <code>SUMMARIZE_ONNX_CANARY_CMD</code> (no flag needed)</li><li>Force a model: <code>--transcriber parakeet|canary|whisper|auto</code></li><li>Docs: <code>docs/nvidia-onnx-transcription.md</code></li></ul><h3>Verified podcast services (2025-12-25)</h3><ul><li>Amazon Music / Audible podcast pages</li><li>RSS feeds (Podcasting 2.0 transcripts when available)</li><li>Embedded YouTube podcast pages (e.g. JREPodcast)</li></ul><p>Transcription: prefers local  when installed; otherwise uses OpenAI Whisper or FAL when keys are set.</p><p> controls the output language of the summary (and other LLM-generated text). Default is .</p><p>When the input is audio/video, the CLI needs a transcript first. The transcript comes from one of these paths:</p><ol><li>Existing transcript (preferred) \n  <ul><li>YouTube: uses  /  when available.</li><li>Podcasts: uses Podcasting 2.0 RSS  (JSON/VTT) when the feed publishes it.</li></ul></li><li>Whisper transcription (fallback) \n  <ul><li>YouTube: falls back to yt-dlp (audio download) + Whisper transcription when configured; Apify is a last resort.</li><li>Prefers local  when installed + model available.</li><li>Otherwise uses cloud Whisper (OpenAI ) or FAL ().</li></ul></li></ol><p>For direct media URLs, use  to force transcribe -&gt; summarize:</p><pre><code>summarize https://example.com/file.mp4 --video-mode transcript --lang en\n</code></pre><pre><code>{\n  \"model\": { \"id\": \"openai/gpt-5-mini\" },\n  \"env\": { \"OPENAI_API_KEY\": \"sk-...\" },\n  \"ui\": { \"theme\": \"ember\" }\n}\n</code></pre><pre><code>{\n  \"model\": \"openai/gpt-5-mini\"\n}\n</code></pre><ul><li> (customize candidates / ordering)</li><li> (define presets selectable via )</li><li> (generic env var defaults; process env still wins)</li><li> (legacy shortcut, mapped to env names; prefer  for new configs)</li><li> (media download cache: TTL 7 days, 2048 MB cap by default;  disables)</li><li><code>media.videoMode: \"auto\"|\"transcript\"|\"understand\"</code></li><li> /  /  /  (defaults for )</li><li><code>ui.theme: \"aurora\"|\"ember\"|\"moss\"|\"mono\"</code></li><li><code>openai.useChatCompletions: true</code> (force OpenAI-compatible chat completions)</li></ul><p>Note: the config is parsed leniently (JSON5), but comments are not allowed. Unknown keys are ignored.</p><pre><code>{\n  \"cache\": {\n    \"media\": { \"enabled\": true, \"ttlDays\": 7, \"maxMb\": 2048, \"verify\": \"size\" }\n  }\n}\n</code></pre><p>Note:  bypasses summary caching only (LLM output). Extract/transcript caches still apply. Use  to skip media files.</p><ol></ol><ol><li> ()</li></ol><p>Environment variable precedence:</p><ol><li> ()</li><li> (, legacy)</li></ol><p>Set the key matching your chosen :</p><ul><li><p>Optional fallback defaults can be stored in config:</p><ul><li> -&gt; <code>\"env\": { \"OPENAI_API_KEY\": \"sk-...\" }</code></li><li>process env always takes precedence</li><li>legacy  still works (mapped to env names)</li></ul></li><li><p> (for )</p></li><li><p> (for )</p></li><li><p> (for )</p></li><li><p> (for ; supports  alias)</p></li><li><p> (for )</p><ul><li>also accepts <code>GOOGLE_GENERATIVE_AI_API_KEY</code> and  as aliases</li></ul></li></ul><p>OpenAI-compatible chat completions toggle:</p><ul><li><code>OPENAI_USE_CHAT_COMPLETIONS=1</code> (or set <code>openai.useChatCompletions</code> in config)</li></ul><ul><li><code>SUMMARIZE_THEME=aurora|ember|moss|mono</code></li><li> (force 24-bit ANSI)</li><li> (disable 24-bit ANSI)</li></ul><p>OpenRouter (OpenAI-compatible):</p><ul><li>Set </li><li>Prefer forcing OpenRouter per model id: <code>--model openrouter/&lt;author&gt;/&lt;slug&gt;</code></li><li>Built-in preset:  (uses a default set of OpenRouter  models)</li></ul><p>Quick start: make free the default (keep  available)</p><pre><code>summarize refresh-free --set-default\nsummarize \"https://example.com\"\nsummarize \"https://example.com\" --model auto\n</code></pre><p>Regenerates the  preset ( in ) by:</p><ul><li>Fetching OpenRouter , filtering </li><li>Skipping models that look very small (&lt;27B by default) based on the model id/name</li><li>Testing which ones return non-empty text (concurrency 4, timeout 10s)</li><li>Picking a mix of smart-ish (bigger  / output cap) and fast models</li><li>Refining timings and writing the sorted list back</li></ul><p>If  stops working, run:</p><ul><li> (default): extra timing runs per selected model (total runs = 1 + runs)</li><li> (default): how many smart-first picks (rest filled by fastest)</li><li> (default): ignore models with inferred size smaller than N billion parameters</li><li> (default): ignore models older than N days (set 0 to disable)</li><li>: also sets  in </li></ul><pre><code>OPENROUTER_API_KEY=sk-or-... summarize \"https://example.com\" --model openrouter/meta-llama/llama-3.1-8b-instruct:free\nOPENROUTER_API_KEY=sk-or-... summarize \"https://example.com\" --model openrouter/minimax/minimax-m2.5\n</code></pre><p>If your OpenRouter account enforces an allowed-provider list, make sure at least one provider is allowed for the selected model. When routing fails,  prints the exact providers to allow.</p><p>Legacy: <code>OPENAI_BASE_URL=https://openrouter.ai/api/v1</code> (and either  or ) also works.</p><p>NVIDIA API Catalog (OpenAI-compatible; free credits):</p><ul><li>Optional: <code>NVIDIA_BASE_URL=https://integrate.api.nvidia.com/v1</code></li><li>Credits: API Catalog trial starts with 1000 free API credits on signup (up to 5000 total via ‚ÄúRequest More‚Äù in the API Catalog profile)</li><li>Pick a model id from  (examples: fast <code>stepfun-ai/step-3.5-flash</code>, strong but slower )</li></ul><pre><code>export NVIDIA_API_KEY=\"nvapi-...\"\nsummarize \"https://example.com\" --model nvidia/stepfun-ai/step-3.5-flash\n</code></pre><p>Z.AI (OpenAI-compatible):</p><ul><li> (or )</li><li>Optional base URL override: </li></ul><ul><li> (website extraction fallback)</li><li> (path to yt-dlp binary for audio extraction)</li><li> (FAL AI API key for audio transcription via Whisper)</li><li> (YouTube transcript fallback)</li></ul><p>The CLI uses the LiteLLM model catalog for model limits (like max output tokens):</p><ul><li>Downloaded from: <code>https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json</code></li><li>Cached at: </li></ul><p>Recommended (minimal deps):</p><ul><li><code>@steipete/summarize-core/content</code></li><li><code>@steipete/summarize-core/prompts</code></li></ul><p>Compatibility (pulls in CLI deps):</p><ul><li><code>@steipete/summarize/content</code></li><li><code>@steipete/summarize/prompts</code></li></ul><ul><li>\"Receiving end does not exist\": Chrome did not inject the content script yet. \n  <ul><li>Extension details -&gt; Site access -&gt; On all sites (or allow this domain)</li></ul></li><li>\"Failed to fetch\" / daemon unreachable: \n  <ul><li>Logs: <code>~/.summarize/logs/daemon.err.log</code></li></ul></li></ul>",
      "contentLength": 18337,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "OpenCTI-Platform/opencti",
      "url": "https://github.com/OpenCTI-Platform/opencti",
      "date": 1771383966,
      "author": "",
      "guid": 45959,
      "unread": true,
      "content": "<p>Open Cyber Threat Intelligence Platform</p><p>OpenCTI is an open source platform allowing organizations to manage their cyber threat intelligence knowledge and observables. It has been created in order to structure, store, organize and visualize technical and non-technical information about cyber threats.</p><p>The structuration of the data is performed using a knowledge schema based on the <a href=\"https://oasis-open.github.io/cti-documentation/\">STIX2 standards</a>. It has been designed as a modern web application including a <a href=\"https://graphql.org\">GraphQL API</a> and an UX oriented frontend. Also, OpenCTI can be integrated with other tools and applications such as <a href=\"https://github.com/MISP/MISP\">MISP</a>, <a href=\"https://github.com/TheHive-Project/TheHive\">TheHive</a>, <a href=\"https://github.com/mitre/cti\">MITRE ATT&amp;CK</a>, etc.</p><p>The goal is to create a comprehensive tool allowing users to capitalize technical (such as TTPs and observables) and non-technical information (such as suggested attribution, victimology etc.) while linking each piece of information to its primary source (a report, a MISP event, etc.), with features such as links between each information, first and last seen dates, levels of confidence, etc. The tool is able to use the <a href=\"https://attack.mitre.org\">MITRE ATT&amp;CK framework</a> (through a <a href=\"https://github.com/OpenCTI-Platform/connectors\">dedicated connector</a>) to help structure the data. The user can also choose to implement their own datasets.</p><p>Once data has been capitalized and processed by the analysts within OpenCTI, new relations may be inferred from existing ones to facilitate the understanding and the representation of this information. This allows the user to extract and leverage meaningful knowledge from the raw data.</p><p>OpenCTI not only allows <a href=\"https://docs.opencti.io/latest/usage/import-automated/\">imports</a> but also <a href=\"https://docs.opencti.io/latest/usage/feeds/\">exports of data</a> under different formats (CSV, STIX2 bundles, etc.). <a href=\"https://hub.filigran.io/cybersecurity-solutions/open-cti-integrations\">Connectors</a> are currently developed to accelerate interactions between the tool and other platforms.</p><p>OpenCTI platform has 2 different editions: Community (CE) and Enterprise (EE). The purpose of the Enterprise Edition is to provide <a href=\"https://filigran.io/offering/subscribe\">additional and powerful features</a> which require specific investments in research and development. You can enable the Enterprise Edition directly in the settings of the platform.</p><p>To understand what OpenCTI Enterprise Edition brings in terms of features, just check the <a href=\"https://filigran.io/offering/subscribe\">Enterprise Editions page</a> on the Filigran website. You can also try this edition by enabling it in the settings of the platform.</p><h2>Documentation and demonstration</h2><p>If you want to know more on OpenCTI, you can read the <a href=\"https://docs.opencti.io\">documentation on the tool</a>. If you wish to discover how the OpenCTI platform is working, a <a href=\"https://demo.opencti.io\">demonstration instance</a> is available and open to everyone. This instance is reset every night and is based on reference data maintained by the OpenCTI developers.</p><p>All you need to install the OpenCTI platform can be found in the <a href=\"https://docs.opencti.io\">official documentation</a>. For installation, you can:</p><p>OpenCTI has adopted a <a href=\"https://raw.githubusercontent.com/OpenCTI-Platform/opencti/master/CODE_OF_CONDUCT.md\">Code of Conduct</a> that we expect project participants to adhere to. Please read the <a href=\"https://raw.githubusercontent.com/OpenCTI-Platform/opencti/master/CODE_OF_CONDUCT.md\">full text</a> so that you can understand what actions will and will not be tolerated.</p><p>Read our <a href=\"https://raw.githubusercontent.com/OpenCTI-Platform/opencti/master/CONTRIBUTING.md\">contributing guide</a> to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes to OpenCTI.</p><p>To help you get you familiar with our contribution process, we have a list of <a href=\"https://github.com/OpenCTI-Platform/opencti/labels/beginner%20friendly%20issue\">beginner friendly issues</a> which are fairly easy to implement. This is a great place to get started.</p><p>If you want to actively help OpenCTI, we created a <a href=\"https://docs.opencti.io/latest/development/environment_ubuntu/\">dedicated documentation</a> about the deployment of a development environment and how to start the source code modification.</p><p>Currently OpenCTI is under heavy development, if you wish to report bugs or ask for new features, you can directly use the <a href=\"https://github.com/OpenCTI-Platform/opencti/issues\">Github issues module</a>.</p><p>If you need support or you wish to engage a discussion about the OpenCTI platform, feel free to join us on our <a href=\"https://community.filigran.io\">Slack channel</a>. You can also send us an email to <a href=\"mailto:contact@filigran.io\">contact@filigran.io</a>.</p><p>OpenCTI is a product designed and developed by the company <a href=\"https://filigran.io\">Filigran</a>.</p><p>To improve the features and the performances of OpenCTI, the platform collects anonymous statistical data related to its usage and health.</p><p>To provide OpenCTI users with cartography features, the platform uses a dedicated OpenStreetMap server (<a href=\"https://map.opencti.io\">https://map.opencti.io</a>). To monitor usage and adapt services performances, Filigran collects access log to this server (including IP addresses).</p><p>By using this server, you authorize Filigran to collect this information. Otherwise, you are free to deploy your own OpenStreetMap server and modify the platform configuration accordingly.</p><p>If you have started using the Filigran server and change your mind, you have the right to access, limit, rectify, erase and receive your data. To exercise your rights, please send your request to <a href=\"mailto:privacy@filigran.io\">privacy@filigran.io</a>.</p>",
      "contentLength": 4511,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "anthropics/claude-quickstarts",
      "url": "https://github.com/anthropics/claude-quickstarts",
      "date": 1771383966,
      "author": "",
      "guid": 45960,
      "unread": true,
      "content": "<p>A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API</p><p>Claude Quickstarts is a collection of projects designed to help developers quickly get started with building applications using the Claude API. Each quickstart provides a foundation that you can easily build upon and customize for your specific needs.</p><p>To use these quickstarts, you'll need an Claude API key. If you don't have one yet, you can sign up for free at <a href=\"https://console.anthropic.com\">console.anthropic.com</a>.</p><p>A customer support agent powered by Claude. This project demonstrates how to leverage Claude's natural language understanding and generation capabilities to create an AI-assisted customer support system with access to a knowledge base.</p><p>A financial data analyst powered by Claude. This project demonstrates how to leverage Claude's capabilities with interactive data visualization to analyze financial data via chat.</p><p>An environment and tools that Claude can use to control a desktop computer. This project demonstrates how to leverage the computer use capabilities of Claude, including support for the latest  tool version with zoom actions.</p><p>A complete reference implementation for browser automation powered by Claude. This project demonstrates how to leverage Claude's browser tools API for web interaction, including navigation, DOM inspection, and form manipulation using Playwright.</p><p>An autonomous coding agent powered by the Claude Agent SDK. This project demonstrates a two-agent pattern (initializer + coding agent) that can build complete applications over multiple sessions, with progress persisted via git and a feature list that the agent works through incrementally.</p><p>Each quickstart project comes with its own README and setup instructions. Generally, you'll follow these steps:</p><ol><li>Navigate to the specific quickstart directory</li><li>Install the required dependencies</li><li>Set up your Claude API key as an environment variable</li><li>Run the quickstart application</li></ol><p>To deepen your understanding of working with Claude and the Claude API, check out these resources:</p><p>We welcome contributions to the Claude Quickstarts repository! If you have ideas for new quickstart projects or improvements to existing ones, please open an issue or submit a pull request.</p><p>This project is licensed under the MIT License - see the <a href=\"https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/LICENSE\">LICENSE</a> file for details.</p>",
      "contentLength": 2330,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ashishps1/awesome-system-design-resources",
      "url": "https://github.com/ashishps1/awesome-system-design-resources",
      "date": 1771383966,
      "author": "",
      "guid": 45961,
      "unread": true,
      "content": "<p>Learn System Design concepts and prepare for interviews using free resources.</p><p>This repository contains free resources to learn System Design concepts and prepare for interviews.</p><h2>üåê Networking Fundamentals</h2><h2>üîÑ Asynchronous Communication</h2><h2>üß© Distributed System and Microservices</h2><h2>üñáÔ∏è Architectural Patterns</h2><h2>‚öñÔ∏è System Design Tradeoffs</h2><h2>üíª System Design Interview Problems</h2><h2>üìú Must-Read Engineering Articles</h2><h2>üóûÔ∏è Must-Read Distributed Systems Papers</h2><p align=\"center\"><i>If you find this resource helpful, please give it a star ‚≠êÔ∏è and share it with others!</i></p>",
      "contentLength": 541,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "davila7/claude-code-templates",
      "url": "https://github.com/davila7/claude-code-templates",
      "date": 1771383966,
      "author": "",
      "guid": 45962,
      "unread": true,
      "content": "<p>CLI tool for configuring and monitoring Claude Code</p><p><strong>Ready-to-use configurations for Anthropic's Claude Code.</strong> A comprehensive collection of AI agents, custom commands, settings, hooks, external integrations (MCPs), and project templates to enhance your development workflow.</p><h2>Browse &amp; Install Components and Templates</h2><p> - Interactive web interface to explore and install 100+ agents, commands, settings, hooks, and MCPs.</p><img width=\"1049\" height=\"855\" alt=\"Screenshot 2025-08-19 at 08 09 24\" src=\"https://github.com/user-attachments/assets/e3617410-9b1c-4731-87b7-a3858800b737\"><pre><code># Install a complete development stack\nnpx claude-code-templates@latest --agent development-team/frontend-developer --command testing/generate-tests --mcp development/github-integration --yes\n\n# Browse and install interactively\nnpx claude-code-templates@latest\n\n# Install specific components\nnpx claude-code-templates@latest --agent development-tools/code-reviewer --yes\nnpx claude-code-templates@latest --command performance/optimize-bundle --yes\nnpx claude-code-templates@latest --setting performance/mcp-timeouts --yes\nnpx claude-code-templates@latest --hook git/pre-commit-validation --yes\nnpx claude-code-templates@latest --mcp database/postgresql-integration --yes\n</code></pre><table><thead><tr></tr></thead><tbody><tr><td>AI specialists for specific domains</td><td>Security auditor, React performance optimizer, database architect</td></tr><tr><td>, , </td></tr><tr><td>External service integrations</td><td>GitHub, PostgreSQL, Stripe, AWS, OpenAI</td></tr><tr><td>Claude Code configurations</td><td>Timeouts, memory settings, output styles</td></tr><tr><td>Pre-commit validation, post-completion actions</td></tr><tr><td>Reusable capabilities with progressive disclosure</td><td>PDF processing, Excel automation, custom workflows</td></tr></tbody></table><p>Beyond the template catalog, Claude Code Templates includes powerful development tools:</p><p>Monitor your AI-powered development sessions in real-time with live state detection and performance metrics.</p><pre><code>npx claude-code-templates@latest --analytics\n</code></pre><p>Mobile-optimized interface to view Claude responses in real-time with secure remote access.</p><pre><code># Local access\nnpx claude-code-templates@latest --chats\n\n# Secure remote access via Cloudflare Tunnel\nnpx claude-code-templates@latest --chats --tunnel\n</code></pre><p>Comprehensive diagnostics to ensure your Claude Code installation is optimized.</p><pre><code>npx claude-code-templates@latest --health-check\n</code></pre><p>View marketplaces, installed plugins, and manage permissions from a unified interface.</p><pre><code>npx claude-code-templates@latest --plugins\n</code></pre><p> - Complete guides, examples, and API reference for all components and tools.</p><p>This collection includes components from multiple sources:</p><p><strong>Community Skills &amp; Agents:</strong></p><ul><li> - Community contribution - MIT License (specialized enterprise skills)</li></ul><p>Each of these resources retains its <strong>original license and attribution</strong>, as defined by their respective authors. We respect and credit all original creators for their work and contributions to the Claude ecosystem.</p><p>This project is licensed under the MIT License - see the <a href=\"https://raw.githubusercontent.com/davila7/claude-code-templates/main/LICENSE\">LICENSE</a> file for details.</p><p><strong>‚≠ê Found this useful? Give us a star to support the project!</strong></p>",
      "contentLength": 2793,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "seerr-team/seerr",
      "url": "https://github.com/seerr-team/seerr",
      "date": 1771297503,
      "author": "",
      "guid": 45613,
      "unread": true,
      "content": "<p>Open-source media request and discovery manager for Jellyfin, Plex, and Emby.</p><p> is a free and open source software application for managing requests for your media library. It integrates with the media server of your choice: <a href=\"https://jellyfin.org\">Jellyfin</a>, <a href=\"https://plex.tv\">Plex</a>, and <a href=\"https://emby.media/\">Emby</a>. In addition, it integrates with your existing services, such as , .</p><ul><li>Full Jellyfin/Emby/Plex integration including authentication with user import &amp; management.</li><li>Support for  and  databases.</li><li>Supports Movies, Shows and Mixed Libraries.</li><li>Ability to change email addresses for SMTP purposes.</li><li>Easy integration with your existing services. Currently, Seerr supports Sonarr and Radarr. More to come!</li><li>Jellyfin/Emby/Plex library scan, to keep track of the titles which are already available.</li><li>Customizable request system, which allows users to request individual seasons or movies in a friendly, easy-to-use interface.</li><li>Incredibly simple request management UI. Don't dig through the app to simply approve recent requests!</li><li>Granular permission system.</li><li>Support for various notification agents.</li><li>Mobile-friendly design, for when you need to approve requests on the go!</li><li>Support for watchlisting &amp; blocklisting media.</li></ul><p>With more features on the way! Check out our <a href=\"https://raw.githubusercontent.com/seerr-team/issues\">issue tracker</a> to see the features which have already been requested.</p><p>Check out our documentation for instructions on how to install and run Seerr:</p><img src=\"https://raw.githubusercontent.com/seerr-team/seerr/develop/public/preview.jpg\" alt=\"Seerr application preview\"><h2>Migrating from Overseerr/Jellyseerr to Seerr</h2><p>Please follow our <a href=\"https://docs.seerr.dev/migration-guide\">migration guide</a> for detailed instructions on migrating from Overseerr or Jellyseerr.</p>",
      "contentLength": 1470,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "hummingbot/hummingbot",
      "url": "https://github.com/hummingbot/hummingbot",
      "date": 1771297503,
      "author": "",
      "guid": 45614,
      "unread": true,
      "content": "<p>Open source software that helps you create and deploy high-frequency crypto trading bots</p><p>Hummingbot is an open-source framework that helps you design and deploy automated trading strategies, or , that can run on many centralized or decentralized exchanges. Over the past year, Hummingbot users have generated over $34 billion in trading volume across 140+ unique trading venues.</p><p>The Hummingbot codebase is free and publicly available under the Apache 2.0 open-source license. Our mission is to <strong>democratize high-frequency trading</strong> by creating a global community of algorithmic traders and developers that share knowledge and contribute to the codebase.</p><ul><li><a href=\"https://discord.gg/hummingbot\">Discord</a>: The main gathering spot for the global Hummingbot community</li><li><a href=\"https://www.youtube.com/c/hummingbot\">YouTube</a>: Videos that teach you how to get the most out of Hummingbot</li><li><a href=\"https://twitter.com/_hummingbot\">Twitter</a>: Get the latest announcements about Hummingbot</li><li><a href=\"https://hummingbot.substack.com\">Newsletter</a>: Get our newsletter whenever we ship a new release</li></ul><p>The easiest way to get started with Hummingbot is using Docker:</p><ul><li><p>To install the CLI-based Hummingbot client, follow the instructions below.</p></li></ul><p>Alternatively, if you are building new connectors/strategies or adding custom code, see the <a href=\"https://hummingbot.org/client/installation/#source-installation\">Install from Source</a> section in the documentation.</p><h3>Install Hummingbot with Docker</h3><p>Clone the repo and use the provided  file:</p><pre><code># Clone the repository\ngit clone https://github.com/hummingbot/hummingbot.git\ncd hummingbot\n\n# Run Setup &amp; Deploy\nmake setup\nmake deploy\n\n# Attach to the running instance\ndocker attach hummingbot\n</code></pre><h3>Install Hummingbot + Gateway DEX Middleware</h3><p>Gateway provides standardized connectors for interacting with automatic market maker (AMM) decentralized exchanges (DEXs) across different blockchain networks.</p><p>To run Hummingbot with Gateway, clone the repo and answer  when prompted after running </p><pre><code># Clone the repository\ngit clone https://github.com/hummingbot/hummingbot.git\ncd hummingbot\n</code></pre><pre><code>make setup\n\n# Answer `y` when prompted\nInclude Gateway? [y/N]\n</code></pre><pre><code>make deploy\n\n# Attach to the running instance\ndocker attach hummingbot\n</code></pre><p>By default, Gateway will start in development mode with unencrypted HTTP endpoints. To run in production model with encrypted HTTPS, use the  flag and run  in Hummingbot to generate the certificates needed. See <a href=\"http://hummingbot.org/gateway/installation/#development-vs-production-modes\">Development vs Production Modes</a> for more information.</p><p>For comprehensive installation instructions and troubleshooting, visit our <a href=\"https://hummingbot.org/installation/\">Installation</a> documentation.</p><p>If you encounter issues or have questions, here's how you can get assistance:</p><p>We pledge that we will not use the information/data you provide us for trading purposes nor share them with third parties.</p><p>Hummingbot connectors standardize REST and WebSocket API interfaces to different types of exchanges, enabling you to build sophisticated trading strategies that can be deployed across many exchanges with minimal changes.</p><p>We classify exchange connectors into three main categories:</p><ul><li><p>: Centralized exchanges with central limit order books that take custody of your funds. Connect via API keys.</p><ul><li>: Trading spot markets</li><li>: Trading perpetual futures markets</li></ul></li><li><p>: Decentralized exchanges with on-chain central limit order books. Non-custodial, connect via wallet keys.</p><ul><li>: Trading spot markets on-chain</li><li>: Trading perpetual futures on-chain</li></ul></li><li><p>: Decentralized exchanges using Automated Market Maker protocols. Non-custodial, connect via Gateway middleware.</p><ul><li>: DEX aggregators that find optimal swap routes</li><li>: Traditional constant product (x*y=k) pools</li><li>: Concentrated Liquidity Market Maker pools with custom price ranges</li></ul></li></ul><p>We are grateful for the following exchanges that support the development and maintenance of Hummingbot via broker partnerships and sponsorships.</p><h3>Other Exchange Connectors</h3><p>Currently, the master branch of Hummingbot also includes the following exchange connectors, which are maintained and updated through the Hummingbot Foundation governance process. See <a href=\"https://hummingbot.org/governance/\">Governance</a> for more information.</p><ul><li><a href=\"https://github.com/hummingbot/condor\">Condor</a>: Telegram Interface for Hummingbot</li><li><a href=\"https://github.com/hummingbot/mcp\">Hummingbot MCP</a>: Enables AI assistants like Claude and Gemini to interact with Hummingbot for automated cryptocurrency trading across multiple exchanges.</li><li><a href=\"https://github.com/hummingbot/quants-lab\">Quants Lab</a>: Jupyter notebooks that enable you to fetch data and perform research using Hummingbot</li><li><a href=\"https://github.com/hummingbot/gateway\">Gateway</a>: Typescript based API client for DEX connectors</li><li><a href=\"https://github.com/hummingbot/hummingbot-site\">Hummingbot Site</a>: Official documentation for Hummingbot - we welcome contributions here too!</li></ul><p>The Hummingbot architecture features modular components that can be maintained and extended by individual community members.</p><p>We welcome contributions from the community! Please review these <a href=\"https://raw.githubusercontent.com/hummingbot/hummingbot/master/CONTRIBUTING.md\">guidelines</a> before submitting a pull request.</p><p>To have your exchange connector or other pull request merged into the codebase, please submit a New Connector Proposal or Pull Request Proposal, following these <a href=\"https://hummingbot.org/about/proposals/\">guidelines</a>. Note that you will need some amount of <a href=\"https://etherscan.io/token/0xe5097d9baeafb89f9bcb78c9290d545db5f9e9cb\">HBOT tokens</a> in your Ethereum wallet to submit a proposal.</p><ul><li>: Hummingbot is open source and licensed under <a href=\"https://raw.githubusercontent.com/hummingbot/hummingbot/master/LICENSE\">Apache 2.0</a>.</li><li>: See <a href=\"https://hummingbot.org/reporting/\">Reporting</a> for information on anonymous data collection and reporting in Hummingbot.</li></ul>",
      "contentLength": 4882,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "moonshine-ai/moonshine",
      "url": "https://github.com/moonshine-ai/moonshine",
      "date": 1771211223,
      "author": "",
      "guid": 45351,
      "unread": true,
      "content": "<p>Fast and accurate automatic speech recognition (ASR) for edge devices</p><p><strong>Voice Interfaces for Everyone</strong></p><p><a href=\"https://moonshine.ai\">Moonshine</a> Voice is an open source AI toolkit for developers building real-time voice applications.</p><ul><li>Everything runs on-device, so it's fast, private, and you don't need an account, credit card, or API keys.</li><li>The framework and models are optimized for live streaming applications, offering low latency responses by doing a lot of the work while the user is still talking.</li><li>Batteries are included. Its high-level APIs offer complete solutions for common tasks like transcription, speaker identification (diarization) and command recognition, so you don't need to be an expert to build a voice application.</li><li>It supports multiple languages, including English, Spanish, Mandarin, Japanese, Korean, Vietnamese, Ukrainian, and Arabic.</li></ul><pre><code>pip install moonshine-voice\npython -m moonshine_voice.mic_transcriber --language en\n</code></pre><p>Listens to the microphone and prints updates to the transcript as they come in.</p><pre><code>python -m moonshine_voice.intent_recognizer\n</code></pre><p><a href=\"https://github.com/moonshine-ai/moonshine/archive/refs/heads/main.zip\">Download</a> or  this repository and then run:</p><pre><code>cd core\nmkdir build\ncmake ..\ncmake --build .\n./moonshine-cpp-test\n</code></pre><pre><code>pip install moonshine-voice\ncd examples\\windows\\cli-transcriber\n.\\download-lib.bat\nmsbuild cli-transcriber.sln /p:Configuration=Release /p:Platform=x64\npython -m moonshine_voice.download --language en\nx64\\Release\\cli-transcriber.exe --model-path &lt;path from the download command&gt; --model-arch &lt;number from the download command&gt;\n</code></pre><p>You'll need a USB microphone plugged in to get audio input, but the Python pip package has been optimized for the Pi, so you can run:</p><pre><code> sudo pip install --break-system-packages moonshine-voice\n python -m moonshine_voice.mic_transcriber --language en\n</code></pre><h2>When should you choose Moonshine over Whisper?</h2><p>TL;DR - When you're working with live speech.</p><table><thead><tr></tr></thead><tbody><tr><td>Moonshine Medium Streaming</td></tr><tr></tr><tr><td>Moonshine Small Streaming</td></tr><tr></tr><tr></tr><tr></tr></tbody></table><p><a href=\"https://github.com/moonshine-ai/moonshine\">OpenAI's release of their Whisper family of models</a> was a massive step forward for open-source speech to text. They offered a range of sizes, allowing developers to trade off compute and storage space against accuracy to fit their applications. Their biggest models, like Large v3, also gave accuracy scores that were higher than anything available outside of large tech companies like Google or Apple. At Moonshine we were early and enthusiastic adopters of Whisper, and we still remain big fans of the models and the great frameworks like <a href=\"https://github.com/SYSTRAN/faster-whisper\">FasterWhisper</a> and others that have been built around them.</p><p>However, as we built applications that needed a live voice interface we found we needed features that weren't available through Whisper:</p><ul><li><strong>Whisper always operates on a 30-second input window</strong>. This isn't an issue when you're processing audio in large batches, you can usually just look ahead in the file and find a 30-second-ish chunk of speech to apply it to. Voice interfaces can't look ahead to create larger chunks from their input stream, and phrases are seldom longer than five to ten seconds. This means there's a lot of wasted computation encoding zero padding in the encoder and decoder, which means longer latency in returning results. Since one of the most important requirements for any interface is responsiveness, usually defined as latency below 200ms, this hurts the user experience even on platforms that have compute to spare, and makes it unusable on more constrained devices.</li><li><strong>Whisper doesn't cache anything</strong>. Another common requirement for voice interfaces is that they display feedback as the user is talking, so that they know the app is listening and understanding them. This means calling the speech to text model repeatedly over time as a sentence is spoken. Most of the audio input is the same, with only a short addition to the end. Even though a lot of the input is constant, Whisper starts from scratch every time, doing a lot of redundant work on audio that it has seen before. Like the fixed input window, this unnecessary latency impairs the user experience.</li><li><strong>Whisper supports a lot of languages poorly</strong>. Whisper's multilingual support is an incredible feat of engineering, and demonstrated a single model could handle many languages, and even offer translations. This chart from OpenAI (<a href=\"https://cdn.openai.com/papers/whisper.pdf\">raw data in Appendix D-2.4</a>) shows the drop-off in Word Error Rate (WER) with the very largest 1.5 billion parameter model.</li></ul><p>82 languages are listed, but only 33 have sub-20% WER (what we consider usable). For the Base model size commonly used on edge devices, only 5 languages are under 20% WER. Asian languages like Korean and Japanese stand out as the native tongue of large markets with a lot of tech innovation, but Whisper doesn't offer good enough accuracy to use in most applications The proprietary in-house versions of Whisper that are available through OpenAI's cloud API seem to offer better accuracy, but aren't available as open models.</p><ul><li>. A fantastic ecosystem has grown up around Whisper, there are a lot of mature frameworks you can use to deploy the models. However these often tend to be focused on desktop-class machines and operating systems. There are projects you can use across edge platforms like iOS, Android, or Raspberry Pi OS, but they tend to have different interfaces, capabilities, and levels of optimization. This made building applications that need to run on a variety of devices unnecessarily difficult.</li></ul><p>All these limitations drove us to create our own family of models that better meet the needs of live voice interfaces. It took us some time since the combined size of the open speech datasets available is tiny compared to the amount of web-derived text data, but after extensive data-gathering work, we were able to release <a href=\"https://arxiv.org/abs/2410.15608\">the first generation of Moonshine models</a>. These removed the fixed-input window limitation along with some other architectural improvements, and gave significantly lower latency than Whisper in live speech applications, often running 5x faster or more.</p><p>However we kept encountering applications that needed even lower latencies on even more constrained platforms. We also wanted to offer higher accuracy than the Base-equivalent that was the top end of the initial models. That led us to this second generation of Moonshine models, which offer:</p><ul><li>. You can supply any length of audio (though we recommend staying below around 30 seconds) and the model will only spend compute on that input, no zero-padding required. This gives us a significant latency boost.</li><li>. Our models now support incremental addition of audio over time, and they cache the input encoding and part of the decoder's state so that we're able to skip even more of the compute, driving latency down dramatically.</li><li>. We have gathered data and trained models for multiple languages, including Arabic, Japanese, Korean, Spanish, Ukrainian, Vietnamese, and Chinese. As we discuss in our <a href=\"https://arxiv.org/abs/2509.02523\">Flavors of Moonshine paper</a>, we've found that we can get much higher accuracy for the same size and compute if we restrict a model to focus on just one language, compared to training one model across many.</li><li><strong>Cross-platform library support</strong>. We're building applications ourselves, and needed to be able to deploy these models across Linux, MacOS, Windows, iOS, and Android, as well as use them from languages like Python, Swift, Java, and C++. To support this we architected a portable C++ core library that handles all of the processing, uses OnnxRuntime for good performance across systems, and then built native interfaces for all the required high-level languages. This allows developers to learn one API, and then deploy it almost anywhere they want to run.</li><li><strong>Better accuracy than Whisper V3 Large</strong>. On <a href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\">HuggingFace's OpenASR leaderboard</a>, our newest streaming model for English, Medium Streaming, achieves a lower word-error rate than the most-accurate Whisper model from OpenAI. This is despite Moonshine's version using 250 million parameters, versus Large v3's 1.5 billion, making it much easier to deploy on the edge.</li></ul><p>Hopefully this gives you a good idea of how Moonshine compares to Whisper. If you're working with GPUs in the cloud on data in bulk where throughput is most important then Whisper (or Nvidia alternatives like Parakeet) offer advantages like batch processing, but we believe we can't be beat for live speech. We've built the framework and models we wished we'd had when we first started building applications with voice interfaces, so if you're working with live voice inputs, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#quickstart\">give Moonshine a try</a>.</p><p>The Moonshine API is designed to take care of the details around capturing and transcribing live speech, giving application developers a high-level API focused on actionable events. I'll use Python to illustrate how it works, but the API is consistent across all the supported languages.</p><p>Our goal is to build a framework that any developer can pick up and use, even with no previous experience of speech technologies. We've abstracted away a lot of the unnecessary details and provide a simple interface that lets you focus on building your application, and that's reflected in our system architecture.</p><ul><li>Create a  or  object, depending on whether you want the text that's spoken, or just to know that a user has requested an action.</li><li>Attach an  that gets called when important things occur, like the end of a phrase or an action being triggered, so your application can respond.</li></ul><p>Traditionally, adding a voice interface to an application or product required integrating a lot of different libraries to handle all the processing that's needed to capture audio and turn it into something actionable. The main steps involved are microphone capture, voice activity detection (to break a continuous stream of audio into sections of speech), speech to text, speaker identification, and intent recognition. Each of these steps typically involved a different framework, which greatly increased the complexity of integrating, optimizing, and maintaining these dependencies.</p><p>Moonshine Voice includes all of these stages in a single library, and abstracts away everything but the essential information your application needs to respond to user speech, whether you want to transcribe it or trigger actions.</p><p>Most developers should be able to treat the library as a black box that tells them when something interesting has happened, using our event-based classes to implement application logic. Of course the framework is fully open source, so speech experts can dive as deep under the hood as they'd like, but it's not necessary to use it.</p><p>A <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/mic_transcriber.py#L10\"></a> is a helper class based on the general transcriber that takes care of connecting to a microphone using your platform's built-in support (for example sounddevice in Python) and then feeding the audio in as it's captured.</p><p>A <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/transcriber.py#L297\"></a> is a handler for audio input. The reason streams exist is because you may want to process multiple audio inputs at once, and a transcriber can support those through multiple streams, without duplicating the model resources. If you only have one input, the transcriber class includes the same methods (start/stop/add_audio) as a stream, and you can use that interface instead and forget about streams.</p><p>A <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/moonshine_api.py#L51\"></a> is a data structure holding information about one line in the transcript. When someone is speaking, the library waits for short pauses (where punctuation might go in written language) and starts a new line. These aren't exactly sentences, since a speech pause isn't a sure sign of the end of a sentence, but this does break the spoken audio into segments that can be considered phrases. A line includes state such as whether the line has just started, is still being spoken, or is complete, along with its start time and duration.</p><p>A <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/moonshine_api.py#67\"></a> is a list of lines in time order holding information about what text has already been recognized, along with other state like when it was captured.</p><p>A <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/transcriber.py#L22\"></a> contains information about changes to the transcript. Events include a new line being started, the text in a line being updated, and a line being completed. The event object includes the transcript line it's referring to as a member, holding the latest state of that line.</p><p>A <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/transcriber.py#L266\"></a> is a protocol that allows app-defined functions to be called when transcript events happen. This is the main way that most applications interact with the results of the transcription. When live speech is happening, applications usually need to respond or display results as new speech is recognized, and this approach allows you to handle those changes in a similar way to events from traditional user interfaces like touch screen gestures or mouse clicks on buttons.</p><p>An <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/intent_recognizer.py#L44\"></a> is a type of TranscriptEventListener that allows you to invoke different callback functions when preprogrammed intents are detected. This is useful for building voice command recognition features.</p><h3>Getting Started with Transcription</h3><p>We have <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#examples\">examples</a> for most platforms so as a first step I recommend checking out what we have for the systems you're targeting.</p><p>Next, you'll need to <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#adding-the-library-to-your-own-app\">add the library to your project</a>. We aim to provide pre-built binaries for all major platforms using their native package managers. On Python this means a pip install, for Android it's a Maven package, and for MacOS and iOS we provide a Swift package through SPM.</p><p>The transcriber needs access to the files for the model you're using, so after <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#downloading-models\">downloading them</a> you'll need to place them somewhere the application can find them, and make a note of the path. This usually means adding them as resources in your IDE if you're planning to distribute the app, or you can use hard-wired paths if you're just experimenting. The download script gives you the location of the models and their architecture type on your drive after it completes.</p><p>Now you can try creating a transcriber. Here's what that looks like in Python:</p><pre><code>transcriber = Transcriber(model_path=model_path, model_arch=model_arch)\n</code></pre><p>If the model isn't found, or if there's any other error, this will throw an exception with information about the problem. You can also check the console for logs from the core library, these are printed to  or your system's equivalent.</p><p>Now we'll create a listener that contains the app logic that you want triggered when the transcript updates, and attach it to your transcriber:</p><pre><code>class TestListener(TranscriptEventListener):\n    def on_line_started(self, event):\n        print(f\"Line started: {event.line.text}\")\n\n    def on_line_text_changed(self, event):\n        print(f\"Line text changed: {event.line.text}\")\n\n    def on_line_completed(self, event):\n        print(f\"Line completed: {event.line.text}\")\n\ntranscriber.add_listener(listener)\n</code></pre><p>The transcriber needs some audio data to work with. If you want to try it with the microphone you can update your transcriber creation line to use a MicTranscriber instead, but if you want to start with a .wav file for testing purposes here's how you feed that in:</p><pre><code>    audio_data, sample_rate = load_wav_file(wav_path)\n\n    transcriber.start()\n\n    # Loop through the audio data in chunks to simulate live streaming\n    # from a microphone or other source.\n    chunk_duration = 0.1\n    chunk_size = int(chunk_duration * sample_rate)\n    for i in range(0, len(audio_data), chunk_size):\n        chunk = audio_data[i: i + chunk_size]\n        transcriber.add_audio(chunk, sample_rate)\n\n    transcriber.stop()\n</code></pre><p>The important things to notice here are:</p><ul><li>We create an array of mono audio data from a wav file, using the convenience  function that's part of the Moonshine library.</li><li>We start the transcriber to activate its processing code.</li><li>The loop adds audio in chunks. These chunks can be any length and any sample rate, the library takes care of all the housekeeping.</li><li>As audio is added, the event listener you added will be called, giving information about the latest speech.</li></ul><p>In a real application you'd be calling  from an audio handler that's receiving it from your source. Since the library can handle arbitrary durations and sample rates, just make sure it's mono and otherwise feed it in as-is.</p><p>The transcriber analyses the speech at a default interval of every 500ms of input. You can change this with the  argument to the transcriber constructor. For streaming models most of the work is done as the audio is being added, and it's automatically done at the end of a phrase, so changing this won't usually affect the workload or latency massively.</p><p>The key takeaway is that you usually don't need to worry about the transcript data structure itself, the event system tells you when something important happens. You can manually trigger a transcript update by calling  which returns a transcript object with all of the information about the current session if you do need to examine the state.</p><p>By calling  and  on a transcriber (or stream) we're beginning and ending a session. Each session has one transcript document associated with it, and it is started fresh on every  call, so you should make copies of any data you need from the transcript object before that.</p><p>The transcriber class also offers a simpler <code>transcribe_without_streaming()</code> method, for when you have an array of data from the past that you just want to analyse, such as a file or recording.</p><p>We also offer a specialization of the base  class called . How this is implemented will depend on the language and platform, but it should provide a transcriber that's automatically attached to the main microphone on the system. This makes it straightforward to start transcribing speech from that common source, since it supports all of the same listener callbacks as the base class.</p><p>The main communication channel between the library and your application is through events that are passed to any listener functions you have registered. There are four major event types:</p><ul><li>. This is sent to listeners when the beginning of a new speech segment is detected. It may or may not contain any text, but since it's dispatched near the start of an utterance, that text is likely to change over time.</li><li>. Called whenever any of the information about a line changes, including the duration, audio data, and text.</li><li>. Called only when the text associated with a line is updated. This is a subset of  that focuses on the common need to refresh the text shown to users as often as possible to keep the experience interactive.</li><li>. Sent when we detect that someone has paused speaking, and we've ended the current segment. The line data structure has the final values for the text, duration, and speaker ID.</li></ul><p>We offer some guarantees about these events:</p><ul><li> is always called exactly once for any segment.</li><li> is always called exactly once after  for any segment.</li><li> and  will only ever be called after the  and before the  events for a segment.</li><li>Those update events are not guaranteed to be called (and in practice can be disabled by setting  to a very large value).</li><li>There will only be one line active at any one time for any given stream.</li><li>Once  has been called, the library will never alter that line's data again.</li><li>If  is called on a transcriber or stream, any active lines will have  called.</li><li>Each line has a 64-bit  that is designed to be unique enough to avoid collisions.</li><li>This  remains the same for the line over time, from the first  event onwards.</li></ul><h3>Getting Started with Command Recognition</h3><p>If you want your application to respond when users talk, you need to understand what they're saying. The previous generation of voice interfaces could only recognize speech that was phrased in exactly the form they expected. For example \"Alexa, turn on living-room lights\" might work, but \"Alexa, lights on in the living room please\" might not. The general problem of figuring out what a user wants from natural speech is known as intent recognition. There have been decades of research into this area, but the rise of transformer-based LLMs has given us new tools. We have integrated some of these advances into Moonshine Voice's command recognition API.</p><p>The basic idea is that your application registers some general actions you're interested in, like \"Turn the lights on\" or \"Move left\", and then Moonshine sends an event when the user says something that matches the meaning of those phrases. It works a lot like a graphical user interface - you define a button (action) and an event callback that is triggered when the user presses that button.</p><p>To give it a try for yourself, run this built-in example:</p><pre><code>python -m moonshine_voice.intent_recognizer\n</code></pre><p>This will present you with a menu of command phrases, and then start listening to the microphone. If you say something that's a variant on one of the phrases you'll see a \"triggered\" log message telling you which action was matched, along with how confident the system is in the match.</p><pre><code>üìù Let there be light.\n'TURN ON THE LIGHTS' triggered by 'Let there be light.' with 76% confidence\n</code></pre><p>To show that you can modify these at run time, try supplying your own list of phrases as a comma-separated string argument to .</p><pre><code>python -m moonshine_voice.intent_recognizer --intents \"Turn left, turn right, go backwards, go forward\"\n</code></pre><p>This could be the core command set to control a robot's movement for example. It's worth spending a bit of time experimenting with different wordings of the command phrases, and different variations on the user side, to get a feel for how the system works.</p><p>Under the hood this is all accomplished using two main classes. We've met the  above, the new addition is . This listens to the results of the transcriber, fuzzily matches completed lines against any intents that have been registered with it, and calls back the client-supplied code.</p><p>The fuzzy matching uses a sentence-embedding model based on Gemma300m, so the first step is downloading it and getting the path:</p><pre><code>embedding_model_path, embedding_model_arch = get_embedding_model(\n    args.embedding_model, args.quantization\n)\n</code></pre><p>Once we have the model's location, we create an  using that path. The only other argument is the  we use for fuzzy matching. It's between 0 and 1, with low numbers producing more matches but at the cost of less accuracy, and vice versa for high values.</p><pre><code>intent_recognizer = IntentRecognizer(\n    model_path=embedding_model_path,\n    model_arch=embedding_model_arch,\n    model_variant=args.quantization,\n    threshold=args.threshold,\n)\n</code></pre><p>Next we tell the recognizer what kinds of phrases to listen out for, and what to do when there's a match.</p><pre><code>def on_intent_triggered_on(trigger: str, utterance: str, similarity: float):\n    print(f\"\\n'{trigger.upper()}' triggered by '{utterance}' with {similarity:.0%} confidence\")\n\nfor intent in intents:\n    intent_recognizer.register_intent(intent, on_intent_triggered_on)\n</code></pre><p>The recognizer supports the transcript event listener interface, so the final stage is adding it as a listener to the .</p><pre><code>mic_transcriber.add_listener(intent_recognizer)\n</code></pre><p>Once you start the transcriber, it will listen out for any variations on the supplied phrases, and call  whenever there's a match.</p><p>The current intent recognition is designed for full-sentence matching, which works well for straightforward commands, but we will be expanding into more advanced \"slot filling\" techniques in the future, to handle extracting the quantity from \"I want ten bananas\" for example.</p><p>The <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/\"></a> folder has code samples organized by platform. We offer these for <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/android/\">Android</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/c++/\">portable C++</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/ios/\">iOS</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/macos/\">MacOS</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/python\">Python</a>, and <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/windows/\">Windows</a>. We have tried to use the most common build system for each platform, so Android uses Android Studio and Maven, iOS and MacOS use Xcode and Swift, while Windows uses Visual Studio.</p><p>The examples usually include one minimal project that just creates a transcriber and then feeds it data from a WAV file, and another that's pulling audio from a microphone using the platform's default framework for accessing audio devices.</p><h3>Adding the Library to your own App</h3><p>We distribute the library through the most widely-used package managers for each platform. Here's how you can use these to add the framework to an existing project on different systems.</p><p>The Python package is <a href=\"https://pypi.org/project/moonshine-voice/\">hosted on PyPi</a>, so all you should need to do to install it is <code>pip install moonshine-voice</code>, and then  in your project.</p><p>For iOS we use the Swift Package Manager, with <a href=\"https://github.com/moonshine-ai/moonshine-swift/\">an auto-updated GitHub repository</a> holding each version. To use this right-click on the file view sidebar in Xcode and choose \"Add Package Dependencies...\" from the menu. A dialog should open up, paste <code>https://github.com/moonshine-ai/moonshine-swift/</code> into the top search box and you should see . Select it and choose \"Add Package\", and it should be added to your project. You should now be able to  and use the library. You will need to add any model files you use to your app bundle and ensure they're copied during the deployment phase, so they can be accessed on-device.</p><p>On Android we publish <a href=\"https://mvnrepository.com/artifact/ai.moonshine/moonshine-voice\">the package to Maven</a>. To include it in your project using Android Studio and Gradle, first add the version number you want to the <code>gradle/libs.versions.toml</code> file by inserting a line in the  section, for example <code>moonshineVoice = \"0.0.48\"</code>. Then in the  part, add a reference to the package: <code>moonshine-voice = { group = \"ai.moonshine\", name = \"moonshine-voice\", version.ref = \"moonshineVoice\" }</code>.</p><p>Finally, in your  add the library to the  list: <code>implementation(libs.moonshine.voice)</code>. You can find a working example of all these changes in [<code>examples/android/Transcriber</code>].</p><p>We couldn't find a single package manager that is used by most Windows developers, so instead we've made the raw library and headers available as a download. The script in <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/windows/cli-transcriber/download-lib.bat\"><code>examples/windows/cli-transcriber/download-lib.bat</code></a> will fetch these for you. You'll see an  folder that you should add to the include search paths in your project settings, and a  directory that you should add to the include search paths. Then add all of the library files in the  folder to your project's linker dependencies.</p><p>The recommended interface to use on Windows is the C++ language binding. This is a header-only library that offers a higher-level API than the underlying C version. You can <code>#include \"moonshine-cpp.h\"</code> to access Moonshine from your C++ code. If you want to see an example of all these changes together, take a look at <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/windows/cli-transcriber\"><code>examples/windows/cli-transcriber</code></a>.</p><p>The library is designed to help you understand what's going wrong when you hit an issue. If something isn't working as expected, the first place to look is the console for log messages. Whenever there's a failure point or an exception within the core library, you should see a message that adds more information about what went wrong. Your language bindings should also recognize when the core library has returned an error and raise an appropriate exception, but sometimes the logs can be helpful because they contain more details.</p><p>If no errors are being reported but the quality of the transcription isn't what you expect, it's worth ruling out an issue with the audio data that the transcriber is receiving. To make this easier, you can pass in the  option when you create a transcriber. That will save any audio received into .wav files in the folder you specify. Here's a Python example:</p><pre><code>python -m moonshine_voice.transcriber --options='save_input_wav_path=.'\n</code></pre><p>This will run test audio through a transcriber, and write out the audio it has received into an  file in the current directory. If you're running multiple streams, you'll see , etc for each additional one. These wavs only contain the audio data from the latest session, and are overwritten after each one is started. Listening to these files should help you confirm that the input you're providing is as you expect it, and not distorted or corrupted.</p><p>If you're running into errors it can be hard to keep track of the timeline of your interactions with the library. The  option will print out the underlying API calls that have been triggered to the console, so you can investigate any ordering or timing issues.</p><pre><code>uv run -m moonshine_voice.transcriber --options='log_api_calls=true'\n</code></pre><p>If you want to debug into the library internals, or add instrumentation to help understand its operation, or add improvements or customizations, all of the source is available for you to build it for yourself.</p><p>The core engine of the library is contained in the  folder of this repo. It's written in C++ with a C interface for easy integration with other languages. We use cmake to build on all our platforms, and so the easiest way to get started is something like this:</p><pre><code>cd core\nmkdir -p build\ncd build\ncmake ..\ncmake --build .\n</code></pre><p>After that completes you should have a set of binary executables you can run on your own system. These executables are all unit tests, and expect to be run from the  folder. You can run the build and test process in one step using the <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/scripts/run-core-tests.sh\"><code>scripts/run-core-tests.sh</code></a>, or <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/scripts/run-core-tests.bat\"><code>scripts/run-core-tests.bat</code></a> for Windows. All tests should compile and run without any errors.</p><p>There are various scripts for building for different platforms and languages, but to see examples of how to build for all of the supported systems you should look at <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/scripts/build-all-platforms.sh\"><code>scripts/build-all-platforms.sh</code></a>. This is the script we call for every release, and it builds all of the artifacts we upload to the various package manager systems.</p><p>The different platforms and languages have a layer on top of the C interfaces to enable idiomatic use of the library within the different environments. The major systems have their own top-level folders in this repo, for example: <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/\"></a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/android/\"></a>, and <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/swift/\"></a> for iOS and MacOS. This is where you'll find the code that calls the underlying core library routines, and handles the event system for each platform.</p><p>If you have a device that isn't supported, you can try <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#cmake\">building using cmake</a> on your system. The only major dependency that the C++ core library has is <a href=\"https://github.com/microsoft/onnxruntime\">the Onnx Runtime</a>. We include <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/core/third-party/onnxruntime/lib/\">pre-built binary library files</a> for all our supported systems, but you'll need to find or build your own version if the libraries we offer don't cover your use case.</p><p>If you want to call this library from a language we don't support, then you should take a look at <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/core/moonshine-c-api.h\">the C interface bindings</a>. Most languages have some way to call into C functions, so you can use these and the binding examples for other languages to guide your implementation.</p><p>The easiest way to get the model files is using the Python module. After <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#python\">installing it</a> run the downloader like this:</p><pre><code>python -m moonshine_voice.download --language en\n</code></pre><p>You can use either the two-letter code or the English name for the  argument. If you want to see which languages are supported by your current version they're <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#available-models\">listed below</a>, or you can supply a bogus language as the argument to this command:</p><pre><code>python -m moonshine_voice.download --language foo\n</code></pre><p>You can also optionally request a specific model architecture using the  flag, chosen from the numbers in <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/core/moonshine-c-api.h\">moonshine-c-api.h</a>. If no architecture is set, the script will load the highest-quality model available.</p><p>The download script will log the location of the downloaded model files and the model architecture, for example:</p><pre><code>encoder_model.ort: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.9M/29.9M [00:00&lt;00:00, 34.5MB/s]\ndecoder_model_merged.ort: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 104M/104M [00:02&lt;00:00, 52.6MB/s]\ntokenizer.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 244k/244k [00:00&lt;00:00, 1.44MB/s]\nModel download url: https://download.moonshine.ai/model/base-en/quantized/base-en\nModel components: ['encoder_model.ort', 'decoder_model_merged.ort', 'tokenizer.bin']\nModel arch: 1\nDownloaded model path: /Users/petewarden/Library/Caches/moonshine_voice/download.moonshine.ai/model/base-en/quantized/base-en\n</code></pre><p>The last two lines tell you which model architecture is being used, and where the model files are on disk. By default it uses your user cache directory, which is <code>~/Library/Caches/moonshine_voice</code> on MacOS, but you can use a different location by setting the  environment variable before running the script.</p><p>The core library includes a benchmarking tool that simulates processing live audio by loading a .wav audio file and feeding it in chunks to the model. To run it:</p><pre><code>cd core\nmd build\ncd build\ncmake ..\ncmake --build . --config Release\n./benchmark\n</code></pre><p>This will report the absolute time taken to process the audio, what percentage of the audio file's duration that is, and the average latency for a response.</p><p>The percentage is helpful because it approximates how much of a compute load the model will be on your hardware. For example, if it shows 20% then that means the speech processing will take a fifth of the compute time when running in your application, leaving 80% for the rest of your code.</p><p>The latency metric needs a bit of explanation. What most applications care about is how soon they are notified about a phrase after the user has finished talking, since this determines how fast the product can respond. As with any user interface, the time between speech ending and the app doing something determines how responsive the voice interface feels, with a goal of keeping it below 200ms. The latency figure logged here is the average time between when the library determines the user has stopped talking and the delivery of the final transcript of that phrase to the client. This is where streaming models have the most impact, since they do a lot of their work upfront, while speech is still happening, so they can usually finish very quickly.</p><p>By default the benchmark binary uses the Tiny English model that's embedded in the framework, but you can pass in the  and  parameters to choose <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#downloading-models\">one that you've downloaded</a>.</p><p>You can also choose how often the transcript should be updated using the  argument. This defaults to 0.5 seconds, but the right value will depend on how fast your application needs updates. Longer intervals reduce the compute required a bit, at the cost of slower updates.</p><p>For platforms that support Python, you can run the <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/scripts/run-benchmarks.py\"><code>scripts/run-benchmarks.py</code></a> script which will evaluate similar metrics, with the advantage that it can also download the models so you don't need to worry about path handling.</p><p>It also evaluates equivalent Whisper models. This is a pretty opinionated benchmark that looks at the latency and total compute cost of the two families of models in a situation that is representative of many common real-time voice applications' requirements:</p><ul><li>Speech needs to be responded to as quickly as possible once a user completes a phrase.</li><li>The phrases are of durations between a range of one to ten seconds.</li></ul><p>These are very different requirements from bulk offline processing scenarios, where the overall throughput of the system is more important, and so the latency on a single segment of speech is less important than the overall throughput of the system. This allows optimizations like batch processing.</p><p>We are not claiming that Whisper is not a great model for offline processing, but we do want to highlight the advantages we that Moonshine offers for live speech applications with real-time latency requirements.</p><p>The experimental setup is as follows:</p><ul><li>We use the two_cities.wav audio file as a test case, since it has a mix of short and long phrases. You can vary this by passing in your own audio file with the --wav_path argument.</li><li>We use the Moonshine Tiny, Base, Tiny Streaming, Small Streaming, and Medium Streaming models.</li><li>We compare these to the Whisper Tiny, Base, Small, and Large v3 models. Since the Moonshine Medium Streaming model achieves lower WER than Whisper Large v3 we compare those two, otherwise we compare each with their namesake.</li><li>We use the Moonshine VAD segmenter to split the audio into phrases, and feed each phrase to Whisper for transcription.</li><li>Response latency for both models is measured as the time between a phrase being identified as complete by the VAD segmenter and the transcribed text being returned. For Whisper this means the full transcription time, but since the Moonshine models are streaming we can do a lot of the work while speech is still happening, so the latency is much lower.</li><li>We measure the total compute cost of the models by totalling the duration of the audio processing times for each model, and then expressing that as a percentage of the total audio duration. This is the inverse of the commonly used real-time factor (RTF) metric, but it reflects the compute load required for a real-time application.</li><li>We're using faster-whisper for Whisper, since that seems to provide the best cross-platform performance. We're also sticking with the CPU, since most applications can't rely on GPU or NPU acceleration being present on all the platforms they target. We know there are a lot of great GPU/NPU-accelerated Whisper implementations out there, but these aren't portable enough to be useful for the applications we care about.</li></ul><p>Moonshine Voice is based on a family of speech to text models created by the team at Moonshine AI. If you want to download models to use with the framework, you can use <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#downloading-models\">the Python package to access them</a>. This section contains more information about the history and characteristics of the models we offer.</p><p>These research papers are a good resource for understanding the architectures and performance strategies behind the models:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>One common issue to watch out for if you're using models that don't use the Latin alphabet (so any languages except English and Spanish) is that you'll need to set the <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber-options\"> option</a> to 13.0 when you create the transcriber. This is because the most common pattern for hallucinations is endlessly repeating the last few words, and our heuristic to detect this is to check if there's an unusually high number of tokens for the duration of a segment. Unfortunately the base number of tokens per second for non-Latin languages is much higher than for English, thanks to how we're tokenizing, so you have to manually set the threshold higher to avoid cutting off valid outputs.</p><p>This documentation covers the Python API, but the same functions and classes are present in all the other supported languages, just with native adaptations (for example CamelCase). You should be able to use this as a reference for all platforms the library runs on.</p><p>Represents a single \"line\" or speech segment in a transcript. It includes information about the timing, speaker, and text content of the utterance, as well as state such as whether the speech is ongoing or done. If you're building an application that involves transcription, this data structure has all of the information available about each line of speech. Be aware that each line can be updated multiple times with new text and other information as the user keeps speaking.</p><ul><li><p>: A string containing the UTF-8 encoded text that has been extracted from the audio of this segment.</p></li><li><p>: A float value representing the time in seconds since the start of the current session that the current utterance was first detected.</p></li><li><p>: A float that represents the duration in seconds of the current utterance.</p></li><li><p>: An unsigned 64-bit integer that represents a line in a collision-resistant way, for use in storage and ensuring the application can keep track of lines as they change over time. See <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcription-event-flow\">Transcription Event Flow</a> for more details.</p></li><li><p>: A boolean that is false until the segment has been completed, and true for the remainder of the line's lifetime.</p></li><li><p>: A boolean that's true if any information about the line has changed since the last time the transcript was updated. Since the transcript will be periodically updated internally by the library as you add audio chunks, you can't rely on polling this to detect changes. You should rely on the event/listener flow to catch modifications instead. This applies to all of the booleans below too.</p></li><li><p>: A boolean indicating whether the line has been added to the transcript by the last update call.</p></li><li><p>: A boolean that's set if the contents of the line's text was modified by the last transcript update. If this is set,  will always be set too, but if other properties of the line (for example the duration or the audio data) have changed but the text remains the same, then  can be true while  is false.</p></li><li><p>: Whether a speaker has been identified for this line. Unless the  option passed to the Transcriber is set to false, this will always be true by the time the line is complete, and potentially it may be set earlier. The speaker identification process is still experimental, so the current accuracy may not be reliable enough for some applications.</p></li><li><p>: A unique-ish unsigned 64-bit integer that is designed for storage or used to identify the same speaker across multiple sessions.</p></li><li><p>: An integer that represents the order in which the speaker appeared in the transcript, to make it easy to give speakers default names like \"Speaker 1:\", etc.</p></li><li><p>: An array of 32-bit floats representing the raw audio data that the line is based on, as 16KHz mono PCM data between 0.0 and 1.0. This can be useful for further processing (for example to drive a visual indicator or to feed into a specialized speech to text model after the line is complete).</p></li></ul><p>A Transcript contains a list of TranscriberLines, arranged in descending time order. The transcript is reset at every  call, so if you need to retain information from it, you should make explicit copies. Most applications won't work with this structure, since all of the same information is available through event callbacks.</p><p>Contains information about a change to the transcript. It has four subclasses, which are explained in more detail in <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcription-event-flow\">the transcription event flow section</a>. Most of the information is contained in the  member, but there's also a  that your application can use to tell the source of a line if you're running multiple streams.</p><p>This event is sent to any listeners you have registered when an  finds a match to a command you've specified.</p><ul><li>: The string representing the canonical command, exactly as you registered it with the recognizer.</li><li>: The text of the utterance that triggered the match.</li><li>: A float value that reflects how confident the recognizer is that the utterance has the same meaning as the command, with zero being the least confident and one the most.</li></ul><p>Handles the speech to text pipeline.</p><ul><li><p>: Loads and initializes the transcriber.</p><ul><li>: The path to the directory holding the component model files needed for the complete flow. Note that this is a path to the , not an individual . You can download and get a path to a cached version of the standard models using the <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#downloading-models\">download_model()</a> function.</li><li>: The architecture of the model to load, from the selection defined in .</li><li>: By default the transcriber will periodically run text transcription as new audio data is fed, so that update events can be triggered. This value is how often the speech to text model should be run. You can set this to a large duration to suppress updates between a line starting and ending, but because the streaming models do a lot of their work before the final speech to text stage, this may not reduce overall latency by much.</li><li>: These are flags that affect how the transcription process works inside the library, often enabling performance optimizations or debug logging. They are passed as a dictionary mapping strings to strings, even if the values are to be interpreted as numbers - for example <code>{\"max_tokens_per_second\", \"15\"}</code>. \n    <ul><li>: If you only want the voice-activity detection and segmentation, but want to do further processing in your app, you can set this to \"true\" and then use the  array in each line.</li><li>: The models occassionally get caught in an infinite decoder loop, where the same words are repeated over and over again. As a heuristic to catch this we compare the number of tokens in the current run to the duration of the audio, and if there seem to be too many tokens we truncate the decoding. By default this is set to 6.5, but for non-English languages where the models produce a lot more raw tokens per second, you may want to bump this to 13.0.</li><li>: How often to run transcription, in seconds.</li><li>: Controls the sensitivity of the initial voice-activity detection stage that decides how to break raw audio into segments. This defaults to 0.5, with lower values creating longer segments, potentially with more background noise sections, and higher values breaking up speech into smaller chunks, at the risk of losing some actual speech by clipping.</li><li>: One of the most common causes of poor transcription quality is incorrect conversion or corruption of the audio that's fed into the pipeline. If you set this option to a folder path, the transcriber will save out exactly what it has received as 16KHz mono WAV files, so you can ensure that your input audio is as you expect.</li><li>: Another debugging option, turning this on causes all calls to the C API entry points in the library to write out information on their arguments to stderr or the console each time they're run.</li><li>: Prints information about the ONNXRuntime inference runs and how long they take.</li><li>: The VAD runs every 30ms, but to get higher-confidence values we average the results over time. This value is the time in seconds to average over. The default is 0.5s, shorter durations will spot speech faster at the cost of lower accuracy, higher values may increase accuracy, but at the cost of missing shorter utterances.</li><li><code>vad_look_behind_sample_count</code>: Because we're averaging over time, the mean VAD signal will lag behind the initial speech detection. To compensate for that, when speech is detected we pull in some of the audio immediately before the average passed the threshold. This value is the number of samples to prepend, and defaults to 8192 (all at 16KHz).</li><li>: It can be hard to find gaps in rapid-fire speech, but a lot of applications want their text in chunks that aren't endless. This option sets the longest duration a line can be before it's marked as complete and a new segment is started. The default is 15 seconds, and to increase the chance that a natural break is found, the  is linearly decreased over time from two thirds of the maximum duration until the maximum is reached.</li><li>: A boolean that controls whether to run the speaker identification stage in the pipeline.</li></ul></li></ul></li><li><p><code>transcribe_without_streaming()</code>: A convenience function to extract text from a non-live audio source, such as a file. We optimize for streaming use cases, so you're probably better off using libraries that specialize in bulk, batched transcription if you use this a lot and have performance constraints. This will still call any registered event listeners as it processes the lines, so this can be useful to test your application using pre-recorded files, or to easily integrate offline audio sources.</p><ul><li>: An array of 32-bit float values, representing mono PCM audio between -1.0 and 1.0, to be analyzed for speech.</li><li>: The number of samples per second. The library uses this to convert to its working rate (16KHz) internally.</li><li>: Integer, currently unused.</li></ul></li><li><p>: Begins a new transcription session. You need to call this after you've created the  and before you add any audio.</p></li><li><p>: Ends a transcription session. If a speech segment was still active, it's marked as complete and the appropriate event handlers are called.</p></li><li><p>: Call this every time you have a new chunk of audio from your input, to begin processing. The size and sample rate of the audio should be whatever's natural for your source, since the library will handle all conversions.</p><ul><li>: Array of 32-bit floats representing a mono PCM chunk of audio.</li><li>: How many samples per second are present in the input audio. The library uses this to convert the data to its preferred rate.</li></ul></li><li><p>: The transcript is usually updated periodically as audio data is added, but if you need to trigger one yourself, for example when a user presses refresh, or want access to the complete transcript, you can call this manually.</p><ul><li>: Integer holding flags that are combined using bitwise or (). \n    <ul><li><code>MOONSHINE_FLAG_FORCE_UPDATE</code>: By default the transcriber returns a cached version of the transcript if less than 200ms of new audio has come in since the last transcription, but by setting this you can ensure that a transcription happens regardless.</li></ul></li></ul></li><li><p>: If your application is taking audio input from multiple sources, for example a microphone and system audio, then you'll want to create multiple streams on a single transcriber to avoid loading multiple copies of the models. Each stream has its own transcript, and line events are tagged with the stream handle they came from. You don't need to worry about this if you only need to deal with a single input though, just use the  class's , , etc. This function returns  class object.</p><ul><li>: Integer, reserved for future expansion.</li><li>: Period in seconds between transcription updates.</li></ul></li><li><p>: Registers a callable object with the transcriber. This object will be called back as audio is fed in and text is extracted.</p><ul><li>: This is often a subclass of , but can be a plain function. It defines what code is called when a speech event happens.</li></ul></li><li><p>: Deletes a listener so that it no longer receives events.</p><ul><li>: An object you previously passed into .</li></ul></li><li><p>: Deletes all registered listeners so than none of them receive events anymore.</p></li></ul><p>This class supports the []](#transcriber-start), <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber-stop\"></a> and listener functions of <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber\"></a>, but internally creates and attaches to the system's microphone input, so you don't need to call <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber-add-audio\"></a> yourself. In Python this uses the <a href=\"https://github.com/moonshine-ai/moonshine\"> library</a>, but in other languages the class uses the native audio API under the hood.</p><p>A convenience class to derive from to create your own listener code. Override any or all of , , , and , and they'll be called back when the corresponding event occurs.</p><p>A specialized kind of event listener that you add as a listener to a , and it then analyzes the transcription results to determine if any of the specified commands have been spoken, using natural-language fuzzy matching.</p><ul><li>: Constructs a new recognizer, loading required models. \n  <ul><li>: String holding a path to a folder that contains the required embedding model files. You can download and obtain a path by calling <code>download_embedding_model()</code>.</li><li>: An , obtained from the <code>download_embedding_model()</code> function.</li><li>: The precision to run the model at. \"q4\" is recommended.</li><li>: How close an utterance has to be to the target sentence to trigger an event.</li></ul></li><li>: Asks the recognizer to look for utterances that match a given command, and call back into the application when one is found. \n  <ul><li>: The canonical command sentence to match against.</li><li>: A callable function or object that contains code you want to trigger when the command is recognized.</li></ul></li><li>: Removes an intent handler from the event callback process. \n  <ul><li>: A handler that had previously been registered with the recognizer.</li></ul></li><li>: Removes all intent listeners from the recognizer.</li><li>: Sets a callable that is called when any registered action is triggered, not just a single command as for .</li></ul><p>Our primary support channel is <a href=\"https://discord.gg/27qp9zSRXF\">the Moonshine Discord</a>. We make our best efforts to respond to questions there, and other channels like <a href=\"https://github.com/moonshine-ai/moonshine/issues\">GitHub issues</a>. We also offer paid support for commercial customers who need porting or acceleration on other platforms, model customization, more languages, or any other services, please <a href=\"mailto:contact@moonshine.ai\">get in touch</a>.</p><p>This library is in active development, and we aim to implement:</p><ul><li>Binary size reduction for mobile deployment.</li><li>Improved speaker identification.</li><li>Lightweight domain customization.</li></ul><p>This code, apart from the source in , is licensed under the MIT License, see LICENSE in this repository.</p><p>The English-language models are also released under the MIT License. Models for other languages are released under the <a href=\"https://moonshine.ai\">Moonshine Community License</a>, which is a non-commercial license.</p><p>The code in  is licensed according to the terms of the open source projects it originates from, with details in a LICENSE file in each subfolder.</p>",
      "contentLength": 51510,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "steipete/gogcli",
      "url": "https://github.com/steipete/gogcli",
      "date": 1771211223,
      "author": "",
      "guid": 45352,
      "unread": true,
      "content": "<p>Google Suite CLI: Gmail, GCal, GDrive, GContacts.</p><p>Fast, script-friendly CLI for Gmail, Calendar, Chat, Classroom, Drive, Docs, Slides, Sheets, Forms, Apps Script, Contacts, Tasks, People, Groups (Workspace), and Keep (Workspace-only). JSON-first output, multiple accounts, and least-privilege auth built in.</p><ul><li> - search threads and messages, send emails, view attachments, manage labels/drafts/filters/delegation/vacation settings, history, and watch (Pub/Sub push)</li><li> - track opens for  with a small Cloudflare Worker backend</li><li> - list/create/update events, detect conflicts, manage invitations, check free/busy status, team calendars, propose new times, focus/OOO/working-location events, recurrence + reminders</li><li> - manage courses, roster, coursework/materials, submissions, announcements, topics, invitations, guardians, profiles</li><li> - list/find/create spaces, list messages/threads (filter by thread/unread), send messages and DMs (Workspace-only)</li><li> - list/search/upload/download files, manage permissions/comments, organize folders, list shared drives</li><li> - search/create/update contacts, access Workspace directory/other contacts</li><li> - manage tasklists and tasks: get/create/add/update/done/undo/delete/clear, repeat schedules</li><li> - read/write/update spreadsheets, insert rows/cols, format cells, read notes, create new sheets (and export via Drive)</li><li> - create/get forms and inspect responses</li><li> - create/get projects, inspect content, and run functions</li><li> - export to PDF/DOCX/PPTX via Drive (plus create/copy, docs-to-text)</li><li> - access profile information</li><li> - list/get/search notes and download attachments (service account + domain-wide delegation)</li><li> - list groups you belong to, view group members (Google Workspace)</li><li> - quick local/UTC time display for scripts and agents</li><li> - manage multiple Google accounts simultaneously (with aliases)</li><li> - restrict top-level commands for sandboxed/agent runs</li><li><strong>Secure credential storage</strong> using OS keyring or encrypted on-disk keyring (configurable)</li><li> - authenticate once, use indefinitely</li><li> -  and  to request fewer scopes</li><li><strong>Workspace service accounts</strong> - domain-wide delegation auth (preferred when configured)</li><li> - JSON mode for scripting and automation (Calendar adds day-of-week fields)</li></ul><pre><code>brew install steipete/tap/gogcli\n</code></pre><pre><code>git clone https://github.com/steipete/gogcli.git\ncd gogcli\nmake\n</code></pre><ul><li> shows top-level command groups.</li><li>Drill down with  (and deeper subcommands).</li><li>For the full expanded command list: .</li><li>Make shortcut:  (or ).</li><li> shows CLI help (note:  is Make‚Äôs own help; use ).</li><li>Version:  or .</li></ul><h3>1. Get OAuth2 Credentials</h3><p>Before adding an account, create OAuth2 credentials from Google Cloud Console:</p><pre><code>gog auth credentials ~/Downloads/client_secret_....json\n</code></pre><p>For multiple OAuth clients/projects:</p><pre><code>gog --client work auth credentials ~/Downloads/work-client.json\ngog auth credentials list\n</code></pre><h3>3. Authorize Your Account</h3><pre><code>gog auth add you@gmail.com\n</code></pre><p>This will open a browser window for OAuth authorization. The refresh token is stored securely in your system keychain.</p><p>Headless / remote server flows (no browser on the server):</p><p>Manual interactive flow (recommended):</p><pre><code>gog auth add you@gmail.com --services user --manual\n</code></pre><ul><li>The CLI prints an auth URL. Open it in a local browser.</li><li>After approval, copy the full loopback redirect URL from the browser address bar.</li><li>Paste that URL back into the terminal when prompted.</li></ul><p>Split remote flow (, useful for two-step/scripted handoff):</p><pre><code># Step 1: print auth URL (open it locally in a browser)\ngog auth add you@gmail.com --services user --remote --step 1\n\n# Step 2: paste the full redirect URL from your browser address bar\ngog auth add you@gmail.com --services user --remote --step 2 --auth-url 'http://127.0.0.1:&lt;port&gt;/oauth2/callback?code=...&amp;state=...'\n</code></pre><ul><li>The  is cached on disk for a short time (about 10 minutes). If it expires, rerun step 1.</li><li>Remote step 2 requires a redirect URL that includes  (state check mandatory).</li></ul><pre><code>export GOG_ACCOUNT=you@gmail.com\ngog gmail labels list\n</code></pre><p> stores your OAuth refresh tokens in a ‚Äúkeyring‚Äù backend. Default is  (best available backend for your OS/environment).</p><p>Before you can run , you must store OAuth client credentials once via <code>gog auth credentials &lt;credentials.json&gt;</code> (download a Desktop app OAuth client JSON from the Cloud Console). For multiple clients, use <code>gog --client &lt;name&gt; auth credentials ...</code>; tokens are isolated per client.</p><p>Verify tokens are usable (helps spot revoked/expired tokens):</p><p>Accounts can be authorized either via OAuth refresh tokens or Workspace service accounts (domain-wide delegation). If a service account key is configured for an account, it takes precedence over OAuth refresh tokens (see ).</p><p>Show current auth state/services for the active account:</p><p>Use  (or ) to select a named OAuth client:</p><pre><code>gog --client work auth credentials ~/Downloads/work.json\ngog --client work auth add you@company.com\n</code></pre><p>Optional domain mapping for auto-selection:</p><pre><code>gog --client work auth credentials ~/Downloads/work.json --domain example.com\n</code></pre><ul><li>Default client is  (stored in ).</li><li>Named clients are stored as <code>credentials-&lt;client&gt;.json</code>.</li><li>Tokens are isolated per client (); defaults are per client too.</li></ul><p>Client selection order (when  is not set):</p><ol><li> config (email -&gt; client)</li><li> config (domain -&gt; client)</li><li>Credentials file named after the email domain (<code>credentials-example.com.json</code>)</li></ol><pre><code>{\n  account_clients: { \"you@company.com\": \"work\" },\n  client_domains: { \"example.com\": \"work\" },\n}\n</code></pre><pre><code>gog auth credentials list\n</code></pre><p>See  for the full client selection and mapping rules.</p><h3>Keyring backend: Keychain vs encrypted file</h3><ul><li> (default): picks the best backend for the platform.</li><li>: macOS Keychain (recommended on macOS; avoids password management).</li><li>: encrypted on-disk keyring (requires a password).</li></ul><p>Set backend via command (writes  into ):</p><pre><code>gog auth keyring file\ngog auth keyring keychain\ngog auth keyring auto\n</code></pre><p>Show current backend + source (env/config/default) and config path:</p><p>Non-interactive runs (CI/ssh): file backend requires .</p><pre><code>export GOG_KEYRING_PASSWORD='...'\ngog --no-input auth status\n</code></pre><p>Force backend via env (overrides config):</p><pre><code>export GOG_KEYRING_BACKEND=file\n</code></pre><p>Precedence:  env var overrides .</p><p>Specify the account using either a flag or environment variable:</p><pre><code># Via flag\ngog gmail search 'newer_than:7d' --account you@gmail.com\n\n# Via alias\ngog auth alias set work work@company.com\ngog gmail search 'newer_than:7d' --account work\n\n# Via environment\nexport GOG_ACCOUNT=you@gmail.com\ngog gmail search 'newer_than:7d'\n\n# Auto-select (default account or the single stored token)\ngog gmail labels list --account auto\n</code></pre><p>List configured accounts:</p><ul><li>Default: human-friendly tables on stdout.</li><li>: stable TSV on stdout (tabs preserved; best for piping to tools that expect ).</li><li>: JSON on stdout (best for scripting).</li><li>Human-facing hints/progress go to stderr.</li><li>Colors are enabled only in rich TTY output and are disabled automatically for  and .</li></ul><p>By default,  requests access to the  services (see  for the current list and scopes).</p><pre><code>gog auth add you@gmail.com --services drive,calendar\n</code></pre><p>To request read-only scopes (write operations will fail with 403 insufficient scopes):</p><pre><code>gog auth add you@gmail.com --services drive,calendar --readonly\n</code></pre><p>To control Drive‚Äôs scope (default: ):</p><pre><code>gog auth add you@gmail.com --services drive --drive-scope full\ngog auth add you@gmail.com --services drive --drive-scope readonly\ngog auth add you@gmail.com --services drive --drive-scope file\n</code></pre><ul><li> is enough for listing/downloading/exporting via Drive (write operations will 403).</li><li> is write-capable (limited to files created/opened by this app) and can‚Äôt be combined with .</li></ul><p>If you need to add services later and Google doesn't return a refresh token, re-run with :</p><pre><code>gog auth add you@gmail.com --services user --force-consent\n# Or add just Sheets\ngog auth add you@gmail.com --services sheets --force-consent\n</code></pre><p> is accepted as an alias for  for backwards compatibility.</p><p>Docs commands are implemented via the Drive API, and  requests both Drive and Docs API scopes.</p><p>Service scope matrix (auto-generated; run <code>go run scripts/gen-auth-services-md.go</code>):</p><table><thead><tr></tr></thead><tbody><tr><td><code>https://www.googleapis.com/auth/gmail.modify</code><code>https://www.googleapis.com/auth/gmail.settings.basic</code><code>https://www.googleapis.com/auth/gmail.settings.sharing</code></td></tr><tr><td><code>https://www.googleapis.com/auth/calendar</code></td></tr><tr><td><code>https://www.googleapis.com/auth/chat.spaces</code><code>https://www.googleapis.com/auth/chat.messages</code><code>https://www.googleapis.com/auth/chat.memberships</code><code>https://www.googleapis.com/auth/chat.users.readstate.readonly</code></td></tr><tr><td><code>https://www.googleapis.com/auth/classroom.courses</code><code>https://www.googleapis.com/auth/classroom.rosters</code><code>https://www.googleapis.com/auth/classroom.coursework.students</code><code>https://www.googleapis.com/auth/classroom.coursework.me</code><code>https://www.googleapis.com/auth/classroom.courseworkmaterials</code><code>https://www.googleapis.com/auth/classroom.announcements</code><code>https://www.googleapis.com/auth/classroom.topics</code><code>https://www.googleapis.com/auth/classroom.guardianlinks.students</code><code>https://www.googleapis.com/auth/classroom.profile.emails</code><code>https://www.googleapis.com/auth/classroom.profile.photos</code></td></tr><tr><td><code>https://www.googleapis.com/auth/drive</code></td></tr><tr><td><code>https://www.googleapis.com/auth/drive</code><code>https://www.googleapis.com/auth/documents</code></td><td>Export/copy/create via Drive</td></tr><tr><td><code>https://www.googleapis.com/auth/drive</code><code>https://www.googleapis.com/auth/presentations</code></td><td>Create/edit presentations</td></tr><tr><td><code>https://www.googleapis.com/auth/contacts</code><code>https://www.googleapis.com/auth/contacts.other.readonly</code><code>https://www.googleapis.com/auth/directory.readonly</code></td><td>Contacts + other contacts + directory</td></tr><tr><td><code>https://www.googleapis.com/auth/tasks</code></td></tr><tr><td><code>https://www.googleapis.com/auth/drive</code><code>https://www.googleapis.com/auth/spreadsheets</code></td></tr><tr></tr><tr><td><code>https://www.googleapis.com/auth/forms.body</code><code>https://www.googleapis.com/auth/forms.responses.readonly</code></td></tr><tr><td><code>https://www.googleapis.com/auth/script.projects</code><code>https://www.googleapis.com/auth/script.deployments</code><code>https://www.googleapis.com/auth/script.processes</code></td></tr><tr><td><code>https://www.googleapis.com/auth/cloud-identity.groups.readonly</code></td></tr><tr><td><code>https://www.googleapis.com/auth/keep.readonly</code></td><td>Workspace only; service account (domain-wide delegation)</td></tr></tbody></table><h3>Service Accounts (Workspace only)</h3><p>A service account is a non-human Google identity that belongs to a Google Cloud project. In Google Workspace, a service account can impersonate a user via  (admin-controlled) and access APIs like Gmail/Calendar/Drive as that user.</p><p>In , service accounts are an  that can be configured per account email. If a service account key is configured for an account, it takes precedence over OAuth refresh tokens (see ).</p><h4>1) Create a Service Account (Google Cloud)</h4><ol><li>Create (or pick) a Google Cloud project.</li><li>Enable the APIs you‚Äôll use (e.g. Gmail, Calendar, Drive, Sheets, Docs, People, Tasks, Cloud Identity).</li><li>Go to <strong>IAM &amp; Admin ‚Üí Service Accounts</strong> and create a service account.</li><li>In the service account details, enable .</li><li>Create a key (<strong>Keys ‚Üí Add key ‚Üí Create new key ‚Üí JSON</strong>) and download the JSON key file.</li></ol><h4>2) Allowlist scopes (Google Workspace Admin Console)</h4><p>Domain-wide delegation is enforced by Workspace admin settings.</p><ol><li>Open <strong>Admin console ‚Üí Security ‚Üí API controls ‚Üí Domain-wide delegation</strong>.</li><li>Add a new API client: \n  <ul><li>Client ID: use the service account‚Äôs ‚ÄúClient ID‚Äù from Google Cloud.</li><li>OAuth scopes: comma-separated list of scopes you want to allow (copy from  and/or your <code>gog auth add --services ...</code> usage).</li></ul></li></ol><p>If a scope is missing from the allowlist, service-account token minting can fail (or API calls will 403 with insufficient permissions).</p><h4>3) Configure  to use the service account</h4><p>Store the key for the user you want to impersonate:</p><pre><code>gog auth service-account set you@yourdomain.com --key ~/Downloads/service-account.json\n</code></pre><p>Verify  is preferring the service account for that account:</p><pre><code>gog --account you@yourdomain.com auth status\ngog auth list\n</code></pre><h3>Google Keep (Workspace only)</h3><p>Keep requires Workspace + domain-wide delegation. You can configure it via the generic service-account command above (recommended), or the legacy Keep helper:</p><pre><code>gog auth service-account set you@yourdomain.com --key ~/Downloads/service-account.json\ngog keep list --account you@yourdomain.com\ngog keep get &lt;noteId&gt; --account you@yourdomain.com\n</code></pre><ul><li> - Default account email or alias to use (avoids repeating ; otherwise uses keyring default or a single stored token)</li><li> - OAuth client name (selects stored credentials + token bucket)</li><li> - Default JSON output</li><li> - Default plain output</li><li> - Color mode:  (default), , or </li><li> - Default output timezone for Calendar/Gmail (IANA name, , or )</li><li> - Comma-separated allowlist of top-level commands (e.g., )</li></ul><p>Find the actual config path in  or .</p><ul><li>macOS: <code>~/Library/Application Support/gogcli/config.json</code></li><li>Linux: <code>~/.config/gogcli/config.json</code> (or <code>$XDG_CONFIG_HOME/gogcli/config.json</code>)</li><li>Windows: <code>%AppData%\\\\gogcli\\\\config.json</code></li></ul><p>Example (JSON5 supports comments and trailing commas):</p><pre><code>{\n  // Avoid macOS Keychain prompts\n  keyring_backend: \"file\",\n  // Default output timezone for Calendar/Gmail (IANA, UTC, or local)\n  default_timezone: \"UTC\",\n  // Optional account aliases\n  account_aliases: {\n    work: \"work@company.com\",\n    personal: \"me@gmail.com\",\n  },\n  // Optional per-account OAuth client selection\n  account_clients: {\n    \"work@company.com\": \"work\",\n  },\n  // Optional domain -&gt; client mapping\n  client_domains: {\n    \"example.com\": \"work\",\n  },\n}\n</code></pre><pre><code>gog config path\ngog config list\ngog config keys\ngog config get default_timezone\ngog config set default_timezone UTC\ngog config unset default_timezone\n</code></pre><pre><code>gog auth alias set work work@company.com\ngog auth alias list\ngog auth alias unset work\n</code></pre><p>Aliases work anywhere you pass  or  (reserved: , ).</p><h3>Command Allowlist (Sandboxing)</h3><pre><code># Only allow calendar + tasks commands for an agent\ngog --enable-commands calendar,tasks calendar events --today\n\n# Same via env\nexport GOG_ENABLE_COMMANDS=calendar,tasks\ngog tasks list &lt;tasklistId&gt;\n</code></pre><p>OAuth credentials are stored securely in your system's keychain:</p><ul><li>: Secret Service (GNOME Keyring, KWallet)</li><li>: Credential Manager</li></ul><p>If no OS keychain backend is available (e.g., Linux/WSL/container), keyring can fall back to an encrypted on-disk store and may prompt for a password; for non-interactive runs set .</p><p>macOS Keychain may prompt more than you‚Äôd expect when the ‚Äúapp identity‚Äù keeps changing (different binary path,  temp builds, rebuilding to new , multiple copies). Keychain treats those as different apps, so it asks again.</p><ul><li> keep using Keychain (secure) and run a stable  binary path to reduce repeat prompts.</li><li><code>GOG_KEYRING_BACKEND=keychain</code> (disables any file-backend fallback).</li><li><strong>Avoid Keychain prompts entirely:</strong> (stores encrypted entries on disk under your config dir). \n  <ul><li>To avoid password prompts too (CI/non-interactive): set  (tradeoff: secret in env).</li></ul></li></ul><ul><li><strong>Never commit OAuth client credentials</strong> to version control</li><li>Store client credentials outside your project directory</li><li>Use different OAuth clients for development and production</li><li>Re-authorize with  if you suspect token compromise</li><li>Remove unused accounts with </li></ul><h3>OAuth Client IDs in Open Source</h3><p>Some open source Google CLIs ship a pre-configured OAuth client ID/secret copied from other desktop apps to avoid OAuth consent verification, testing-user limits, or quota issues. This makes the consent screen/security emails show the other app‚Äôs name and can stop working at any time.</p><p> does not do this. Supported auth:</p><ul><li>Your own OAuth Desktop client JSON via  + </li><li>Google Workspace service accounts with domain-wide delegation (Workspace only)</li></ul><ul><li> also accepts .</li><li> also accepts  (Gmail thread attachment downloads).</li></ul><pre><code>gog auth credentials &lt;path&gt;           # Store OAuth client credentials\ngog auth credentials list             # List stored OAuth client credentials\ngog --client work auth credentials &lt;path&gt;  # Store named OAuth client credentials\ngog auth add &lt;email&gt;                  # Authorize and store refresh token\ngog auth service-account set &lt;email&gt; --key &lt;path&gt;  # Configure service account impersonation (Workspace only)\ngog auth service-account status &lt;email&gt;            # Show service account status\ngog auth service-account unset &lt;email&gt;             # Remove service account\ngog auth keep &lt;email&gt; --key &lt;path&gt;                 # Legacy alias (Keep)\ngog auth keyring [backend]            # Show/set keyring backend (auto|keychain|file)\ngog auth status                       # Show current auth state/services\ngog auth services                     # List available services and OAuth scopes\ngog auth list                         # List stored accounts\ngog auth list --check                 # Validate stored refresh tokens\ngog auth remove &lt;email&gt;               # Remove a stored refresh token\ngog auth manage                       # Open accounts manager in browser\ngog auth tokens                       # Manage stored refresh tokens\n</code></pre><pre><code>gog keep list --account you@yourdomain.com\ngog keep get &lt;noteId&gt; --account you@yourdomain.com\ngog keep search &lt;query&gt; --account you@yourdomain.com\ngog keep attachment &lt;attachmentName&gt; --account you@yourdomain.com --out ./attachment.bin\n</code></pre><pre><code># Search and read\ngog gmail search 'newer_than:7d' --max 10\ngog gmail thread get &lt;threadId&gt;\ngog gmail thread get &lt;threadId&gt; --download              # Download attachments to current dir\ngog gmail thread get &lt;threadId&gt; --download --out-dir ./attachments\ngog gmail get &lt;messageId&gt;\ngog gmail get &lt;messageId&gt; --format metadata\ngog gmail attachment &lt;messageId&gt; &lt;attachmentId&gt;\ngog gmail attachment &lt;messageId&gt; &lt;attachmentId&gt; --out ./attachment.bin\ngog gmail url &lt;threadId&gt;              # Print Gmail web URL\ngog gmail thread modify &lt;threadId&gt; --add STARRED --remove INBOX\n\n# Send and compose\ngog gmail send --to a@b.com --subject \"Hi\" --body \"Plain fallback\"\ngog gmail send --to a@b.com --subject \"Hi\" --body-file ./message.txt\ngog gmail send --to a@b.com --subject \"Hi\" --body-file -   # Read body from stdin\ngog gmail send --to a@b.com --subject \"Hi\" --body \"Plain fallback\" --body-html \"&lt;p&gt;Hello&lt;/p&gt;\"\n# Reply + include quoted original message (auto-generates HTML quote unless you pass --body-html)\ngog gmail send --reply-to-message-id &lt;messageId&gt; --quote --to a@b.com --subject \"Re: Hi\" --body \"My reply\"\ngog gmail drafts list\ngog gmail drafts create --subject \"Draft\" --body \"Body\"\ngog gmail drafts create --to a@b.com --subject \"Draft\" --body \"Body\"\ngog gmail drafts update &lt;draftId&gt; --subject \"Draft\" --body \"Body\"\ngog gmail drafts update &lt;draftId&gt; --to a@b.com --subject \"Draft\" --body \"Body\"\ngog gmail drafts send &lt;draftId&gt;\n\n# Labels\ngog gmail labels list\ngog gmail labels get INBOX --json  # Includes message counts\ngog gmail labels create \"My Label\"\ngog gmail labels modify &lt;threadId&gt; --add STARRED --remove INBOX\ngog gmail labels delete &lt;labelIdOrName&gt;  # Deletes user label (guards system labels; confirm)\n\n# Batch operations\ngog gmail batch delete &lt;messageId&gt; &lt;messageId&gt;\ngog gmail batch modify &lt;messageId&gt; &lt;messageId&gt; --add STARRED --remove INBOX\n\n# Filters\ngog gmail filters list\ngog gmail filters create --from 'noreply@example.com' --add-label 'Notifications'\ngog gmail filters delete &lt;filterId&gt;\n\n# Settings\ngog gmail autoforward get\ngog gmail autoforward enable --email forward@example.com\ngog gmail autoforward disable\ngog gmail forwarding list\ngog gmail forwarding add --email forward@example.com\ngog gmail sendas list\ngog gmail sendas create --email alias@example.com\ngog gmail vacation get\ngog gmail vacation enable --subject \"Out of office\" --message \"...\"\ngog gmail vacation disable\n\n# Delegation (G Suite/Workspace)\ngog gmail delegates list\ngog gmail delegates add --email delegate@example.com\ngog gmail delegates remove --email delegate@example.com\n\n# Watch (Pub/Sub push)\ngog gmail watch start --topic projects/&lt;p&gt;/topics/&lt;t&gt; --label INBOX\ngog gmail watch serve --bind 127.0.0.1 --token &lt;shared&gt; --hook-url http://127.0.0.1:18789/hooks/agent\ngog gmail watch serve --bind 0.0.0.0 --verify-oidc --oidc-email &lt;svc@...&gt; --hook-url &lt;url&gt;\ngog gmail watch serve --bind 127.0.0.1 --token &lt;shared&gt; --exclude-labels SPAM,TRASH --hook-url http://127.0.0.1:18789/hooks/agent\ngog gmail history --since &lt;historyId&gt;\n</code></pre><p>Gmail watch (Pub/Sub push):</p><ul><li>Create Pub/Sub topic + push subscription (OIDC preferred; shared token ok for dev).</li><li>Full flow + payload details: .</li><li><code>watch serve --exclude-labels</code> defaults to ; IDs are case-sensitive.</li></ul><p>Track when recipients open your emails:</p><pre><code># Set up local tracking config (per-account; generates keys; follow printed deploy steps)\ngog gmail track setup --worker-url https://gog-email-tracker.&lt;acct&gt;.workers.dev\n\n# Send with tracking\ngog gmail send --to recipient@example.com --subject \"Hello\" --body-html \"&lt;p&gt;Hi!&lt;/p&gt;\" --track\n\n# Check opens\ngog gmail track opens &lt;tracking_id&gt;\ngog gmail track opens --to recipient@example.com\n\n# View status\ngog gmail track status\n</code></pre><p>Docs:  (setup/deploy) + <code>docs/email-tracking-worker.md</code> (internals).</p><p> requires exactly 1 recipient (no cc/bcc) and an HTML body ( or ). Use  to send per-recipient messages with individual tracking ids. The tracking worker stores IP/user-agent + coarse geo by default.</p><pre><code># Calendars\ngog calendar calendars\ngog calendar acl &lt;calendarId&gt;         # List access control rules\ngog calendar colors                   # List available event/calendar colors\ngog calendar time --timezone America/New_York\ngog calendar users                    # List workspace users (use email as calendar ID)\n\n# Events (with timezone-aware time flags)\ngog calendar events &lt;calendarId&gt; --today                    # Today's events\ngog calendar events &lt;calendarId&gt; --tomorrow                 # Tomorrow's events\ngog calendar events &lt;calendarId&gt; --week                     # This week (Mon-Sun by default; use --week-start)\ngog calendar events &lt;calendarId&gt; --days 3                   # Next 3 days\ngog calendar events &lt;calendarId&gt; --from today --to friday   # Relative dates\ngog calendar events &lt;calendarId&gt; --from today --to friday --weekday   # Include weekday columns\ngog calendar events &lt;calendarId&gt; --from 2025-01-01T00:00:00Z --to 2025-01-08T00:00:00Z\ngog calendar events --all             # Fetch events from all calendars\ngog calendar events --calendars 1,3   # Fetch events from calendar indices (see gog calendar calendars)\ngog calendar events --cal Work --cal Personal  # Fetch events from calendars by name/ID\ngog calendar event &lt;calendarId&gt; &lt;eventId&gt;\ngog calendar get &lt;calendarId&gt; &lt;eventId&gt;                     # Alias for event\ngog calendar search \"meeting\" --today\ngog calendar search \"meeting\" --tomorrow\ngog calendar search \"meeting\" --days 365\ngog calendar search \"meeting\" --from 2025-01-01T00:00:00Z --to 2025-01-31T00:00:00Z --max 50\n\n# Search defaults to 30 days ago through 90 days ahead unless you set --from/--to/--today/--week/--days.\n# Tip: set GOG_CALENDAR_WEEKDAY=1 to default --weekday for calendar events output.\n\n# JSON event output includes timezone and localized times (useful for agents).\ngog calendar get &lt;calendarId&gt; &lt;eventId&gt; --json\n# {\n#   \"event\": {\n#     \"id\": \"...\",\n#     \"summary\": \"...\",\n#     \"startDayOfWeek\": \"Friday\",\n#     \"endDayOfWeek\": \"Friday\",\n#     \"timezone\": \"America/Los_Angeles\",\n#     \"eventTimezone\": \"America/New_York\",\n#     \"startLocal\": \"2026-01-23T20:45:00-08:00\",\n#     \"endLocal\": \"2026-01-23T22:45:00-08:00\",\n#     \"start\": { \"dateTime\": \"2026-01-23T23:45:00-05:00\" },\n#     \"end\": { \"dateTime\": \"2026-01-24T01:45:00-05:00\" }\n#   }\n# }\n\n# Team calendars (requires Cloud Identity API for Google Workspace)\ngog calendar team &lt;group-email&gt; --today           # Show team's events for today\ngog calendar team &lt;group-email&gt; --week            # Show team's events for the week (use --week-start)\ngog calendar team &lt;group-email&gt; --freebusy        # Show only busy/free blocks (faster)\ngog calendar team &lt;group-email&gt; --query \"standup\" # Filter by event title\n\n# Create and update\ngog calendar create &lt;calendarId&gt; \\\n  --summary \"Meeting\" \\\n  --from 2025-01-15T10:00:00Z \\\n  --to 2025-01-15T11:00:00Z\n\ngog calendar create &lt;calendarId&gt; \\\n  --summary \"Team Sync\" \\\n  --from 2025-01-15T14:00:00Z \\\n  --to 2025-01-15T15:00:00Z \\\n  --attendees \"alice@example.com,bob@example.com\" \\\n  --location \"Zoom\"\n\ngog calendar update &lt;calendarId&gt; &lt;eventId&gt; \\\n  --summary \"Updated Meeting\" \\\n  --from 2025-01-15T11:00:00Z \\\n  --to 2025-01-15T12:00:00Z\n\n# Send notifications when creating/updating\ngog calendar create &lt;calendarId&gt; \\\n  --summary \"Team Sync\" \\\n  --from 2025-01-15T14:00:00Z \\\n  --to 2025-01-15T15:00:00Z \\\n  --send-updates all\n\ngog calendar update &lt;calendarId&gt; &lt;eventId&gt; \\\n  --send-updates externalOnly\n\n# Default: no attendee notifications unless you pass --send-updates.\ngog calendar delete &lt;calendarId&gt; &lt;eventId&gt; \\\n  --send-updates all --force\n\n# Recurrence + reminders\ngog calendar create &lt;calendarId&gt; \\\n  --summary \"Payment\" \\\n  --from 2025-02-11T09:00:00-03:00 \\\n  --to 2025-02-11T09:15:00-03:00 \\\n  --rrule \"RRULE:FREQ=MONTHLY;BYMONTHDAY=11\" \\\n  --reminder \"email:3d\" \\\n  --reminder \"popup:30m\"\n\n# Special event types via --event-type (focus-time/out-of-office/working-location)\ngog calendar create primary \\\n  --event-type focus-time \\\n  --from 2025-01-15T13:00:00Z \\\n  --to 2025-01-15T14:00:00Z\n\ngog calendar create primary \\\n  --event-type out-of-office \\\n  --from 2025-01-20 \\\n  --to 2025-01-21 \\\n  --all-day\n\ngog calendar create primary \\\n  --event-type working-location \\\n  --working-location-type office \\\n  --working-office-label \"HQ\" \\\n  --from 2025-01-22 \\\n  --to 2025-01-23\n\n# Dedicated shortcuts (same event types, more opinionated defaults)\ngog calendar focus-time --from 2025-01-15T13:00:00Z --to 2025-01-15T14:00:00Z\ngog calendar out-of-office --from 2025-01-20 --to 2025-01-21 --all-day\ngog calendar working-location --type office --office-label \"HQ\" --from 2025-01-22 --to 2025-01-23\n# Add attendees without replacing existing attendees/RSVP state\ngog calendar update &lt;calendarId&gt; &lt;eventId&gt; \\\n  --add-attendee \"alice@example.com,bob@example.com\"\n\ngog calendar delete &lt;calendarId&gt; &lt;eventId&gt;\n\n# Invitations\ngog calendar respond &lt;calendarId&gt; &lt;eventId&gt; --status accepted\ngog calendar respond &lt;calendarId&gt; &lt;eventId&gt; --status declined\ngog calendar respond &lt;calendarId&gt; &lt;eventId&gt; --status tentative\ngog calendar respond &lt;calendarId&gt; &lt;eventId&gt; --status declined --send-updates externalOnly\n\n# Propose a new time (browser-only flow; API limitation)\ngog calendar propose-time &lt;calendarId&gt; &lt;eventId&gt;\ngog calendar propose-time &lt;calendarId&gt; &lt;eventId&gt; --open\ngog calendar propose-time &lt;calendarId&gt; &lt;eventId&gt; --decline --comment \"Can we do 5pm?\"\n\n# Availability\ngog calendar freebusy --calendars \"primary,work@example.com\" \\\n  --from 2025-01-15T00:00:00Z \\\n  --to 2025-01-16T00:00:00Z\n\ngog calendar conflicts --calendars \"primary,work@example.com\" \\\n  --today                             # Today's conflicts\n</code></pre><pre><code>gog time now\ngog time now --timezone UTC\n</code></pre><pre><code># List and search\ngog drive ls --max 20\ngog drive ls --parent &lt;folderId&gt; --max 20\ngog drive ls --no-all-drives            # Only list from \"My Drive\"\ngog drive search \"invoice\" --max 20\ngog drive search \"invoice\" --no-all-drives\ngog drive search \"mimeType = 'application/pdf'\" --raw-query\ngog drive get &lt;fileId&gt;                # Get file metadata\ngog drive url &lt;fileId&gt;                # Print Drive web URL\ngog drive copy &lt;fileId&gt; \"Copy Name\"\n\n# Upload and download\ngog drive upload ./path/to/file --parent &lt;folderId&gt;\ngog drive upload ./path/to/file --replace &lt;fileId&gt;  # Replace file content in-place (preserves shared link)\ngog drive upload ./report.docx --convert\ngog drive upload ./chart.png --convert-to sheet\ngog drive upload ./report.docx --convert --name report.docx\ngog drive download &lt;fileId&gt; --out ./downloaded.bin\ngog drive download &lt;fileId&gt; --format pdf --out ./exported.pdf     # Google Workspace files only\ngog drive download &lt;fileId&gt; --format docx --out ./doc.docx\ngog drive download &lt;fileId&gt; --format pptx --out ./slides.pptx\n\n# Organize\ngog drive mkdir \"New Folder\"\ngog drive mkdir \"New Folder\" --parent &lt;parentFolderId&gt;\ngog drive rename &lt;fileId&gt; \"New Name\"\ngog drive move &lt;fileId&gt; --parent &lt;destinationFolderId&gt;\ngog drive delete &lt;fileId&gt;             # Move to trash\ngog drive delete &lt;fileId&gt; --permanent # Permanently delete\n\n# Permissions\ngog drive permissions &lt;fileId&gt;\ngog drive share &lt;fileId&gt; --to user --email user@example.com --role reader\ngog drive share &lt;fileId&gt; --to user --email user@example.com --role writer\ngog drive share &lt;fileId&gt; --to domain --domain example.com --role reader\ngog drive unshare &lt;fileId&gt; --permission-id &lt;permissionId&gt;\n\n# Shared drives (Team Drives)\ngog drive drives --max 100\n</code></pre><pre><code># Docs\ngog docs info &lt;docId&gt;\ngog docs cat &lt;docId&gt; --max-bytes 10000\ngog docs create \"My Doc\"\ngog docs create \"My Doc\" --file ./doc.md            # Import markdown\ngog docs copy &lt;docId&gt; \"My Doc Copy\"\ngog docs export &lt;docId&gt; --format pdf --out ./doc.pdf\ngog docs list-tabs &lt;docId&gt;\ngog docs cat &lt;docId&gt; --tab \"Notes\"\ngog docs cat &lt;docId&gt; --all-tabs\ngog docs update &lt;docId&gt; --format markdown --content-file ./doc.md\ngog docs write &lt;docId&gt; --replace --markdown --file ./doc.md\ngog docs find-replace &lt;docId&gt; \"old\" \"new\"\n\n# Slides\ngog slides info &lt;presentationId&gt;\ngog slides create \"My Deck\"\ngog slides create-from-markdown \"My Deck\" --content-file ./slides.md\ngog slides copy &lt;presentationId&gt; \"My Deck Copy\"\ngog slides export &lt;presentationId&gt; --format pdf --out ./deck.pdf\ngog slides list-slides &lt;presentationId&gt;\ngog slides add-slide &lt;presentationId&gt; ./slide.png --notes \"Speaker notes\"\ngog slides update-notes &lt;presentationId&gt; &lt;slideId&gt; --notes \"Updated notes\"\ngog slides replace-slide &lt;presentationId&gt; &lt;slideId&gt; ./new-slide.png --notes \"New notes\"\n\n# Sheets\ngog sheets copy &lt;spreadsheetId&gt; \"My Sheet Copy\"\ngog sheets export &lt;spreadsheetId&gt; --format pdf --out ./sheet.pdf\ngog sheets format &lt;spreadsheetId&gt; 'Sheet1!A1:B2' --format-json '{\"textFormat\":{\"bold\":true}}' --format-fields 'userEnteredFormat.textFormat.bold'\ngog sheets insert &lt;spreadsheetId&gt; \"Sheet1\" rows 2 --count 3\ngog sheets notes &lt;spreadsheetId&gt; 'Sheet1!A1:B10'\n</code></pre><pre><code># Personal contacts\ngog contacts list --max 50\ngog contacts search \"Ada\" --max 50\ngog contacts get people/&lt;resourceName&gt;\ngog contacts get user@example.com     # Get by email\n\n# Other contacts (people you've interacted with)\ngog contacts other list --max 50\ngog contacts other search \"John\" --max 50\n\n# Create and update\ngog contacts create \\\n  --given \"John\" \\\n  --family \"Doe\" \\\n  --email \"john@example.com\" \\\n  --phone \"+1234567890\"\n\ngog contacts update people/&lt;resourceName&gt; \\\n  --given \"Jane\" \\\n  --email \"jane@example.com\" \\\n  --birthday \"1990-05-12\" \\\n  --notes \"Met at WWDC\"\n\n# Update via JSON (see docs/contacts-json-update.md)\ngog contacts get people/&lt;resourceName&gt; --json | \\\n  jq '(.contact.urls //= []) | (.contact.urls += [{\"value\":\"obsidian://open?vault=notes&amp;file=People/John%20Doe\",\"type\":\"profile\"}])' | \\\n  gog contacts update people/&lt;resourceName&gt; --from-file -\n\ngog contacts delete people/&lt;resourceName&gt;\n\n# Workspace directory (requires Google Workspace)\ngog contacts directory list --max 50\ngog contacts directory search \"Jane\" --max 50\n</code></pre><pre><code># Task lists\ngog tasks lists --max 50\ngog tasks lists create &lt;title&gt;\n\n# Tasks in a list\ngog tasks list &lt;tasklistId&gt; --max 50\ngog tasks get &lt;tasklistId&gt; &lt;taskId&gt;\ngog tasks add &lt;tasklistId&gt; --title \"Task title\"\ngog tasks add &lt;tasklistId&gt; --title \"Weekly sync\" --due 2025-02-01 --repeat weekly --repeat-count 4\ngog tasks add &lt;tasklistId&gt; --title \"Daily standup\" --due 2025-02-01 --repeat daily --repeat-until 2025-02-05\ngog tasks update &lt;tasklistId&gt; &lt;taskId&gt; --title \"New title\"\ngog tasks done &lt;tasklistId&gt; &lt;taskId&gt;\ngog tasks undo &lt;tasklistId&gt; &lt;taskId&gt;\ngog tasks delete &lt;tasklistId&gt; &lt;taskId&gt;\ngog tasks clear &lt;tasklistId&gt;\n\n# Note: Google Tasks treats due dates as date-only; time components may be ignored.\n# See docs/dates.md for all supported date/time input formats across commands.\n</code></pre><pre><code># Read\ngog sheets metadata &lt;spreadsheetId&gt;\ngog sheets get &lt;spreadsheetId&gt; 'Sheet1!A1:B10'\n\n# Export (via Drive)\ngog sheets export &lt;spreadsheetId&gt; --format pdf --out ./sheet.pdf\ngog sheets export &lt;spreadsheetId&gt; --format xlsx --out ./sheet.xlsx\n\n# Write\ngog sheets update &lt;spreadsheetId&gt; 'A1' 'val1|val2,val3|val4'\ngog sheets update &lt;spreadsheetId&gt; 'A1' --values-json '[[\"a\",\"b\"],[\"c\",\"d\"]]'\ngog sheets update &lt;spreadsheetId&gt; 'Sheet1!A1:C1' 'new|row|data' --copy-validation-from 'Sheet1!A2:C2'\ngog sheets append &lt;spreadsheetId&gt; 'Sheet1!A:C' 'new|row|data'\ngog sheets append &lt;spreadsheetId&gt; 'Sheet1!A:C' 'new|row|data' --copy-validation-from 'Sheet1!A2:C2'\ngog sheets clear &lt;spreadsheetId&gt; 'Sheet1!A1:B10'\n\n# Format\ngog sheets format &lt;spreadsheetId&gt; 'Sheet1!A1:B2' --format-json '{\"textFormat\":{\"bold\":true}}' --format-fields 'userEnteredFormat.textFormat.bold'\n\n# Insert rows/cols\ngog sheets insert &lt;spreadsheetId&gt; \"Sheet1\" rows 2 --count 3\ngog sheets insert &lt;spreadsheetId&gt; \"Sheet1\" cols 3 --after\n\n# Notes\ngog sheets notes &lt;spreadsheetId&gt; 'Sheet1!A1:B10'\n\n# Create\ngog sheets create \"My New Spreadsheet\" --sheets \"Sheet1,Sheet2\"\n</code></pre><pre><code># Forms\ngog forms get &lt;formId&gt;\ngog forms create --title \"Weekly Check-in\" --description \"Friday async update\"\n\n# Responses\ngog forms responses list &lt;formId&gt; --max 20\ngog forms responses get &lt;formId&gt; &lt;responseId&gt;\n</code></pre><pre><code># Projects\ngog appscript get &lt;scriptId&gt;\ngog appscript content &lt;scriptId&gt;\ngog appscript create --title \"Automation Helpers\"\ngog appscript create --title \"Bound Script\" --parent-id &lt;driveFileId&gt;\n\n# Execute functions\ngog appscript run &lt;scriptId&gt; myFunction --params '[\"arg1\", 123, true]'\ngog appscript run &lt;scriptId&gt; myFunction --dev-mode\n</code></pre><pre><code># Profile\ngog people me\ngog people get people/&lt;userId&gt;\n\n# Search the Workspace directory\ngog people search \"Ada Lovelace\" --max 5\n\n# Relations (defaults to people/me)\ngog people relations\ngog people relations people/&lt;userId&gt; --type manager\n</code></pre><pre><code># Spaces\ngog chat spaces list\ngog chat spaces find \"Engineering\"\ngog chat spaces create \"Engineering\" --member alice@company.com --member bob@company.com\n\n# Messages\ngog chat messages list spaces/&lt;spaceId&gt; --max 5\ngog chat messages list spaces/&lt;spaceId&gt; --thread &lt;threadId&gt;\ngog chat messages list spaces/&lt;spaceId&gt; --unread\ngog chat messages send spaces/&lt;spaceId&gt; --text \"Build complete!\" --thread spaces/&lt;spaceId&gt;/threads/&lt;threadId&gt;\n\n# Threads\ngog chat threads list spaces/&lt;spaceId&gt;\n\n# Direct messages\ngog chat dm space user@company.com\ngog chat dm send user@company.com --text \"ping\"\n</code></pre><p>Note: Chat commands require a Google Workspace account (consumer @gmail.com accounts are not supported).</p><h3>Groups (Google Workspace)</h3><pre><code># List groups you belong to\ngog groups list\n\n# List members of a group\ngog groups members engineering@company.com\n</code></pre><p>Note: Groups commands require the Cloud Identity API and the <code>cloud-identity.groups.readonly</code> scope. If you get a permissions error, re-authenticate:</p><pre><code>gog auth add your@email.com --services groups --force-consent\n</code></pre><h3>Classroom (Google Workspace for Education)</h3><pre><code># Courses\ngog classroom courses list\ngog classroom courses list --role teacher\ngog classroom courses get &lt;courseId&gt;\ngog classroom courses create --name \"Math 101\"\ngog classroom courses update &lt;courseId&gt; --name \"Math 102\"\ngog classroom courses archive &lt;courseId&gt;\ngog classroom courses unarchive &lt;courseId&gt;\ngog classroom courses url &lt;courseId&gt;\n\n# Roster\ngog classroom roster &lt;courseId&gt;\ngog classroom roster &lt;courseId&gt; --students\ngog classroom students add &lt;courseId&gt; &lt;userId&gt;\ngog classroom teachers add &lt;courseId&gt; &lt;userId&gt;\n\n# Coursework\ngog classroom coursework list &lt;courseId&gt;\ngog classroom coursework get &lt;courseId&gt; &lt;courseworkId&gt;\ngog classroom coursework create &lt;courseId&gt; --title \"Homework 1\" --type ASSIGNMENT --state PUBLISHED\ngog classroom coursework update &lt;courseId&gt; &lt;courseworkId&gt; --title \"Updated\"\ngog classroom coursework assignees &lt;courseId&gt; &lt;courseworkId&gt; --mode INDIVIDUAL_STUDENTS --add-student &lt;studentId&gt;\n\n# Materials\ngog classroom materials list &lt;courseId&gt;\ngog classroom materials create &lt;courseId&gt; --title \"Syllabus\" --state PUBLISHED\n\n# Submissions\ngog classroom submissions list &lt;courseId&gt; &lt;courseworkId&gt;\ngog classroom submissions get &lt;courseId&gt; &lt;courseworkId&gt; &lt;submissionId&gt;\ngog classroom submissions grade &lt;courseId&gt; &lt;courseworkId&gt; &lt;submissionId&gt; --grade 85\ngog classroom submissions return &lt;courseId&gt; &lt;courseworkId&gt; &lt;submissionId&gt;\ngog classroom submissions turn-in &lt;courseId&gt; &lt;courseworkId&gt; &lt;submissionId&gt;\ngog classroom submissions reclaim &lt;courseId&gt; &lt;courseworkId&gt; &lt;submissionId&gt;\n\n# Announcements\ngog classroom announcements list &lt;courseId&gt;\ngog classroom announcements create &lt;courseId&gt; --text \"Welcome!\"\ngog classroom announcements update &lt;courseId&gt; &lt;announcementId&gt; --text \"Updated\"\ngog classroom announcements assignees &lt;courseId&gt; &lt;announcementId&gt; --mode INDIVIDUAL_STUDENTS --add-student &lt;studentId&gt;\n\n# Topics\ngog classroom topics list &lt;courseId&gt;\ngog classroom topics create &lt;courseId&gt; --name \"Unit 1\"\ngog classroom topics update &lt;courseId&gt; &lt;topicId&gt; --name \"Unit 2\"\n\n# Invitations\ngog classroom invitations list\ngog classroom invitations create &lt;courseId&gt; &lt;userId&gt; --role student\ngog classroom invitations accept &lt;invitationId&gt;\n\n# Guardians\ngog classroom guardians list &lt;studentId&gt;\ngog classroom guardians get &lt;studentId&gt; &lt;guardianId&gt;\ngog classroom guardians delete &lt;studentId&gt; &lt;guardianId&gt;\n\n# Guardian invitations\ngog classroom guardian-invitations list &lt;studentId&gt;\ngog classroom guardian-invitations create &lt;studentId&gt; --email parent@example.com\n\n# Profiles\ngog classroom profile get\ngog classroom profile get &lt;userId&gt;\n</code></pre><p>Note: Classroom commands require a Google Workspace for Education account. Personal Google accounts have limited Classroom functionality.</p><pre><code># Export (via Drive)\ngog docs export &lt;docId&gt; --format pdf --out ./doc.pdf\ngog docs export &lt;docId&gt; --format docx --out ./doc.docx\ngog docs export &lt;docId&gt; --format txt --out ./doc.txt\n</code></pre><pre><code># Export (via Drive)\ngog slides export &lt;presentationId&gt; --format pptx --out ./deck.pptx\ngog slides export &lt;presentationId&gt; --format pdf --out ./deck.pdf\n</code></pre><p>Human-readable output with colors (default):</p><pre><code>$ gog gmail search 'newer_than:7d' --max 3\nTHREAD_ID           SUBJECT                           FROM                  DATE\n18f1a2b3c4d5e6f7    Meeting notes                     alice@example.com     2025-01-10\n17e1d2c3b4a5f6e7    Invoice #12345                    billing@vendor.com    2025-01-09\n16d1c2b3a4e5f6d7    Project update                    bob@example.com       2025-01-08\n</code></pre><p>Message-level search (one row per email; add  to fetch/decode bodies):</p><pre><code>$ gog gmail messages search 'newer_than:7d' --max 3\nID                  THREAD             SUBJECT                           FROM                  DATE\n18f1a2b3c4d5e6f7    9e8d7c6b5a4f3e2d    Meeting notes                     alice@example.com     2025-01-10\n17e1d2c3b4a5f6e7    9e8d7c6b5a4f3e2d    Invoice #12345                    billing@vendor.com    2025-01-09\n16d1c2b3a4e5f6d7    7f6e5d4c3b2a1908    Project update                    bob@example.com       2025-01-08\n</code></pre><p>Machine-readable output for scripting and automation:</p><pre><code>$ gog gmail search 'newer_than:7d' --max 3 --json\n{\n  \"threads\": [\n    {\n      \"id\": \"18f1a2b3c4d5e6f7\",\n      \"snippet\": \"Meeting notes from today...\",\n      \"messages\": [...]\n    },\n    ...\n  ]\n}\n</code></pre><pre><code>$ gog gmail messages search 'newer_than:7d' --max 3 --json\n{\n  \"messages\": [\n    {\n      \"id\": \"18f1a2b3c4d5e6f7\",\n      \"threadId\": \"9e8d7c6b5a4f3e2d\",\n      \"subject\": \"Meeting notes\",\n      \"from\": \"alice@example.com\",\n      \"date\": \"2025-01-10\"\n    },\n    ...\n  ]\n}\n</code></pre><pre><code>$ gog gmail messages search 'newer_than:7d' --max 1 --include-body --json\n{\n  \"messages\": [\n    {\n      \"id\": \"18f1a2b3c4d5e6f7\",\n      \"threadId\": \"9e8d7c6b5a4f3e2d\",\n      \"subject\": \"Meeting notes\",\n      \"from\": \"alice@example.com\",\n      \"date\": \"2025-01-10\",\n      \"body\": \"Hi team ‚Äî meeting notes...\"\n    }\n  ]\n}\n</code></pre><p>Data goes to stdout, errors and progress to stderr for clean piping:</p><pre><code>gog --json drive ls --max 5 | jq '.files[] | select(.mimeType==\"application/pdf\")'\n</code></pre><p>Calendar JSON convenience fields:</p><ul><li> /  on event payloads (derived from start/end).</li></ul><h3>Search recent emails and download attachments</h3><pre><code># Search for emails from the last week\ngog gmail search 'newer_than:7d has:attachment' --max 10\n\n# Get thread details and download attachments\ngog gmail thread get &lt;threadId&gt; --download\n</code></pre><h3>Modify labels on a thread</h3><pre><code># Archive and star a thread\ngog gmail thread modify &lt;threadId&gt; --remove INBOX --add STARRED\n</code></pre><h3>Create a calendar event with attendees</h3><pre><code># Find a free time slot\ngog calendar freebusy --calendars \"primary\" \\\n  --from 2025-01-15T00:00:00Z \\\n  --to 2025-01-16T00:00:00Z\n\n# Create the meeting\ngog calendar create primary \\\n  --summary \"Team Standup\" \\\n  --from 2025-01-15T10:00:00Z \\\n  --to 2025-01-15T10:30:00Z \\\n  --attendees \"alice@example.com,bob@example.com\"\n</code></pre><h3>Find and download files from Drive</h3><pre><code># Search for PDFs\ngog drive search \"invoice filetype:pdf\" --max 20 --json | \\\n  jq -r '.files[] | .id' | \\\n  while read fileId; do\n    gog drive download \"$fileId\"\n  done\n</code></pre><pre><code># Check personal Gmail\ngog gmail search 'is:unread' --account personal@gmail.com\n\n# Check work Gmail\ngog gmail search 'is:unread' --account work@company.com\n\n# Or set default\nexport GOG_ACCOUNT=work@company.com\ngog gmail search 'is:unread'\n</code></pre><h3>Update a Google Sheet from a CSV</h3><pre><code># Convert CSV to pipe-delimited format and update sheet\ncat data.csv | tr ',' '|' | \\\n  gog sheets update &lt;spreadsheetId&gt; 'Sheet1!A1'\n</code></pre><h3>Export Sheets / Docs / Slides</h3><pre><code># Sheets\ngog sheets export &lt;spreadsheetId&gt; --format pdf\n\n# Docs\ngog docs export &lt;docId&gt; --format docx\n\n# Slides\ngog slides export &lt;presentationId&gt; --format pptx\n</code></pre><h3>Batch process Gmail threads</h3><pre><code># Mark all emails from a sender as read\ngog --json gmail search 'from:noreply@example.com' --max 200 | \\\n  jq -r '.threads[].id' | \\\n  xargs -n 50 gog gmail labels modify --remove UNREAD\n\n# Archive old emails\ngog --json gmail search 'older_than:1y' --max 200 | \\\n  jq -r '.threads[].id' | \\\n  xargs -n 50 gog gmail labels modify --remove INBOX\n\n# Label important emails\ngog --json gmail search 'from:boss@example.com' --max 200 | \\\n  jq -r '.threads[].id' | \\\n  xargs -n 50 gog gmail labels modify --add IMPORTANT\n</code></pre><p>Enable verbose logging for troubleshooting:</p><pre><code>gog --verbose gmail search 'newer_than:7d'\n# Shows API requests and responses\n</code></pre><p>All commands support these flags:</p><ul><li><code>--account &lt;email|alias|auto&gt;</code> - Account to use (overrides GOG_ACCOUNT)</li><li> - Allowlist top-level commands (e.g., )</li><li> - Output JSON to stdout (best for scripting)</li><li> - Output stable, parseable text to stdout (TSV; no colors)</li><li> - Color mode: , , or  (default: auto)</li><li> - Skip confirmations for destructive commands</li><li> - Never prompt; fail instead (useful for CI)</li><li> - Enable verbose logging</li><li> - Show help for any command</li></ul><p>Generate shell completions for your preferred shell:</p><pre><code># macOS (with Homebrew)\ngog completion bash &gt; $(brew --prefix)/etc/bash_completion.d/gog\n\n# Linux\ngog completion bash &gt; /etc/bash_completion.d/gog\n\n# Or load directly in your current session\nsource &lt;(gog completion bash)\n</code></pre><pre><code># Generate completion file\ngog completion zsh &gt; \"${fpath[1]}/_gog\"\n\n# Or add to .zshrc for automatic loading\necho 'eval \"$(gog completion zsh)\"' &gt;&gt; ~/.zshrc\n\n# Enable completions if not already enabled\necho \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc\n</code></pre><pre><code>gog completion fish &gt; ~/.config/fish/completions/gog.fish\n</code></pre><pre><code># Load for current session\ngog completion powershell | Out-String | Invoke-Expression\n\n# Or add to profile for all sessions\ngog completion powershell &gt;&gt; $PROFILE\n</code></pre><p>After installing completions, start a new shell session for changes to take effect.</p><p>After cloning, install tools:</p><p>Pinned tools (installed into ):</p><ul><li>Format:  (goimports + gofumpt)</li><li>Lint:  (golangci-lint)</li></ul><p>CI runs format checks, tests, and lint on push/PR.</p><h3>Integration Tests (Live Google APIs)</h3><p>Opt-in tests that hit real Google APIs using your stored  credentials/tokens.</p><pre><code># Optional: override which account to use\nexport GOG_IT_ACCOUNT=you@gmail.com\nexport GOG_CLIENT=work\ngo test -tags=integration ./...\n</code></pre><p>Tip: if you want to avoid macOS Keychain prompts during these runs, set  and  (uses encrypted on-disk keyring).</p><p>Fast end-to-end smoke checks against live APIs:</p><pre><code>scripts/live-test.sh --fast\nscripts/live-test.sh --account you@gmail.com --skip groups,keep,calendar-enterprise\nscripts/live-test.sh --client work --account you@company.com\n</code></pre><ul><li> to re-auth before running</li><li> to select OAuth client credentials</li><li> to fail on optional features (groups/keep/enterprise)</li><li> to override the test-account guardrail</li></ul><p>Go test wrapper (opt-in):</p><pre><code>GOG_LIVE=1 go test -tags=integration ./internal/integration -run Live\n</code></pre><ul><li><code>GOG_LIVE_SKIP=groups,keep</code></li><li><code>GOG_LIVE_EMAIL_TEST=steipete+gogtest@gmail.com</code></li><li><code>GOG_LIVE_GROUP_EMAIL=group@domain</code></li><li><code>GOG_LIVE_CLASSROOM_COURSE=&lt;courseId&gt;</code></li><li><code>GOG_LIVE_CLASSROOM_CREATE=1</code></li><li><code>GOG_LIVE_CLASSROOM_ALLOW_STATE=1</code></li><li><code>GOG_LIVE_GMAIL_BATCH_DELETE=1</code></li><li><code>GOG_LIVE_GMAIL_WATCH_TOPIC=projects/.../topics/...</code></li><li><code>GOG_LIVE_CALENDAR_RESPOND=1</code></li><li><code>GOG_LIVE_CALENDAR_RECURRENCE=1</code></li><li><code>GOG_KEEP_SERVICE_ACCOUNT=/path/to/service-account.json</code></li><li><code>GOG_KEEP_IMPERSONATE=user@workspace-domain</code></li></ul><pre><code>make gog auth add you@gmail.com\n</code></pre><p>For clean stdout when scripting:</p><ul><li>Use  when the first arg is a flag: <code>make gog -- --json gmail search \"from:me\" | jq .</code></li></ul><p>This project is inspired by Mario Zechner's original CLIs:</p>",
      "contentLength": 44302,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "brave/brave-browser",
      "url": "https://github.com/brave/brave-browser",
      "date": 1771211223,
      "author": "",
      "guid": 45353,
      "unread": true,
      "content": "<p>Brave browser for Android, iOS, Linux, macOS, Windows.</p><p>This repository holds the build tools needed to build the Brave desktop browser for macOS, Windows, and Linux. In particular, it fetches and syncs code from the projects defined in  and :</p><p>Our <a href=\"https://github.com/brave/brave-browser/wiki\">Wiki</a> also has some useful technical information.</p><p>Follow <a href=\"https://x.com/brave\">@brave</a> on X for important news and announcements.</p><p>Follow the instructions for your platform:</p><h2>Clone and initialize the repo</h2><p>Once you have the prerequisites installed, you can get the code and initialize the build environment.</p><pre><code>git clone git@github.com:brave/brave-core.git path-to-your-project-folder/src/brave\ncd path-to-your-project-folder/src/brave\nnpm install\n\n# the Chromium source is downloaded, which has a large history (gigabytes of data)\n# this might take really long to finish depending on internet speed\n\nnpm run init\n</code></pre><p>brave-core based android builds should use <code>npm run init -- --target_os=android --target_arch=arm</code> (or whichever CPU type you want to build for) brave-core based iOS builds should use <code>npm run init -- --target_os=ios</code></p><p>You can also set the target_os and target_arch for init and build using:</p><pre><code>npm config set target_os android\nnpm config set target_arch arm\n</code></pre><p>The default build type is component.</p><pre><code># start the component build compile\nnpm run build\n</code></pre><pre><code># start the release compile\nnpm run build Release\n</code></pre><p>brave-core based android builds should use <code>npm run build -- --target_os=android --target_arch=arm</code> or set the npm config variables as specified above for </p><p>brave-core based iOS builds should use the Xcode project found in . You can open this project directly or run <code>npm run ios_bootstrap -- --open_xcodeproj</code> to have it opened in Xcode. See the <a href=\"https://github.com/brave/brave-browser/wiki/iOS-Development-Environment#Building\">iOS Developer Environment</a> for more information on iOS builds.</p><p>Running a release build with  can be very slow and use a lot of RAM, especially on Linux with the Gold LLVM plugin.</p><p>To run a statically linked build (takes longer to build, but starts faster):</p><p>To run a debug build (Component build with is_debug=true):</p><p>NOTE: the build will take a while to complete. Depending on your processor and memory, it could potentially take a few hours.</p><p><code>npm start [Release|Component|Static|Debug]</code></p><p><code>npm run sync -- [--force] [--init] [--create] [brave_core_ref]</code></p><p><strong>This will attempt to stash your local changes in brave-core, but it's safer to commit local changes before running this</strong></p><p> will (depending on the below flags):</p><ol><li>üì• Update sub-projects (chromium, brave-core) to latest commit of a git ref (e.g. tag or branch)</li><li>üîÑ Update gclient DEPS dependencies</li><li>‚è© Run hooks (e.g. to perform  on child projects)</li></ol><table><tbody><tr><td>updates chromium if needed and re-applies patches. If the chromium version did not change, it will only re-apply patches that have changed. Will update child dependencies <strong>only if any project needed updating during this script run</strong>.  **Use this if you want the script to manage keeping you up to date instead of pulling or switching branches manually. **</td></tr><tr><td>updates both  and  to the latest remote commit for the current brave-core branch and the  ref specified in brave-core/package.json (e.g.  or ). Will re-apply all patches. Will force update all child dependencies.  **Use this if you're having trouble and want to force the branches back to a known state. **</td></tr><tr><td>force update both  and  to the versions specified in brave-core/package.json and force updates all dependent repos - same as </td></tr><tr><td><code>--sync_chromium (true/false)</code></td><td>Will force or skip the chromium version update when applicable. Useful if you want to avoid a minor update when not ready for the larger build time a chromium update may result in. A warning will be output about the current code state expecting a different chromium version. Your build may fail as a result.</td></tr><tr><td>Will delete from the working copy any dependencies that have been removed since the last sync. Mimics .</td></tr></tbody></table><p>Run <code>npm run sync brave_core_ref</code> to checkout the specified  ref and update all dependent repos including chromium if needed.</p><pre><code>&gt; cd src/brave\nsrc/brave&gt; git checkout -b branch_name\n</code></pre><h4>Checkout an existing branch or tag:</h4><pre><code>src/brave&gt; git fetch origin\nsrc/brave&gt; git checkout [-b] branch_name\nsrc/brave&gt; npm run sync\n...Updating 2 patches...\n...Updating child dependencies...\n...Running hooks...\n</code></pre><h4>Update the current branch to the latest remote:</h4><pre><code>src/brave&gt; git pull\nsrc/brave&gt; npm run sync\n...Updating 2 patches...\n...Updating child dependencies...\n...Running hooks...\n</code></pre><h4>Reset to latest brave-core master (via , will always result in a longer build and will remove any pending changes in your brave-core working directory):</h4><pre><code>src/brave&gt; git checkout master\nsrc/brave&gt; git pull\nsrc/brave&gt; npm run sync -- --init\n</code></pre><h4>When you know that DEPS didn't change, but .patch files did (quickest attempt to perform a mini-sync before a build):</h4><pre><code>src/brave&gt; git checkout featureB\nsrc/brave&gt; git pull\nsrc/brave&gt; npm run apply_patches\n...Applying 2 patches...\n</code></pre>",
      "contentLength": 4770,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "nautechsystems/nautilus_trader",
      "url": "https://github.com/nautechsystems/nautilus_trader",
      "date": 1771211223,
      "author": "",
      "guid": 45354,
      "unread": true,
      "content": "<p>A high-performance algorithmic trading platform and event-driven backtester</p><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>NautilusTrader is an open-source, high-performance, production-grade algorithmic trading platform, providing quantitative traders with the ability to backtest portfolios of automated trading strategies on historical data with an event-driven engine, and also deploy those same strategies live, with no code changes.</p><p>The platform is , designed to develop and deploy algorithmic trading strategies within a highly performant and robust Python-native environment. This helps to address the parity challenge of keeping the Python research/backtest environment consistent with the production live trading environment.</p><p>NautilusTrader's design, architecture, and implementation philosophy prioritizes software correctness and safety at the highest level, with the aim of supporting Python-native, mission-critical, trading system backtesting and live deployment workloads.</p><p>The platform is also universal, and asset-class-agnostic ‚Äî with any REST API or WebSocket feed able to be integrated via modular adapters. It supports high-frequency trading across a wide range of asset classes and instrument types including FX, Equities, Futures, Options, Crypto, DeFi, and Betting ‚Äî enabling seamless operations across multiple venues simultaneously.</p><ul><li>: Core is written in Rust with asynchronous networking using <a href=\"https://crates.io/crates/tokio\">tokio</a>.</li><li>: Rust-powered type- and thread-safety, with optional Redis-backed state persistence.</li><li>: OS independent, runs on Linux, macOS, and Windows. Deploy using Docker.</li><li>: Modular adapters mean any REST API or WebSocket feed can be integrated.</li><li>: Time in force , , , , , , , advanced order types and conditional triggers. Execution instructions , , and icebergs. Contingency orders including , , .</li><li>: Add user-defined custom components, or assemble entire systems from scratch leveraging the <a href=\"https://nautilustrader.io/docs/latest/concepts/cache\">cache</a> and <a href=\"https://nautilustrader.io/docs/latest/concepts/message_bus\">message bus</a>.</li><li>: Run with multiple venues, instruments and strategies simultaneously using historical quote tick, trade tick, bar, order book and custom data with nanosecond resolution.</li><li>: Use identical strategy implementations between backtesting and live deployments.</li><li>: Multiple venue capabilities facilitate market-making and statistical arbitrage strategies.</li><li>: Backtest engine fast enough to be used to train AI trading agents (RL/ES).</li></ul><blockquote><p><em>nautilus - from ancient Greek 'sailor' and naus 'ship'.</em></p><p><em>The nautilus shell consists of modular chambers with a growth factor which approximates a logarithmic spiral. The idea is that this can be translated to the aesthetics of design and architecture.</em></p></blockquote><ul><li><strong>Highly performant event-driven Python</strong>: Native binary core components.</li><li><strong>Parity between backtesting and live trading</strong>: Identical strategy code.</li><li>: Enhanced risk management functionality, logical accuracy, and type safety.</li><li>: Message bus, custom components and actors, custom data, custom adapters.</li></ul><p>Traditionally, trading strategy research and backtesting might be conducted in Python using vectorized methods, with the strategy then needing to be reimplemented in a more event-driven way using C++, C#, Java or other statically typed language(s). The reasoning here is that vectorized backtesting code cannot express the granular time and event dependent complexity of real-time trading, where compiled languages have proven to be more suitable due to their inherently higher performance, and type safety.</p><p>One of the key advantages of NautilusTrader here, is that this reimplementation step is now circumvented - as the critical core components of the platform have all been written entirely in <a href=\"https://www.rust-lang.org/\">Rust</a> or <a href=\"https://cython.org/\">Cython</a>. This means we're using the right tools for the job, where systems programming languages compile performant binaries, with CPython C extension modules then able to offer a Python-native environment, suitable for professional quantitative traders and trading firms.</p><p>Python was originally created decades ago as a simple scripting language with a clean straightforward syntax. It has since evolved into a fully fledged general purpose object-oriented programming language. Based on the TIOBE index, Python is currently the most popular programming language in the world. Not only that, Python has become the  of data science, machine learning, and artificial intelligence.</p><p><a href=\"https://www.rust-lang.org/\">Rust</a> is a multi-paradigm programming language designed for performance and safety, especially safe concurrency. Rust is \"blazingly fast\" and memory-efficient (comparable to C and C++) with no garbage collector. It can power mission-critical systems, run on embedded devices, and easily integrates with other languages.</p><p>Rust's rich type system and ownership model guarantee memory-safety and thread-safety in safe code, eliminating many classes of bugs at compile-time. Overall safety in this project also depends on correctly upheld invariants in unsafe blocks and FFI boundaries.</p><p>The project utilizes Rust for core performance-critical components. Python bindings are implemented via Cython and <a href=\"https://pyo3.rs\">PyO3</a>‚Äîno Rust toolchain is required at install time.</p><blockquote><p>‚ÄúThe intent of this project is to be free of soundness bugs. The developers will do their best to avoid them, and welcome help in analyzing and fixing them.‚Äù</p></blockquote><blockquote><p> NautilusTrader relies heavily on improvements in the Rust language and compiler. As a result, the Minimum Supported Rust Version (MSRV) is generally equal to the latest stable release of Rust.</p></blockquote><p>NautilusTrader is modularly designed to work with , enabling connectivity to trading venues and data providers by translating their raw APIs into a unified interface and normalized domain model.</p><ul><li>: The default client ID for the integrations adapter clients.</li><li>: The type of integration (often the venue type).</li></ul><ul><li>: Planned for future development.</li><li>: Under construction and likely not in a usable state.</li><li>: Completed to a minimally working state and in a beta testing phase.</li><li>: Stabilized feature set and API, the integration has been tested by both developers and users to a reasonable level (some bugs may still remain).</li></ul><p>The <a href=\"https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md\">Roadmap</a> outlines NautilusTrader's strategic direction. Current priorities include porting the core to Rust, improving documentation, and enhancing code ergonomics.</p><p>The open-source project focuses on single-node backtesting and live trading for individual and small-team quantitative traders. UI dashboards, distributed orchestration, and built-in AI/ML tooling are out of scope to maintain focus on the core engine and ecosystem sustainability.</p><blockquote><p><strong>NautilusTrader is still under active development</strong>. Some features may be incomplete, and while the API is becoming more stable, breaking changes can occur between releases. We strive to document these changes in the release notes on a .</p></blockquote><p>We aim to follow a <strong>bi-weekly release schedule</strong>, though experimental or larger features may cause delays.</p><p>We aim to maintain a stable, passing build across all branches.</p><ul><li>: Reflects the source code for the latest released version; recommended for production use.</li><li>: Daily snapshots of the  branch for early testing; merged at  and as required.</li><li>: Active development branch for contributors and feature work.</li></ul><blockquote><p>Our <a href=\"https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md\">roadmap</a> aims to achieve a <strong>stable API for version 2.x</strong> (likely after the Rust port). Once this milestone is reached, we plan to implement a formal deprecation process for any API changes. This approach allows us to maintain a rapid development pace for now.</p></blockquote><p>NautilusTrader supports two precision modes for its core value types (, , ), which differ in their internal bit-width and maximum decimal precision.</p><ul><li>: 128-bit integers with up to 16 decimals of precision, and a larger value range.</li><li>: 64-bit integers with up to 9 decimals of precision, and a smaller value range.</li></ul><blockquote><p>By default, the official Python wheels ship in high-precision (128-bit) mode on Linux and macOS. On Windows, only standard-precision (64-bit) Python wheels are available because MSVC's C/C++ frontend does not support , preventing the Cython/FFI layer from handling 128-bit integers.</p><p>For pure Rust crates, high-precision works on all platforms (including Windows) since Rust handles / via software emulation. The default is standard-precision unless you explicitly enable the  feature flag.</p></blockquote><p>: To enable high-precision mode in Rust, add the  feature to your Cargo.toml:</p><pre><code>[dependencies]\nnautilus_model = { version = \"*\", features = [\"high-precision\"] }\n</code></pre><p>We recommend using the latest supported version of Python and installing <a href=\"https://pypi.org/project/nautilus_trader/\">nautilus_trader</a> inside a virtual environment to isolate dependencies.</p><p><strong>There are two supported ways to install</strong>:</p><ol><li>Pre-built binary wheel from PyPI  the Nautech Systems package index.</li></ol><blockquote><p>We highly recommend installing using the <a href=\"https://docs.astral.sh/uv\">uv</a> package manager with a \"vanilla\" CPython.</p><p>Conda and other Python distributions  work but aren‚Äôt officially supported.</p></blockquote><p>To install the latest binary wheel (or sdist package) from PyPI using Python's pip package manager:</p><pre><code>pip install -U nautilus_trader\n</code></pre><p>Install optional dependencies as 'extras' for specific integrations (e.g., , , , , , ):</p><pre><code>pip install -U \"nautilus_trader[docker,ib]\"\n</code></pre><h3>From the Nautech Systems package index</h3><p>The Nautech Systems package index (<code>packages.nautechsystems.io</code>) complies with <a href=\"https://peps.python.org/pep-0503/\">PEP-503</a> and hosts both stable and development binary wheels for . This enables users to install either the latest stable release or pre-release versions for testing.</p><p>Stable wheels correspond to official releases of  on PyPI, and use standard versioning.</p><p>To install the latest stable release:</p><pre><code>pip install -U nautilus_trader --index-url=https://packages.nautechsystems.io/simple\n</code></pre><blockquote><p>Use  instead of  if you want pip to fall back to PyPI automatically.</p></blockquote><p>Development wheels are published from both the  and  branches, allowing users to test features and fixes ahead of stable releases.</p><p>This process also helps preserve compute resources and provides easy access to the exact binaries tested in CI pipelines, while adhering to <a href=\"https://peps.python.org/pep-0440/\">PEP-440</a> versioning standards:</p><ul><li> wheels use the version format  (e.g., ).</li><li> wheels use the version format  (alpha) (e.g., ).</li></ul><table><tbody></tbody></table><p>: Development wheels from the  branch publish for every supported platform except Linux ARM64. Skipping that target keeps CI feedback fast while avoiding unnecessary build resource usage.</p><blockquote><p>We do not recommend using development wheels in production environments, such as live trading controlling real capital.</p></blockquote><p>By default, pip will install the latest stable release. Adding the  flag ensures that pre-release versions, including development wheels, are considered.</p><p>To install the latest available pre-release (including development wheels):</p><pre><code>pip install -U nautilus_trader --pre --index-url=https://packages.nautechsystems.io/simple\n</code></pre><p>To install a specific development wheel (e.g.,  for October 26, 2025):</p><pre><code>pip install nautilus_trader==1.221.0a20251026 --index-url=https://packages.nautechsystems.io/simple\n</code></pre><p>You can view all available versions of  on the <a href=\"https://packages.nautechsystems.io/simple/nautilus-trader/index.html\">package index</a>.</p><p>To programmatically fetch and list available versions:</p><pre><code>curl -s https://packages.nautechsystems.io/simple/nautilus-trader/index.html | grep -oP '(?&lt;=&lt;a href=\")[^\"]+(?=\")' | awk -F'#' '{print $1}' | sort\n</code></pre><blockquote><p>On Linux, confirm your glibc version with  and ensure it reports  or newer before installing binary wheels.</p></blockquote><ul><li> branch wheels (): Build and publish continuously with every merged commit.</li><li> branch wheels (): Build and publish daily when we automatically merge the  branch at  (if there are changes).</li></ul><ul><li> branch wheels (): We retain only the most recent wheel build.</li><li> branch wheels (): We retain only the 30 most recent wheel builds.</li></ul><h4>Verifying build provenance</h4><p>All release artifacts (wheels and source distributions) published to PyPI, GitHub Releases, and the Nautech Systems package index include cryptographic attestations that prove their authenticity and build provenance.</p><p>These attestations are generated automatically during the CI/CD pipeline using <a href=\"https://slsa.dev/\">SLSA</a> build provenance, and can be verified to ensure:</p><ul><li>The artifact was built by the official NautilusTrader GitHub Actions workflow.</li><li>The artifact corresponds to a specific commit SHA in the repository.</li><li>The artifact hasn't been tampered with since it was built.</li></ul><p>To verify a wheel file using the GitHub CLI:</p><pre><code>gh attestation verify nautilus_trader-1.220.0-*.whl --owner nautechsystems\n</code></pre><p>This provides supply chain security by allowing you to cryptographically verify that the installed package came from the official NautilusTrader build process.</p><blockquote><p>Attestation verification requires the <a href=\"https://cli.github.com/\">GitHub CLI</a> () to be installed. Development wheels from  and  branches are also attested and can be verified the same way.</p></blockquote><p>It's possible to install from source using pip if you first install the build dependencies as specified in the .</p><ol><li><p>Install <a href=\"https://rustup.rs/\">rustup</a> (the Rust toolchain installer):</p></li><li><p>Enable  in the current shell:</p><ul></ul></li><li><p>Install <a href=\"https://clang.llvm.org/\">clang</a> (a C language frontend for LLVM):</p><ul><li><pre><code>sudo apt-get install clang\n</code></pre></li><li><ol><li><p>Enable  in the current shell:</p><pre><code>[System.Environment]::SetEnvironmentVariable('path', \"C:\\Program Files\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\Llvm\\x64\\bin\\;\" + $env:Path,\"User\")\n</code></pre></li></ol></li><li><p>Verify (any system): from a terminal session run: </p></li></ul></li><li><ul><li><pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></li><li><pre><code>irm https://astral.sh/uv/install.ps1 | iex\n</code></pre></li></ul></li><li><p>Clone the source with , and install from the project's root directory:</p><pre><code>git clone --branch develop --depth 1 https://github.com/nautechsystems/nautilus_trader\ncd nautilus_trader\nuv sync --all-extras\n</code></pre></li></ol><blockquote><p>The  flag fetches just the latest commit for a faster, lightweight clone.</p></blockquote><ol start=\"6\"><li><p>Set environment variables for PyO3 compilation (Linux and macOS only):</p><pre><code># Linux only: Set the library path for the Python interpreter\nexport LD_LIBRARY_PATH=\"$(python -c 'import sys; print(sys.base_prefix)')/lib:$LD_LIBRARY_PATH\"\n\n# Set the Python executable path for PyO3\nexport PYO3_PYTHON=$(pwd)/.venv/bin/python\n\n# Required for Rust tests when using uv-installed Python\nexport PYTHONHOME=$(python -c \"import sys; print(sys.base_prefix)\")\n</code></pre></li></ol><blockquote><p>The  export is Linux-specific and not needed on macOS.</p><p>The  variable is required when running  with a -installed Python. Without it, tests that depend on PyO3 may fail to locate the Python runtime.</p></blockquote><p>A  is provided to automate most installation and build tasks for development. Some of the targets include:</p><ul><li>: Installs in  build mode with all dependency groups and extras.</li><li>: Same as  but with  build mode.</li><li>: Installs just the ,  and  dependencies (does not install package).</li><li>: Runs the build script in  build mode (default).</li><li>: Runs the build script in  build mode.</li><li>: Runs uv build with a wheel format in  mode.</li><li>: Runs uv build with a wheel format in  mode.</li><li>: Runs all Rust crate tests using .</li><li>: Deletes all build results, such as  or  files.</li><li>:  Removes all artifacts not in the git index from the repository. This includes source files which have not been ed.</li><li>: Builds the documentation HTML using Sphinx.</li><li>: Runs the pre-commit checks over all files.</li><li>: Runs ruff over all files using the  config (with autofix).</li><li>: Runs all tests with .</li><li>: Runs performance tests with <a href=\"https://codspeed.io\">codspeed</a>.</li></ul><blockquote><p>Run  for documentation on all available make targets.</p></blockquote><p>Indicators and strategies can be developed in both Python and Cython. For performance and latency-sensitive applications, we recommend using Cython. Below are some examples:</p><p>Docker containers are built using the base image  with the following variant tags:</p><ul><li> has the latest release version installed.</li><li> has the head of the  branch installed.</li><li> has the latest release version installed along with  and an example backtest notebook with accompanying data.</li><li> has the head of the  branch installed along with  and an example backtest notebook with accompanying data.</li></ul><p>You can pull the container images as follows:</p><pre><code>docker pull ghcr.io/nautechsystems/&lt;image_variant_tag&gt; --platform linux/amd64\n</code></pre><p>You can launch the backtest example container by running:</p><pre><code>docker pull ghcr.io/nautechsystems/jupyterlab:nightly --platform linux/amd64\ndocker run -p 8888:8888 ghcr.io/nautechsystems/jupyterlab:nightly\n</code></pre><p>Then open your browser at the following address:</p><pre><code>http://127.0.0.1:8888/lab\n</code></pre><p>We aim to provide the most pleasant developer experience possible for this hybrid codebase of Python, Cython and Rust. See the <a href=\"https://nautilustrader.io/docs/latest/developer_guide/\">Developer Guide</a> for helpful information.</p><blockquote><p>Run  to compile after changes to Rust or Cython code for the most efficient development workflow.</p></blockquote><p><a href=\"https://nexte.st\">cargo-nextest</a> is the standard Rust test runner for NautilusTrader. Its key benefit is isolating each test in its own process, ensuring test reliability by avoiding interference.</p><p>You can install cargo-nextest by running:</p><pre><code>cargo install cargo-nextest\n</code></pre><blockquote><p>Run Rust tests with , which uses  with an efficient profile.</p></blockquote><p>Thank you for considering contributing to NautilusTrader! We welcome any and all help to improve the project. If you have an idea for an enhancement or a bug fix, the first step is to open an <a href=\"https://github.com/nautechsystems/nautilus_trader/issues\">issue</a> on GitHub to discuss it with the team. This helps to ensure that your contribution will be well-aligned with the goals of the project and avoids duplication of effort.</p><p>Before getting started, be sure to review the <a href=\"https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md#open-source-scope\">open-source scope</a> outlined in the project‚Äôs roadmap to understand what‚Äôs in and out of scope.</p><p>Once you're ready to start working on your contribution, make sure to follow the guidelines outlined in the <a href=\"https://github.com/nautechsystems/nautilus_trader/raw/develop/CONTRIBUTING.md\">CONTRIBUTING.md</a> file. This includes signing a Contributor License Agreement (CLA) to ensure that your contributions can be included in the project.</p><blockquote><p>Pull requests should target the  branch (the default branch). This is where new features and improvements are integrated before release.</p></blockquote><p>Thank you again for your interest in NautilusTrader! We look forward to reviewing your contributions and working with you to improve the project.</p><p>Join our community of users and contributors on <a href=\"https://discord.gg/NautilusTrader\">Discord</a> to chat and stay up-to-date with the latest announcements and features of NautilusTrader. Whether you're a developer looking to contribute or just want to learn more about the platform, all are welcome on our Discord server.</p><blockquote><p>NautilusTrader does not issue, promote, or endorse any cryptocurrency tokens. Any claims or communications suggesting otherwise are unauthorized and false.</p><p>If you encounter any suspicious activity, please report it to the appropriate platform and contact us at <a href=\"mailto:info@nautechsystems.io\">info@nautechsystems.io</a>.</p></blockquote><p>NautilusTrader‚Ñ¢ is developed and maintained by Nautech Systems, a technology company specializing in the development of high-performance trading systems. For more information, visit <a href=\"https://nautilustrader.io\">https://nautilustrader.io</a>.</p><p>¬© 2015-2026 Nautech Systems Pty Ltd. All rights reserved.</p>",
      "contentLength": 18125,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ruby/ruby",
      "url": "https://github.com/ruby/ruby",
      "date": 1771124854,
      "author": "",
      "guid": 45137,
      "unread": true,
      "content": "<p>The Ruby Programming Language</p><p>Ruby is an interpreted object-oriented programming language often used for web development. It also offers many scripting features to process plain text and serialized files, or manage system tasks. It is simple, straightforward, and extensible.</p><ul><li> Object-oriented Features (e.g. class, method calls)</li><li> Object-oriented Features (e.g. mix-in, singleton-method)</li><li>Dynamic Loading of Object Files (on some architectures)</li></ul><p>For a complete list of ways to install Ruby, including using third-party tools like rvm, see:</p><p>The mirror of the Ruby source tree can be checked out with the following command:</p><pre><code>$ git clone https://github.com/ruby/ruby.git\n</code></pre><p>There are some other branches under development. Try the following command to see the list of branches:</p><pre><code>$ git ls-remote https://github.com/ruby/ruby.git\n</code></pre><p>There is a mailing list to discuss Ruby. To subscribe to this list, please send the following phrase:</p><p>Ruby was originally designed and developed by Yukihiro Matsumoto (Matz) in 1995.</p>",
      "contentLength": 990,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Zipstack/unstract",
      "url": "https://github.com/Zipstack/unstract",
      "date": 1771124854,
      "author": "",
      "guid": 45138,
      "unread": true,
      "content": "<p>No-code LLM Platform to launch APIs and ETL Pipelines to structure unstructured documents</p><p>Prompt Studio is a purpose-built environment that supercharges your schema definition efforts. Compare outputs from different LLMs side-by-side, keep tab on costs while you develop generic prompts that work across wide-ranging document variations. And when you're ready, launch extraction APIs with a single click.</p><h2>üîå Integrations that suit your environment</h2><p>Once you've used Prompt Studio to define your schema, Unstract makes it easy to integrate into your existing workflows. Simply choose the integration type that best fits your environment:</p><table><thead><tr></tr></thead><tbody><tr><td>Run Unstract as an MCP Server to provide structured data extraction to Agents or LLMs in your ecosystem.</td><td>Developers building  that speak MCP.</td></tr><tr><td>Turn any document into JSON with an API call. Deploy any Prompt Studio project as a REST API endpoint with a single click.</td><td>Teams needing  in apps, services, or custom tooling.</td></tr><tr><td>Embed Unstract directly into your ETL jobs to transform unstructured data before loading it into your warehouse / database.</td><td><strong>Engineering and Data engineering teams</strong> that need to batch process documents into clean JSON.</td></tr><tr><td>Use Unstract as ready-made nodes in n8n workflows for drag-and-drop automation.</td><td> and  automating workflows.</td></tr></tbody></table><h2>‚òÅÔ∏è Getting Started (Cloud / Enterprise)</h2><p>Unstract Cloud also comes with some really awesome features that give serious accuracy boosts to agentic/LLM-powered document-centric workflows in the enterprise.</p><table><thead><tr></tr></thead><tbody><tr><td>Uses two Large Language Models to ensure trustworthy output. You either get the right response or no response at all.</td></tr><tr><td>Reduces LLM token usage by up to , dramatically cutting costs.</td></tr><tr><td>Reduces LLM token usage by up to , saving costs while keeping accuracy.</td></tr><tr><td>Side-by-side comparison of extracted value and source document, with highlighting for human review and tweaking.</td></tr><tr><td>Enterprise-ready authentication options for seamless onboarding and off-boarding.</td></tr></tbody></table><p>Unstract comes well documented. You can get introduced to the <a href=\"https://docs.unstract.com/unstract/\">basics of Unstract</a>, and <a href=\"https://docs.unstract.com/unstract/unstract_platform/setup_accounts/whats_needed\">learn how to connect</a> various systems like LLMs, Vector Databases, Embedding Models and Text Extractors to it. The easiest way to wet your feet is to go through our <a href=\"https://docs.unstract.com/unstract/unstract_platform/quick_start\">Quick Start Guide</a> where you actually get to do some prompt engineering in Prompt Studio and launch an API to structure varied credit card statements!</p><h2>üöÄ Getting started (self-hosted)</h2><ul><li>Linux or MacOS (Intel or M-series)</li><li>Docker Compose (if you need to install it separately)</li></ul><p>Next, either download a release or clone this repo and do the following:</p><p>That's all there is to it!</p><p>Follow <a href=\"https://raw.githubusercontent.com/Zipstack/unstract/main/backend/README.md#authentication\">these steps</a> to change the default username and password. See <a href=\"https://docs.unstract.com/unstract/unstract_platform/user_guides/run_platform\">user guide</a> for more details on managing the platform.</p><p>Another really quick way to experience Unstract is by signing up for our <a href=\"https://us-central.unstract.com/\">hosted version</a>. It comes with a 14 day free trial!</p><p>Unstract supports a wide range of file formats for document processing:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr><td>Microsoft PowerPoint Open XML</td></tr><tr><td>OpenDocument Presentation</td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>JavaScript Object Notation</td></tr><tr><td>Graphics Interchange Format</td></tr><tr><td>Joint Photographic Experts Group</td></tr><tr><td>Joint Photographic Experts Group</td></tr><tr><td>Portable Network Graphics</td></tr><tr></tr><tr></tr></tbody></table><p>Contributions are welcome! Please see <a href=\"https://raw.githubusercontent.com/Zipstack/unstract/main/CONTRIBUTING.md\">CONTRIBUTING.md</a> for further details to get started easily.</p><h2>üëã Join the LLM-powered automation community</h2><p>Do copy the value of  config in either  or  file to a secure location.</p><p>Adapter credentials are encrypted by the platform using this key. Its loss or change will make all existing adapters inaccessible!</p><p>In full disclosure, Unstract integrates Posthog to track usage analytics. As you can inspect the relevant code here, we collect the minimum possible metrics. Posthog can be disabled if desired by setting  to  in the frontend's .env file.</p>",
      "contentLength": 3625,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ruvnet/wifi-densepose",
      "url": "https://github.com/ruvnet/wifi-densepose",
      "date": 1771124854,
      "author": "",
      "guid": 45139,
      "unread": true,
      "content": "<p>Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers</p><p>A cutting-edge WiFi-based human pose estimation system that leverages Channel State Information (CSI) data and advanced machine learning to provide real-time, privacy-preserving pose detection without cameras.</p><ul><li>: No cameras required - uses WiFi signals for pose detection</li><li>: Sub-50ms latency with 30 FPS pose estimation</li><li>: Simultaneous tracking of up to 10 individuals</li><li><strong>Domain-Specific Optimization</strong>: Healthcare, fitness, smart home, and security applications</li><li>: Production-grade API with authentication, rate limiting, and monitoring</li><li>: Works with standard WiFi routers and access points</li><li>: Fall detection, activity recognition, and occupancy monitoring</li><li>: Real-time pose data streaming for live applications</li><li>: Thoroughly tested with comprehensive test suite</li></ul><h2>ü¶Ä Rust Implementation (v2)</h2><p>A high-performance Rust port is available in <code>/rust-port/wifi-densepose-rs/</code>:</p><h3>Performance Benchmarks (Validated)</h3><table><thead><tr></tr></thead><tbody><tr></tr><tr><td>Phase Sanitization (4x64)</td></tr><tr><td>Feature Extraction (4x64)</td></tr><tr></tr><tr></tr></tbody></table><table><tbody><tr></tr><tr></tr><tr></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr></tr></tbody></table><pre><code>cd rust-port/wifi-densepose-rs\ncargo build --release\ncargo test --workspace\ncargo bench --package wifi-densepose-signal\n</code></pre><p>Mathematical correctness validated:</p><ul><li>‚úÖ Phase unwrapping: 0.000000 radians max error</li><li>‚úÖ Amplitude RMS: Exact match</li><li>‚úÖ Doppler shift: 33.33 Hz (exact)</li><li>‚úÖ Correlation: 1.0 for identical signals</li><li>‚úÖ Phase coherence: 1.0 for coherent signals</li></ul><h2>üö® WiFi-Mat: Disaster Response Module</h2><p>A specialized extension for <strong>search and rescue operations</strong> - detecting and localizing survivors trapped in rubble, earthquakes, and natural disasters.</p><table><tbody><tr><td>Breathing (4-60 BPM), heartbeat via micro-Doppler</td></tr><tr><td>Position estimation through debris up to 5m depth</td></tr><tr><td>Automatic Immediate/Delayed/Minor/Deceased classification</td></tr><tr><td>Priority-based notifications with escalation</td></tr></tbody></table><ul><li>Earthquake search and rescue</li><li>Building collapse response</li><li>Avalanche victim location</li></ul><pre><code>use wifi_densepose_mat::{DisasterResponse, DisasterConfig, DisasterType, ScanZone, ZoneBounds};\n\nlet config = DisasterConfig::builder()\n    .disaster_type(DisasterType::Earthquake)\n    .sensitivity(0.85)\n    .max_depth(5.0)\n    .build();\n\nlet mut response = DisasterResponse::new(config);\nresponse.initialize_event(location, \"Building collapse\")?;\nresponse.add_zone(ScanZone::new(\"North Wing\", ZoneBounds::rectangle(0.0, 0.0, 30.0, 20.0)))?;\nresponse.start_scanning().await?;\n\n// Get survivors prioritized by triage status\nlet immediate = response.survivors_by_triage(TriageStatus::Immediate);\nprintln!(\"{} survivors require immediate rescue\", immediate.len());\n</code></pre><pre><code>cd rust-port/wifi-densepose-rs\ncargo build --release --package wifi-densepose-mat\ncargo test --package wifi-densepose-mat\n</code></pre><p>WiFi DensePose consists of several key components working together:</p><pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   WiFi Router   ‚îÇ    ‚îÇ   WiFi Router   ‚îÇ    ‚îÇ   WiFi Router   ‚îÇ\n‚îÇ   (CSI Source)  ‚îÇ    ‚îÇ   (CSI Source)  ‚îÇ    ‚îÇ   (CSI Source)  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n          ‚îÇ                      ‚îÇ                      ‚îÇ\n          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                 ‚îÇ\n                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                    ‚îÇ     CSI Data Collector    ‚îÇ\n                    ‚îÇ   (Hardware Interface)    ‚îÇ\n                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                  ‚îÇ\n                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                    ‚îÇ    Signal Processor       ‚îÇ\n                    ‚îÇ  (Phase Sanitization)     ‚îÇ\n                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                  ‚îÇ\n                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                    ‚îÇ   Neural Network Model    ‚îÇ\n                    ‚îÇ    (DensePose Head)       ‚îÇ\n                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                  ‚îÇ\n                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                    ‚îÇ   Person Tracker          ‚îÇ\n                    ‚îÇ  (Multi-Object Tracking)  ‚îÇ\n                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                  ‚îÇ\n          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n          ‚îÇ                       ‚îÇ                       ‚îÇ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   REST API        ‚îÇ   ‚îÇ  WebSocket API    ‚îÇ   ‚îÇ   Analytics       ‚îÇ\n‚îÇ  (CRUD Operations)‚îÇ   ‚îÇ (Real-time Stream)‚îÇ   ‚îÇ  (Fall Detection) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</code></pre><ul><li>: Extracts and processes Channel State Information from WiFi signals</li><li>: Removes hardware-specific phase offsets and noise</li><li>: Converts CSI data to human pose keypoints</li><li>: Maintains consistent person identities across frames</li><li>: Comprehensive API for data access and system control</li><li>: Real-time pose data broadcasting</li><li>: Advanced analytics including fall detection and activity recognition</li></ul><p>WiFi-DensePose is now available on PyPI for easy installation:</p><pre><code># Install the latest stable version\npip install wifi-densepose\n\n# Install with specific version\npip install wifi-densepose==1.0.0\n\n# Install with optional dependencies\npip install wifi-densepose[gpu]  # For GPU acceleration\npip install wifi-densepose[dev]  # For development\npip install wifi-densepose[all]  # All optional dependencies\n</code></pre><pre><code>git clone https://github.com/ruvnet/wifi-densepose.git\ncd wifi-densepose\npip install -r requirements.txt\npip install -e .\n</code></pre><pre><code>docker pull ruvnet/wifi-densepose:latest\ndocker run -p 8000:8000 ruvnet/wifi-densepose:latest\n</code></pre><ul><li>: Linux (Ubuntu 18.04+), macOS (10.15+), Windows 10+</li><li>: Minimum 4GB RAM, Recommended 8GB+</li><li>: 2GB free space for models and data</li><li>: WiFi interface with CSI capability</li><li>: Optional but recommended (NVIDIA GPU with CUDA support)</li></ul><pre><code># Install the package\npip install wifi-densepose\n\n# Copy example configuration\ncp example.env .env\n\n# Edit configuration (set your WiFi interface)\nnano .env\n</code></pre><pre><code>from wifi_densepose import WiFiDensePose\n\n# Initialize with default configuration\nsystem = WiFiDensePose()\n\n# Start pose estimation\nsystem.start()\n\n# Get latest pose data\nposes = system.get_latest_poses()\nprint(f\"Detected {len(poses)} persons\")\n\n# Stop the system\nsystem.stop()\n</code></pre><pre><code># Start the API server\nwifi-densepose start\n\n# Start with custom configuration\nwifi-densepose -c /path/to/config.yaml start\n\n# Start with verbose logging\nwifi-densepose -v start\n\n# Check server status\nwifi-densepose status\n</code></pre><p>The API will be available at </p><pre><code>import asyncio\nimport websockets\nimport json\n\nasync def stream_poses():\n    uri = \"ws://localhost:8000/ws/pose/stream\"\n    async with websockets.connect(uri) as websocket:\n        while True:\n            data = await websocket.recv()\n            poses = json.loads(data)\n            print(f\"Received poses: {len(poses['persons'])} persons detected\")\n\n# Run the streaming client\nasyncio.run(stream_poses())\n</code></pre><p>WiFi DensePose provides a comprehensive command-line interface for easy system management, configuration, and monitoring.</p><p>The CLI is automatically installed with the package:</p><pre><code># Install WiFi DensePose with CLI\npip install wifi-densepose\n\n# Verify CLI installation\nwifi-densepose --help\nwifi-densepose version\n</code></pre><p>The WiFi-DensePose CLI provides the following commands:</p><pre><code>wifi-densepose [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  -c, --config PATH  Path to configuration file\n  -v, --verbose      Enable verbose logging\n  --debug            Enable debug mode\n  --help             Show this message and exit.\n\nCommands:\n  config   Configuration management commands.\n  db       Database management commands.\n  start    Start the WiFi-DensePose API server.\n  status   Show the status of the WiFi-DensePose API server.\n  stop     Stop the WiFi-DensePose API server.\n  tasks    Background task management commands.\n  version  Show version information.\n</code></pre><pre><code># Start the WiFi-DensePose API server\nwifi-densepose start\n\n# Start with custom configuration\nwifi-densepose -c /path/to/config.yaml start\n\n# Start with verbose logging\nwifi-densepose -v start\n\n# Start with debug mode\nwifi-densepose --debug start\n\n# Check server status\nwifi-densepose status\n\n# Stop the server\nwifi-densepose stop\n\n# Show version information\nwifi-densepose version\n</code></pre><pre><code># Configuration management commands\nwifi-densepose config [SUBCOMMAND]\n\n# Examples:\n# Show current configuration\nwifi-densepose config show\n\n# Validate configuration file\nwifi-densepose config validate\n\n# Create default configuration\nwifi-densepose config init\n\n# Edit configuration\nwifi-densepose config edit\n</code></pre><pre><code># Database management commands\nwifi-densepose db [SUBCOMMAND]\n\n# Examples:\n# Initialize database\nwifi-densepose db init\n\n# Run database migrations\nwifi-densepose db migrate\n\n# Check database status\nwifi-densepose db status\n\n# Backup database\nwifi-densepose db backup\n\n# Restore database\nwifi-densepose db restore\n</code></pre><pre><code># Background task management commands\nwifi-densepose tasks [SUBCOMMAND]\n\n# Examples:\n# List running tasks\nwifi-densepose tasks list\n\n# Start background tasks\nwifi-densepose tasks start\n\n# Stop background tasks\nwifi-densepose tasks stop\n\n# Check task status\nwifi-densepose tasks status\n</code></pre><pre><code># Show help for main command\nwifi-densepose --help\n\n# Show help for specific command\nwifi-densepose start --help\nwifi-densepose config --help\nwifi-densepose db --help\n\n# Use global options with commands\nwifi-densepose -v status          # Verbose status check\nwifi-densepose --debug start      # Start with debug logging\nwifi-densepose -c custom.yaml start  # Start with custom config\n</code></pre><pre><code># Basic server lifecycle\nwifi-densepose start              # Start the server\nwifi-densepose status             # Check if running\nwifi-densepose stop               # Stop the server\n\n# Configuration management\nwifi-densepose config show        # View current config\nwifi-densepose config validate    # Check config validity\n\n# Database operations\nwifi-densepose db init            # Initialize database\nwifi-densepose db migrate         # Run migrations\nwifi-densepose db status          # Check database health\n\n# Task management\nwifi-densepose tasks list         # List background tasks\nwifi-densepose tasks status       # Check task status\n\n# Version and help\nwifi-densepose version            # Show version info\nwifi-densepose --help             # Show help message\n</code></pre><pre><code># 1. Check version and help\nwifi-densepose version\nwifi-densepose --help\n\n# 2. Initialize configuration\nwifi-densepose config init\n\n# 3. Initialize database\nwifi-densepose db init\n\n# 4. Start the server\nwifi-densepose start\n\n# 5. Check status\nwifi-densepose status\n</code></pre><pre><code># Start with debug logging\nwifi-densepose --debug start\n\n# Use custom configuration\nwifi-densepose -c dev-config.yaml start\n\n# Check database status\nwifi-densepose db status\n\n# Manage background tasks\nwifi-densepose tasks start\nwifi-densepose tasks list\n</code></pre><pre><code># Start with production config\nwifi-densepose -c production.yaml start\n\n# Check system status\nwifi-densepose status\n\n# Manage database\nwifi-densepose db migrate\nwifi-densepose db backup\n\n# Monitor tasks\nwifi-densepose tasks status\n</code></pre><pre><code># Enable verbose logging\nwifi-densepose -v status\n\n# Check configuration\nwifi-densepose config validate\n\n# Check database health\nwifi-densepose db status\n\n# Restart services\nwifi-densepose stop\nwifi-densepose start\n</code></pre><p>Comprehensive documentation is available to help you get started and make the most of WiFi-DensePose:</p><ul><li> - Complete guide covering installation, setup, basic usage, and examples</li><li> - Detailed documentation of all public classes, methods, and endpoints</li><li> - Production deployment, Docker setup, Kubernetes, and scaling strategies</li></ul><p>The system provides a comprehensive REST API and WebSocket streaming:</p><pre><code># Pose estimation\nGET /api/v1/pose/latest          # Get latest pose data\nGET /api/v1/pose/history         # Get historical data\nGET /api/v1/pose/zones/{zone_id} # Get zone-specific data\n\n# System management\nGET /api/v1/system/status        # System health and status\nPOST /api/v1/system/calibrate    # Calibrate environment\nGET /api/v1/analytics/summary    # Analytics dashboard data\n</code></pre><pre><code>// Real-time pose data\nws://localhost:8000/ws/pose/stream\n\n// Analytics events (falls, alerts)\nws://localhost:8000/ws/analytics/events\n\n// System status updates\nws://localhost:8000/ws/system/status\n</code></pre><pre><code>from wifi_densepose import WiFiDensePoseClient\n\n# Initialize client\nclient = WiFiDensePoseClient(base_url=\"http://localhost:8000\")\n\n# Get latest poses with confidence filtering\nposes = client.get_latest_poses(min_confidence=0.7)\nprint(f\"Detected {len(poses)} persons\")\n\n# Get zone occupancy\noccupancy = client.get_zone_occupancy(\"living_room\")\nprint(f\"Living room occupancy: {occupancy.person_count}\")\n</code></pre><p>WiFi DensePose works with standard WiFi equipment that supports CSI extraction:</p><ul><li> (RT-AX88U) - Excellent CSI quality</li><li> - High performance</li><li> - Budget-friendly option</li><li> - Enterprise grade</li></ul><ul><li>Intel WiFi cards (5300, 7260, 8260, 9260)</li></ul><ol><li>: Position routers to create overlapping coverage areas</li><li>: Mount routers 2-3 meters high for optimal coverage</li><li>: 5-10 meter spacing between routers depending on environment</li><li>: Ensure antennas are positioned for maximum signal diversity</li></ol><pre><code># Configure WiFi interface for CSI extraction\nsudo iwconfig wlan0 mode monitor\nsudo iwconfig wlan0 channel 6\n\n# Set up CSI extraction (Intel 5300 example)\necho 0x4101 | sudo tee /sys/kernel/debug/ieee80211/phy0/iwlwifi/iwldvm/debug/monitor_tx_rate\n</code></pre><pre><code>from wifi_densepose import Calibrator\n\n# Run environment calibration\ncalibrator = Calibrator()\ncalibrator.calibrate_environment(\n    duration_minutes=10,\n    environment_id=\"room_001\"\n)\n\n# Apply calibration\ncalibrator.apply_calibration()\n</code></pre><p>Copy  to  and configure:</p><pre><code># Application Settings\nAPP_NAME=WiFi-DensePose API\nVERSION=1.0.0\nENVIRONMENT=production  # development, staging, production\nDEBUG=false\n\n# Server Settings\nHOST=0.0.0.0\nPORT=8000\nWORKERS=4\n\n# Security Settings\nSECRET_KEY=your-secure-secret-key-here\nJWT_ALGORITHM=HS256\nJWT_EXPIRE_HOURS=24\n\n# Hardware Settings\nWIFI_INTERFACE=wlan0\nCSI_BUFFER_SIZE=1000\nHARDWARE_POLLING_INTERVAL=0.1\n\n# Pose Estimation Settings\nPOSE_CONFIDENCE_THRESHOLD=0.7\nPOSE_PROCESSING_BATCH_SIZE=32\nPOSE_MAX_PERSONS=10\n\n# Feature Flags\nENABLE_AUTHENTICATION=true\nENABLE_RATE_LIMITING=true\nENABLE_WEBSOCKETS=true\nENABLE_REAL_TIME_PROCESSING=true\nENABLE_HISTORICAL_DATA=true\n</code></pre><h3>Domain-Specific Configurations</h3><pre><code>config = {\n    \"domain\": \"healthcare\",\n    \"detection\": {\n        \"confidence_threshold\": 0.8,\n        \"max_persons\": 5,\n        \"enable_tracking\": True\n    },\n    \"analytics\": {\n        \"enable_fall_detection\": True,\n        \"enable_activity_recognition\": True,\n        \"alert_thresholds\": {\n            \"fall_confidence\": 0.9,\n            \"inactivity_timeout\": 300\n        }\n    },\n    \"privacy\": {\n        \"data_retention_days\": 30,\n        \"anonymize_data\": True,\n        \"enable_encryption\": True\n    }\n}\n</code></pre><pre><code>config = {\n    \"domain\": \"fitness\",\n    \"detection\": {\n        \"confidence_threshold\": 0.6,\n        \"max_persons\": 20,\n        \"enable_tracking\": True\n    },\n    \"analytics\": {\n        \"enable_activity_recognition\": True,\n        \"enable_form_analysis\": True,\n        \"metrics\": [\"rep_count\", \"form_score\", \"intensity\"]\n    }\n}\n</code></pre><pre><code>from wifi_densepose.config import Settings\n\n# Load custom configuration\nsettings = Settings(\n    pose_model_path=\"/path/to/custom/model.pth\",\n    neural_network={\n        \"batch_size\": 64,\n        \"enable_gpu\": True,\n        \"inference_timeout\": 500\n    },\n    tracking={\n        \"max_age\": 30,\n        \"min_hits\": 3,\n        \"iou_threshold\": 0.3\n    }\n)\n</code></pre><p>WiFi DensePose maintains 100% test coverage with comprehensive testing:</p><pre><code># Run all tests\npytest\n\n# Run with coverage report\npytest --cov=wifi_densepose --cov-report=html\n\n# Run specific test categories\npytest tests/unit/          # Unit tests\npytest tests/integration/   # Integration tests\npytest tests/e2e/          # End-to-end tests\npytest tests/performance/  # Performance tests\n</code></pre><h4>Unit Tests (95% coverage)</h4><ul><li>CSI processing algorithms</li><li>Neural network components</li></ul><ul><li>Hardware interface integration</li></ul><ul><li>Complete pose estimation pipeline</li><li>Multi-person tracking scenarios</li></ul><ul></ul><p>For development without hardware:</p><pre><code># Enable mock mode\nexport MOCK_HARDWARE=true\nexport MOCK_POSE_DATA=true\n\n# Run tests with mocked hardware\npytest tests/ --mock-hardware\n</code></pre><pre><code># .github/workflows/test.yml\nname: Test Suite\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: 3.8\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install -e .\n      - name: Run tests\n        run: pytest --cov=wifi_densepose --cov-report=xml\n      - name: Upload coverage\n        uses: codecov/codecov-action@v1\n</code></pre><pre><code># Build production image\ndocker build -t wifi-densepose:latest .\n\n# Run with production configuration\ndocker run -d \\\n  --name wifi-densepose \\\n  -p 8000:8000 \\\n  -v /path/to/data:/app/data \\\n  -v /path/to/models:/app/models \\\n  -e ENVIRONMENT=production \\\n  -e SECRET_KEY=your-secure-key \\\n  wifi-densepose:latest\n</code></pre><pre><code># docker-compose.yml\nversion: '3.8'\nservices:\n  wifi-densepose:\n    image: wifi-densepose:latest\n    ports:\n      - \"8000:8000\"\n    environment:\n      - ENVIRONMENT=production\n      - DATABASE_URL=postgresql://user:pass@db:5432/wifi_densepose\n      - REDIS_URL=redis://redis:6379/0\n    volumes:\n      - ./data:/app/data\n      - ./models:/app/models\n    depends_on:\n      - db\n      - redis\n\n  db:\n    image: postgres:13\n    environment:\n      POSTGRES_DB: wifi_densepose\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\n  redis:\n    image: redis:6-alpine\n    volumes:\n      - redis_data:/data\n\nvolumes:\n  postgres_data:\n  redis_data:\n</code></pre><pre><code># k8s/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wifi-densepose\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: wifi-densepose\n  template:\n    metadata:\n      labels:\n        app: wifi-densepose\n    spec:\n      containers:\n      - name: wifi-densepose\n        image: wifi-densepose:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: ENVIRONMENT\n          value: \"production\"\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: wifi-densepose-secrets\n              key: database-url\n        resources:\n          requests:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n          limits:\n            memory: \"4Gi\"\n            cpu: \"2000m\"\n</code></pre><pre><code># terraform/main.tf\nresource \"aws_ecs_cluster\" \"wifi_densepose\" {\n  name = \"wifi-densepose\"\n}\n\nresource \"aws_ecs_service\" \"wifi_densepose\" {\n  name            = \"wifi-densepose\"\n  cluster         = aws_ecs_cluster.wifi_densepose.id\n  task_definition = aws_ecs_task_definition.wifi_densepose.arn\n  desired_count   = 3\n\n  load_balancer {\n    target_group_arn = aws_lb_target_group.wifi_densepose.arn\n    container_name   = \"wifi-densepose\"\n    container_port   = 8000\n  }\n}\n</code></pre><pre><code># ansible/playbook.yml\n- hosts: servers\n  become: yes\n  tasks:\n    - name: Install Docker\n      apt:\n        name: docker.io\n        state: present\n\n    - name: Deploy WiFi DensePose\n      docker_container:\n        name: wifi-densepose\n        image: wifi-densepose:latest\n        ports:\n          - \"8000:8000\"\n        env:\n          ENVIRONMENT: production\n          DATABASE_URL: \"{{ database_url }}\"\n        restart_policy: always\n</code></pre><pre><code># monitoring/prometheus.yml\nglobal:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: 'wifi-densepose'\n    static_configs:\n      - targets: ['localhost:8000']\n    metrics_path: '/metrics'\n</code></pre><pre><code>{\n  \"dashboard\": {\n    \"title\": \"WiFi DensePose Monitoring\",\n    \"panels\": [\n      {\n        \"title\": \"Pose Detection Rate\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(pose_detections_total[5m])\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Processing Latency\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, pose_processing_duration_seconds_bucket)\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre><ul><li>: 45.2ms per frame</li><li>: 30 FPS sustained</li></ul><ul><li>: 94.2% (compared to camera-based systems)</li><li>: 91.8%</li><li><strong>Fall Detection Sensitivity</strong>: 96.5%</li><li><strong>Fall Detection Specificity</strong>: 94.1%</li></ul><ul><li>: 65% (4-core system)</li><li>: 78% (NVIDIA RTX 3080)</li><li>: 15 Mbps (CSI data)</li></ul><ul><li>: 1000+ WebSocket connections</li><li>: 10,000 requests/minute</li><li>: 50GB/month (with compression)</li><li><strong>Multi-Environment Support</strong>: Up to 50 simultaneous environments</li></ul><pre><code># Enable GPU acceleration\nconfig = {\n    \"neural_network\": {\n        \"enable_gpu\": True,\n        \"batch_size\": 64,\n        \"mixed_precision\": True\n    },\n    \"processing\": {\n        \"num_workers\": 4,\n        \"prefetch_factor\": 2\n    }\n}\n</code></pre><pre><code># Enable performance optimizations\nconfig = {\n    \"caching\": {\n        \"enable_redis\": True,\n        \"cache_ttl\": 300\n    },\n    \"database\": {\n        \"connection_pool_size\": 20,\n        \"enable_query_cache\": True\n    }\n}\n</code></pre><pre><code># API load testing with Apache Bench\nab -n 10000 -c 100 http://localhost:8000/api/v1/pose/latest\n\n# WebSocket load testing\npython scripts/websocket_load_test.py --connections 1000 --duration 300\n</code></pre><p>We welcome contributions to WiFi DensePose! Please follow these guidelines:</p><pre><code># Clone the repository\ngit clone https://github.com/ruvnet/wifi-densepose.git\ncd wifi-densepose\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install development dependencies\npip install -r requirements-dev.txt\npip install -e .\n\n# Install pre-commit hooks\npre-commit install\n</code></pre><ul><li>: Follow PEP 8, enforced by Black and Flake8</li><li>: Use type hints for all functions and methods</li><li>: Comprehensive docstrings for all public APIs</li><li>: Maintain 100% test coverage for new code</li><li>: Follow OWASP guidelines for security</li></ul><ol><li> a feature branch (<code>git checkout -b feature/amazing-feature</code>)</li><li> your changes (<code>git commit -m 'Add amazing feature'</code>)</li><li> to the branch (<code>git push origin feature/amazing-feature</code>)</li></ol><pre><code>**Describe the bug**\nA clear description of the bug.\n\n**To Reproduce**\nSteps to reproduce the behavior.\n\n**Expected behavior**\nWhat you expected to happen.\n\n**Environment**\n- OS: [e.g., Ubuntu 20.04]\n- Python version: [e.g., 3.8.10]\n- WiFi DensePose version: [e.g., 1.0.0]\n</code></pre><pre><code>**Feature Description**\nA clear description of the feature.\n\n**Use Case**\nDescribe the use case and benefits.\n\n**Implementation Ideas**\nAny ideas on how to implement this feature.\n</code></pre><p>This project is licensed under the MIT License - see the <a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/LICENSE\">LICENSE</a> file for details.</p><pre><code>MIT License\n\nCopyright (c) 2025 WiFi DensePose Contributors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre><ul><li>: Based on groundbreaking research in WiFi-based human sensing</li><li>: Built on PyTorch, FastAPI, and other excellent open source projects</li><li>: Thanks to all contributors and users who make this project possible</li><li>: Special thanks to router manufacturers for CSI support</li></ul><p> - Revolutionizing human pose estimation through privacy-preserving WiFi technology.</p>",
      "contentLength": 25038,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "alibaba/zvec",
      "url": "https://github.com/alibaba/zvec",
      "date": 1771124854,
      "author": "",
      "guid": 45140,
      "unread": true,
      "content": "<p>A lightweight, lightning-fast, in-process vector database</p><p> is an open-source, in-process vector database ‚Äî lightweight, lightning-fast, and designed to embed directly into applications. Built on  (Alibaba's battle-tested vector search engine), it delivers production-grade, low-latency, scalable similarity search with minimal setup.</p><ul><li>: Searches billions of vectors in milliseconds.</li><li>: <a href=\"https://raw.githubusercontent.com/alibaba/zvec/main/#-installation\">Install</a> and start searching in seconds. No servers, no config, no fuss.</li><li>: Work with both dense and sparse embeddings, with native support for multi-vector queries in a single call.</li><li>: Combine semantic similarity with structured filters for precise results.</li><li>: As an in-process library, Zvec runs wherever your code runs ‚Äî notebooks, servers, CLI tools, or even edge devices.</li></ul><p>: Python 3.10 - 3.12</p><ul></ul><pre><code>import zvec\n\n# Define collection schema\nschema = zvec.CollectionSchema(\n    name=\"example\",\n    vectors=zvec.VectorSchema(\"embedding\", zvec.DataType.VECTOR_FP32, 4),\n)\n\n# Create collection\ncollection = zvec.create_and_open(path=\"./zvec_example\", schema=schema)\n\n# Insert documents\ncollection.insert([\n    zvec.Doc(id=\"doc_1\", vectors={\"embedding\": [0.1, 0.2, 0.3, 0.4]}),\n    zvec.Doc(id=\"doc_2\", vectors={\"embedding\": [0.2, 0.3, 0.4, 0.1]}),\n])\n\n# Search by vector similarity\nresults = collection.query(\n    zvec.VectorQuery(\"embedding\", vector=[0.4, 0.3, 0.3, 0.1]),\n    topk=10\n)\n\n# Results: list of {'id': str, 'score': float, ...}, sorted by relevance\nprint(results)\n</code></pre><p>Zvec delivers exceptional speed and efficiency, making it ideal for demanding production workloads.</p><img src=\"https://zvec.oss-cn-hongkong.aliyuncs.com/qps_10M.svg?sanitize=true\" width=\"800\" alt=\"Zvec Performance Benchmarks\"><p>We welcome and appreciate contributions from the community! Whether you're fixing a bug, adding a feature, or improving documentation, your help makes Zvec better for everyone.</p>",
      "contentLength": 1721,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "letta-ai/letta-code",
      "url": "https://github.com/letta-ai/letta-code",
      "date": 1771124854,
      "author": "",
      "guid": 45141,
      "unread": true,
      "content": "<p>The memory-first coding agent</p><p>Letta Code is a memory-first coding harness, built on top of the Letta API. Instead of working in independent sessions, you work with a persisted agent that learns over time and is portable across models (Claude Sonnet/Opus 4.5, GPT-5.2-Codex, Gemini 3 Pro, GLM-4.7, and more).</p><p>Install the package via <a href=\"https://docs.npmjs.com/downloading-and-installing-node-js-and-npm\">npm</a>:</p><pre><code>npm install -g @letta-ai/letta-code\n</code></pre><p>Navigate to your project directory and run  (see various command-line options <a href=\"https://docs.letta.com/letta-code/commands\">on the docs</a>).</p><p>Run  to configure your own LLM API keys (OpenAI, Anthropic, etc.), and use  to swap models.</p><blockquote><p>[!NOTE] By default, Letta Code will to connect to the <a href=\"https://app.letta.com/\">Letta API</a>. Use  to use your own LLM API keys and coding plans (Codex, zAI, Minimax) for free. Set  to connect to an external <a href=\"https://docs.letta.com/letta-code/docker\">Docker server</a>.</p></blockquote><p>Letta Code is built around long-lived agents that persist across sessions and improve with use. Rather than working in independent sessions, each session is tied to a persisted agent that learns.</p><p><strong>Claude Code / Codex / Gemini CLI</strong> (Session-Based)</p><ul><li>No learning between sessions</li><li>Context = messages in the current session + </li><li>Relationship: Every conversation is like meeting a new contractor</li></ul><ul><li>Same agent across sessions</li><li>Persistent memory and learning over time</li><li> starts a new conversation (aka \"thread\" or \"session\"), but memory persists</li><li>Relationship: Like having a coworker or mentee that learns and remembers</li></ul><p>If you‚Äôre using Letta Code for the first time, you will likely want to run the  command to initialize the agent‚Äôs memory system:</p><p>Over time, the agent will update its memory as it learns. To actively guide your agents memory, you can use the  command:</p><pre><code>&gt; /remember [optional instructions on what to remember]\n</code></pre><p>Letta Code works with skills (reusable modules that teach your agent new capabilities in a  directory), but additionally supports <a href=\"https://www.letta.com/blog/skill-learning\">skill learning</a>. You can ask your agent to learn a skill from it's current trajectory with the command:</p><pre><code>&gt; /skill [optional instructions on what skill to learn]\n</code></pre><p>Community maintained packages are available for Arch Linux users on the <a href=\"https://aur.archlinux.org/packages/letta-code\">AUR</a>:</p><pre><code>yay -S letta-code # release\nyay -S letta-code-git # nightly\n</code></pre><p>Made with üíú in San Francisco</p>",
      "contentLength": 2102,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "minio/minio",
      "url": "https://github.com/minio/minio",
      "date": 1771124854,
      "author": "",
      "guid": 45142,
      "unread": true,
      "content": "<p>MinIO is a high-performance, S3 compatible object store, open sourced under GNU AGPLv3 license.</p><blockquote><p>[!NOTE] <strong>THIS REPOSITORY IS NO LONGER MAINTAINED.</strong></p><ul><li> ‚Äî Full-featured, standalone edition for community use (free license)</li></ul></blockquote><p>MinIO is a high-performance, S3-compatible object storage solution released under the GNU AGPL v3.0 license. Designed for speed and scalability, it powers AI/ML, analytics, and data-intensive workloads with industry-leading performance.</p><ul><li>S3 API Compatible ‚Äì Seamless integration with existing S3 tools</li><li>Built for AI &amp; Analytics ‚Äì Optimized for large-scale data pipelines</li><li>High Performance ‚Äì Ideal for demanding storage workloads.</li></ul><p>This README provides instructions for building MinIO from source and deploying onto baremetal hardware. Use the <a href=\"https://github.com/minio/docs\">MinIO Documentation</a> project to build and host a local copy of the documentation.</p><h2>MinIO is Open Source Software</h2><p>We designed MinIO as Open Source software for the Open Source software community. We encourage the community to remix, redesign, and reshare MinIO under the terms of the AGPLv3 license.</p><p>All usage of MinIO in your application stack requires validation against AGPLv3 obligations, which include but are not limited to the release of modified code to the community from which you have benefited. Any commercial/proprietary usage of the AGPLv3 software, including repackaging or reselling services/features, is done at your own risk.</p><p>The AGPLv3 provides no obligation by any party to support, maintain, or warranty the original or any modified work. All support is provided on a best-effort basis through Github and our <a href=\"https://slack.min.io\">Slack</a> channel, and any member of the community is welcome to contribute and assist others in their usage of the software.</p><p>MinIO <a href=\"https://www.min.io/product/aistor\">AIStor</a> includes enterprise-grade support and licensing for workloads which require commercial or proprietary usage and production-level SLA/SLO-backed support. For more information, <a href=\"https://min.io/pricing\">reach out for a quote</a>.</p><p> The MinIO community edition is now distributed as source code only. We will no longer provide pre-compiled binary releases for the community version.</p><h3>Installing Latest MinIO Community Edition</h3><p>To use MinIO community edition, you have two options:</p><ol><li> using <code>go install github.com/minio/minio@latest</code> (recommended)</li><li> from the provided Dockerfile</li></ol><p>See the sections below for detailed instructions on each method.</p><p>Historical pre-compiled binary releases remain available for reference but are no longer maintained:</p><p><strong>These legacy binaries will not receive updates.</strong> We strongly recommend using source builds for access to the latest features, bug fixes, and security updates.</p><p>Use the following commands to compile and run a standalone MinIO server from source. If you do not have a working Golang environment, please follow <a href=\"https://golang.org/doc/install\">How to install Golang</a>. Minimum version required is <a href=\"https://golang.org/dl/#stable\">go1.24</a></p><pre><code>go install github.com/minio/minio@latest\n</code></pre><p>You can alternatively run  and use the  and  environment variables to control the OS and architecture target. For example:</p><pre><code>env GOOS=linux GOARCH=arm64 go build\n</code></pre><p>Start MinIO by running  where  is any empty folder on your local filesystem.</p><p>The MinIO deployment starts using default root credentials . You can test the deployment using the MinIO Console, an embedded web-based object browser built into MinIO Server. Point a web browser running on the host machine to <a href=\"http://127.0.0.1:9000\">http://127.0.0.1:9000</a> and log in with the root credentials. You can use the Browser to create buckets, upload objects, and browse the contents of the MinIO server.</p><p>You can also connect using any S3-compatible tool, such as the MinIO Client  commandline tool:</p><pre><code>mc alias set local http://localhost:9000 minioadmin minioadmin\nmc admin info local\n</code></pre><blockquote><p>[!NOTE] Production environments using compiled-from-source MinIO binaries do so at their own risk. The AGPLv3 license provides no warranties nor liabilities for any such usage.</p></blockquote><p>You can use the  command to build a Docker image on your local host machine. You must first <a href=\"https://raw.githubusercontent.com/minio/minio/master/#install-from-source\">build MinIO</a> and ensure the  binary exists in the project root.</p><p>The following command builds the Docker image using the default  in the root project directory with the repository and image tag </p><pre><code>docker build -t myminio:minio .\n</code></pre><p>Use  to confirm the image exists in your local repository. You can run the server using standard Docker invocation:</p><pre><code>docker run -p 9000:9000 -p 9001:9001 myminio:minio server /tmp/minio --console-address :9001\n</code></pre><p>Complete documentation for building Docker containers, managing custom images, or loading images into orchestration platforms is out of scope for this documentation. You can modify the  and <code>dockerscripts/docker-entrypoint.sh</code> as-needed to reflect your specific image requirements.</p><p>See the <a href=\"https://docs.min.io/community/minio-object-store/operations/deployments/baremetal-deploy-minio-as-a-container.html#deploy-minio-container\">MinIO Container</a> documentation for more guidance on running MinIO within a Container image.</p><h2>Install using Helm Charts</h2><p>There are two paths for installing MinIO onto Kubernetes infrastructure:</p><p>See the <a href=\"https://docs.min.io/community/minio-object-store/operations/deployments/kubernetes.html\">MinIO Documentation</a> for guidance on deploying using the Operator. The Community Helm chart has instructions in the folder-level README.</p><p>MinIO Server comes with an embedded web based object browser. Point your web browser to <a href=\"http://127.0.0.1:9000\">http://127.0.0.1:9000</a> to ensure your server has started successfully.</p><blockquote><p>[!NOTE] MinIO runs console on random port by default, if you wish to choose a specific port use  to pick a specific interface and port.</p></blockquote><h3>Test using MinIO Client </h3><p> provides a modern alternative to UNIX commands like ls, cat, cp, mirror, diff etc. It supports filesystems and Amazon S3 compatible cloud storage services.</p><p>The following commands set a local alias, validate the server information, create a bucket, copy data to that bucket, and list the contents of the bucket.</p><pre><code>mc alias set local http://localhost:9000 minioadmin minioadmin\nmc admin info\nmc mb data\nmc cp ~/Downloads/mydata data/\nmc ls data/\n</code></pre><h2>Contribute to MinIO Project</h2><p>Please follow MinIO <a href=\"https://github.com/minio/minio/raw/master/CONTRIBUTING.md\">Contributor's Guide</a> for guidance on making new contributions to the repository.</p>",
      "contentLength": 5811,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "DebugSwift/DebugSwift",
      "url": "https://github.com/DebugSwift/DebugSwift",
      "date": 1771038090,
      "author": "",
      "guid": 44930,
      "unread": true,
      "content": "<p>A toolkit to make debugging iOS applications easier üöÄ</p><img width=\"1970\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a569b038-9058-4260-ae7c-47f3376cf629\"><img width=\"1970\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/334ccefa-5951-494f-8faa-5f016d39f946\"><img width=\"1970\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/246cde3c-7a14-45de-ae01-e810c42d8e65\"><img width=\"1970\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fadde188-dcba-46d8-9460-762f9be98bd6\"><img width=\"1970\" height=\"1184\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8085e55c-a7e6-4e3b-8ceb-8fc7034480fe\"><img width=\"1970\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a435a660-a4b2-4a3f-852e-a7bf0709e75e\"><img width=\"1970\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/15f34de1-214f-4bc3-95bc-b25efc2d383e\"><ul></ul><ul><li> Capture all requests/responses with detailed logs and filtering</li><li> Zero-config automatic monitoring of WebSocket connections and frames</li><li> Set thresholds to monitor and control API usage</li><li> Automatic JSON formatting with syntax highlighting</li><li> Automatic decryption of encrypted API responses with AES-256/128 and custom decryptors</li></ul><ul><li> Monitor CPU, memory, and FPS in real-time</li><li> Automatic detection of leaked ViewControllers and Views</li><li> Detect main thread violations with detailed stack traces</li><li> Overlay displaying live performance stats</li></ul><ul><li> Detailed crash analysis with screenshots and stack traces</li><li> Real-time console output monitoring and filtering</li><li> App version, build, device details, and more</li><li> Easy access and copying of push notification tokens</li><li> Add your own debugging actions and info</li></ul><ul><li> Visual alignment grid with customizable colors and opacity</li><li> 3D interactive view hierarchy inspector</li><li> Visual feedback for touch interactions</li><li> Slow down animations for easier debugging</li><li> Highlight view boundaries with colorization</li><li><strong>SwiftUI Render Tracking (Beta):</strong> Automatically detect and visualize SwiftUI view re-renders with dedicated settings screen</li></ul><ul><li> Navigate app sandbox and shared app group containers</li><li> View and modify app preferences at runtime</li><li> Inspect keychain entries</li><li> SQLite and Realm database inspection</li><li> Simulate push notifications with templates and test scenarios</li></ul><h3>üçÉ Swift Package Manager (Recommended)</h3><p>Add to your :</p><pre><code>dependencies: [\n    .package(url: \"https://github.com/DebugSwift/DebugSwift.git\", from: \"1.0.0\")\n]\n</code></pre><p>Or add through Xcode:  &gt;  &gt; Enter URL:</p><pre><code>https://github.com/DebugSwift/DebugSwift\n</code></pre><h4>Option 1: Source Distribution (Standard)</h4><h4>Option 2: XCFramework Distribution (Faster Builds) ‚ö°</h4><pre><code>pod 'DebugSwift', :http =&gt; 'https://github.com/DebugSwift/DebugSwift/releases/latest/download/DebugSwift.xcframework.zip'\n</code></pre><p>DebugSwift <strong>fully supports Apple Silicon Macs</strong> with native arm64 simulator builds! No more architecture exclusions or compatibility issues.</p><ul><li>üñ•Ô∏è : arm64 (Apple Silicon) + x86_64 (Intel)</li></ul><p> If you were using architecture exclusions like <code>'EXCLUDED_ARCHS[sdk=iphonesimulator*]' =&gt; 'arm64'</code>, you can now  as they are no longer needed.</p><pre><code>import DebugSwift\n\n@main\nclass AppDelegate: UIResponder, UIApplicationDelegate {\n    private let debugSwift = DebugSwift()\n    \n    func application(\n        _ application: UIApplication,\n        didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?\n    ) -&gt; Bool {\n        \n        #if DEBUG\n        debugSwift.setup()\n        // debugSwift.setup(disable: [.leaksDetector])\n        debugSwift.show()\n        #endif\n        \n        return true\n    }\n}\n</code></pre><h3>Shake to Toggle (Optional)</h3><pre><code>extension UIWindow {\n    open override func motionEnded(_ motion: UIEvent.EventSubtype, with event: UIEvent?) {\n        super.motionEnded(motion, with: event)\n        \n        #if DEBUG\n        if motion == .motionShake {\n            if let appDelegate = UIApplication.shared.delegate as? AppDelegate {\n                appDelegate.debugSwift.toggle()\n            }\n        }\n        #endif\n    }\n}\n</code></pre><h3>Apple Silicon Build Issues</h3><p>If you encounter build errors like <code>error unsupported Swift architecture</code> or <code>DebugSwift.framework only contains x86_64 slice for simulator</code> on Apple Silicon Macs:</p><h4>Solution 1: Update to Latest Version</h4><p>Ensure you're using the latest version of DebugSwift which includes full Apple Silicon support:</p><pre><code># CocoaPods\npod 'DebugSwift', '~&gt; 1.8.1'\n\n# Swift Package Manager - update to latest\n</code></pre><h4>Solution 2: Use XCFramework Distribution (Recommended)</h4><p>For faster builds and guaranteed architecture compatibility:</p><pre><code>pod 'DebugSwift', :http =&gt; 'https://github.com/DebugSwift/DebugSwift/releases/latest/download/DebugSwift.xcframework.zip'\n</code></pre><h4>Solution 3: Remove Architecture Exclusions</h4><p>If you have custom architecture exclusions in your project, remove them:</p><pre><code># Remove this from your Podfile or target configuration:\n# config.build_settings['EXCLUDED_ARCHS[sdk=iphonesimulator*]'] = 'arm64'\n</code></pre><p>Clean your build folder and derived data:</p><pre><code># Xcode\nProduct ‚Üí Clean Build Folder (‚åò‚áßK)\n\n# Command line\nrm -rf ~/Library/Developer/Xcode/DerivedData\n</code></pre><ul><li>: Up to 50% faster build times</li><li>: Full source access and debugging capabilities</li></ul><p>Choose XCFramework for production builds, source for active development.</p><h3>Enhanced Hierarchy Tree for Deeper Application Insights (Beta)</h3><p>Harness the Power of Visual Information within the iOS Hierarchy Tree to Uncover Intricate Layouts and Element Relationships in Your Application.</p><p>Simply press and hold the circle button to reveal the Snapshot and Hierarchy for a comprehensive overview.</p><h4>Explore Additional Details</h4><p>Enhance your understanding by pressing and holding on a specific view to reveal information such as:</p><ul><li>Specific attributes based on the type (e.g., UILabel: Text, Font, and TextColor).</li></ul><h3>App Custom ViewControllers in Tab Bar</h3><pre><code>DebugSwift.App.shared.customControllers = {\n    let controller1 = UITableViewController()\n    controller1.title = \"Custom TableVC 1\"\n\n    let controller2 = UITableViewController()\n    controller2.title = \"Custom TableVC 2\"\n    return [controller1, controller2]\n}\n</code></pre><pre><code>// Add custom debugging actions\nDebugSwift.App.shared.customAction = {\n    [\n        .init(title: \"Development Tools\", actions: [\n            .init(title: \"Clear User Data\") {\n                UserDefaults.standard.removeObject(forKey: \"userData\")\n            },\n            .init(title: \"Reset App State\") {\n                // Your reset logic here\n            }\n        ])\n    ]\n}\n</code></pre><pre><code>DebugSwift.App.shared.customInfo = {\n    [\n        .init(\n            title: \"Info 1\",\n            infos: [\n                .init(title: \"title 1\", subtitle: \"title 2\")\n            ]\n        )\n    ]\n}\n</code></pre><pre><code>// In your AppDelegate\nfunc application(_ application: UIApplication, didRegisterForRemoteNotificationsWithDeviceToken deviceToken: Data) {\n    DebugSwift.APNSToken.didRegister(deviceToken: deviceToken)\n    // Your existing token handling code\n}\n\nfunc application(_ application: UIApplication, didFailToRegisterForRemoteNotificationsWithError error: Error) {\n    DebugSwift.APNSToken.didFailToRegister(error: error)\n    // Your existing error handling code\n}\n</code></pre><pre><code>// Ignore specific URLs\nDebugSwift.Network.shared.ignoredURLs = [\"https://analytics.com\"]\n\n// Monitor only specific URLs\nDebugSwift.Network.shared.onlyURLs = [\"https://api.myapp.com\"]\n</code></pre><h3>Network History Management</h3><pre><code>// Clear HTTP/HTTPS request history (useful when switching environments)\nDebugSwift.Network.shared.clearNetworkHistory()\n\n// Clear WebSocket connection history\nawait DebugSwift.Network.shared.clearWebSocketHistory()\n\n// Clear all network data (HTTP + WebSocket)\nawait DebugSwift.Network.shared.clearAllNetworkData()\n\n// Example: Clear network data when switching environments\nDebugSwift.App.shared.customAction = {\n    [\n        .init(title: \"Environment\", actions: [\n            .init(title: \"Switch to Production\") {\n                // Your environment switch logic\n                DebugSwift.Network.shared.clearNetworkHistory()\n            },\n            .init(title: \"Switch to Development\") {\n                // Your environment switch logic\n                DebugSwift.Network.shared.clearNetworkHistory()\n            }\n        ])\n    ]\n}\n</code></pre><h3>Manual URLSessionConfiguration Injection</h3><p>If you create  instances  calling , you can manually inject the network monitoring protocol:</p><pre><code>// Option 1: Inject into existing configuration\nlet config = URLSessionConfiguration.default\nDebugSwift.Network.shared.injectIntoConfiguration(config)\nlet session = URLSession(configuration: config)\n\n// Option 2: Get pre-configured default configuration\nlet config = DebugSwift.Network.shared.defaultConfiguration()\nlet session = URLSession(configuration: config)\n\n// Option 3: Get pre-configured ephemeral configuration\nlet config = DebugSwift.Network.shared.ephemeralConfiguration()\nlet session = URLSession(configuration: config)\n\n// Option 4: Direct protocol class injection (advanced)\nvar config = URLSessionConfiguration.default\nvar protocolClasses = config.protocolClasses ?? []\nprotocolClasses.insert(CustomHTTPProtocol.self, at: 0)\nconfig.protocolClasses = protocolClasses\n</code></pre><p> This is particularly useful when migrating from other network debugging tools like Netfox or when working with pre-existing URLSession configurations.</p><h3>Network Encryption/Decryption</h3><p>DebugSwift supports automatic decryption of encrypted API responses, making it easier to debug apps with end-to-end encryption.</p><pre><code>// Enable decryption feature\nDebugSwift.Network.shared.setDecryptionEnabled(true)\n\n// Register decryption key for specific API endpoints\nif let key = \"your-32-byte-aes-key-here-123456\".data(using: .utf8) {\n    DebugSwift.Network.shared.registerDecryptionKey(for: \"api.example.com\", key: key)\n}\n\n// Register custom decryptor for complex encryption schemes\nDebugSwift.Network.shared.registerCustomDecryptor(for: \"api.myapp.com\") { encryptedData in\n    // Your custom decryption logic here\n    return customDecrypt(encryptedData)\n}\n</code></pre><pre><code>debugSwift.setup(\n    hideFeatures: [.performance, .interface], // Hide specific tabs\n    disable: [.leaksDetector, .console]       // Disable specific monitoring\n)\n</code></pre><pre><code>// Enable beta features (disabled by default)\ndebugSwift.setup(\n    enableBetaFeatures: [.swiftUIRenderTracking] // Enable experimental SwiftUI render tracking\n)\n</code></pre><pre><code>// Configure app groups for file browser access\nDebugSwift.Resources.shared.configureAppGroups([\n    \"group.com.yourcompany.yourapp\"\n])\n</code></pre><pre><code>// Configure memory leak detection\nDebugSwift.Performance.shared.onLeakDetected { leakData in\n    print(\"üî¥ Memory leak detected: \\(leakData.message)\")\n}\n</code></pre><h3>Push Notification Simulation</h3><pre><code>// Enable push notification simulation\nDebugSwift.PushNotification.enableSimulation()\n\n// Simulate a notification\nDebugSwift.PushNotification.simulate(\n    title: \"Test Notification\",\n    body: \"This is a test notification\"\n)\n</code></pre><h3>SwiftUI Render Tracking (Beta)</h3><p>‚ö†Ô∏è : SwiftUI render tracking is experimental and must be enabled explicitly.</p><pre><code>// First enable the beta feature in setup\ndebugSwift.setup(enableBetaFeatures: [.swiftUIRenderTracking])\n\n// Then enable SwiftUI render tracking\nDebugSwift.SwiftUIRender.shared.isEnabled = true\n\n// Configure persistent overlays (stay visible until manually cleared)\nDebugSwift.SwiftUIRender.shared.persistentOverlays = true\n\n// Set overlay style (border, borderWithCount, none)\nDebugSwift.SwiftUIRender.shared.overlayStyle = .borderWithCount\n\n// Configure overlay duration\nDebugSwift.SwiftUIRender.shared.overlayDuration = 1.0\n\n// Enable console logging\nDebugSwift.SwiftUIRender.shared.loggingEnabled = true\n\n// Clear render statistics\nDebugSwift.SwiftUIRender.shared.clearStats()\n\n// Clear persistent overlays\nDebugSwift.SwiftUIRender.shared.clearPersistentOverlays()\n</code></pre><p>If you find DebugSwift helpful, please consider giving us a star on GitHub! Your support helps us continue improving and adding new features.</p><p>Our contributors have made this project possible. Thank you!</p><a href=\"https://github.com/DebugSwift/DebugSwift/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=DebugSwift/DebugSwift\"></a><p>Contributions are welcome! If you have suggestions, improvements, or bug fixes, please submit a pull request. Let's make DebugSwift even more powerful together!</p><p>DebugSwift is licensed under the MIT License - see the <a href=\"https://raw.githubusercontent.com/DebugSwift/DebugSwift/main/LICENSE\">LICENSE</a> file for details.</p>",
      "contentLength": 11104,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "TelegramMessenger/MTProxy",
      "url": "https://github.com/TelegramMessenger/MTProxy",
      "date": 1771038090,
      "author": "",
      "guid": 44931,
      "unread": true,
      "content": "<p>Install dependencies, you would need common set of tools for building from source, and development packages for  and .</p><pre><code>apt install git curl build-essential libssl-dev zlib1g-dev\n</code></pre><pre><code>yum install openssl-devel zlib-devel\nyum groupinstall \"Development Tools\"\n</code></pre><pre><code>git clone https://github.com/TelegramMessenger/MTProxy\ncd MTProxy\n</code></pre><p>To build, simply run , the binary will be in :</p><p>If the build has failed, you should run  before building it again.</p><ol><li>Obtain a secret, used to connect to telegram servers.</li></ol><pre><code>curl -s https://core.telegram.org/getProxySecret -o proxy-secret\n</code></pre><ol start=\"2\"><li>Obtain current telegram configuration. It can change (occasionally), so we encourage you to update it once per day.</li></ol><pre><code>curl -s https://core.telegram.org/getProxyConfig -o proxy-multi.conf\n</code></pre><ol start=\"3\"><li>Generate a secret to be used by users to connect to your proxy.</li></ol><pre><code>head -c 16 /dev/urandom | xxd -ps\n</code></pre><pre><code>./mtproto-proxy -u nobody -p 8888 -H 443 -S &lt;secret&gt; --aes-pwd proxy-secret proxy-multi.conf -M 1\n</code></pre><ul><li> is the username.  calls  to drop privileges.</li><li> is the port, used by clients to connect to the proxy.</li><li> is the local port. You can use it to get statistics from . Like <code>wget localhost:8888/stats</code>. You can only get this stat via loopback.</li><li> is the secret generated at step 3. Also you can set multiple secrets: <code>-S &lt;secret1&gt; -S &lt;secret2&gt;</code>.</li><li> and  are obtained at steps 1 and 2.</li><li> is the number of workers. You can increase the number of workers, if you have a powerful server.</li></ul><p>Also feel free to check out other options using .</p><ol start=\"5\"><li>Generate the link with following schema: <code>tg://proxy?server=SERVER_NAME&amp;port=PORT&amp;secret=SECRET</code> (or let the official bot generate it for you).</li><li>Set received tag with arguments: </li></ol><p>Due to some ISPs detecting MTProxy by packet sizes, random padding is added to packets if such mode is enabled.</p><p>It's only enabled for clients which request it.</p><p>Add  prefix to secret ( =&gt; ) to enable this mode on client side.</p><h2>Systemd example configuration</h2><ol><li>Create systemd service file (it's standard path for the most Linux distros, but you should check it before):</li></ol><pre><code>nano /etc/systemd/system/MTProxy.service\n</code></pre><ol start=\"2\"><li>Edit this basic service (especially paths and params):</li></ol><pre><code>[Unit]\nDescription=MTProxy\nAfter=network.target\n\n[Service]\nType=simple\nWorkingDirectory=/opt/MTProxy\nExecStart=/opt/MTProxy/mtproto-proxy -u nobody -p 8888 -H 443 -S &lt;secret&gt; -P &lt;proxy tag&gt; &lt;other params&gt;\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n</code></pre><ol start=\"4\"><li>Test fresh MTProxy service:</li></ol><pre><code>systemctl restart MTProxy.service\n# Check status, it should be active\nsystemctl status MTProxy.service\n</code></pre><ol start=\"5\"><li>Enable it, to autostart service after reboot:</li></ol><pre><code>systemctl enable MTProxy.service\n</code></pre>",
      "contentLength": 2534,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "google-deepmind/superhuman",
      "url": "https://github.com/google-deepmind/superhuman",
      "date": 1771038090,
      "author": "",
      "guid": 44932,
      "unread": true,
      "content": "<p>This repository hosts projects and datasets created by Google DeepMind's Superhuman Reasoning team, led by Thang Luong.</p><p>A suite of advanced benchmarks designed to evaluate robust mathematical reasoning in AI. Following our <a href=\"https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/\">2025 IMO-gold achievement</a>, this release includes:</p><ul><li><p>: 400 challenging short-answer problems.</p></li><li><p>: 60 proof-based problems vetted by experts.</p></li><li><p>: A dataset of 1000 human gradings to advance automatic evaluation.</p></li></ul><p>A math research agent, powered by Gemini Deep Think, that can iteratively generate, verify, and revise solutions. See <a href=\"https://raw.githubusercontent.com/google-deepmind/superhuman/main/aletheia/Aletheia.pdf\">paper</a>.</p><p>This release includes prompts and outputs from Aletheia on research level math problems.</p><p>Copyright 2025 Google LLC</p><p>All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license. You may obtain a copy of the Apache 2.0 license at: <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">https://www.apache.org/licenses/LICENSE-2.0</a></p><p>Unless required by applicable law or agreed to in writing, all software and materials distributed here under the Apache 2.0 or CC-BY licenses are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the licenses for the specific language governing permissions and limitations under those licenses.</p><p>This is not an official Google product.</p>",
      "contentLength": 1300,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "THUDM/slime",
      "url": "https://github.com/THUDM/slime",
      "date": 1771038090,
      "author": "",
      "guid": 44933,
      "unread": true,
      "content": "<p>slime is an LLM post-training framework for RL Scaling.</p><p> is an LLM post-training framework for RL scaling, providing two core capabilities:</p><ol><li><strong>High-Performance Training</strong>: Supports efficient training in various modes by connecting Megatron with SGLang;</li><li>: Enables arbitrary training data generation workflows through custom data generation interfaces and server-based engines.</li></ol><p>slime is the RL-framework behind <a href=\"https://z.ai/blog/glm-4.7\">GLM-4.7</a>, <a href=\"https://z.ai/blog/glm-4.6\">GLM-4.6</a>, <a href=\"https://z.ai/blog/glm-4.5\">GLM-4.5</a> and apart from models from Z.ai, we also supports the following models:</p><ul><li>Qwen3 series (Qwen3Next, Qwen3MoE, Qwen3), Qwen2.5 series;</li><li>DeepSeek V3 series (DeepSeek V3, V3.1, DeepSeek R1);</li></ul><ul><li>: Responsible for the main training process, reads data from the Data Buffer, and synchronizes parameters to the rollout module after training.</li><li><strong>rollout (SGLang + router)</strong>: Generates new data (including rewards/verifier outputs) and stores it in the Data Buffer.</li><li>: A bridge module that manages prompt initialization, custom data, and rollout generation methods.</li></ul><p>For a comprehensive quick start guide covering environment setup, data preparation, training startup, and key code analysis, please refer to:</p><p>We also provide examples for some use cases not covered in the quick start guide; please check <a href=\"https://raw.githubusercontent.com/THUDM/slime/main/examples/\">examples</a>.</p><h2>Projects Built upon slime</h2><p>slime has powered several novel research projects and production systems. Here are some notable examples:</p><h3>‚öõÔ∏è P1: Mastering Physics Olympiads with Reinforcement Learning</h3><p><a href=\"https://prime-rl.github.io/P1/\"></a> is a family of open-source physics reasoning models trained entirely through reinforcement learning. P1 leverages slime as the RL post training framework, and introduces a multi-stage RL training algorithm that progressively enhances reasoning ability through adaptive learnability adjustment and stabilization mechanisms. Enpowered by this training paradigm, P1 delivers breakthrough performance in open-source physics reasoning.</p><h3>üìàRLVE: Scaling LM RL with Adaptive Verifiable Environments</h3><p><a href=\"https://github.com/Zhiyuan-Zeng/RLVE\"></a> introduces an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). With joint training across 400 verifiable environments, RLVE enables each environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses.</p><h3>‚ö° TritonForge: Agentic RL Training Framework for Kernel Generation</h3><p><a href=\"https://github.com/RLsys-Foundation/TritonForge\"></a> leverages slime's SFT &amp; RL capabilities to train LLMs that automatically generate optimized GPU kernels. By using a two-stage training approach‚Äîsupervised fine-tuning followed by reinforcement learning with multi-turn compilation feedback‚ÄîTritonForge achieves remarkable results in converting PyTorch operations into high-performance Triton kernels.</p><h3>üöÄ APRIL: Accelerating RL Training with Active Partial Rollouts</h3><p><a href=\"https://github.com/RLsys-Foundation/APRIL\"></a> introduces a system-level optimization that seamlessly integrates with slime to accelerate the rollout generation phase in RL training. By intelligently over-provisioning requests and actively managing partial completions, APRIL addresses the long-tail generation bottleneck that typically consumes over 90% of RL training time.</p><h3>üèüÔ∏è qqr: Scaling Open-Ended Agents with ArenaRL &amp; MCP</h3><p><a href=\"https://github.com/Alibaba-NLP/qqr\"></a> (a.k.a. hilichurl) is a lightweight extension for slime designed to evolve open-ended agents. It implements the  algorithm to tackle discriminative collapse through tournament-based relative ranking (<strong>e.g., Seeded Single-Elimination, Round-Robin</strong>) and seamlessly integrates the <strong>Model Context Protocol (MCP)</strong>. qqr leverages slime's high-throughput training capabilities to enable scalable, distributed evolution of agents in standardized, decoupled tool environments.</p><p>These projects showcase slime's versatility‚Äîfrom training code-generation models to optimizing RL training systems‚Äîmaking it a powerful foundation for both research and production deployments.</p><p>Arguments in slime are divided into three categories:</p><ol><li>: slime reads all arguments in Megatron. You can configure Megatron by passing arguments like <code>--tensor-model-parallel-size 2</code>.</li><li>: All arguments for the installed SGLang are supported. These arguments must be prefixed with . For example,  should be passed as <code>--sglang-mem-fraction-static</code>.</li></ol><ul><li><p><strong>Contributions are welcome!</strong> If you have suggestions for new features, performance tuning, or feedback on user experience, feel free to submit an Issue or PR üòä</p></li><li><p>Use <a href=\"https://pre-commit.com/\">pre-commit</a> to ensure code style consistency for your commits:</p></li></ul><pre><code>apt install pre-commit -y\npre-commit install\n\n# run pre-commit to ensure code style consistency\npre-commit run --all-files --show-diff-on-failure --color=always\n</code></pre><ul><li>For frequently asked questions, please see the <a href=\"https://raw.githubusercontent.com/THUDM/slime/main/docs/en/get_started/qa.md\">Q&amp;A</a></li><li>Special thanks to the following projects &amp; communities: SGLang, Megatron‚ÄëLM, mbridge, OpenRLHF, veRL, Pai-Megatron-Patch and others.</li><li>To quote slime, please use:</li></ul><pre><code>@misc{slime_github,\n  author       = {Zilin Zhu and Chengxing Xie and Xin Lv and slime Contributors},\n  title        = {slime: An LLM post-training framework for RL Scaling},\n  year         = {2025},\n  howpublished = {\\url{https://github.com/THUDM/slime}},\n  note         = {GitHub repository. Corresponding author: Xin Lv},\n  urldate      = {2025-06-19}\n}\n</code></pre>",
      "contentLength": 5102,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "SynkraAI/aios-core",
      "url": "https://github.com/SynkraAI/aios-core",
      "date": 1771038090,
      "author": "",
      "guid": 44934,
      "unread": true,
      "content": "<p>Synkra AIOS: AI-Orchestrated System for Full Stack Development - Core Framework v4.0</p><p>Framework de Desenvolvimento Auto-Modific√°vel Alimentado por IA. Fundado em Desenvolvimento √Ågil Dirigido por Agentes, oferecendo capacidades revolucion√°rias para desenvolvimento dirigido por IA e muito mais. Transforme qualquer dom√≠nio com expertise especializada de IA: desenvolvimento de software, entretenimento, escrita criativa, estrat√©gia de neg√≥cios, bem-estar pessoal e muito mais.</p><p>Se √© sua primeira vez no AIOS, siga este caminho linear:</p><ol><li>Instale em um projeto novo ou existente:</li></ol><pre><code># novo projeto\nnpx aios-core init meu-projeto\n\n# projeto existente\ncd seu-projeto\nnpx aios-core install\n</code></pre><ol start=\"2\"><li>Escolha sua IDE/CLI e o caminho de ativa√ß√£o:</li></ol><ul><li>Gemini CLI:  ‚Üí </li><li>Codex CLI:  ‚Üí </li><li>Cursor/Copilot/AntiGravity: siga os limites e workarounds em </li></ul><ol start=\"3\"><li>Ative 1 agente e confirme o greeting.</li><li>Rode 1 comando inicial ( ou equivalente) para validar first-value.</li></ol><p>Defini√ß√£o de first-value (bin√°ria): ativa√ß√£o de agente + greeting v√°lido + comando inicial com output √∫til em &lt;= 10 minutos.</p><h2>Compatibilidade de Hooks por IDE (Realidade AIOS 4.2)</h2><p>Muitos recursos avan√ßados do AIOS dependem de eventos de ciclo de vida (hooks). A tabela abaixo mostra a paridade real entre IDEs/plataformas:</p><table><thead><tr><th>Paridade de Hooks vs Claude</th></tr></thead><tbody><tr><td>Automa√ß√£o m√°xima de contexto, guardrails e auditoria</td></tr><tr><td>Cobertura forte de automa√ß√µes pre/post tool e sess√£o</td></tr><tr><td>Parte das automa√ß√µes depende de , , MCP e fluxo operacional</td></tr><tr><td>Sem lifecycle hooks equivalentes</td><td>Menor automa√ß√£o de pre/post tool; foco em regras, MCP e fluxo do agente</td></tr><tr><td>Sem lifecycle hooks equivalentes</td><td>Menor automa√ß√£o de sess√£o/tooling; foco em instru√ß√µes de reposit√≥rio + MCP no VS Code</td></tr><tr><td>Workflow-based (n√£o hook-based)</td><td>Integra√ß√£o por workflows, n√£o por eventos de hook equivalentes ao Claude</td></tr></tbody></table><p>Impactos e mitiga√ß√£o detalhados: .</p><h2>Acknowledgments &amp; Attribution</h2><p>Synkra AIOS was originally derived from the <a href=\"https://github.com/bmad-code-org/BMAD-METHOD\">BMad Method</a>, created by <a href=\"https://github.com/bmadcode\">Brian Madison</a> (BMad Code, LLC). We gratefully acknowledge the BMad Method for providing the foundation from which this project began.</p><p> This project is <strong>NOT affiliated with, endorsed by, or sanctioned by</strong> the BMad Method or BMad Code, LLC. Contributors appearing in the git history from the original BMad Method repository do not imply active participation in or endorsement of Synkra AIOS.</p><p>Since its origin, AIOS has evolved significantly with its own architecture, terminology, and features (v4.x+), and does not depend on BMad for current operation. The BMad Method remains an excellent framework in its own right ‚Äî please visit the <a href=\"https://github.com/bmad-code-org/BMAD-METHOD\">official BMad Method repository</a> to learn more.</p><p>BMad, BMad Method, and BMad Core are trademarks of BMad Code, LLC. See <a href=\"https://github.com/bmad-code-org/BMAD-METHOD/raw/main/TRADEMARK.md\">TRADEMARK.md</a> for usage guidelines.</p><h3>Premissa Arquitetural: CLI First</h3><p>O Synkra AIOS segue uma hierarquia clara de prioridades:</p><pre><code>CLI First ‚Üí Observability Second ‚Üí UI Third\n</code></pre><table><thead><tr></tr></thead><tbody><tr><td>Onde a intelig√™ncia vive. Toda execu√ß√£o, decis√µes e automa√ß√£o acontecem aqui.</td><td>Agentes (, ), workflows, comandos</td></tr><tr><td>Observar e monitorar o que acontece no CLI em tempo real.</td><td>Dashboard SSE, logs, m√©tricas, timeline</td></tr><tr><td>Gest√£o pontual e visualiza√ß√µes quando necess√°rio.</td><td>Kanban, settings, story management</td></tr></tbody></table><ul><li>A CLI √© a fonte da verdade - dashboards apenas observam</li><li>Funcionalidades novas devem funcionar 100% via CLI antes de ter UI</li><li>A UI nunca deve ser requisito para opera√ß√£o do sistema</li><li>Observabilidade serve para entender o que o CLI est√° fazendo, n√£o para control√°-lo</li></ul><p><strong>As Duas Inova√ß√µes Chave do Synkra AIOS:</strong></p><p><strong>1. Planejamento Ag√™ntico:</strong> Agentes dedicados (analyst, pm, architect) colaboram com voc√™ para criar documentos de PRD e Arquitetura detalhados e consistentes. Atrav√©s de engenharia avan√ßada de prompts e refinamento com human-in-the-loop, estes agentes de planejamento produzem especifica√ß√µes abrangentes que v√£o muito al√©m da gera√ß√£o gen√©rica de tarefas de IA.</p><p><strong>2. Desenvolvimento Contextualizado por Engenharia:</strong> O agente sm (Scrum Master) ent√£o transforma estes planos detalhados em hist√≥rias de desenvolvimento hiperdetalhadas que cont√™m tudo que o agente dev precisa - contexto completo, detalhes de implementa√ß√£o e orienta√ß√£o arquitetural incorporada diretamente nos arquivos de hist√≥rias.</p><p>Esta abordagem de duas fases elimina tanto a <strong>inconsist√™ncia de planejamento</strong> quanto a  - os maiores problemas no desenvolvimento assistido por IA. Seu agente dev abre um arquivo de hist√≥ria com compreens√£o completa do que construir, como construir e por qu√™.</p><ul><li>Node.js &gt;=18.0.0 (v20+ recomendado)</li><li>GitHub CLI (opcional, necess√°rio para colabora√ß√£o em equipe)</li></ul><p><strong>Guias espec√≠ficos por plataforma:</strong></p><h3>Entendendo o Fluxo de Trabalho AIOS</h3><p><strong>Antes de mergulhar, revise estes diagramas cr√≠ticos de fluxo de trabalho que explicam como o AIOS funciona:</strong></p><blockquote><p>‚ö†Ô∏è <strong>Estes diagramas explicam 90% da confus√£o sobre o fluxo Synkra AIOS Agentic Agile</strong> - Entender a cria√ß√£o de PRD+Arquitetura e o fluxo de trabalho sm/dev/qa e como os agentes passam notas atrav√©s de arquivos de hist√≥rias √© essencial - e tamb√©m explica por que isto N√ÉO √© taskmaster ou apenas um simples executor de tarefas!</p></blockquote><h3>O que voc√™ gostaria de fazer?</h3><h2>Importante: Mantenha Sua Instala√ß√£o AIOS Atualizada</h2><p><strong>Mantenha-se atualizado sem esfor√ßo!</strong> Para atualizar sua instala√ß√£o AIOS existente:</p><pre><code>npx aios-core@latest install\n</code></pre><ul><li>‚úÖ Detectar automaticamente sua instala√ß√£o existente</li><li>‚úÖ Atualizar apenas os arquivos que mudaram</li><li>‚úÖ Criar arquivos de backup  para quaisquer modifica√ß√µes customizadas</li><li>‚úÖ Preservar suas configura√ß√µes espec√≠ficas do projeto</li></ul><p>Isto facilita beneficiar-se das √∫ltimas melhorias, corre√ß√µes de bugs e novos agentes sem perder suas customiza√ß√µes!</p><h3>üöÄ Instala√ß√£o via NPX (Recomendado)</h3><p><strong>Instale o Synkra AIOS com um √∫nico comando:</strong></p><pre><code># Criar um novo projeto com assistente interativo moderno\nnpx aios-core init meu-projeto\n\n# Ou instalar em projeto existente\ncd seu-projeto\nnpx aios-core install\n\n# Ou usar uma vers√£o espec√≠fica\nnpx aios-core@latest init meu-projeto\n</code></pre><h3>‚ú® Assistente de Instala√ß√£o Moderno</h3><p>O Synkra AIOS agora inclui uma experi√™ncia de instala√ß√£o interativa de √∫ltima gera√ß√£o, inspirada em ferramentas modernas como Vite e Next.js:</p><p><strong>Recursos do Instalador Interativo:</strong></p><ul><li>üé® : Prompts coloridos e visuais com @clack/prompts</li><li>‚úÖ : Feedback instant√¢neo sobre entradas inv√°lidas</li><li>üîÑ : Spinners para opera√ß√µes longas (c√≥pia de arquivos, instala√ß√£o de deps)</li><li>üì¶ : Escolha quais componentes instalar com interface intuitiva</li><li>‚öôÔ∏è <strong>Escolha de Gerenciador de Pacotes</strong>: Selecione entre npm, yarn ou pnpm</li><li>‚å®Ô∏è : Ctrl+C ou ESC para sair graciosamente a qualquer momento</li><li>üìä : Visualize todas as configura√ß√µes antes de prosseguir</li><li>‚è±Ô∏è : Veja quanto tempo levou a instala√ß√£o</li></ul><ul><li>‚úÖ Download da vers√£o mais recente do NPM</li><li>‚úÖ Assistente de instala√ß√£o interativo moderno</li><li>‚úÖ Configura√ß√£o autom√°tica do IDE (Codex CLI, Cursor ou Claude Code)</li><li>‚úÖ Configura√ß√£o de todos os agentes e fluxos de trabalho AIOS</li><li>‚úÖ Cria√ß√£o dos arquivos de configura√ß√£o necess√°rios</li><li>‚úÖ Inicializa√ß√£o do sistema de meta-agentes</li><li>‚úÖ Verifica√ß√µes de sa√∫de do sistema</li><li>‚úÖ : Testado em Windows, macOS e Linux</li></ul><blockquote><p> Sem clonar, sem configura√ß√£o manual - apenas um comando e voc√™ est√° pronto para come√ßar com uma experi√™ncia de instala√ß√£o moderna e profissional.</p></blockquote><h3>Atualizando uma Instala√ß√£o Existente</h3><p>Se voc√™ j√° tem o AIOS instalado:</p><pre><code>npx aios-core@latest install\n# O instalador detectar√° sua instala√ß√£o existente e a atualizar√°\n</code></pre><h3>Configure Seu IDE para Desenvolvimento AIOS</h3><p>O Synkra AIOS inclui regras pr√©-configuradas para IDE para melhorar sua experi√™ncia de desenvolvimento:</p><ol><li>Abra as configura√ß√µes do Cursor</li><li>Copie o conte√∫do de </li><li>Cole na se√ß√£o de regras e salve</li></ol><ul><li>‚úÖ J√° configurado! O arquivo  √© carregado automaticamente</li><li>Sync dedicado de agentes: </li><li>Validacao dedicada: <code>npm run validate:claude-sync &amp;&amp; npm run validate:claude-integration</code></li></ul><ul><li>‚úÖ Integra√ß√£o de primeira classe no AIOS 4.2 (pipeline de ativa√ß√£o e greeting compartilhado)</li><li>‚úÖ J√° configurado! O arquivo  na raiz √© carregado automaticamente</li><li>Opcional: sincronize agentes auxiliares com </li><li>Recomendado neste reposit√≥rio: gerar e versionar skills locais com <code>npm run sync:skills:codex</code></li><li>Use <code>npm run sync:skills:codex:global</code> apenas fora deste projeto (para evitar duplicidade no )</li><li>Validacao dedicada: <code>npm run validate:codex-sync &amp;&amp; npm run validate:codex-integration</code></li><li>Guardrails de skills/paths: <code>npm run validate:codex-skills &amp;&amp; npm run validate:paths</code></li></ul><ul><li>‚úÖ Regras e agentes sincronizaveis com </li><li>Arquivos gerados em , <code>.gemini/rules/AIOS/agents/</code> e </li><li>‚úÖ Hooks e settings locais no fluxo de instalacao ( + )</li><li>‚úÖ Ativacao rapida por slash commands (, , , etc.)</li><li>Validacao dedicada: <code>npm run validate:gemini-sync &amp;&amp; npm run validate:gemini-integration</code></li><li>Paridade multi-IDE em um comando: </li></ul><ul><li>ü§ñ Reconhecimento e integra√ß√£o de comandos de agentes</li><li>üìã Fluxo de trabalho de desenvolvimento dirigido por hist√≥rias</li><li>‚úÖ Rastreamento autom√°tico de checkboxes</li><li>üß™ Padr√µes de teste e valida√ß√£o</li><li>üìù Padr√µes de c√≥digo espec√≠ficos do AIOS</li></ul><h3>In√≠cio Mais R√°pido com Interface Web (2 minutos)</h3><ol><li>: Execute <code>npx aios-core init meu-projeto</code></li><li>: Siga as instru√ß√µes de configura√ß√£o para Codex CLI, Cursor ou Claude Code</li><li>: Ative um agente como  para come√ßar a criar seu briefing</li><li>: Digite  para ver comandos dispon√≠veis</li></ol><h3>Refer√™ncia de Comandos CLI</h3><p>O Synkra AIOS oferece uma CLI moderna e cross-platform com comandos intuitivos:</p><pre><code># Gerenciamento de Projeto (com assistente interativo)\nnpx aios-core init &lt;nome-projeto&gt; [op√ß√µes]\n  --force              For√ßar cria√ß√£o em diret√≥rio n√£o vazio\n  --skip-install       Pular instala√ß√£o de depend√™ncias npm\n  --template &lt;nome&gt;    Usar template espec√≠fico (default, minimal, enterprise)\n\n# Instala√ß√£o e Configura√ß√£o (com prompts modernos)\nnpx aios-core install [op√ß√µes]\n  --force              Sobrescrever configura√ß√£o existente\n  --quiet              Sa√≠da m√≠nima durante instala√ß√£o\n  --dry-run            Simular instala√ß√£o sem modificar arquivos\n\n# Comandos do Sistema\nnpx aios-core --version   Exibir vers√£o instalada\nnpx aios-core --help      Exibir ajuda detalhada\nnpx aios-core info        Exibir informa√ß√µes do sistema\nnpx aios-core doctor      Executar diagn√≥sticos do sistema\nnpx aios-core doctor --fix Corrigir problemas detectados automaticamente\n\n# Manuten√ß√£o\nnpx aios-core update      Atualizar para vers√£o mais recente\nnpx aios-core uninstall   Remover Synkra AIOS\n</code></pre><ul><li>‚úÖ :  em qualquer comando mostra documenta√ß√£o detalhada</li><li>‚úÖ : Feedback imediato sobre par√¢metros inv√°lidos</li><li>‚úÖ : Erros em vermelho, sucessos em verde, avisos em amarelo</li><li>‚úÖ : Funciona perfeitamente em Windows, macOS e Linux</li><li>‚úÖ : Teste instala√ß√µes sem modificar arquivos</li></ul><h4>Instala√ß√£o Interativa Completa</h4><pre><code>$ npx aios-core install\n\nüöÄ Synkra AIOS Installation\n\n‚óÜ What is your project name?\n‚îÇ  my-awesome-project\n‚îÇ\n‚óá Which directory should we use?\n‚îÇ  ./my-awesome-project\n‚îÇ\n‚óÜ Choose components to install:\n‚îÇ  ‚óè Core Framework (Required)\n‚îÇ  ‚óè Agent System (Required)\n‚îÇ  ‚óè Squads (optional)\n‚îÇ  ‚óã Example Projects (optional)\n‚îÇ\n‚óá Select package manager:\n‚îÇ  ‚óè npm\n‚îÇ  ‚óã yarn\n‚îÇ  ‚óã pnpm\n‚îÇ\n‚óÜ Initialize Git repository?\n‚îÇ  Yes\n‚îÇ\n‚óÜ Install dependencies?\n‚îÇ  Yes\n‚îÇ\n‚ñ∏ Creating project directory...\n‚ñ∏ Copying framework files...\n‚ñ∏ Initializing Git repository...\n‚ñ∏ Installing dependencies (this may take a minute)...\n‚ñ∏ Configuring environment...\n‚ñ∏ Running post-installation setup...\n\n‚úî Installation completed successfully! (34.2s)\n\nNext steps:\n  cd my-awesome-project\n  aios-core doctor     # Verify installation\n  aios-core --help     # See available commands\n</code></pre><h4>Instala√ß√£o Silenciosa (CI/CD)</h4><pre><code># Instala√ß√£o automatizada sem prompts\n$ npx aios-core install --quiet --force\n‚úî Synkra AIOS installed successfully\n</code></pre><h4>Simula√ß√£o de Instala√ß√£o (Dry-Run)</h4><pre><code># Testar instala√ß√£o sem modificar arquivos\n$ npx aios-core install --dry-run\n\n[DRY RUN] Would create: ./my-project/\n[DRY RUN] Would copy: .aios-core/ (45 files)\n[DRY RUN] Would initialize: Git repository\n[DRY RUN] Would install: npm dependencies\n‚úî Dry run completed - no files were modified\n</code></pre><pre><code>$ npx aios-core doctor\n\nüè• AIOS System Diagnostics\n\n‚úî Node.js version: v20.10.0 (meets requirement: &gt;=18.0.0)\n‚úî npm version: 10.2.3\n‚úî Git installed: version 2.43.0\n‚úî GitHub CLI: gh 2.40.1\n‚úî Synkra AIOS: v4.2.11\n\nConfiguration:\n‚úî .aios-core/ directory exists\n‚úî Agent files: 11 found\n‚úî Workflow files: 8 found\n‚úî Templates: 15 found\n\nDependencies:\n‚úî @clack/prompts: ^0.7.0\n‚úî commander: ^12.0.0\n‚úî execa: ^9.0.0\n‚úî fs-extra: ^11.0.0\n‚úî picocolors: ^1.0.0\n\n‚úÖ All checks passed! Your installation is healthy.\n</code></pre><pre><code>$ npx aios-core --help\n\nUsage: aios-core [options] [command]\n\nSynkra AIOS: AI-Orchestrated System for Full Stack Development\n\nOptions:\n  -V, --version                output the version number\n  -h, --help                   display help for command\n\nCommands:\n  init &lt;project-name&gt;          Create new AIOS project with interactive wizard\n  install [options]            Install AIOS in current directory\n  info                         Display system information\n  doctor [options]             Run system diagnostics and health checks\n  help [command]               display help for command\n\nRun 'aios-core &lt;command&gt; --help' for detailed information about each command.\n</code></pre><h3>Alternativa: Clonar e Construir</h3><p>Para contribuidores ou usu√°rios avan√ßados que queiram modificar o c√≥digo fonte:</p><pre><code># Clonar o reposit√≥rio\ngit clone https://github.com/SynkraAI/aios-core.git\ncd aios-core\n\n# Instalar depend√™ncias\nnpm install\n\n# Executar o instalador\nnpm run install:aios\n</code></pre><h3>Configura√ß√£o R√°pida para Equipe</h3><p>Para membros da equipe ingressando no projeto:</p><pre><code># Instalar AIOS no projeto\nnpx aios-core@latest install\n\n# Isto vai:\n# 1. Detectar instala√ß√£o existente (se houver)\n# 2. Instalar/atualizar framework AIOS\n# 3. Configurar agentes e workflows\n</code></pre><h2>üåü Al√©m do Desenvolvimento de Software - Squads</h2><p>O framework de linguagem natural do AIOS funciona em QUALQUER dom√≠nio. Os Squads fornecem agentes IA especializados para escrita criativa, estrat√©gia de neg√≥cios, sa√∫de e bem-estar, educa√ß√£o e muito mais. Al√©m disso, os Squads podem expandir o n√∫cleo do Synkra AIOS com funcionalidade espec√≠fica que n√£o √© gen√©rica para todos os casos. <a href=\"https://raw.githubusercontent.com/SynkraAI/aios-core/main/docs/guides/squads-guide.md\">Veja o Guia de Squads</a> e aprenda a criar os seus pr√≥prios!</p><p>O Synkra AIOS vem com 11 agentes especializados:</p><ul><li> - Agente mestre de orquestra√ß√£o (inclui capacidades de desenvolvimento de framework)</li><li> - Orquestrador de fluxo de trabalho e coordena√ß√£o de equipe</li></ul><h3>Agentes de Planejamento (Interface Web)</h3><ul><li> - Especialista em an√°lise de neg√≥cios e cria√ß√£o de PRD</li><li> (Product Manager) - Gerente de produto e prioriza√ß√£o</li><li> - Arquiteto de sistema e design t√©cnico</li><li> - Design de experi√™ncia do usu√°rio e usabilidade</li></ul><h3>Agentes de Desenvolvimento (IDE)</h3><ul><li> (Scrum Master) - Gerenciamento de sprint e cria√ß√£o de hist√≥rias</li><li> - Desenvolvedor e implementa√ß√£o</li><li> - Garantia de qualidade e testes</li><li> (Product Owner) - Gerenciamento de backlog e hist√≥rias</li></ul><h2>ü§ñ AIOS Autonomous Development Engine (ADE)</h2><p>O Synkra AIOS introduz o <strong>Autonomous Development Engine (ADE)</strong> - um sistema completo para desenvolvimento aut√¥nomo que transforma requisitos em c√≥digo funcional.</p><p>O ADE √© um conjunto de  que habilitam execu√ß√£o aut√¥noma de desenvolvimento:</p><table><tbody><tr><td>Isolamento de branches via Git worktrees</td></tr><tr><td>Migra√ß√£o para formato autoClaude V3</td></tr><tr><td>Transforma requisitos em specs execut√°veis</td></tr><tr><td>Executa specs com 13 steps + self-critique</td></tr><tr><td>Recupera√ß√£o autom√°tica de falhas</td></tr><tr><td>Review estruturado em 10 fases</td></tr><tr><td>Mem√≥ria persistente de padr√µes e insights</td></tr></tbody></table><pre><code>User Request ‚Üí Spec Pipeline ‚Üí Execution Engine ‚Üí QA Review ‚Üí Working Code\n                                      ‚Üì\n                              Recovery System\n                                      ‚Üì\n                               Memory Layer\n</code></pre><pre><code># 1. Criar spec a partir de requisito\n@pm *gather-requirements\n@architect *assess-complexity\n@analyst *research-deps\n@pm *write-spec\n@qa *critique-spec\n\n# 2. Executar spec aprovada\n@architect *create-plan\n@architect *create-context\n@dev *execute-subtask 1.1\n\n# 3. QA Review\n@qa *review-build STORY-42\n</code></pre><h3>üÜï Novos Comandos por Agente</h3><ul><li>, , , </li><li>, , , </li></ul><ul><li>, </li></ul><ul><li>, , , </li></ul><ul><li>, </li></ul><ul><li>, , , </li></ul><ul><li>, , , , , </li></ul><h2>Criando Seu Pr√≥prio Squad</h2><p>Squads permitem estender o AIOS para qualquer dom√≠nio. Estrutura b√°sica:</p><pre><code>squads/seu-squad/\n‚îú‚îÄ‚îÄ config.yaml           # Configura√ß√£o do squad\n‚îú‚îÄ‚îÄ agents/              # Agentes especializados\n‚îú‚îÄ‚îÄ tasks/               # Fluxos de trabalho de tarefas\n‚îú‚îÄ‚îÄ templates/           # Templates de documentos\n‚îú‚îÄ‚îÄ checklists/          # Checklists de valida√ß√£o\n‚îú‚îÄ‚îÄ data/                # Base de conhecimento\n‚îú‚îÄ‚îÄ README.md            # Documenta√ß√£o do squad\n‚îî‚îÄ‚îÄ user-guide.md        # Guia do usu√°rio\n</code></pre><ul><li> - Opera√ß√µes h√≠bridas humano-agente (reposit√≥rio separado)</li></ul><p>O  () √© o m√≥dulo premium do Synkra AIOS, oferecendo funcionalidades avan√ßadas para equipes e projetos de maior escala.</p><pre><code>npm install @aios-fullstack/pro\n</code></pre><ul><li> - Squads especializados com capacidades expandidas</li><li> - Mem√≥ria persistente de padr√µes e insights entre sess√µes</li><li> - Dashboard de produtividade e m√©tricas de desenvolvimento</li><li> - Conectores para Jira, Linear, Notion e mais</li><li> - Sistema de configura√ß√£o L1-L4 com heran√ßa</li><li> - Gerenciamento de licen√ßa via <code>aios pro activate --key &lt;KEY&gt;</code></li></ul><p>Para mais informa√ß√µes, execute  ap√≥s a instala√ß√£o.</p><p>O Synkra AIOS implementa um sistema de valida√ß√£o de m√∫ltiplas camadas para garantir qualidade do c√≥digo e consist√™ncia:</p><h3>üõ°Ô∏è Defense in Depth - 3 Camadas de Valida√ß√£o</h3><p><strong>Camada 1: Pre-commit (Local - R√°pida)</strong></p><ul><li>‚úÖ ESLint - Qualidade de c√≥digo</li><li>‚úÖ TypeScript - Verifica√ß√£o de tipos</li></ul><p><strong>Camada 2: Pre-push (Local - Valida√ß√£o de Stories)</strong></p><ul><li>‚úÖ Valida√ß√£o de checkboxes de hist√≥rias</li></ul><p><strong>Camada 3: CI/CD (Cloud - Obrigat√≥rio para merge)</strong></p><ul><li>‚úÖ Cobertura de testes (80% m√≠nimo)</li></ul><pre><code># Valida√ß√µes locais\nnpm run lint           # ESLint\nnpm run typecheck      # TypeScript\nnpm test              # Testes\nnpm run test:coverage # Testes com cobertura\n\n# Validador AIOS\nnode .aios-core/utils/aios-validator.js pre-commit   # Valida√ß√£o pre-commit\nnode .aios-core/utils/aios-validator.js pre-push     # Valida√ß√£o pre-push\nnode .aios-core/utils/aios-validator.js stories      # Validar todas stories\n</code></pre><p>Configure prote√ß√£o da branch master com:</p><pre><code>node scripts/setup-branch-protection.js\n</code></pre><ul><li>GitHub CLI (gh) instalado e autenticado</li><li>Acesso de admin ao reposit√≥rio</li></ul><p><strong>Estamos empolgados com contribui√ß√µes e acolhemos suas ideias, melhorias e Squads!</strong> üéâ</p><ol><li>Crie uma branch para sua feature (<code>git checkout -b feature/MinhaNovaFeature</code>)</li><li>Commit suas mudan√ßas (<code>git commit -m 'feat: Adicionar nova feature'</code>)</li><li>Push para a branch (<code>git push origin feature/MinhaNovaFeature</code>)</li></ol><p>This project was originally derived from the <a href=\"https://github.com/bmad-code-org/BMAD-METHOD\">BMad Method</a> by <a href=\"https://github.com/bmadcode\">Brian Madison</a>. We thank Brian and all BMad Method contributors for the original work that made this project possible.</p><p> Some contributors shown in the GitHub contributors graph are inherited from the original BMad Method git history and do not represent active participation in or endorsement of Synkra AIOS.</p><p><sub>Constru√≠do com ‚ù§Ô∏è para a comunidade de desenvolvimento assistido por IA</sub></p>",
      "contentLength": 18960,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "iOfficeAI/AionUi",
      "url": "https://github.com/iOfficeAI/AionUi",
      "date": 1770952083,
      "author": "",
      "guid": 44597,
      "unread": true,
      "content": "<p>Free, local, open-source 24/7 Cowork and OpenClaw for Gemini CLI, Claude Code, Codex, OpenCode, Qwen Code, Goose CLI, Auggie, and more | üåü Star if you like it!</p><p align=\"center\"><strong>üöÄ Cowork with Your AI, Gemini CLI, Claude Code, Codex, Qwen Code, Goose CLI, OpenClaw, Auggie, and more</strong><em>User-friendly | Visual graphical interface | Multi-model support | Local data security</em></p><h3>ü§ñ <strong>Multi-Agent Mode - Cowork for Your Command-Line AI Tools, Unified Graphical Interface</strong></h3><p>AionUi provides a unified graphical interface for your command-line AI tools. Built-in Gemini CLI included, no setup required.</p><p> Gemini CLI (built-in) ‚Ä¢ Claude Code ‚Ä¢ CodeX ‚Ä¢ Qwen Code ‚Ä¢ Goose AI ‚Ä¢ OpenClaw ‚Ä¢ Augment Code</p><ul><li>‚úÖ  - Automatically recognizes and integrates local CLI tools</li><li>‚úÖ  - One interface for all your AI tools, no more command line</li><li>‚úÖ <strong>Local Storage + Multi-Session</strong> - Conversations saved locally, multiple parallel sessions with independent context</li></ul><h3>üåê <strong>Access Your AionUi Anywhere</strong></h3><p><em>Your 7√ó24 hour AI assistant - Access AionUi from any device, anywhere! On business trips, at home, in the office, use your AI tools anytime, anywhere through WebUI or various chat platforms</em></p><p>AionUi provides multiple remote access methods:</p><ul><li><p>Access AionUi from any device via browser - phone, tablet, computer. Supports LAN, cross-network, and server deployment. You can log in by scanning a QR code or using account password, making it simple and convenient.</p></li><li><p><strong>üì± Chat Platform Integration</strong></p><ul><li> - Chat with your AI assistant directly from Telegram on any device. Simple pairing code system for secure access.</li><li> - Interact with your AI assistant through Feishu bots, supporting enterprise collaboration scenarios.</li><li> and more platforms coming soon üöß</li></ul><blockquote><p>üí°  Go to AionUi Settings ‚Üí WebUI Settings ‚Üí Channel, configure the corresponding Bot Token to get started!</p></blockquote></li></ul><h3>‚è∞ <strong>Scheduled Tasks - Let AionUi Automate Your Work</strong></h3><p><em>After setting up scheduled tasks, the AI assistant will automatically execute according to your set time, truly achieving 7√ó24 hours unattended operation</em></p><ul><li> - Tell AI what to do using natural language, just like chatting normally</li><li> - Daily, weekly, monthly are all possible</li><li> - Create, modify, enable/disable, delete, view and adjust anytime</li></ul><blockquote><p>üí°  Scheduled data aggregation, regular report generation, automatic file organization, scheduled reminders, etc.</p></blockquote><h3>üìÅ <strong>Smart File Management (AI Cowork)</strong></h3><p><em>Batch renaming, automatic organization, smart classification, file merging</em></p><ul><li>: Intelligently identify content and auto-classify, keeping folders tidy.</li><li>: One-click rename, merge files, say goodbye to tedious manual tasks.</li></ul><h3>üìÑ <strong>Preview Panel - Quickly View AI-Generated Results</strong></h3><p><em>Supports 9+ formats of visual preview (PDF, Word, Excel, PPT, code, Markdown, images, HTML, Diff, etc.)</em></p><ul><li>‚úÖ  - After AI generates files, view preview immediately without switching apps</li><li>‚úÖ <strong>Real-time Tracking + Editable</strong> - Automatically tracks file changes, editor and preview sync intelligently; supports real-time editing of Markdown, code, HTML, WYSIWYG</li></ul><h3>üé® <strong>AI Image Generation &amp; Editing</strong></h3><p><em>Intelligent image generation, editing, and recognition, powered by Gemini</em></p><p><em>Supports mainstream models like Gemini, OpenAI, Claude, Qwen, as well as local models like Ollama, LM Studio. AionUi also supports <a href=\"https://github.com/QuantumNous/new-api\">NewAPI</a> gateway service(a unified AI model hub that aggregates and distributes various LLMs). Flexibly switch between different models to meet various task requirements.</em></p><h3>üõ†Ô∏è <strong>AI Assistants &amp; Skills Ecosystem</strong></h3><p><em>Extensible assistant system with built-in specialized assistants and custom skill support</em></p><h3>üé® <strong>Personalized Interface Customization</strong></h3><p><em>Customize with your own CSS code, make your interface match your preferences</em></p><ul><li>‚úÖ  - Freely customize interface colors, styles, layout through CSS code, create your exclusive experience</li></ul><h3>üí¨ <strong>Multi-Task Parallel Processing</strong></h3><p><em>Open multiple conversations, tasks don't get mixed up, independent memory, double efficiency</em></p><p><strong>Just like Claude Cowork makes Claude Code easier to use, AionUi is the Cowork platform for all your command-line AI tools</strong></p><p>While command-line tools like Gemini CLI, Claude Code, Codex, Qwen Code are powerful, they share common pain points: conversations can't be saved, single-session limitations, cumbersome file operations, and only support a single model.</p><p>AionUi provides unified  for these command-line tools:</p><ul><li>üéØ  - One interface to manage all command-line AI tools, no switching needed; built-in Gemini CLI, ready to use out of the box and completely free</li><li>üöÄ  - Not only supports Claude Code, but also Gemini CLI, Codex, Qwen Code, and more</li><li>üñ•Ô∏è  - Full platform support for macOS, Windows, Linux (Claude Cowork currently only macOS)</li><li>üåê  - Your remote 24/7 assistant, access anytime, anywhere, and completely free</li><li>üîÑ  - Flexibly switch between different models in the same interface, meeting different task requirements</li><li>üìÑ  - Visual preview for 9+ formats, immediately view the effects of AI-generated files</li><li>üíæ  - All conversations and files saved locally, data never leaves your device</li></ul><h2>üé¨ See How People Use AionUi</h2><p><em>Watch how content creators review and use AionUi in real-world scenarios</em></p><p align=\"center\"><em>üé¨ WorldofAI (200K subscribers)</em><em>üé¨ Julian Goldie SEO (318K subscribers)</em></p><blockquote><p>üí° <strong>Have you made a video about AionUi?</strong><a href=\"https://x.com/AionUi\">Let us know on X</a> and we'll feature it here! We value your feedback and will continue to improve based on your suggestions.</p></blockquote><ul><li><strong>Multi-Session + Independent Context</strong> - Open multiple chats simultaneously, each session has independent context memory, no confusion</li><li> - All conversations are saved locally and will not be lost</li></ul><ul><li> - Supports mainstream models like Gemini, OpenAI, Claude, Qwen, flexible switching</li><li> - Supports local model deployment like Ollama, LM Studio, select Custom platform and set local API address (e.g., <code>http://localhost:11434/v1</code>) to connect</li><li><strong>Gemini 3 Subscription Optimization</strong> - Automatically identifies subscribed users, recommends advanced models</li></ul><ul><li><strong>File Tree Browsing + Drag &amp; Drop Upload</strong> - Browse files like folders, support drag and drop files or folders for one-click import</li><li> - You can let AI help organize folders, automatic classification</li></ul><h3>üìÑ <strong>Preview Panel - Give AI Agent a Display</strong></h3><ul><li> - Supports PDF, Word, Excel, PPT, code, Markdown, images, etc., view results immediately after AI generation</li><li><strong>Real-time Tracking + Editable</strong> - Automatically tracks file changes, supports real-time editing and debugging of Markdown, code, HTML</li></ul><h3>üé® <strong>AI Image Generation &amp; Editing</strong></h3><ul><li><strong>Intelligent Image Generation</strong> - Supports multiple image generation models like Gemini 2.5 Flash Image Preview, Nano, Banana</li><li><strong>Image Recognition &amp; Editing</strong> - AI-driven image analysis and editing features</li></ul><ul><li> - Access from any device on the network via browser, supports mobile devices</li><li> - All data stored locally in SQLite database, suitable for server deployment</li></ul><ul><li>: Windows 10 or higher</li><li>: Ubuntu 18.04+ / Debian 10+ / Fedora 32+</li><li>: Recommended 4GB or more</li><li>: At least 500MB available space</li></ul><h3>üç∫ Install via Homebrew (macOS)</h3><ol><li> AionUi application</li><li> - Support Google account login or API Key authentication</li><li> - Immediately experience modern AI chat interface</li></ol><p> We highly value every user's suggestions and feedback. Whether it's feature ideas, user experience, or issues you encounter, feel free to contact us anytime!</p><p>Welcome to submit Issues and Pull Requests!</p><ol><li>Create a feature branch (<code>git checkout -b feature/AmazingFeature</code>)</li><li>Commit your changes (<code>git commit -m 'Add some AmazingFeature'</code>)</li><li>Push to the branch (<code>git push origin feature/AmazingFeature</code>)</li></ol><p>Thanks to all developers who have contributed to AionUi!</p>",
      "contentLength": 7392,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    }
  ],
  "tags": []
}
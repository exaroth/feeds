{
  "id": "KWcVEiwEzuiDsTqLn6LojC9X2yBX45N36DRtYHsFby6oiM4t597YhKBSdxr5wqMdj8kvSgDuiUuV6yvogw",
  "title": "GitHub All Languages Daily Trending",
  "displayTitle": "Github Trending",
  "url": "https://mshibanami.github.io/GitHubTrendingRSS/daily/all.xml",
  "feedLink": "http://mshibanami.github.io/GitHubTrendingRSS",
  "isQuery": false,
  "isEmpty": false,
  "isHidden": false,
  "itemCount": 50,
  "items": [
    {
      "title": "PaddlePaddle/Paddle",
      "url": "https://github.com/PaddlePaddle/Paddle",
      "date": 1772334486,
      "author": "",
      "guid": 49280,
      "unread": true,
      "content": "<li><p><strong>Unified Dynamic/Static Graphs and Automatic Parallelism</strong></p><p>By requiring only minimal tensor partitioning annotations based on a single-card configuration, PaddlePaddle automatically discovers the most efficient distributed parallel strategy. This significantly reduces the costs of industrial development and training, enabling developers to focus more intently on model and algorithm innovation.</p></li><li><p><strong>Integrated Training and Inference for Large Models</strong></p><p>The same framework supports both training and inference, achieving code reuse and seamless integration between these stages. This provides a unified development experience and maximum training efficiency for the entire large model workflow, offering the industry a superior development experience.</p></li><li><p><strong>High-Order Differentiation for Scientific Computing</strong></p><p>Provides capabilities such as high-order automatic differentiation, complex number operations, Fourier transforms, compilation optimization, and distributed training support. It facilitates scientific exploration in fields including mathematics, mechanics, materials science, meteorology, and biology, substantially improving the speed of solving differential equations.</p></li><li><p>Adopting an integrated framework design, it supports efficient training and flexible inference for diverse models, including generative and scientific computing models. It achieves an effective balance between computational flexibility and high performance, significantly lowering performance optimization costs.</p></li><li><p><strong>Heterogeneous Multi-Chip Adaptation</strong> Features a mature and complete unified adaptation solution for multiple hardware types. Through standardized interfaces, it abstracts the variations in development interfaces across different chip software stacks, realizing a pluggable architecture.</p></li>",
      "contentLength": 1757,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/a8540ddc6d809889b59554dc520151c04d3ec8960923d3c45c3d200129679a6d/PaddlePaddle/Paddle",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "datagouv/datagouv-mcp",
      "url": "https://github.com/datagouv/datagouv-mcp",
      "date": 1772334486,
      "author": "",
      "guid": 49281,
      "unread": true,
      "content": "<p>Official data.gouv.fr Model Context Protocol (MCP) server that allows AI chatbots to search, explore, and analyze datasets from the French national Open Data platform, directly through conversation.</p><img width=\"1200\" height=\"675\" alt=\"image\" src=\"https://github.com/user-attachments/assets/5d20e992-349a-4b3b-9a0a-ebe308735cc9\"><p>Model Context Protocol (MCP) server that allows AI chatbots (Claude, ChatGPT, Gemini, etc.) to search, explore, and analyze datasets from <a href=\"https://www.data.gouv.fr\">data.gouv.fr</a>, the French national Open Data platform, directly through conversation.</p><p>Instead of manually browsing the website, you can simply ask questions like \"Quels jeux de donn√©es sont disponibles sur les prix de l'immobilier ?\" or \"Montre-moi les derni√®res donn√©es de population pour Paris\" and get instant answers.</p><h2>üåê Connect your chatbot to the MCP server</h2><p>Use the hosted endpoint <code>https://mcp.data.gouv.fr/mcp</code> (recommended). If you self-host, swap in your own URL.</p><p>The MCP server configuration depends on your client. Use the appropriate configuration format for your client:</p><ol><li><p>Locate the <code>anythingllm_mcp_servers.json</code> file in your AnythingLLM storage plugins directory:</p><ul><li>: <code>~/Library/Application Support/anythingllm-desktop/storage/plugins/anythingllm_mcp_servers.json</code></li><li>: <code>~/.config/anythingllm-desktop/storage/plugins/anythingllm_mcp_servers.json</code></li><li>: <code>C:\\Users\\&lt;username&gt;\\AppData\\Roaming\\anythingllm-desktop\\storage\\plugins\\anythingllm_mcp_servers.json</code></li></ul></li><li><p>Add the following configuration:</p></li></ol><pre><code>{\n  \"mcpServers\": {\n    \"datagouv\": {\n      \"type\": \"streamable\",\n      \"url\": \"https://mcp.data.gouv.fr/mcp\"\n    }\n  }\n}\n</code></pre><p><em>Available for paid plans only (Plus, Pro, Team, and Enterprise).</em></p><ol><li>: Open ChatGPT in your browser, go to , then .</li><li>: Open  and enable .</li><li>: Return to  &gt;  &gt;  and click .</li><li>: Set the URL to <code>https://mcp.data.gouv.fr/mcp</code> and save to activate the tools.</li></ol><p>Use the  command to add the MCP server:</p><pre><code>claude mcp add --transport http datagouv https://mcp.data.gouv.fr/mcp\n</code></pre><p>Add the following to your Claude Desktop configuration file (typically <code>~/Library/Application Support/Claude/claude_desktop_config.json</code> on MacOS, or <code>%APPDATA%\\Claude\\claude_desktop_config.json</code> on Windows):</p><pre><code>{\n  \"mcpServers\": {\n    \"datagouv\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mcp.data.gouv.fr/mcp\"\n      ]\n    }\n  }\n}\n</code></pre><p>Cursor supports MCP servers through its settings. To configure the server:</p><ol><li>Search for \"MCP\" or \"Model Context Protocol\"</li><li>Add a new MCP server with the following configuration:</li></ol><pre><code>{\n  \"mcpServers\": {\n    \"datagouv\": {\n      \"url\": \"https://mcp.data.gouv.fr/mcp\",\n      \"transport\": \"http\"\n    }\n  }\n}\n</code></pre><p>Add the following to your  file:</p><pre><code>{\n  \"mcpServers\": {\n    \"datagouv\": {\n      \"httpUrl\": \"https://mcp.data.gouv.fr/mcp\"\n    }\n  }\n}\n</code></pre><p>Edit your Vibe config (default ) and add the MCP server:</p><pre><code>[[mcp_servers]]\nname = \"datagouv\"\ntransport = \"streamable-http\"\nurl = \"https://mcp.data.gouv.fr/mcp\"\n</code></pre><p>Add the following to your Kiro MCP configuration file ( in your workspace, or <code>~/.kiro/settings/mcp.json</code> for global config):</p><pre><code>{\n  \"mcpServers\": {\n    \"datagouv\": {\n      \"url\": \"https://mcp.data.gouv.fr/mcp\"\n    }\n  }\n}\n</code></pre><p>Add the following to <code>~/.kiro/settings/mcp.json</code>:</p><pre><code>{\n  \"mcpServers\": {\n    \"datagouv\": {\n      \"url\": \"https://mcp.data.gouv.fr/mcp\"\n    }\n  }\n}\n</code></pre><p>IBM Bob supports MCP servers through its settings. To configure the server:</p><ol><li>Click the setting icon in the Bob panel.</li><li>Click the appropriate button:</li></ol><ul><li>Edit Global MCP: Opens the global  file</li><li>Edit Project MCP: Opens the project-specific  file (Bob creates it if it does not exist)</li></ul><p>Both files use JSON format with an mcpServers object containing named server configurations.</p><pre><code>{\n  \"mcpServers\": {\n    \"datagouv\": {\n      \"url\": \"https://mcp.data.gouv.fr/mcp\",\n      \"type\": \"streamable-http\"\n    }\n  }\n}\n</code></pre><p>Add the following to your VS Code :</p><pre><code>{\n  \"servers\": {\n    \"datagouv\": {\n      \"url\": \"https://mcp.data.gouv.fr/mcp\",\n      \"type\": \"http\"\n    }\n  }\n}\n</code></pre><p>Add the following to your <code>~/.codeium/mcp_config.json</code>:</p><pre><code>{\n  \"mcpServers\": {\n    \"datagouv\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-remote\",\n        \"https://mcp.data.gouv.fr/mcp\"\n      ]\n    }\n  }\n}\n</code></pre><ul><li>The hosted endpoint is <code>https://mcp.data.gouv.fr/mcp</code>. If you run the server yourself, replace it with your own URL (see ‚ÄúRun locally‚Äù below for the default local endpoint).</li><li>This MCP server only exposes read-only tools for now, so no API key is required.</li></ul><p>Before starting, clone this repository and browse into it:</p><pre><code>git clone git@github.com:datagouv/datagouv-mcp.git\ncd datagouv-mcp\n</code></pre><p>Docker is required for the recommended setup. Install it via <a href=\"https://www.docker.com/products/docker-desktop/\">Docker Desktop</a> or any compatible Docker Engine before continuing.</p><h4>üê≥ With Docker (Recommended)</h4><pre><code># With default settings (port 8000, prod environment)\ndocker compose up -d\n\n# With custom environment variables\nMCP_PORT=8007 DATAGOUV_ENV=demo docker compose up -d\n\n# Stop\ndocker compose down\n</code></pre><ul><li>: host to bind to (defaults to ). Set to  for local development to follow MCP security best practices.</li><li>: port for the MCP HTTP server (defaults to  when unset).</li><li>:  (default) or . This controls which data.gouv.fr environement it uses the data from (<a href=\"https://www.data.gouv.fr\">https://www.data.gouv.fr</a> or <a href=\"https://demo.data.gouv.fr\">https://demo.data.gouv.fr</a>). By default the MCP server talks to the production data.gouv.fr. Set  if you specifically need the demo environment.</li></ul><p>You will need <a href=\"https://github.com/astral-sh/uv\">uv</a> to install dependencies and run the server.</p><ol start=\"2\"><li><strong>Prepare the environment file</strong></li></ol><p>Then optionally edit  and set the variables that matter for your run:</p><pre><code>MCP_HOST=127.0.0.1  # (defaults to 0.0.0.0, use 127.0.0.1 for local dev)\nMCP_PORT=8007  # (defaults to 8000 when unset)\nDATAGOUV_ENV=prod  # Allowed values: demo | prod (defaults to prod when unset)\n</code></pre><p>Load the variables with your preferred method, e.g.:</p><pre><code>set -a &amp;&amp; source .env &amp;&amp; set +a\n</code></pre><ol start=\"3\"><li><strong>Start the HTTP MCP server</strong></li></ol><h3>2. Connect your chatbot to the local MCP server</h3><p><strong>STDIO and SSE are not supported</strong>.</p><p><strong>Streamable HTTP transport (standards-compliant):</strong></p><ul><li> - JSON-RPC messages (client ‚Üí server)</li><li> - Simple JSON health probe (<code>{\"status\":\"ok\",\"timestamp\":\"...\"}</code>)</li></ul><p>The MCP server provides tools to interact with data.gouv.fr datasets and dataservices.</p><p> \"Dataservices\" are external third-party APIs (e.g., Adresse API, Sirene API) registered in the data.gouv.fr catalog. They are distinct from data.gouv.fr's own internal APIs (Main/Tabular/Metrics) which power this MCP server.</p><h3>Datasets (static data files)</h3><ul><li><p> - Search for datasets by keywords. Returns datasets with metadata (title, description, organization, tags, resource count).</p><p>Parameters:  (required),  (optional, default: 1),  (optional, default: 20, max: 100)</p></li><li><p> - Get detailed information about a specific dataset (metadata, organization, tags, dates, license, etc.).</p><p>Parameters:  (required)</p></li><li><p> - List all resources (files) in a dataset with their metadata (format, size, type, URL).</p><p>Parameters:  (required)</p></li><li><p> - Get detailed information about a specific resource (format, size, MIME type, URL, dataset association, Tabular API availability).</p><p>Parameters:  (required)</p></li><li><p> - Query data from a specific resource via the Tabular API. Fetches rows from a resource to answer questions.</p><p>Parameters:  (required),  (required),  (optional, default: 1),  (optional, default: 20, max: 200)</p><p>Note: Recommended workflow: 1) Use  to find the dataset, 2) Use  to see available resources, 3) Use  with default  (20) to preview data structure. For small datasets (&lt;500 rows), increase  or paginate. For large datasets (&gt;1000 rows), use <code>download_and_parse_resource</code> instead. Works for CSV/XLS resources within Tabular API size limits (CSV ‚â§ 100 MB, XLSX ‚â§ 12.5 MB).</p></li><li><p><strong><code>download_and_parse_resource</code></strong> - Download and parse a resource that is not accessible via Tabular API (files too large, formats not supported, external URLs).</p><p>Parameters:  (required),  (optional, default: 20),  (optional, default: 500)</p><p>Supported formats: CSV, CSV.GZ, JSON, JSONL. Useful for files exceeding Tabular API limits or formats not supported by Tabular API. Start with default max_rows (20) to preview, then call again with higher max_rows if you need all data.</p></li></ul><h3>Dataservices (external APIs)</h3><ul><li><p> - Search for dataservices (APIs) registered on data.gouv.fr by keywords. Returns dataservices with metadata (title, description, organization, base API URL, tags).</p><p>Parameters:  (required),  (optional, default: 1),  (optional, default: 20, max: 100)</p></li><li><p> - Get detailed metadata about a specific dataservice (title, description, organization, base API URL, OpenAPI spec URL, license, dates, related datasets).</p><p>Parameters:  (required)</p></li><li><p><strong><code>get_dataservice_openapi_spec</code></strong> - Fetch and summarize the OpenAPI/Swagger specification for a dataservice. Returns a concise overview of available endpoints with their parameters.</p><p>Parameters:  (required)</p><p>Note: Recommended workflow: 1) Use  to find the API, 2) Use  to get its metadata and documentation URL, 3) Use <code>get_dataservice_openapi_spec</code> to understand available endpoints and parameters, 4) Call the API using the  per the spec.</p></li></ul><ul><li><p> - Get metrics (visits, downloads) for a dataset and/or a resource.</p><p>Parameters:  (optional),  (optional),  (optional, default: 12, max: 100)</p><p>Returns monthly statistics including visits and downloads, sorted by month in descending order (most recent first). At least one of  or  must be provided.  This tool only works with the production environment (). The Metrics API does not have a demo/preprod environment.</p></li></ul><h3>‚úÖ Automated Tests with pytest</h3><p>Run the tests with pytest (these cover helper modules; the MCP server wiring is best exercised via the MCP Inspector):</p><pre><code># Run all tests\nuv run pytest\n\n# Run with verbose output\nuv run pytest -v\n\n# Run specific test file\nuv run pytest tests/test_tabular_api.py\n\n# Run with custom resource ID\nRESOURCE_ID=3b6b2281-b9d9-4959-ae9d-c2c166dff118 uv run pytest tests/test_tabular_api.py\n\n# Run with prod environment\nDATAGOUV_ENV=prod uv run pytest\n</code></pre><h3>üîç Interactive Testing with MCP Inspector</h3><p>Use the official <a href=\"https://modelcontextprotocol.io/docs/tools/inspector\">MCP Inspector</a> to interactively test the server tools and resources.</p><ol><li>Start the MCP server (see above)</li><li>In another terminal, launch the inspector: <pre><code>npx @modelcontextprotocol/inspector --http-url \"http://127.0.0.1:${MCP_PORT}/mcp\"\n</code></pre> Adjust the URL if you exposed the server on another host/port.</li></ol><p>We welcome contributions! To keep the project stable and reviews manageable, please observe these rules before submitting:</p><ul><li> We strictly follow a  workflow.</li><li> Do not submit raw AI-generated code. All code must be reviewed and tested by a human prior to submission.</li></ul><p>We use a standard review-and-deploy process:</p><ol><li> Propose your changes via a Pull Request against the  branch.</li><li> All PRs must be reviewed and approved by a maintainer before merging.</li></ol><h3>üßπ Code Linting and Formatting</h3><p>This project follows PEP 8 style guidelines using <a href=\"https://astral.sh/ruff/\">Ruff</a> for linting and formatting, and <a href=\"https://docs.astral.sh/ty/\">ty</a> for type checking.</p><pre><code># Lint (including import sorting) and format code\nuv run ruff check --fix &amp;&amp; uv run ruff format\n\n# Type check (ty)\nuv run ty check\n</code></pre><p>This repository uses a <a href=\"https://pre-commit.com/\">pre-commit</a> hook which lint and format code before each commit. Installing the pre-commit hook is strongly recommended so the checks run automatically.</p><p><strong>Install pre-commit hooks:</strong></p><pre><code>uv run pre-commit install\n</code></pre><p>The pre-commit hook that automatically:</p><ul><li>Remove trailing whitespace</li><li>Run Ruff linting and formatting</li></ul><h3>üè∑Ô∏è Releases and versioning</h3><p>The release process uses the <a href=\"https://raw.githubusercontent.com/datagouv/datagouv-mcp/main/tag_version.sh\"></a> script to create git tags, GitHub releases and update <a href=\"https://raw.githubusercontent.com/datagouv/datagouv-mcp/main/CHANGELOG.md\">CHANGELOG.md</a> automatically. Package version numbers are automatically derived from git tags using <a href=\"https://github.com/pypa/setuptools_scm\">setuptools_scm</a>, so no manual version updates are needed in .</p><p>: <a href=\"https://cli.github.com/\">GitHub CLI</a> must be installed and authenticated, and you must be on the main branch with a clean working directory.</p><pre><code># Create a new release\n./tag_version.sh &lt;version&gt;\n\n# Example\n./tag_version.sh 2.5.0\n\n# Dry run to see what would happen\n./tag_version.sh 2.5.0 --dry-run\n</code></pre><p>The script automatically:</p><ul><li>Extracts commits since the last tag and formats them for CHANGELOG.md</li><li>Identifies breaking changes (commits with  in the subject)</li><li>Creates a git tag and pushes it to the remote repository</li><li>Creates a GitHub release with the changelog content</li></ul><p>This project is licensed under the MIT License - see the <a href=\"https://raw.githubusercontent.com/datagouv/datagouv-mcp/main/LICENSE\">LICENSE</a> file for details.</p>",
      "contentLength": 11792,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/e0ad152ea4052666029c3fef544b236db6d54a81e97b46b8b0239eaf1f6c2e6b/datagouv/datagouv-mcp",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Wei-Shaw/sub2api",
      "url": "https://github.com/Wei-Shaw/sub2api",
      "date": 1772334486,
      "author": "",
      "guid": 49282,
      "unread": true,
      "content": "<p>Sub2API-CRS2 ‰∏ÄÁ´ôÂºèÂºÄÊ∫ê‰∏≠ËΩ¨ÊúçÂä°ÔºåËÆ© Claude„ÄÅOpenai „ÄÅGemini„ÄÅAntigravityËÆ¢ÈòÖÁªü‰∏ÄÊé•ÂÖ•ÔºåÊîØÊåÅÊãºËΩ¶ÂÖ±‰∫´ÔºåÊõ¥È´òÊïàÂàÜÊëäÊàêÊú¨ÔºåÂéüÁîüÂ∑•ÂÖ∑Êó†Áºù‰ΩøÁî®„ÄÇ</p><p>Demo credentials (shared demo environment;  created automatically for self-hosted installs):</p><p>Sub2API is an AI API gateway platform designed to distribute and manage API quotas from AI product subscriptions (like Claude Code $200/month). Users can access upstream AI services through platform-generated API Keys, while the platform handles authentication, billing, load balancing, and request forwarding.</p><ul><li> - Support multiple upstream account types (OAuth, API Key)</li><li> - Generate and manage API Keys for users</li><li> - Token-level usage tracking and cost calculation</li><li> - Intelligent account selection with sticky sessions</li><li> - Per-user and per-account concurrency limits</li><li> - Configurable request and token rate limits</li><li> - Web interface for monitoring and management</li></ul><table><tbody><tr></tr><tr><td>Vue 3.4+, Vite 5+, TailwindCSS</td></tr></tbody></table><ul><li>Dependency Security: <code>docs/dependency-security.md</code></li></ul><h3>Method 1: Script Installation (Recommended)</h3><p>One-click installation script that downloads pre-built binaries from GitHub Releases.</p><ul><li>Linux server (amd64 or arm64)</li><li>PostgreSQL 15+ (installed and running)</li><li>Redis 7+ (installed and running)</li></ul><pre><code>curl -sSL https://raw.githubusercontent.com/Wei-Shaw/sub2api/main/deploy/install.sh | sudo bash\n</code></pre><ol><li>Detect your system architecture</li><li>Download the latest release</li><li>Install binary to </li><li>Configure system user and permissions</li></ol><pre><code># 1. Start the service\nsudo systemctl start sub2api\n\n# 2. Enable auto-start on boot\nsudo systemctl enable sub2api\n\n# 3. Open Setup Wizard in browser\n# http://YOUR_SERVER_IP:8080\n</code></pre><p>The Setup Wizard will guide you through:</p><ul></ul><p>You can upgrade directly from the  by clicking the  button in the top-left corner.</p><ul><li>Check for new versions automatically</li><li>Download and apply updates with one click</li><li>Support rollback if needed</li></ul><pre><code># Check status\nsudo systemctl status sub2api\n\n# View logs\nsudo journalctl -u sub2api -f\n\n# Restart service\nsudo systemctl restart sub2api\n\n# Uninstall\ncurl -sSL https://raw.githubusercontent.com/Wei-Shaw/sub2api/main/deploy/install.sh | sudo bash -s -- uninstall -y\n</code></pre><h3>Method 2: Docker Compose (Recommended)</h3><p>Deploy with Docker Compose, including PostgreSQL and Redis containers.</p><ul></ul><h4>Quick Start (One-Click Deployment)</h4><p>Use the automated deployment script for easy setup:</p><pre><code># Create deployment directory\nmkdir -p sub2api-deploy &amp;&amp; cd sub2api-deploy\n\n# Download and run deployment preparation script\ncurl -sSL https://raw.githubusercontent.com/Wei-Shaw/sub2api/main/deploy/docker-deploy.sh | bash\n\n# Start services\ndocker-compose -f docker-compose.local.yml up -d\n\n# View logs\ndocker-compose -f docker-compose.local.yml logs -f sub2api\n</code></pre><ul><li>Downloads  and </li><li>Generates secure credentials (JWT_SECRET, TOTP_ENCRYPTION_KEY, POSTGRES_PASSWORD)</li><li>Creates  file with auto-generated secrets</li><li>Creates data directories (uses local directories for easy backup/migration)</li><li>Displays generated credentials for your reference</li></ul><p>If you prefer manual setup:</p><pre><code># 1. Clone the repository\ngit clone https://github.com/Wei-Shaw/sub2api.git\ncd sub2api/deploy\n\n# 2. Copy environment configuration\ncp .env.example .env\n\n# 3. Edit configuration (generate secure passwords)\nnano .env\n</code></pre><p><strong>Required configuration in :</strong></p><pre><code># PostgreSQL password (REQUIRED)\nPOSTGRES_PASSWORD=your_secure_password_here\n\n# JWT Secret (RECOMMENDED - keeps users logged in after restart)\nJWT_SECRET=your_jwt_secret_here\n\n# TOTP Encryption Key (RECOMMENDED - preserves 2FA after restart)\nTOTP_ENCRYPTION_KEY=your_totp_key_here\n\n# Optional: Admin account\nADMIN_EMAIL=admin@example.com\nADMIN_PASSWORD=your_admin_password\n\n# Optional: Custom port\nSERVER_PORT=8080\n</code></pre><pre><code># Generate JWT_SECRET\nopenssl rand -hex 32\n\n# Generate TOTP_ENCRYPTION_KEY\nopenssl rand -hex 32\n\n# Generate POSTGRES_PASSWORD\nopenssl rand -hex 32\n</code></pre><pre><code># 4. Create data directories (for local version)\nmkdir -p data postgres_data redis_data\n\n# 5. Start all services\n# Option A: Local directory version (recommended - easy migration)\ndocker-compose -f docker-compose.local.yml up -d\n\n# Option B: Named volumes version (simple setup)\ndocker-compose up -d\n\n# 6. Check status\ndocker-compose -f docker-compose.local.yml ps\n\n# 7. View logs\ndocker-compose -f docker-compose.local.yml logs -f sub2api\n</code></pre><table><thead><tr></tr></thead><tbody><tr><td>‚úÖ Easy (tar entire directory)</td><td>Production, frequent backups</td></tr><tr><td>‚ö†Ô∏è Requires docker commands</td></tr></tbody></table><p> Use  (deployed by script) for easier data management.</p><p>Open <code>http://YOUR_SERVER_IP:8080</code> in your browser.</p><p>If admin password was auto-generated, find it in logs:</p><pre><code>docker-compose -f docker-compose.local.yml logs sub2api | grep \"admin password\"\n</code></pre><pre><code># Pull latest image and recreate container\ndocker-compose -f docker-compose.local.yml pull\ndocker-compose -f docker-compose.local.yml up -d\n</code></pre><h4>Easy Migration (Local Directory Version)</h4><p>When using , migrate to a new server easily:</p><pre><code># On source server\ndocker-compose -f docker-compose.local.yml down\ncd ..\ntar czf sub2api-complete.tar.gz sub2api-deploy/\n\n# Transfer to new server\nscp sub2api-complete.tar.gz user@new-server:/path/\n\n# On new server\ntar xzf sub2api-complete.tar.gz\ncd sub2api-deploy/\ndocker-compose -f docker-compose.local.yml up -d\n</code></pre><pre><code># Stop all services\ndocker-compose -f docker-compose.local.yml down\n\n# Restart\ndocker-compose -f docker-compose.local.yml restart\n\n# View all logs\ndocker-compose -f docker-compose.local.yml logs -f\n\n# Remove all data (caution!)\ndocker-compose -f docker-compose.local.yml down\nrm -rf data/ postgres_data/ redis_data/\n</code></pre><h3>Method 3: Build from Source</h3><p>Build and run from source code for development or customization.</p><ul></ul><pre><code># 1. Clone the repository\ngit clone https://github.com/Wei-Shaw/sub2api.git\ncd sub2api\n\n# 2. Install pnpm (if not already installed)\nnpm install -g pnpm\n\n# 3. Build frontend\ncd frontend\npnpm install\npnpm run build\n# Output will be in ../backend/internal/web/dist/\n\n# 4. Build backend with embedded frontend\ncd ../backend\ngo build -tags embed -o sub2api ./cmd/server\n\n# 5. Create configuration file\ncp ../deploy/config.example.yaml ./config.yaml\n\n# 6. Edit configuration\nnano config.yaml\n</code></pre><blockquote><p> The  flag embeds the frontend into the binary. Without this flag, the binary will not serve the frontend UI.</p></blockquote><p><strong>Key configuration in :</strong></p><pre><code>server:\n  host: \"0.0.0.0\"\n  port: 8080\n  mode: \"release\"\n\ndatabase:\n  host: \"localhost\"\n  port: 5432\n  user: \"postgres\"\n  password: \"your_password\"\n  dbname: \"sub2api\"\n\nredis:\n  host: \"localhost\"\n  port: 6379\n  password: \"\"\n\njwt:\n  secret: \"change-this-to-a-secure-random-string\"\n  expire_hour: 24\n\ndefault:\n  user_concurrency: 5\n  user_balance: 0\n  api_key_prefix: \"sk-\"\n  rate_multiplier: 1.0\n</code></pre><h3>Sora Status (Temporarily Unavailable)</h3><blockquote><p>‚ö†Ô∏è Sora-related features are temporarily unavailable due to technical issues in upstream integration and media delivery. Please do not rely on Sora in production at this time. Existing  configuration keys are reserved and may not take effect until these issues are resolved.</p></blockquote><p>Additional security-related options are available in :</p><ul><li> for CORS allowlist</li><li> for upstream/pricing/CRS host allowlists</li><li><code>security.url_allowlist.enabled</code> to disable URL validation (use with caution)</li><li><code>security.url_allowlist.allow_insecure_http</code> to allow HTTP URLs when validation is disabled</li><li><code>security.url_allowlist.allow_private_hosts</code> to allow private/local IP addresses</li><li><code>security.response_headers.enabled</code> to enable configurable response header filtering (disabled uses default allowlist)</li><li> to control Content-Security-Policy headers</li><li> to fail closed on billing errors</li><li> to enable X-Forwarded-For parsing</li><li> to require Turnstile in release mode</li></ul><p><strong>‚ö†Ô∏è Security Warning: HTTP URL Configuration</strong></p><p>When <code>security.url_allowlist.enabled=false</code>, the system performs minimal URL validation by default,  and only allowing HTTPS. To allow HTTP URLs (e.g., for development or internal testing), you must explicitly set:</p><pre><code>security:\n  url_allowlist:\n    enabled: false                # Disable allowlist checks\n    allow_insecure_http: true     # Allow HTTP URLs (‚ö†Ô∏è INSECURE)\n</code></pre><p><strong>Or via environment variable:</strong></p><pre><code>SECURITY_URL_ALLOWLIST_ENABLED=false\nSECURITY_URL_ALLOWLIST_ALLOW_INSECURE_HTTP=true\n</code></pre><ul><li>API keys and data transmitted in  (vulnerable to interception)</li><li>Susceptible to <strong>man-in-the-middle (MITM) attacks</strong></li><li><strong>NOT suitable for production</strong> environments</li></ul><ul><li>‚úÖ Internal networks with trusted endpoints</li><li>‚úÖ Testing account connectivity before obtaining HTTPS</li><li>‚ùå Production environments (use HTTPS only)</li></ul><p><strong>Example error without this setting:</strong></p><pre><code>Invalid base URL: invalid url scheme: http\n</code></pre><p>If you disable URL validation or response header filtering, harden your network layer:</p><ul><li>Enforce an egress allowlist for upstream domains/IPs</li><li>Block private/loopback/link-local ranges</li><li>Enforce TLS-only outbound traffic</li><li>Strip sensitive upstream response headers at the proxy</li></ul><pre><code># 6. Run the application\n./sub2api\n</code></pre><pre><code># Backend (with hot reload)\ncd backend\ngo run ./cmd/server\n\n# Frontend (with hot reload)\ncd frontend\npnpm run dev\n</code></pre><p>When editing , regenerate Ent + Wire:</p><pre><code>cd backend\ngo generate ./ent\ngo generate ./cmd/server\n</code></pre><p>Simple Mode is designed for individual developers or internal teams who want quick access without full SaaS features.</p><ul><li>Enable: Set environment variable </li><li>Difference: Hides SaaS-related features and skips billing process</li><li>Security note: In production, you must also set  to allow startup</li></ul><p>Sub2API supports <a href=\"https://antigravity.so/\">Antigravity</a> accounts. After authorization, dedicated endpoints are available for Claude and Gemini models.</p><table><tbody><tr></tr><tr></tr></tbody></table><h3>Claude Code Configuration</h3><pre><code>export ANTHROPIC_BASE_URL=\"http://localhost:8080/antigravity\"\nexport ANTHROPIC_AUTH_TOKEN=\"sk-xxx\"\n</code></pre><p>Antigravity accounts support optional . When enabled, the general endpoints  and  will also route requests to Antigravity accounts.</p><blockquote><p>: Anthropic Claude and Antigravity Claude <strong>cannot be mixed within the same conversation context</strong>. Use groups to isolate them properly.</p></blockquote><p>In Claude Code, Plan Mode cannot exit automatically. (Normally when using the native Claude API, after planning is complete, Claude Code will pop up options for users to approve or reject the plan.)</p><p>: Press  to manually exit Plan Mode, then type your response to approve or reject the plan.</p><pre><code>sub2api/\n‚îú‚îÄ‚îÄ backend/                  # Go backend service\n‚îÇ   ‚îú‚îÄ‚îÄ cmd/server/           # Application entry\n‚îÇ   ‚îú‚îÄ‚îÄ internal/             # Internal modules\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/           # Configuration\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model/            # Data models\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ service/          # Business logic\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ handler/          # HTTP handlers\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gateway/          # API gateway core\n‚îÇ   ‚îî‚îÄ‚îÄ resources/            # Static resources\n‚îÇ\n‚îú‚îÄ‚îÄ frontend/                 # Vue 3 frontend\n‚îÇ   ‚îî‚îÄ‚îÄ src/\n‚îÇ       ‚îú‚îÄ‚îÄ api/              # API calls\n‚îÇ       ‚îú‚îÄ‚îÄ stores/           # State management\n‚îÇ       ‚îú‚îÄ‚îÄ views/            # Page components\n‚îÇ       ‚îî‚îÄ‚îÄ components/       # Reusable components\n‚îÇ\n‚îî‚îÄ‚îÄ deploy/                   # Deployment files\n    ‚îú‚îÄ‚îÄ docker-compose.yml    # Docker Compose configuration\n    ‚îú‚îÄ‚îÄ .env.example          # Environment variables for Docker Compose\n    ‚îú‚îÄ‚îÄ config.example.yaml   # Full config file for binary deployment\n    ‚îî‚îÄ‚îÄ install.sh            # One-click installation script\n</code></pre><div align=\"center\"><p><strong>If you find this project useful, please give it a star!</strong></p></div>",
      "contentLength": 11119,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/f3b9519b7598c996bc58c84f1a9aa9a9aabb53d784f39b8b1a8abf40c97e2d4d/Wei-Shaw/sub2api",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "superset-sh/superset",
      "url": "https://github.com/superset-sh/superset",
      "date": 1772334486,
      "author": "",
      "guid": 49283,
      "unread": true,
      "content": "<p>IDE for the AI Agents Era - Run an army of Claude Code, Codex, etc. on your machine</p><p>Superset is a turbocharged terminal that allows you to run any CLI coding agents along with the tools to 10x your development workflow.</p><ul><li><strong>Run multiple agents simultaneously</strong> without context switching overhead</li><li> in its own git worktree so agents don't interfere with each other</li><li> from one place and get notified when they need attention</li><li> with built-in diff viewer and editor</li></ul><table><tbody><tr><td align=\"left\">Run 10+ coding agents simultaneously on your machine</td></tr><tr><td align=\"left\">Each task gets its own branch and working directory</td></tr><tr><td align=\"left\">Track agent status and get notified when changes are ready</td></tr><tr><td align=\"left\">Inspect and edit agent changes without leaving the app</td></tr><tr><td align=\"left\">Automate env setup, dependency installation, and more</td></tr><tr><td align=\"left\">Works with any CLI agent that runs in a terminal</td></tr><tr><td align=\"left\">Jump between tasks as they need your attention</td></tr><tr><td align=\"left\">Open any workspace in your favorite editor with one click</td></tr></tbody></table><p>Superset works with any CLI-based coding agent, including:</p><p>If it runs in a terminal, it runs on Superset</p><table><tbody><tr><td align=\"left\">macOS (Windows/Linux untested)</td></tr><tr></tr></tbody></table><p>All shortcuts are customizable via <strong>Settings &gt; Keyboard Shortcuts</strong> (). See <a href=\"https://docs.superset.sh/keyboard-shortcuts\">full documentation</a>.</p><table><tbody><tr></tr><tr></tr><tr></tr></tbody></table><table><tbody></tbody></table><table><tbody><tr><td align=\"left\">Toggle workspaces sidebar</td></tr></tbody></table><pre><code>{\n  \"setup\": [\"./.superset/setup.sh\"],\n  \"teardown\": [\"./.superset/teardown.sh\"]\n}\n</code></pre><table><tbody><tr><td align=\"left\">Commands to run when creating a workspace</td></tr><tr><td align=\"left\">Commands to run when deleting a workspace</td></tr></tbody></table><pre><code>#!/bin/bash\n# .superset/setup.sh\n\n# Copy environment variables\ncp ../.env .env\n\n# Install dependencies\nbun install\n\n# Run any other setup tasks\necho \"Workspace ready!\"\n</code></pre><p>Scripts have access to environment variables:</p><ul><li> ‚Äî Name of the workspace</li><li> ‚Äî Path to the main repository</li></ul><h2>Internal Dependency Overrides</h2><p>We welcome contributions! If you have a suggestion that would make Superset better:</p><ol><li>Create your feature branch (<code>git checkout -b feature/amazing-feature</code>)</li><li>Commit your changes (<code>git commit -m 'Add amazing feature'</code>)</li><li>Push to the branch (<code>git push origin feature/amazing-feature</code>)</li></ol><a href=\"https://github.com/superset-sh/superset/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=superset-sh/superset\"></a><p>Join the Superset community to get help, share feedback, and connect with other users:</p><p>Distributed under the Apache 2.0 License. See <a href=\"https://raw.githubusercontent.com/superset-sh/superset/main/LICENSE.md\">LICENSE.md</a> for more information.</p>",
      "contentLength": 2013,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/7e3300bade3bc4dc029fb42fcc1c3763a2328f72b6dd75dfaa1f067bb2d5b17b/superset-sh/superset",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "NousResearch/hermes-agent",
      "url": "https://github.com/NousResearch/hermes-agent",
      "date": 1772334486,
      "author": "",
      "guid": 49284,
      "unread": true,
      "content": "<p><strong>The fully open-source AI agent that grows with you.</strong> Install it on a machine, give it your messaging accounts, and it becomes a persistent personal agent ‚Äî learning your projects, building its own skills, running tasks on a schedule, and reaching you wherever you are. An autonomous agent that lives on your server, remembers what it learns, and gets more capable the longer it runs.</p><p>Use any model you want ‚Äî log in with a <a href=\"https://portal.nousresearch.com\">Nous Portal</a> subscription for zero-config access, connect an <a href=\"https://openrouter.ai\">OpenRouter</a> key for 200+ models, or point it at your own VLLM/SGLang endpoint. Switch with  ‚Äî no code changes, no lock-in.</p><table><tbody><tr><td><b>A real terminal interface</b></td><td>Not a web UI ‚Äî a full TUI with multiline editing, slash-command autocomplete, conversation history, interrupt-and-redirect, and streaming tool output. Built for people who live in the terminal and want an agent that keeps up.</td></tr><tr><td>Telegram, Discord, Slack, WhatsApp, and CLI ‚Äî all from a single gateway process. Send it a voice memo from your phone, get a researched answer with citations. Cross-platform message mirroring means a conversation started on Telegram can continue on Discord.</td></tr><tr><td>Persistent memory across sessions ‚Äî the agent remembers your preferences, your projects, your environment. When it solves a hard problem, it writes a skill document for next time. Skills are searchable, shareable, and compatible with the <a href=\"https://agentskills.io\">agentskills.io</a> open standard. A Skills Hub lets you install community skills or publish your own.</td></tr><tr><td>Built-in cron scheduler with delivery to any platform. Set up a daily AI funding report delivered to Telegram, a nightly backup verification on Discord, a weekly dependency audit that opens PRs, or a morning news briefing ‚Äî all in natural language. The gateway runs them unattended.</td></tr><tr><td><b>Delegates and parallelizes</b></td><td>Spawn isolated subagents for parallel workstreams ‚Äî each gets its own conversation and terminal. The agent can also write Python scripts that call its own tools via RPC, collapsing multi-step pipelines into a single turn with zero intermediate context cost.</td></tr><tr><td>Five terminal backends ‚Äî local, Docker, SSH, Singularity, and Modal ‚Äî with persistent workspaces, background process management, with the option to make these machines ephemeral. Run it against a remote machine so it can't modify its own code or read private API keys for added security.</td></tr><tr><td>Batch runner for generating thousands of tool-calling trajectories in parallel. Atropos RL environments for training models with reinforcement learning on agentic tasks. Trajectory compression for fitting training data into token budgets.</td></tr></tbody></table><pre><code>curl -fsSL https://raw.githubusercontent.com/NousResearch/hermes-agent/main/scripts/install.sh | bash\n</code></pre><pre><code>irm https://raw.githubusercontent.com/NousResearch/hermes-agent/main/scripts/install.ps1 | iex\n</code></pre><ul><li>Install <a href=\"https://docs.astral.sh/uv/\">uv</a> (fast Python package manager) if not present</li><li>Install Python 3.11 via uv if not already available (no sudo needed)</li><li>Clone to  (with submodules: mini-swe-agent, tinker-atropos)</li><li>Create a virtual environment with Python 3.11</li><li>Install all dependencies and submodule packages</li><li>Symlink  into  so it works globally (no venv activation needed)</li><li>Run the interactive setup wizard</li></ul><p>After installation, reload your shell and run:</p><pre><code>source ~/.bashrc   # or: source ~/.zshrc\nhermes setup       # Configure API keys (if you skipped during install)\nhermes             # Start chatting!\n</code></pre><p>The installer () walks you through selecting a provider and model. Once that's done:</p><pre><code>hermes          # Start chatting!\nhermes model    # Switch provider or model interactively\nhermes tools    # See all available tools\n</code></pre><p>This lets you switch between  (subscription),  (200+ models, pay-per-use), or a  (VLLM, SGLang, any OpenAI-compatible API) at any time.</p><h3>üîí Recommended: Run with a Sandboxed Terminal</h3><p>By default, Hermes runs commands directly on your machine ( backend). For safer use we recommend running with a <strong>sandboxed terminal backend</strong> so the agent <strong>cannot access its own code, config, or API keys</strong>:</p><pre><code># Option A: SSH into a separate machine (recommended for production)\nhermes config set terminal.backend ssh\nhermes config set TERMINAL_SSH_HOST my-server.example.com\nhermes config set TERMINAL_SSH_USER myuser\n\n# Option B: Docker container (good for local isolation)\nhermes config set terminal.backend docker\n\n# Option C: Modal cloud sandbox (serverless, no infra to manage)\nhermes config set terminal.backend modal\n</code></pre><p>All container/remote backends support  ‚Äî installed packages, files, and state survive across sessions. The agent gets a full working environment but can't read , modify its own source code, or access your host filesystem.</p><pre><code>hermes update    # Update to latest version (prompts for new config)\n</code></pre><pre><code>hermes uninstall          # Uninstall (can keep configs for later reinstall)\n</code></pre><pre><code>rm -f ~/.local/bin/hermes\nrm -rf /path/to/hermes-agent\nrm -rf ~/.hermes            # Optional ‚Äî keep if you plan to reinstall\n</code></pre><p>You need at least one way to connect to an LLM. Use  to switch providers and models interactively, or configure directly:</p><table><tbody><tr><td> (OAuth, subscription-based)</td></tr><tr><td> in </td></tr><tr><td> +  in </td></tr></tbody></table><p> Even when using Nous Portal or a custom endpoint, some tools (vision, web summarization, MoA) use OpenRouter independently. An  enables these tools.</p><p>All your settings are stored in  for easy access:</p><pre><code>~/.hermes/\n‚îú‚îÄ‚îÄ config.yaml     # Settings (model, terminal, TTS, compression, etc.)\n‚îú‚îÄ‚îÄ .env            # API keys and secrets\n‚îú‚îÄ‚îÄ auth.json       # OAuth provider credentials (Nous Portal, etc.)\n‚îú‚îÄ‚îÄ SOUL.md         # Optional: global persona (agent embodies this personality)\n‚îú‚îÄ‚îÄ memories/       # Persistent memory (MEMORY.md, USER.md)\n‚îú‚îÄ‚îÄ skills/         # Agent-created skills (managed via skill_manage tool)\n‚îú‚îÄ‚îÄ cron/           # Scheduled jobs\n‚îú‚îÄ‚îÄ sessions/       # Gateway sessions\n‚îî‚îÄ‚îÄ logs/           # Logs\n</code></pre><pre><code>hermes config              # View current configuration\nhermes config edit         # Open config.yaml in your editor\nhermes config set KEY VAL  # Set a specific value\nhermes config check        # Check for missing options (after updates)\nhermes config migrate      # Interactively add missing options\n\n# Examples:\nhermes config set model anthropic/claude-opus-4\nhermes config set terminal.backend docker\nhermes config set OPENROUTER_API_KEY sk-or-...  # Saves to .env\n</code></pre><p>Settings are resolved in this order (highest priority first):</p><ol><li> ‚Äî <code>hermes chat --max-turns 100</code> (per-invocation override)</li><li> ‚Äî the primary config file for all non-secret settings</li><li> ‚Äî fallback for env vars;  for secrets (API keys, tokens, passwords)</li><li> ‚Äî hardcoded safe defaults when nothing else is set</li></ol><p> Secrets (API keys, bot tokens, passwords) go in . Everything else (model, terminal backend, compression settings, memory limits, toolsets) goes in . When both are set,  wins for non-secret settings.</p><p>The  command automatically routes values to the right file ‚Äî API keys are saved to , everything else to .</p><table><thead><tr></tr></thead><tbody><tr><td>, </td></tr><tr></tr><tr><td>OpenAI TTS + voice transcription</td></tr><tr><td>Cross-session user modeling</td></tr></tbody></table><p>Chat with Hermes from Telegram, Discord, Slack, or WhatsApp. The gateway is a single background process that connects to all your configured platforms, handles sessions, runs cron jobs, and delivers voice messages.</p><pre><code>hermes gateway              # Run in foreground\nhermes gateway install      # Install as systemd service (Linux)\nhermes gateway start        # Start the systemd service\nhermes gateway stop         # Stop the systemd service\nhermes gateway status       # Check service status\n</code></pre><p>The installer will offer to set this up automatically if it detects a bot token.</p><ol><li> Message <a href=\"https://t.me/BotFather\">@BotFather</a> on Telegram, use </li><li> Message <a href=\"https://t.me/userinfobot\">@userinfobot</a> ‚Äî it replies with your numeric ID</li></ol><pre><code># Add to ~/.hermes/.env:\nTELEGRAM_BOT_TOKEN=123456:ABC-DEF...\nTELEGRAM_ALLOWED_USERS=YOUR_USER_ID    # Comma-separated for multiple users\n</code></pre><ol start=\"4\"></ol><ol><li> Bot ‚Üí Privileged Gateway Intents ‚Üí enable Message Content Intent</li><li> Enable Developer Mode in Discord settings, right-click your name ‚Üí Copy ID</li><li> OAuth2 ‚Üí URL Generator ‚Üí scopes: ,  ‚Üí permissions: Send Messages, Read Message History, Attach Files</li></ol><pre><code># Add to ~/.hermes/.env:\nDISCORD_BOT_TOKEN=MTIz...\nDISCORD_ALLOWED_USERS=YOUR_USER_ID\n</code></pre><ol><li> Go to <a href=\"https://api.slack.com/apps\">Slack API</a>, create a new app</li><li> In app settings ‚Üí Socket Mode ‚Üí Enable</li><li><ul><li>Bot Token (): OAuth &amp; Permissions ‚Üí Install to Workspace</li><li>App Token (): Basic Information ‚Üí App-Level Tokens ‚Üí Generate</li></ul></li></ol><pre><code># Add to ~/.hermes/.env:\nSLACK_BOT_TOKEN=xoxb-...\nSLACK_APP_TOKEN=xapp-...\nSLACK_ALLOWED_USERS=U01234ABCDE    # Comma-separated Slack user IDs\n</code></pre><p>WhatsApp doesn't have a simple bot API like Telegram or Discord. Hermes includes a built-in bridge using <a href=\"https://github.com/WhiskeySockets/Baileys\">Baileys</a> that connects via WhatsApp Web. The agent links to your WhatsApp account and responds to incoming messages.</p><ul><li>Enable WhatsApp in your config</li><li>Ask for your phone number (for the allowlist)</li><li>Install bridge dependencies (Node.js required)</li><li>Display a QR code ‚Äî scan it with your phone (WhatsApp ‚Üí Settings ‚Üí Linked Devices ‚Üí Link a Device)</li><li>Exit automatically once paired</li></ul><pre><code>hermes gateway            # Foreground\nhermes gateway install    # Or install as a system service (Linux)\n</code></pre><p>The gateway starts the WhatsApp bridge automatically using the saved session.</p><blockquote><p> WhatsApp Web sessions can disconnect if WhatsApp updates their protocol. The gateway reconnects automatically. If you see persistent failures, re-pair with . Agent responses are prefixed with \"‚öï Hermes Agent\" so you can distinguish them from your own messages in self-chat.</p></blockquote><h3>Gateway Commands (inside chat)</h3><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>Set this chat as the home channel</td></tr><tr></tr><tr><td>Invoke any installed skill (e.g., , )</td></tr></tbody></table><h3>DM Pairing (Alternative to Allowlists)</h3><p>Instead of manually configuring user IDs in allowlists, you can use the pairing system. When an unknown user DMs your bot, they receive a one-time pairing code:</p><pre><code># The user sees: \"Pairing code: XKGH5N7P\"\n# You approve them with:\nhermes pairing approve telegram XKGH5N7P\n\n# Other pairing commands:\nhermes pairing list          # View pending + approved users\nhermes pairing revoke telegram 123456789  # Remove access\n</code></pre><p>Pairing codes expire after 1 hour, are rate-limited, and use cryptographic randomness.</p><p><strong>By default, the gateway denies all users who are not in an allowlist or paired via DM.</strong> This is the safe default for a bot with terminal access.</p><pre><code># Restrict to specific users (recommended):\nTELEGRAM_ALLOWED_USERS=123456789,987654321\nDISCORD_ALLOWED_USERS=123456789012345678\n\n# Or explicitly allow all users (NOT recommended for bots with terminal access):\nGATEWAY_ALLOW_ALL_USERS=true\n</code></pre><table><tbody><tr><td>Current directory where you run the command</td></tr><tr><td>Home directory  (override with )</td></tr><tr><td><strong>Docker / Singularity / Modal / SSH</strong></td><td>User's home directory () inside the container or remote machine</td></tr></tbody></table><p>Override the terminal working directory for any backend:</p><pre><code># In ~/.hermes/.env or ~/.hermes/config.yaml:\nMESSAGING_CWD=/home/myuser/projects    # Gateway sessions\nTERMINAL_CWD=/workspace                # All terminal sessions (local or container)\n</code></pre><h3>Tool Progress Notifications</h3><p>Control how much tool activity is displayed. Set in :</p><pre><code>display:\n  tool_progress: all    # off | new | all | verbose\n</code></pre><table><tbody><tr><td>Silent ‚Äî just the final response</td></tr><tr><td>Tool indicator only when the tool changes (skip repeats)</td></tr><tr><td>Every tool call with a short preview (default)</td></tr><tr><td>Full args, results, and debug logs</td></tr></tbody></table><p>Toggle at runtime in the CLI with  (cycles through all four modes).</p><pre><code># Chat\nhermes                    # Interactive chat (default)\nhermes chat -q \"Hello\"    # Single query mode\nhermes --continue         # Resume the most recent session (-c)\nhermes --resume &lt;id&gt;      # Resume a specific session (-r)\n\n# Provider &amp; model management\nhermes model              # Switch provider and model interactively\nhermes login              # Authenticate with Nous Portal (OAuth)\nhermes logout             # Clear stored OAuth credentials\n\n# Configuration\nhermes setup              # Full setup wizard (provider, terminal, messaging, etc.)\nhermes config             # View/edit configuration\nhermes config check       # Check for missing config (useful after updates)\nhermes config migrate     # Interactively add missing options\nhermes status             # Show configuration status (incl. auth)\nhermes doctor             # Diagnose issues\n\n# Maintenance\nhermes update             # Update to latest version\nhermes uninstall          # Uninstall (can keep configs for later reinstall)\n\n# Gateway (messaging + cron scheduler)\nhermes gateway            # Run gateway in foreground\nhermes gateway install    # Install as system service (messaging + cron)\nhermes gateway status     # Check service status\nhermes whatsapp           # Pair WhatsApp via QR code\n\n# Skills, cron, misc\nhermes skills search k8s  # Search skill registries\nhermes skills install ... # Install a skill (with security scan)\nhermes skills list        # List installed skills\nhermes cron list          # View scheduled jobs\nhermes cron status        # Check if cron scheduler is running\nhermes pairing list       # View/manage DM pairing codes\nhermes version            # Show version info\n</code></pre><h3>CLI Commands (inside chat)</h3><p>Type  to see an autocomplete dropdown of all commands.</p><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>View/set custom system prompt</td></tr><tr><td>Set personality (kawaii, pirate, etc.)</td></tr><tr><td>Clear screen and reset conversation</td></tr><tr><td>Show conversation history</td></tr><tr><td>Reset conversation only (keep screen)</td></tr><tr></tr><tr></tr><tr><td>Save the current conversation</td></tr><tr><td>Show current configuration</td></tr><tr></tr><tr><td>Search, install, inspect, or manage skills from registries</td></tr><tr><td>Show gateway/messaging platform status</td></tr><tr></tr><tr><td>Invoke any installed skill (e.g., , )</td></tr></tbody></table><ul><li> or  ‚Äî new line (multi-line input)</li><li> ‚Äî interrupt agent (double-press to force exit)</li></ul><ul><li>Type a message + Enter while the agent is working to interrupt and send new instructions</li><li> to interrupt (press twice within 2s to force exit)</li><li>In-progress terminal commands are killed immediately (SIGTERM, then SIGKILL after 1s if the process resists)</li><li>Multiple messages typed during interrupt are combined into one prompt</li></ul><p><strong>Messaging Platforms (Telegram, Discord, Slack):</strong></p><ul><li>Send any message while the agent is working to interrupt</li><li>Use  to interrupt without queuing a follow-up message</li><li>Multiple messages sent during interrupt are combined into one prompt</li><li>Interrupt signals are processed with highest priority (before command parsing)</li></ul><p>Tools are organized into logical :</p><pre><code># Use specific toolsets\nhermes --toolsets \"web,terminal\"\n\n# Configure tools per platform (interactive)\nhermes tools\n</code></pre><p>, , , , , , , , , , , , , , , , and more.</p><h3>üñ•Ô∏è Terminal &amp; Process Management</h3><p>The terminal tool can execute commands in different environments, with full background process management via the  tool:</p><p> Start with <code>terminal(command=\"...\", background=true)</code>, then use <code>process(action=\"poll/wait/log/kill/write\")</code> to monitor, wait for completion, read output, terminate, or send input. The  action blocks until the process finishes -- no polling loops needed. PTY mode () enables interactive CLI tools like Codex and Claude Code.</p><table><thead><tr></tr></thead><tbody><tr><td>Run on your machine (default)</td><td>Development, trusted tasks</td></tr><tr><td>Security, reproducibility</td></tr><tr><td>Sandboxing, keep agent away from its own code</td></tr><tr><td>Cluster computing, rootless</td></tr><tr></tr></tbody></table><p><strong>Configure in :</strong></p><pre><code>terminal:\n  backend: local    # or: docker, ssh, singularity, modal\n  cwd: \".\"          # Working directory (\".\" = current dir)\n  timeout: 180      # Command timeout in seconds\n</code></pre><pre><code>terminal:\n  backend: docker\n  docker_image: python:3.11-slim\n</code></pre><p> (recommended for security - agent can't modify its own code):</p><pre><code># Set credentials in ~/.hermes/.env\nTERMINAL_SSH_HOST=my-server.example.com\nTERMINAL_SSH_USER=myuser\nTERMINAL_SSH_KEY=~/.ssh/id_rsa\n</code></pre><p> (for HPC clusters):</p><pre><code># Pre-build SIF for parallel workers\napptainer build ~/python.sif docker://python:3.11-slim\n\n# Configure\nhermes config set terminal.backend singularity\nhermes config set terminal.singularity_image ~/python.sif\n</code></pre><pre><code>uv pip install \"swe-rex[modal]\"   # Installs swe-rex + modal + boto3\nmodal setup                    # Authenticate with Modal\nhermes config set terminal.backend modal\n</code></pre><p> If a command needs sudo, you'll be prompted for your password (cached for the session). Or set  in .</p><p><strong>Container Security (Docker, Singularity, Modal):</strong> All container backends run with security hardening by default:</p><ul><li>Read-only root filesystem (Docker)</li><li>All Linux capabilities dropped</li><li>No privilege escalation (<code>--security-opt no-new-privileges</code>)</li><li>PID limits (256 processes)</li><li>Full namespace isolation ( for Singularity)</li><li>Persistent workspace via volumes, not writable root layer</li></ul><p> Configure CPU, memory, disk, and persistence for all container backends:</p><pre><code># In ~/.hermes/config.yaml under terminal:\nterminal:\n  backend: docker  # or singularity, modal\n  container_cpu: 1              # CPU cores (default: 1)\n  container_memory: 5120        # Memory in MB (default: 5GB)\n  container_disk: 51200         # Disk in MB (default: 50GB)\n  container_persistent: true    # Persist filesystem across sessions (default: true)\n</code></pre><p>When <code>container_persistent: true</code>, the sandbox state (installed packages, files, config) survives across sessions. Docker uses bind mounts, Singularity uses persistent overlays, and Modal uses filesystem snapshots. All persistent data is stored under  (default: ):</p><pre><code># Override where Docker workspaces and Singularity overlays/SIF cache are stored\nTERMINAL_SANDBOX_DIR=/mnt/fast-ssd/hermes-sandboxes\n</code></pre><p>Bounded curated memory that persists across sessions:</p><ul><li> ‚Äî agent's personal notes (environment facts, conventions, things learned). ~800 token budget.</li><li> ‚Äî user profile (preferences, communication style, expectations). ~500 token budget.</li></ul><p>Both are injected into the system prompt as a frozen snapshot at session start. The agent manages its own memory via the  tool (add/replace/remove/read). Character limits keep memory focused ‚Äî when full, the agent consolidates or replaces entries.</p><p>Configure in :</p><pre><code>memory:\n  memory_enabled: true\n  user_profile_enabled: true\n  memory_char_limit: 2200   # ~800 tokens\n  user_char_limit: 1375     # ~500 tokens\n</code></pre><h3>üîó Honcho Integration (Cross-Session User Modeling)</h3><p>Optional cloud-based user modeling via <a href=\"https://honcho.dev/\">Honcho</a> by Plastic Labs. While MEMORY.md and USER.md are local file-based memory, Honcho builds a deeper, AI-generated understanding of the user that persists across sessions and works across tools (Claude Code, Cursor, Hermes, etc.).</p><p>When enabled, Honcho runs  existing memory ‚Äî USER.md stays as-is, and Honcho adds an additional layer of user context:</p><ul><li>: Each turn, Honcho's user representation is fetched and injected into the system prompt</li><li>: After each conversation, messages are synced to Honcho for ongoing user modeling</li><li>: The agent can actively query its understanding of the user via </li></ul><pre><code># 1. Install the optional dependency\nuv pip install honcho-ai\n\n# 2. Get an API key from https://app.honcho.dev\n\n# 3. Create ~/.honcho/config.json (shared with other Honcho-enabled tools)\ncat &gt; ~/.honcho/config.json &lt;&lt; 'EOF'\n{\n  \"enabled\": true,\n  \"apiKey\": \"your-honcho-api-key\",\n  \"peerName\": \"your-name\",\n  \"hosts\": {\n    \"hermes\": {\n      \"workspace\": \"hermes\"\n    }\n  }\n}\nEOF\n</code></pre><p>Or configure via environment variable:</p><pre><code>hermes config set HONCHO_API_KEY your-key\n</code></pre><p>Fully opt-in ‚Äî zero behavior change when disabled or unconfigured. All Honcho calls are non-fatal; if the service is unreachable, the agent continues normally.</p><h3>üìÑ Context Files (SOUL.md, AGENTS.md, .cursorrules)</h3><p>Drop these files in your project directory and the agent automatically picks them up:</p><table><tbody><tr><td>Project-specific instructions, coding conventions, tool usage guidelines</td></tr><tr><td>Persona definition -- the agent embodies this personality and tone</td></tr><tr><td>Cursor IDE rules (also detected)</td></tr><tr><td>Cursor rule files (also detected)</td></tr></tbody></table><ul><li> is hierarchical: if subdirectories also have , all are combined (like Codex/Cline).</li><li> checks cwd first, then  as a global fallback.</li><li>All context files are capped at 20,000 characters with smart truncation.</li></ul><p>Long conversations are automatically summarized when approaching context limits:</p><pre><code># In ~/.hermes/config.yaml\ncompression:\n  enabled: true\n  threshold: 0.85    # Compress at 85% of limit\n</code></pre><p>Control how much \"thinking\" the model does before responding. This works with models that support extended thinking on OpenRouter and Nous Portal.</p><pre><code># In ~/.hermes/config.yaml under agent:\nagent:\n  reasoning_effort: \"xhigh\"   # xhigh (max), high, medium, low, minimal, none\n</code></pre><p>Higher reasoning effort gives better results on complex tasks (multi-step planning, debugging, research) at the cost of more tokens and latency. Set to  to disable extended thinking entirely.</p><p>All CLI and messaging sessions are stored in a SQLite database () with full-text search:</p><ul><li> stored per-session with model config and system prompt snapshots</li><li> via the  tool -- search past conversations with Gemini Flash summarization</li><li><strong>Compression-triggered session splitting</strong> -- when context is compressed, a new session is created linked to the parent, giving clean trajectories</li><li> -- each session is tagged with its origin (cli, telegram, discord, etc.)</li><li> -- pick up where you left off with  (most recent) or  (specific session)</li><li>Batch runner and RL trajectories are NOT stored here (separate systems)</li></ul><p>When you exit a CLI session, the resume command is printed automatically:</p><pre><code>Resume this session with:\n  hermes --resume 20260225_143052_a1b2c3\n\nSession:        20260225_143052_a1b2c3\nDuration:       12m 34s\nMessages:       28 (5 user, 18 tool calls)\n</code></pre><p>Use  to browse past sessions and find IDs to resume.</p><p>Every conversation is logged to  for debugging:</p><pre><code>sessions/\n‚îú‚îÄ‚îÄ session_20260201_143052_a1b2c3.json\n‚îî‚îÄ‚îÄ ...\n</code></pre><p>Schedule tasks to run automatically:</p><pre><code># In the CLI (/cron slash commands)\n/cron add 30m \"Remind me to check the build\"\n/cron add \"every 2h\" \"Check server status\"\n/cron add \"0 9 * * *\" \"Morning briefing\"\n/cron list\n/cron remove &lt;job_id&gt;\n</code></pre><p>The agent can also self-schedule using the  tool from any platform (CLI, Telegram, Discord, etc.).</p><p><strong>Cron execution is handled by the gateway daemon.</strong> The gateway ticks the scheduler every 60 seconds, running any due jobs in isolated agent sessions:</p><pre><code>hermes gateway install     # Install as system service (recommended)\nhermes gateway             # Or run in foreground\n\nhermes cron list           # View scheduled jobs\nhermes cron status         # Check if gateway is running\n</code></pre><p>Even if no messaging platforms are configured, the gateway stays running for cron. A file lock prevents duplicate execution if multiple processes overlap.</p><p>Run custom code at key lifecycle points ‚Äî log activity, send alerts, post to webhooks. Hooks are Python handlers that fire automatically during gateway operation.</p><pre><code>~/.hermes/hooks/\n‚îî‚îÄ‚îÄ my-hook/\n    ‚îú‚îÄ‚îÄ HOOK.yaml      # name + events to subscribe to\n    ‚îî‚îÄ‚îÄ handler.py     # async def handle(event_type, context)\n</code></pre><p>, , , , , ,  (wildcard ‚Äî fires for any slash command).</p><p>Hooks are non-blocking ‚Äî errors are caught and logged, never crashing the agent. See <a href=\"https://raw.githubusercontent.com/NousResearch/hermes-agent/main/docs/hooks.md\">docs/hooks.md</a> for the full event reference, context keys, and examples.</p><h3>üõ°Ô∏è Exec Approval (Messaging Platforms)</h3><p>When the agent tries to run a potentially dangerous command (, , etc.) on Telegram/Discord/WhatsApp, instead of blocking it silently, it asks the user for approval:</p><blockquote><p>‚ö†Ô∏è This command is potentially dangerous (recursive delete). Reply \"yes\" to approve.</p></blockquote><p>Reply \"yes\"/\"y\" to approve or \"no\"/\"n\" to deny. In CLI mode, the existing interactive approval prompt (once/session/always/deny) is preserved.</p><p>Hermes includes multiple layers of security beyond sandboxed terminals and exec approval:</p><table><tbody><tr><td><strong>Shell injection prevention</strong></td><td>Sudo password piping uses  to prevent metacharacter injection</td></tr><tr><td><strong>Cron prompt injection scanning</strong></td><td>Scheduled task prompts are scanned for instruction-override patterns (multi-word variants, Unicode obfuscation)</td></tr><tr><td><strong>Write deny list with symlink resolution</strong></td><td>Protected paths (, , etc.) are resolved via  before comparison, preventing symlink bypass</td></tr><tr><td><strong>Recursive delete false-positive fix</strong></td><td>Dangerous command detection uses precise flag-matching to avoid blocking safe commands</td></tr><tr><td> scripts run in a child process with API keys and credentials stripped from the environment</td></tr><tr><td>Docker containers run with read-only root, all capabilities dropped, no privilege escalation, PID limits</td></tr><tr><td>Cryptographically random pairing codes with 1-hour expiry and rate limiting</td></tr><tr><td>Default deny-all for messaging platforms; explicit allowlists or DM pairing required</td></tr></tbody></table><p>Convert text to speech with three providers:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>On Telegram, audio plays as native voice bubbles (the round, inline-playable kind). On Discord/WhatsApp, sent as audio file attachments. In CLI mode, saved to .</p><p><strong>Configure in :</strong></p><pre><code>tts:\n  provider: \"edge\"              # \"edge\" | \"elevenlabs\" | \"openai\"\n  edge:\n    voice: \"en-US-AriaNeural\"   # 322 voices, 74 languages\n  elevenlabs:\n    voice_id: \"pNInz6obpgDQGcFmaJgB\"  # Adam\n    model_id: \"eleven_multilingual_v2\"\n  openai:\n    model: \"gpt-4o-mini-tts\"\n    voice: \"alloy\"              # alloy, echo, fable, onyx, nova, shimmer\n</code></pre><p><strong>Telegram voice bubbles &amp; ffmpeg:</strong></p><p>Telegram voice bubbles require Opus/OGG audio format. OpenAI and ElevenLabs produce Opus natively ‚Äî no extra dependencies needed. Edge TTS (the default free provider) outputs MP3 and needs  to convert to Opus:</p><pre><code># Ubuntu/Debian\nsudo apt install ffmpeg\n\n# macOS\nbrew install ffmpeg\n\n# Fedora\nsudo dnf install ffmpeg\n</code></pre><p>Without ffmpeg, Edge TTS audio is sent as a regular audio file (playable, but shows as a rectangular player instead of a voice bubble). If you want voice bubbles without installing ffmpeg, switch to the OpenAI or ElevenLabs provider.</p><h3>üéôÔ∏è Voice Message Transcription</h3><p>Voice messages sent on Telegram, Discord, WhatsApp, or Slack are automatically transcribed using OpenAI's Whisper API and injected as text into the conversation. The agent sees the transcript as normal text -- no special handling needed.</p><table><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>Requires  in . Configure the model in :</p><pre><code>stt:\n  enabled: true\n  model: \"whisper-1\"\n</code></pre><p>Browser tools let the agent navigate websites, fill forms, click buttons, and extract content using <a href=\"https://browserbase.com/\">Browserbase</a>.</p><pre><code># 1. Get credentials from browserbase.com\nhermes config set BROWSERBASE_API_KEY your_api_key\nhermes config set BROWSERBASE_PROJECT_ID your_project_id\n\n# 2. Install Node.js dependencies (if not already)\ncd ~/.hermes-agent &amp;&amp; npm install\n</code></pre><p>, , , , , , , , </p><pre><code>hermes --toolsets browser -q \"Go to amazon.com and find the price of the latest Kindle\"\n</code></pre><p>Skills are on-demand knowledge documents the agent can load when needed. They follow a  pattern to minimize token usage and are compatible with the <a href=\"https://agentskills.io/specification\">agentskills.io</a> open standard.</p><p>All skills live in  -- a single directory that is the source of truth. On fresh install, bundled skills are copied there from the repo. Hub-installed skills and agent-created skills also go here. The agent can modify or delete any skill.  adds only genuinely new bundled skills (via a manifest) without overwriting your changes or re-adding skills you deleted.</p><p>Every installed skill is automatically available as a slash command ‚Äî type  to invoke it directly:</p><pre><code># In the CLI or any messaging platform (Telegram, Discord, Slack, WhatsApp):\n/gif-search funny cats\n/axolotl help me fine-tune Llama 3 on my dataset\n/github-pr-workflow create a PR for the auth refactor\n\n# Just the skill name (no prompt) loads the skill and lets the agent ask what you need:\n/excalidraw\n</code></pre><p>The skill's full instructions (SKILL.md) are loaded into the conversation, and any supporting files (references, templates, scripts) are listed for the agent to pull on demand via the  tool. Type  to see all available skill commands.</p><p>You can also use skills through natural conversation:</p><pre><code>hermes --toolsets skills -q \"What skills do you have?\"\nhermes --toolsets skills -q \"Show me the axolotl skill\"\n</code></pre><p><strong>Agent-Managed Skills (skill_manage tool):</strong></p><p>The agent can create, update, and delete its own skills via the  tool. This is the agent's  -- when it figures out a non-trivial workflow, it can save the approach as a skill for future reuse.</p><p>The agent is encouraged to  skills when:</p><ul><li>It completed a complex task (5+ tool calls) successfully</li><li>It hit errors or dead ends and found the working path</li><li>The user corrected its approach and the corrected version worked</li><li>It discovered a non-trivial workflow (deployment, data pipeline, configuration)</li></ul><p>The agent is encouraged to  skills when:</p><ul><li>Instructions were stale or incorrect (outdated API, changed behavior)</li><li>Steps didn't work on the current OS or environment</li><li>Missing critical steps or pitfalls discovered during use</li></ul><table><tbody><tr><td>,  (full SKILL.md), optional </td></tr><tr><td>Targeted fixes (preferred for updates)</td><td>, , </td></tr><tr><td>Major structural rewrites</td><td>,  (full SKILL.md replacement)</td></tr><tr></tr><tr><td>Add/update supporting files</td><td>, , </td></tr><tr></tr></tbody></table><p>The  action uses the same / pattern as the  file tool -- find a unique string and replace it. This is more token-efficient than  for small fixes (updating a command, adding a pitfall, fixing a version) because the model doesn't need to rewrite the entire skill. When patching SKILL.md, frontmatter integrity is validated after the replacement. The  action also works on supporting files via the  parameter.</p><p>User-created skills are stored in  and can optionally be organized into categories (subdirectories). Each skill has a  file and may include supporting files under , , , and .</p><p>The  tool is enabled by default in CLI and all messaging platforms. It is  included in batch_runner or RL training environments.</p><p><strong>Skills Hub ‚Äî Search, install, and manage skills from online registries:</strong></p><pre><code>hermes skills search kubernetes          # Search all sources (GitHub, ClawHub, LobeHub)\nhermes skills install openai/skills/k8s  # Install with security scan\nhermes skills inspect openai/skills/k8s  # Preview before installing\nhermes skills list --source hub          # List hub-installed skills\nhermes skills audit                      # Re-scan all hub skills\nhermes skills uninstall k8s              # Remove a hub skill\nhermes skills publish skills/my-skill --to github --repo owner/repo\nhermes skills snapshot export setup.json # Export skill config\nhermes skills tap add myorg/skills-repo  # Add a custom source\n</code></pre><p>All hub-installed skills go through a  that checks for data exfiltration, prompt injection, destructive commands, and other threats. Trust levels:  (ships with Hermes),  (openai/skills, anthropics/skills),  (everything else ‚Äî any findings = blocked unless ).</p><pre><code>---\nname: my-skill\ndescription: Brief description of what this skill does\nversion: 1.0.0\nmetadata:\n  hermes:\n    tags: [python, automation]\n    category: devops\n---\n\n# Skill Title\n\n## When to Use\nTrigger conditions for this skill.\n\n## Procedure\n1. Step one\n2. Step two\n\n## Pitfalls\n- Known failure modes and fixes\n\n## Verification\nHow to confirm it worked.\n</code></pre><p><strong>Skill Directory Structure:</strong></p><pre><code>~/.hermes/skills/                  # Single source of truth for all skills\n‚îú‚îÄ‚îÄ mlops/                         # Category directory\n‚îÇ   ‚îú‚îÄ‚îÄ axolotl/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SKILL.md               # Main instructions (required)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ references/            # Additional docs\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ templates/             # Output formats\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ assets/                # Supplementary files (agentskills.io standard)\n‚îÇ   ‚îî‚îÄ‚îÄ vllm/\n‚îÇ       ‚îî‚îÄ‚îÄ SKILL.md\n‚îú‚îÄ‚îÄ devops/\n‚îÇ   ‚îî‚îÄ‚îÄ deploy-k8s/                # Agent-created skill\n‚îÇ       ‚îú‚îÄ‚îÄ SKILL.md\n‚îÇ       ‚îî‚îÄ‚îÄ references/\n‚îú‚îÄ‚îÄ .hub/                          # Skills Hub state\n‚îÇ   ‚îú‚îÄ‚îÄ lock.json                  # Installed skill provenance\n‚îÇ   ‚îú‚îÄ‚îÄ quarantine/                # Pending security review\n‚îÇ   ‚îî‚îÄ‚îÄ audit.log                  # Security scan history\n‚îî‚îÄ‚îÄ .bundled_manifest              # Tracks which bundled skills have been offered\n</code></pre><h3>üêç Code Execution (Programmatic Tool Calling)</h3><p>The  tool lets the agent write Python scripts that call Hermes tools programmatically, collapsing multi-step workflows into a single LLM turn. The script runs in a sandboxed child process on the agent host, communicating with the parent via Unix domain socket RPC.</p><pre><code># The agent can write scripts like:\nfrom hermes_tools import web_search, web_extract\nresults = web_search(\"Python 3.13 features\", limit=5)\nfor r in results[\"data\"][\"web\"]:\n    content = web_extract([r[\"url\"]])\n    # ... filter and process ...\nprint(summary)\n</code></pre><p><strong>Available tools in sandbox:</strong>, , , , , ,  (foreground only).</p><p><strong>When the agent uses this:</strong> 3+ tool calls with processing logic between them, bulk data filtering, conditional branching, loops. The intermediate tool results never enter the context window -- only the final  output comes back.</p><p> The child process runs with a minimal environment -- only safe system variables (, , , etc.) are passed through. API keys, tokens, and credentials are stripped entirely. The script accesses tools exclusively via the RPC channel; it cannot read secrets from environment variables.</p><p>Configure via :</p><pre><code>code_execution:\n  timeout: 300       # Max seconds per script (default: 300)\n  max_tool_calls: 50 # Max tool calls per execution (default: 50)\n</code></pre><h3>üîÄ Subagents (Task Delegation)</h3><p>The  tool spawns child AIAgent instances with isolated context, restricted toolsets, and their own terminal sessions. Each child gets a fresh conversation and works independently -- only its final summary enters the parent's context.</p><pre><code>delegate_task(goal=\"Debug why tests fail\", context=\"Error: assertion in test_foo.py line 42\", toolsets=[\"terminal\", \"file\"])\n</code></pre><p><strong>Parallel batch (up to 3 concurrent):</strong></p><pre><code>delegate_task(tasks=[\n    {\"goal\": \"Research topic A\", \"toolsets\": [\"web\"]},\n    {\"goal\": \"Research topic B\", \"toolsets\": [\"web\"]},\n    {\"goal\": \"Fix the build\", \"toolsets\": [\"terminal\", \"file\"]}\n])\n</code></pre><ul><li>Each subagent gets its own terminal session (separate from the parent)</li><li>Depth limit of 2 (no grandchildren)</li><li>Subagents cannot call: , , , , </li><li>Interrupt propagation: interrupting the parent interrupts all active children</li></ul><p>Configure via :</p><pre><code>delegation:\n  max_iterations: 25                        # Max turns per child (default: 25)\n  default_toolsets: [\"terminal\", \"file\", \"web\"]  # Default toolsets\n</code></pre><h3>ü§ñ RL Training (Tinker + Atropos)</h3><blockquote><p> ‚Äî RL training integration is not yet functional. The tools and environments below are under active development.</p></blockquote><p>Train language models with reinforcement learning using the Tinker API and Atropos framework.</p><ol><li> Add to :</li></ol><pre><code>TINKER_API_KEY=your-tinker-key      # Get from https://tinker-console.thinkingmachines.ai/keys\nWANDB_API_KEY=your-wandb-key        # Get from https://wandb.ai/authorize\nOPENROUTER_API_KEY=your-key         # Optional: for rl_test_inference\n</code></pre><ol start=\"3\"><li> tinker-atropos is included as a submodule ‚Äî the installer handles it automatically.</li></ol><p>The agent can now use RL training tools:</p><pre><code>You: Start training on GSM8k with group_size=16\n\nAgent: I'll set up an RL training run on the GSM8k environment...\n[Uses rl_list_environments, rl_select_environment, rl_edit_config, rl_start_training]\n</code></pre><table><tbody><tr><td>List available RL environments</td></tr><tr><td>Select an environment for training</td></tr><tr><td>View all configurable options</td></tr><tr><td>Change a configuration value</td></tr><tr><td>Test environment with OpenRouter (pre-training validation)</td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>List active training runs</td></tr></tbody></table><p>For extended RL workflows with longer timeouts:</p><pre><code>python rl_cli.py --model \"anthropic/claude-sonnet-4-20250514\"\n</code></pre><h3>üß™ Atropos RL Environments</h3><p>Hermes Agent integrates with the <a href=\"https://github.com/NousResearch/atropos\">Atropos</a> RL framework through a layered environment system. This allows training models with reinforcement learning on agentic tasks using Hermes Agent's tools.</p><p>The integration has three layers:</p><table><tbody><tr><td><code>environments/agent_loop.py</code></td><td>Reusable multi-turn tool-calling engine (standard OpenAI spec)</td></tr><tr><td><code>environments/hermes_base_env.py</code></td><td>Abstract Atropos  subclass with toolset resolution, ToolContext, scoring</td></tr><tr><td><code>environments/terminal_test_env.py</code>, <code>environments/hermes_swe_env.py</code></td><td>Task-specific environments</td></tr></tbody></table><ul><li><strong>Phase 1 (OpenAI server type)</strong>: Works with any OpenAI-compatible endpoint (VLLM, SGLang, OpenRouter, OpenAI API). The server handles tool call parsing natively. Good for , , and .</li><li><strong>Phase 2 (VLLM server type)</strong>: Uses ManagedServer for exact token IDs + logprobs via . Client-side tool call parser registry reconstructs structured  from raw output. Required for .</li></ul><pre><code># 1. Launch VLLM with tool parser\nvllm serve YourModel --tool-parser hermes\n\n# 2. Start the Atropos API server\nrun-api\n\n# 3. Run an environment\npython environments/terminal_test_env.py serve \\\n    --openai.base_url http://localhost:8000/v1 \\\n    --openai.model_name YourModel \\\n    --openai.server_type openai\n</code></pre><h4>ToolContext (Reward Functions)</h4><p>Reward functions receive a  with unrestricted access to all hermes-agent tools, scoped to the rollout's sandbox:</p><pre><code>async def compute_reward(self, item, result, ctx: ToolContext) -&gt; float:\n    # Run tests in the model's terminal sandbox\n    test = ctx.terminal(\"pytest -v\")\n    if test[\"exit_code\"] == 0:\n        return 1.0\n    # Or check a file, search the web, navigate a browser...\n    return 0.0\n</code></pre><h4>Creating Custom Environments</h4><p>Subclass  and implement 5 methods:</p><pre><code>from environments.hermes_base_env import HermesAgentBaseEnv\n\nclass MyEnv(HermesAgentBaseEnv):\n    name = \"my-env\"\n    async def setup(self): ...            # Load data\n    async def get_next_item(self): ...    # Return next item\n    def format_prompt(self, item): ...    # Item -&gt; prompt string\n    async def compute_reward(self, item, result, ctx): ...  # Score with ToolContext\n    async def evaluate(self, *args, **kwargs): ...          # Periodic eval\n\nif __name__ == \"__main__\":\n    MyEnv.cli()\n</code></pre><p>Configure which tools are available per group, either explicitly or probabilistically:</p><pre><code># Explicit toolsets\n--env.enabled_toolsets '[\"terminal\",\"file\",\"web\"]'\n\n# Probabilistic distribution (sampled per group)\n--env.distribution development\n</code></pre><h4>Tool Call Parsers (Phase 2)</h4><p>For VLLM server type, a parser registry extracts structured  from raw model output. Supported parsers: , , , , , , , , , , .</p><pre><code>--env.tool_call_parser hermes  # Match your VLLM --tool-parser flag\n</code></pre><p>If you prefer full control over the installation process (or the quick-install script doesn't suit your environment), follow these steps to set everything up by hand.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr><td>Optional ‚Äî needed for browser automation tools</td></tr><tr><td>Optional ‚Äî faster file search in terminal tool (falls back to grep)</td></tr></tbody></table><blockquote><p> Python and pip are  prerequisites. The installer uses <a href=\"https://docs.astral.sh/uv/\">uv</a> to provision Python 3.11 automatically (no sudo needed). If you already have Python 3.11+ installed, uv will use it.</p></blockquote><h3>Step 1: Clone the Repository</h3><p>Clone with  to pull the required submodules (<a href=\"https://github.com/SWE-agent/mini-swe-agent\">mini-swe-agent</a> for the terminal tool backend and <a href=\"https://github.com/nousresearch/tinker-atropos\">tinker-atropos</a> for RL training):</p><pre><code>git clone --recurse-submodules https://github.com/NousResearch/hermes-agent.git\ncd hermes-agent\n</code></pre><p>If you already cloned without , initialize them manually:</p><pre><code>git submodule update --init --recursive\n</code></pre><h3>Step 2: Install uv &amp; Create Virtual Environment</h3><p><a href=\"https://docs.astral.sh/uv/\">uv</a> is a fast Python package manager that can also provision Python itself. Install it and create the venv in one go:</p><pre><code># Install uv (if not already installed)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create venv with Python 3.11 (uv downloads it if not present ‚Äî no sudo needed)\nuv venv venv --python 3.11\n</code></pre><blockquote><p> You do  need to activate the venv to use . The entry point has a hardcoded shebang pointing to the venv Python, so it works globally once symlinked (see Step 8). For installing packages, uv can target the venv directly via .</p></blockquote><h3>Step 3: Install Python Dependencies</h3><p>Install the main package in editable mode with all optional extras (messaging, cron, CLI menus, modal):</p><pre><code># Tell uv which venv to install into\nexport VIRTUAL_ENV=\"$(pwd)/venv\"\n\n# Install with all extras\nuv pip install -e \".[all]\"\n</code></pre><p>If you only want the core agent (no Telegram/Discord/cron support):</p><h3>Step 4: Install Submodule Packages</h3><p>These are local packages checked out as Git submodules. Install them in editable mode:</p><pre><code># Terminal tool backend (required for the terminal/command-execution tool)\nuv pip install -e \"./mini-swe-agent\"\n\n# RL training backend\nuv pip install -e \"./tinker-atropos\"\n</code></pre><p>Both are optional ‚Äî if you skip them, the corresponding toolsets simply won't be available.</p><h3>Step 5: Install Node.js Dependencies (Optional)</h3><p>Only needed if you plan to use the  toolset (Browserbase-powered):</p><p>This installs the  package defined in . Skip this step if you don't need browser tools.</p><h3>Step 6: Create the Configuration Directory</h3><p>Hermes stores all user configuration in :</p><pre><code># Create the directory structure\nmkdir -p ~/.hermes/{cron,sessions,logs,memories,skills}\n\n# Copy the example config file\ncp cli-config.yaml.example ~/.hermes/config.yaml\n\n# Create an empty .env file for API keys\ntouch ~/.hermes/.env\n</code></pre><p>Your  directory should now look like:</p><pre><code>~/.hermes/\n‚îú‚îÄ‚îÄ config.yaml     # Agent settings (model, terminal, toolsets, compression, etc.)\n‚îú‚îÄ‚îÄ .env            # API keys and secrets (one per line: KEY=value)\n‚îú‚îÄ‚îÄ memories/       # Persistent memory (MEMORY.md, USER.md)\n‚îú‚îÄ‚îÄ skills/         # Agent-created skills (auto-created on first use)\n‚îú‚îÄ‚îÄ cron/           # Scheduled job data\n‚îú‚îÄ‚îÄ sessions/       # Messaging gateway sessions\n‚îî‚îÄ‚îÄ logs/           # Conversation logs\n</code></pre><h3>Step 7: Add Your API Keys</h3><p>Open  in your editor and add at minimum an LLM provider key:</p><pre><code># Required ‚Äî at least one LLM provider:\nOPENROUTER_API_KEY=sk-or-v1-your-key-here\n\n# Optional ‚Äî enable additional tools:\nFIRECRAWL_API_KEY=fc-your-key          # Web search &amp; scraping\nBROWSERBASE_API_KEY=bb-your-key        # Browser automation\nBROWSERBASE_PROJECT_ID=your-project-id # Browser automation\nFAL_KEY=your-fal-key                   # Image generation (FLUX)\nTINKER_API_KEY=your-tinker-key         # RL training\nWANDB_API_KEY=your-wandb-key           # RL training metrics\n\n# Optional ‚Äî messaging gateway:\nTELEGRAM_BOT_TOKEN=123456:ABC-DEF      # From @BotFather\nTELEGRAM_ALLOWED_USERS=your-user-id    # Comma-separated\nDISCORD_BOT_TOKEN=MTIz...              # From Developer Portal\nDISCORD_ALLOWED_USERS=your-user-id     # Comma-separated\n</code></pre><p>Or set them one at a time via the CLI:</p><pre><code>hermes config set OPENROUTER_API_KEY sk-or-v1-your-key-here\n</code></pre><h3>Step 8: Add  to Your PATH</h3><p>The  entry point at  has a hardcoded shebang pointing to the venv's Python, so it works <strong>without activating the venv</strong>. The recommended approach is a symlink into  (most distributions already have this on PATH):</p><pre><code>mkdir -p ~/.local/bin\nln -sf \"$(pwd)/venv/bin/hermes\" ~/.local/bin/hermes\n</code></pre><p>If  isn't on your PATH yet, add it:</p><pre><code>echo '' &gt;&gt; ~/.bashrc\necho '# Hermes Agent' &gt;&gt; ~/.bashrc\necho 'export PATH=\"$HOME/.local/bin:$PATH\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre><pre><code>echo '' &gt;&gt; ~/.zshrc\necho '# Hermes Agent' &gt;&gt; ~/.zshrc\necho 'export PATH=\"$HOME/.local/bin:$PATH\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n</code></pre><p> (<code>~/.config/fish/config.fish</code>):</p><pre><code>fish_add_path $HOME/.local/bin\n</code></pre><h3>Step 9: Run the Setup Wizard (Optional)</h3><p>The interactive setup wizard walks you through configuring your API keys and preferences:</p><p>This is optional if you already configured  and  manually in the steps above.</p><h3>Step 10: Verify the Installation</h3><pre><code># Check that the command is available\nhermes version\n\n# Run diagnostics to verify everything is working\nhermes doctor\n\n# Check your configuration\nhermes status\n\n# Test with a quick query\nhermes chat -q \"Hello! What tools do you have available?\"\n</code></pre><p>If  reports issues, it will tell you exactly what's missing and how to fix it.</p><h3>Quick-Reference: Manual Install (Condensed)</h3><p>For those who just want the commands without the explanations:</p><pre><code># Install uv (if not already installed)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Clone &amp; enter\ngit clone --recurse-submodules https://github.com/NousResearch/hermes-agent.git\ncd hermes-agent\n\n# Create venv with Python 3.11 (uv downloads it if needed)\nuv venv venv --python 3.11\nexport VIRTUAL_ENV=\"$(pwd)/venv\"\n\n# Install everything\nuv pip install -e \".[all]\"\nuv pip install -e \"./mini-swe-agent\"\nuv pip install -e \"./tinker-atropos\"\nnpm install  # optional, for browser tools\n\n# Configure\nmkdir -p ~/.hermes/{cron,sessions,logs,memories,skills}\ncp cli-config.yaml.example ~/.hermes/config.yaml\ntouch ~/.hermes/.env\necho 'OPENROUTER_API_KEY=sk-or-v1-your-key' &gt;&gt; ~/.hermes/.env\n\n# Make hermes available globally (no venv activation needed)\nmkdir -p ~/.local/bin\nln -sf \"$(pwd)/venv/bin/hermes\" ~/.local/bin/hermes\n\n# Verify\nhermes doctor\nhermes\n</code></pre><p>If you installed manually (not via ):</p><pre><code>cd /path/to/hermes-agent\nexport VIRTUAL_ENV=\"$(pwd)/venv\"\n\n# Pull latest code and submodules\ngit pull origin main\ngit submodule update --init --recursive\n\n# Reinstall (picks up new dependencies)\nuv pip install -e \".[all]\"\nuv pip install -e \"./mini-swe-agent\"\nuv pip install -e \"./tinker-atropos\"\n\n# Check for new config options added since your last update\nhermes config check\nhermes config migrate   # Interactively add any missing options\n</code></pre><p>Process multiple prompts in parallel with automatic checkpointing:</p><pre><code>python batch_runner.py \\\n  --dataset_file=prompts.jsonl \\\n  --batch_size=20 \\\n  --run_name=my_run \\\n  --num_workers=4 \\\n  --distribution=default\n</code></pre><table><tbody><tr></tr><tr></tr><tr><td>Name for output/checkpoints</td></tr><tr><td>Parallel workers (default: 4)</td></tr><tr></tr><tr></tr><tr><td><code>--ephemeral_system_prompt</code></td><td>Guide behavior without saving to trajectories</td></tr><tr><td>Show available distributions</td></tr></tbody></table><p><code>data/&lt;run_name&gt;/trajectories.jsonl</code></p><p>Compress trajectories to fit token budgets for training:</p><pre><code># Compress a directory\npython trajectory_compressor.py --input=data/my_run\n\n# Compress with sampling\npython trajectory_compressor.py --input=data/my_run --sample_percent=15\n\n# Custom token target\npython trajectory_compressor.py --input=data/my_run --target_max_tokens=16000\n</code></pre><ul><li>Protects first/last turns</li><li>Summarizes middle turns via LLM</li><li>Configurable via <code>configs/trajectory_compression.yaml</code></li></ul><pre><code>from run_agent import AIAgent\n\nagent = AIAgent(\n    model=\"anthropic/claude-sonnet-4\",\n    enabled_toolsets=[\"web\", \"terminal\"]\n)\n\nresult = agent.run_conversation(\"Search for the latest Python news\")\nprint(result[\"final_response\"])\n</code></pre><h2>Environment Variables Reference</h2><p>All variables go in . Run <code>hermes config set VAR value</code> to set them.</p><table><tbody><tr><td>OpenRouter API key (recommended for flexibility)</td></tr><tr><td>API key for custom OpenAI-compatible endpoints (used with )</td></tr><tr><td>Base URL for custom endpoint (VLLM, SGLang, etc.)</td></tr><tr><td>Default model name (fallback when  is not set)</td></tr><tr><td>OpenAI key for TTS and voice transcription (separate from custom endpoint)</td></tr><tr><td>Override Hermes config directory (default: ). All config, sessions, logs, and skills are stored here.</td></tr></tbody></table><table><tbody><tr><td><code>HERMES_INFERENCE_PROVIDER</code></td><td>Override provider selection: , ,  (default: )</td></tr><tr><td>Override Nous Portal URL (for development/testing)</td></tr><tr><td>Override Nous inference API URL</td></tr><tr><td><code>HERMES_NOUS_MIN_KEY_TTL_SECONDS</code></td><td>Min agent key TTL before re-mint (default: 1800 = 30min)</td></tr><tr><td>Dump API request payloads to log files for debugging (/)</td></tr></tbody></table><table><tbody><tr><td>Web scraping (firecrawl.dev)</td></tr><tr></tr><tr></tr><tr><td>Image generation (fal.ai)</td></tr><tr></tr></tbody></table><table><tbody><tr><td>Backend: , , , , </td></tr><tr><td>Docker image (default: )</td></tr><tr><td><code>TERMINAL_SINGULARITY_IMAGE</code></td><td>Singularity image or  path</td></tr><tr><td>Command timeout in seconds</td></tr><tr></tr><tr><td>Enable sudo (stored plaintext - be careful!)</td></tr></tbody></table><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><table><tbody><tr><td>Telegram bot token (@BotFather)</td></tr><tr><td>Comma-separated user IDs allowed to use bot</td></tr><tr><td>Default channel for cron delivery</td></tr><tr></tr><tr><td>Comma-separated user IDs allowed to use bot</td></tr><tr><td>Default channel for cron delivery</td></tr><tr><td>Slack bot token ()</td></tr><tr><td>Slack app-level token (, required for Socket Mode)</td></tr><tr><td>Comma-separated Slack user IDs</td></tr><tr><td>Default Slack channel for cron delivery</td></tr><tr><td>Enable WhatsApp bridge (/)</td></tr><tr><td>Comma-separated phone numbers (with country code)</td></tr><tr><td>Working directory for terminal in messaging (default: ~)</td></tr><tr><td>Allow all users without allowlist (/, default: )</td></tr></tbody></table><p><strong>Container Resources (Docker, Singularity, Modal):</strong></p><table><tbody><tr><td>CPU cores for container backends (default: 1)</td></tr><tr><td><code>TERMINAL_CONTAINER_MEMORY</code></td><td>Memory in MB for container backends (default: 5120)</td></tr><tr><td>Disk in MB for container backends (default: 51200)</td></tr><tr><td><code>TERMINAL_CONTAINER_PERSISTENT</code></td><td>Persist container filesystem across sessions (default: true)</td></tr><tr><td>Host directory for Docker workspaces, Singularity overlays/SIF cache (default: )</td></tr></tbody></table><table><tbody><tr><td>Max tool-calling iterations per conversation (default: 60)</td></tr></tbody></table><table><tbody><tr><td><code>CONTEXT_COMPRESSION_ENABLED</code></td><td>Enable auto-compression (default: true)</td></tr><tr><td><code>CONTEXT_COMPRESSION_THRESHOLD</code></td><td>Trigger at this % of limit (default: 0.85)</td></tr><tr><td><code>CONTEXT_COMPRESSION_MODEL</code></td></tr></tbody></table><table><tbody><tr></tr><tr></tr><tr><td>OAuth provider credentials (managed by )</td></tr><tr></tr><tr></tr><tr></tr><tr><td>Agent internals (context compressor, prompt builder, display, etc.)</td></tr><tr><td>CLI implementation (banner, commands, callbacks, config, auth)</td></tr><tr><td>Tool implementations + central registry ()</td></tr><tr><td>Terminal execution backends (local, docker, ssh, singularity, modal)</td></tr><tr><td>Dangerous command detection + per-session approval state</td></tr><tr><td>Tool orchestration (thin layer over )</td></tr><tr><td>Bundled skill sources (copied to  on install)</td></tr><tr><td>All active skills (bundled + hub-installed + agent-created)</td></tr><tr><td>Messaging platform adapters</td></tr><tr></tr></tbody></table><pre><code>hermes doctor    # Run diagnostics\nhermes status    # Check configuration\nhermes config    # View current settings\n</code></pre><ul><li>: Run  or <code>hermes config set OPENROUTER_API_KEY your_key</code></li><li><strong>\"hermes: command not found\"</strong>: Reload your shell () or check PATH</li><li><strong>\"Run  to re-authenticate\"</strong>: Your Nous Portal session expired. Run  to refresh.</li><li><strong>\"No active paid subscription\"</strong>: Your Nous Portal account needs an active subscription for inference.</li><li>: Check  and logs</li><li><strong>Missing config after update</strong>: Run  to see what's new, then  to add missing options</li><li><strong>Provider auto-detection wrong</strong>: Force a provider with <code>hermes chat --provider openrouter</code> or set <code>HERMES_INFERENCE_PROVIDER</code> in </li></ul><ol></ol><p>MIT License - see <a href=\"https://raw.githubusercontent.com/NousResearch/hermes-agent/main/LICENSE\">LICENSE</a> for details.</p>",
      "contentLength": 48150,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/e7c92474a54090f78d6ba971c1fec309a6448fe30107f46d62ec740f57c2f04a/NousResearch/hermes-agent",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "X-PLUG/MobileAgent",
      "url": "https://github.com/X-PLUG/MobileAgent",
      "date": 1772334486,
      "author": "",
      "guid": 49285,
      "unread": true,
      "content": "<p>Mobile-Agent: The Powerful GUI Agent Family</p><div align=\"left\"><h3>Learn about Mobile-Agent-v3.5</h3></div><div align=\"left\"><h3>Search for the stock prices of Apple and Nvidia respectively. Then create a new spreadsheet in WPS Office. Enter the company names in column A and the retrieved stock prices in column B.</h3></div><div align=\"left\"><h3>Create a new document in WPS Office and write a brief introduction about Alibaba with font size 12. Then search for Alibaba's logo in Edge browser, copy an image, and paste it at the end of the document.</h3></div><div align=\"left\"><h3>Today is Sunday, February 15, 2025. Search for flights from Guangzhou to Chengdu five days from now on Ctrip, check the cheapest flight, then search for the cheapest train ticker on the same route and tell me their prices.</h3></div><div align=\"left\"><h3>Check the \"È≠îÊê≠ModelScopeÁ§æÂå∫\" (ModelScope Community) account on Xiaohongshu and Douyin, then tell me the total follower count across both platforms.</h3></div><p>If you find Mobile-Agent useful for your research and applications, please cite using this BibTeX:</p><pre><code>@article{xu2026mobile,\n  title={Mobile-Agent-v3. 5: Multi-platform Fundamental GUI Agents},\n  author={Xu, Haiyang and Zhang, Xi and Liu, Haowei and Wang, Junyang and Zhu, Zhaozai and Zhou, Shengjie and Hu, Xuhao and Gao, Feiyu and Cao, Junjie and Wang, Zihua and others},\n  journal={arXiv preprint arXiv:2602.16855},\n  year={2026}\n}\n\n@article{ye2025mobile,\n  title={Mobile-Agent-v3: Foundamental Agents for GUI Automation},\n  author={Ye, Jiabo and Zhang, Xi and Xu, Haiyang and Liu, Haowei and Wang, Junyang and Zhu, Zhaoqing and Zheng, Ziwei and Gao, Feiyu and Cao, Junjie and Lu, Zhengxi and others},\n  journal={arXiv preprint arXiv:2508.15144},\n  year={2025}\n}\n\n@article{lu2025ui,\n  title={UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning},\n  author={Lu, Zhengxi and Ye, Jiabo and Tang, Fei and Shen, Yongliang and Xu, Haiyang and Zheng, Ziwei and Lu, Weiming and Yan, Ming and Huang, Fei and Xiao, Jun and others},\n  journal={arXiv preprint arXiv:2509.11543},\n  year={2025}\n}\n\n@article{wanyan2025look,\n  title={Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation},\n  author={Wanyan, Yuyang and Zhang, Xi and Xu, Haiyang and Liu, Haowei and Wang, Junyang and Ye, Jiabo and Kou, Yutong and Yan, Ming and Huang, Fei and Yang, Xiaoshan and others},\n  journal={arXiv preprint arXiv:2506.04614},\n  year={2025}\n}\n\n@article{liu2025pc,\n  title={PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC},\n  author={Liu, Haowei and Zhang, Xi and Xu, Haiyang and Wanyan, Yuyang and Wang, Junyang and Yan, Ming and Zhang, Ji and Yuan, Chunfeng and Xu, Changsheng and Hu, Weiming and Huang, Fei},\n  journal={arXiv preprint arXiv:2502.14282},\n  year={2025}\n}\n\n@article{wang2025mobile,\n  title={Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks},\n  author={Wang, Zhenhailong and Xu, Haiyang and Wang, Junyang and Zhang, Xi and Yan, Ming and Zhang, Ji and Huang, Fei and Ji, Heng},\n  journal={arXiv preprint arXiv:2501.11733},\n  year={2025}\n}\n\n@article{wang2024mobile2,\n  title={Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration},\n  author={Wang, Junyang and Xu, Haiyang and Jia, Haitao and Zhang, Xi and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},\n  journal={arXiv preprint arXiv:2406.01014},\n  year={2024}\n}\n\n@article{wang2024mobile,\n  title={Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception},\n  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},\n  journal={arXiv preprint arXiv:2401.16158},\n  year={2024}\n}\n</code></pre>",
      "contentLength": 3649,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/96cdf5af95511a8ee959f7a0eec92fc02fc9cfbdb1005935ac868f032237e030/X-PLUG/MobileAgent",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Shubhamsaboo/awesome-llm-apps",
      "url": "https://github.com/Shubhamsaboo/awesome-llm-apps",
      "date": 1772334486,
      "author": "",
      "guid": 49286,
      "unread": true,
      "content": "<p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p><p>A curated collection of <strong>Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.</strong> This repository features LLM apps that use models from <img src=\"https://cdn.jsdelivr.net/npm/simple-icons@15/icons/openai.svg?sanitize=true\" alt=\"openai logo\" width=\"25\" height=\"15\"> , <img src=\"https://cdn.simpleicons.org/anthropic\" alt=\"anthropic logo\" width=\"25\" height=\"15\">, <img src=\"https://cdn.simpleicons.org/googlegemini\" alt=\"google logo\" width=\"25\" height=\"18\">, <img src=\"https://cdn.simpleicons.org/x\" alt=\"X logo\" width=\"25\" height=\"15\"> and open-source models like <img src=\"https://cdn.simpleicons.org/alibabacloud\" alt=\"alibaba logo\" width=\"25\" height=\"15\"> or <img src=\"https://cdn.simpleicons.org/meta\" alt=\"meta logo\" width=\"25\" height=\"15\"> that you can run locally on your computer.</p><ul><li>üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.</li><li>üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.</li><li>üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.</li></ul><h3>üéÆ Autonomous Game Playing Agents</h3><h3>üìÄ RAG (Retrieval Augmented Generation)</h3><h3>üíæ LLM Apps with Memory Tutorials</h3><h3>üîß LLM Fine-tuning Tutorials</h3><h3>üßë‚Äçüè´ AI Agent Framework Crash Course</h3><ul><li>Starter agent; model‚Äëagnostic (OpenAI, Claude)</li><li>Structured outputs (Pydantic)</li><li>Tools: built‚Äëin, function, third‚Äëparty, MCP tools</li><li>Memory; callbacks; Plugins</li><li>Simple multi‚Äëagent; Multi‚Äëagent patterns</li></ul><ul><li>Starter agent; function calling; structured outputs</li><li>Tools: built‚Äëin, function, third‚Äëparty integrations</li><li>Memory; callbacks; evaluation</li><li>Multi‚Äëagent patterns; agent handoffs</li><li>Swarm orchestration; routing logic</li></ul><ol><li><pre><code>git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git \n</code></pre></li><li><p><strong>Navigate to the desired project directory</strong></p><pre><code>cd awesome-llm-apps/starter_ai_agents/ai_travel_agent\n</code></pre></li><li><p><strong>Install the required dependencies</strong></p><pre><code>pip install -r requirements.txt\n</code></pre></li><li><p><strong>Follow the project-specific instructions</strong> in each project's  file to set up and run the app.</p></li></ol><h3><img src=\"https://cdn.simpleicons.org/github\" alt=\"github logo\" width=\"25\" height=\"20\"> Thank You, Community, for the Support! üôè</h3><p>üåü <strong>Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.</strong></p>",
      "contentLength": 1845,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/a95d20b1b0b439d0bdebe553bebe0322bd77666e1133c4f1dadfdb9bad24c59e/Shubhamsaboo/awesome-llm-apps",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ruvnet/ruflo",
      "url": "https://github.com/ruvnet/ruflo",
      "date": 1772247153,
      "author": "",
      "guid": 49041,
      "unread": true,
      "content": "<p>üåä The leading agent orchestration platform for Claude. Deploy intelligent multi-agent swarms, coordinate autonomous workflows, and build conversational AI systems. Features enterprise-grade architecture, distributed swarm intelligence, RAG integration, and native Claude Code / Codex Integration</p><blockquote><p> Claude Flow is now Ruflo ‚Äî named by Ruv, who loves Rust, flow states, and building things that feel inevitable. The \"Ru\" is the Ruv. The \"flo\" is the flow. Underneath, WASM kernels written in Rust power the policy engine, embeddings, and proof system. 5,800 commits later, the alpha is over. This is v3.5.</p></blockquote><p>Ruflo is a comprehensive AI agent orchestration framework that transforms Claude Code into a powerful multi-agent development platform. It enables teams to deploy, coordinate, and optimize specialized AI agents working together on complex software engineering tasks.</p><h3>Self-Learning/Self-Optimizing Agent Architecture</h3><pre><code>User ‚Üí Ruflo (CLI/MCP) ‚Üí Router ‚Üí Swarm ‚Üí Agents ‚Üí Memory ‚Üí LLM Providers\n                       ‚Üë                          ‚Üì\n                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ Learning Loop ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</code></pre><pre><code># One-line install (recommended)\ncurl -fsSL https://cdn.jsdelivr.net/gh/ruvnet/claude-flow@main/scripts/install.sh | bash\n\n# Or full setup with MCP + diagnostics\ncurl -fsSL https://cdn.jsdelivr.net/gh/ruvnet/claude-flow@main/scripts/install.sh | bash -s -- --full\n\n# Or via npx\nnpx ruflo@latest init --wizard\n</code></pre><p>ü§ñ  - Ready-to-use AI agents for coding, code review, testing, security audits, documentation, and DevOps. Each agent is optimized for its specific role.</p><p>üêù  - Run unlimited agents simultaneously in organized swarms. Agents spawn sub-workers, communicate, share context, and divide work automatically using hierarchical (queen/workers) or mesh (peer-to-peer) patterns.</p><p>üß† <strong>Learns From Your Workflow</strong> - The system remembers what works. Successful patterns are stored and reused, routing similar tasks to the best-performing agents. Gets smarter over time.</p><p>üîå  - Switch between Claude, GPT, Gemini, Cohere, or local models like Llama. Automatic failover if one provider is unavailable. Smart routing picks the cheapest option that meets quality requirements.</p><p>‚ö°  - Native integration via MCP (Model Context Protocol). Use ruflo commands directly in your Claude Code sessions with full tool access.</p><p>üîí <strong>Production-Ready Security</strong> - Built-in protection against prompt injection, input validation, path traversal prevention, command injection blocking, and safe credential handling.</p><p>üß©  - Add custom capabilities with the plugin SDK. Create workers, hooks, providers, and security modules. Share plugins via the decentralized IPFS marketplace.</p><h3>A multi-purpose Agent Tool Kit</h3><h3>Claude Code: With vs Without Ruflo</h3><table><thead><tr></tr></thead><tbody><tr><td>Agents work in isolation, no shared context</td><td>Agents collaborate via swarms with shared memory and consensus</td></tr><tr><td>Manual orchestration between tasks</td><td>Queen-led hierarchy with 5 consensus algorithms (Raft, Byzantine, Gossip)</td></tr><tr><td>üêù Queen-led swarms with collective intelligence, 3 queen types, 8 worker types</td></tr><tr><td>‚õî No multi-agent decisions</td><td>Byzantine fault-tolerant voting (f &lt; n/3), weighted, majority</td></tr><tr><td>Session-only, no persistence</td><td>HNSW vector memory with sub-ms retrieval + knowledge graph</td></tr><tr><td>üêò RuVector PostgreSQL with 77+ SQL functions, ~61¬µs search, 16,400 QPS</td></tr><tr><td>PageRank + community detection identifies influential insights (ADR-049)</td></tr><tr><td>Shared knowledge base with LRU cache, SQLite persistence, 8 memory types</td></tr><tr><td>Static behavior, no adaptation</td><td>SONA self-learning with &lt;0.05ms adaptation, LearningBridge for insights</td></tr><tr><td>3-scope agent memory (project/local/user) with cross-agent transfer</td></tr><tr><td>You decide which agent to use</td><td>Intelligent routing based on learned patterns (89% accuracy)</td></tr><tr><td>Manual breakdown required</td><td>Automatic decomposition across 5 domains (Security, Core, Integration, Support)</td></tr><tr><td>Nothing runs automatically</td><td>12 context-triggered workers auto-dispatch on file changes, patterns, sessions</td></tr><tr><td>6 providers with automatic failover and cost-based routing (85% savings)</td></tr><tr><td>CVE-hardened with bcrypt, input validation, path traversal prevention</td></tr><tr><td>Faster tasks via parallel swarm spawning and intelligent routing</td></tr></tbody></table><ul><li> /  /  package manager</li></ul><p>: Claude Code must be installed first:</p><pre><code># 1. Install Claude Code globally\nnpm install -g @anthropic-ai/claude-code\n\n# 2. (Optional) Skip permissions check for faster setup\nclaude --dangerously-skip-permissions\n</code></pre><h4>One-Line Install (Recommended)</h4><pre><code># curl-style installer with progress display\ncurl -fsSL https://cdn.jsdelivr.net/gh/ruvnet/claude-flow@main/scripts/install.sh | bash\n\n# Full setup (global + MCP + diagnostics)\ncurl -fsSL https://cdn.jsdelivr.net/gh/ruvnet/claude-flow@main/scripts/install.sh | bash -s -- --full\n</code></pre><pre><code># Quick start (no install needed)\nnpx ruflo@latest init\n\n# Or install globally\nnpm install -g ruflo@latest\nruflo init\n\n# With Bun (faster)\nbunx ruflo@latest init\n</code></pre><table><tbody><tr></tr><tr><td>Full install with ML/embeddings</td></tr></tbody></table><pre><code># Minimal install (skip ML/embeddings)\nnpm install -g ruflo@latest --omit=optional\n</code></pre><pre><code># Initialize project\nnpx ruflo@latest init\n\n# Start MCP server for Claude Code integration\nnpx ruflo@latest mcp start\n\n# Run a task with agents\nnpx ruflo@latest --agent coder --task \"Implement user authentication\"\n\n# List available agents\nnpx ruflo@latest --list\n</code></pre><pre><code># Update helpers and statusline (preserves your data)\nnpx ruflo@v3alpha init upgrade\n\n# Update AND add any missing skills/agents/commands\nnpx ruflo@v3alpha init upgrade --add-missing\n</code></pre><p>The  flag automatically detects and installs new skills, agents, and commands that were added in newer versions, without overwriting your existing customizations.</p><h3>Claude Code MCP Integration</h3><p>Add ruflo as an MCP server for seamless integration:</p><pre><code># Add ruflo MCP server to Claude Code\nclaude mcp add ruflo -- npx -y ruflo@latest mcp start\n\n# Verify installation\nclaude mcp list\n</code></pre><p>Once added, Claude Code can use all 175+ ruflo MCP tools directly:</p><ul><li> - Initialize agent swarms</li><li> - Spawn specialized agents</li><li> - Search patterns with HNSW vector search</li><li> - Intelligent task routing</li></ul><h2>What is it exactly? Agents that learn, build and work perpetually.</h2><p>Connect Ruflo to your development environment.</p><p>Comprehensive capabilities for enterprise-grade AI agent orchestration.</p><p>Real-world scenarios and pre-built workflows for common tasks.</p><h2>üß† Infinite Context &amp; Memory Optimization</h2><p>Ruflo eliminates Claude Code's context window ceiling with a real-time memory management system that archives, optimizes, and restores conversation context automatically.</p><h2>üíæ Storage: RVF (RuVector Format)</h2><p>Ruflo uses RVF ‚Äî a compact binary storage format that replaces the 18MB sql.js WASM dependency with pure TypeScript. No native compilation, no WASM downloads, works everywhere Node.js runs.</p><h2>üß† Intelligence &amp; Learning</h2><p>Self-learning hooks, pattern recognition, and intelligent task routing.</p><p>Scripts, coordination systems, and collaborative development features.</p><p>Use Ruflo packages directly in your applications.</p><h2>üîó Ecosystem &amp; Integrations</h2><p>Core infrastructure packages powering Ruflo's intelligence layer.</p><p>Cloud platform integration and deployment tools.</p><p>AI manipulation defense, threat detection, and input validation.</p><h2>üèóÔ∏è Architecture &amp; Modules</h2><p>Domain-driven design, performance benchmarks, and testing framework.</p><h2>‚öôÔ∏è Configuration &amp; Reference</h2><p>Environment setup, configuration options, and platform support.</p><p>Troubleshooting, migration guides, and documentation links.</p>",
      "contentLength": 7291,
      "flags": null,
      "enclosureUrl": "https://repository-images.githubusercontent.com/995029641/b9acbe16-0f49-420d-804f-468ba2a73ace",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Wei-Shaw/claude-relay-service",
      "url": "https://github.com/Wei-Shaw/claude-relay-service",
      "date": 1772247153,
      "author": "",
      "guid": 49042,
      "unread": true,
      "content": "<p>CRS-Ëá™Âª∫Claude CodeÈïúÂÉèÔºå‰∏ÄÁ´ôÂºèÂºÄÊ∫ê‰∏≠ËΩ¨ÊúçÂä°ÔºåËÆ© Claude„ÄÅOpenAI„ÄÅGemini„ÄÅDroid ËÆ¢ÈòÖÁªü‰∏ÄÊé•ÂÖ•ÔºåÊîØÊåÅÊãºËΩ¶ÂÖ±‰∫´ÔºåÊõ¥È´òÊïàÂàÜÊëäÊàêÊú¨ÔºåÂéüÁîüÂ∑•ÂÖ∑Êó†Áºù‰ΩøÁî®„ÄÇ</p><blockquote><p>[!CAUTION] Ôºöv1.1.248 Âèä‰ª•‰∏ãÁâàÊú¨Â≠òÂú®‰∏•ÈáçÁöÑÁÆ°ÁêÜÂëòËÆ§ËØÅÁªïËøáÊºèÊ¥ûÔºåÊîªÂáªËÄÖÂèØÊú™ÊéàÊùÉËÆøÈóÆÁÆ°ÁêÜÈù¢Êùø„ÄÇ</p></blockquote><div align=\"center\"><table><tbody><tr><td align=\"left\">Êèê‰æõÁ®≥ÂÆöÁöÑ Codex CLI ÊãºËΩ¶ÊúçÂä°ÔºöÊé•ÂÖ•CCÁöÑÊïàÊûúÂ™≤ÁæéÂÆòÊñπ Anthropic Console Ë¥¶Âè∑ÔºåÊöÇ‰∏çÊîØÊåÅ PDF ËØÜÂà´ÂäüËÉΩ üí∞ Âçï‰ª∑Ôºö0.8ÂÖÉ=1ÁæéÈáëÈ¢ùÂ∫¶</td></tr></tbody></table></div><p>üö® : ‰ΩøÁî®Êú¨È°πÁõÆÂèØËÉΩËøùÂèçAnthropicÁöÑÊúçÂä°Êù°Ê¨æ„ÄÇËØ∑Âú®‰ΩøÁî®Ââç‰ªîÁªÜÈòÖËØªAnthropicÁöÑÁî®Êà∑ÂçèËÆÆÔºå‰ΩøÁî®Êú¨È°πÁõÆÁöÑ‰∏ÄÂàáÈ£éÈô©Áî±Áî®Êà∑Ëá™Ë°åÊâøÊãÖ„ÄÇ</p><p>üìñ : Êú¨È°πÁõÆ‰ªÖ‰æõÊäÄÊúØÂ≠¶‰π†ÂíåÁ†îÁ©∂‰ΩøÁî®Ôºå‰ΩúËÄÖ‰∏çÂØπÂõ†‰ΩøÁî®Êú¨È°πÁõÆÂØºËá¥ÁöÑË¥¶Êà∑Â∞ÅÁ¶Å„ÄÅÊúçÂä°‰∏≠Êñ≠ÊàñÂÖ∂‰ªñÊçüÂ§±ÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª„ÄÇ</p><ul><li>üåç : ÊâÄÂú®Âú∞Âå∫Êó†Ê≥ïÁõ¥Êé•ËÆøÈóÆClaude CodeÊúçÂä°Ôºü</li><li>üîí : ÊãÖÂøÉÁ¨¨‰∏âÊñπÈïúÂÉèÊúçÂä°‰ºöËÆ∞ÂΩïÊàñÊ≥ÑÈú≤‰Ω†ÁöÑÂØπËØùÂÜÖÂÆπÔºü</li><li>üë• : ÊÉ≥ÂíåÊúãÂèã‰∏ÄËµ∑ÂàÜÊëäClaude Code MaxËÆ¢ÈòÖË¥πÁî®Ôºü</li><li>‚ö° : Á¨¨‰∏âÊñπÈïúÂÉèÁ´ôÁªèÂ∏∏ÊïÖÈöú‰∏çÁ®≥ÂÆöÔºåÂΩ±ÂìçÊïàÁéá Ôºü</li></ul><p>‚úÖ : ‰∏â‰∫îÂ•ΩÂèã‰∏ÄËµ∑ÂàÜÊëäClaude Code MaxËÆ¢ÈòÖ ‚úÖ : ‰∏çÊÉ≥ËÆ©Á¨¨‰∏âÊñπÈïúÂÉèÁúãÂà∞‰Ω†ÁöÑÂØπËØùÂÜÖÂÆπ ‚úÖ : ÊúâÂü∫Êú¨ÁöÑÊäÄÊúØÂü∫Á°ÄÔºåÊÑøÊÑèËá™Â∑±Êê≠Âª∫ÂíåÁª¥Êä§ ‚úÖ : ÈúÄË¶ÅÈïøÊúüÁ®≥ÂÆöÁöÑClaudeËÆøÈóÆÔºå‰∏çÊÉ≥ÂèóÂà∂‰∫éÈïúÂÉèÁ´ô ‚úÖ : Êó†Ê≥ïÁõ¥Êé•ËÆøÈóÆClaudeÂÆòÊñπÊúçÂä°</p><ul><li>üïµÔ∏è : ‰Ω†ÁöÑÂØπËØùÂÜÖÂÆπÈÉΩË¢´‰∫∫ÂÆ∂ÁúãÂæó‰∏ÄÊ∏Ö‰∫åÊ•öÔºåÂïÜ‰∏öÊú∫ÂØÜ‰ªÄ‰πàÁöÑÂ∞±Âà´ÊÉ≥‰∫Ü</li></ul><ul><li>üîê : ÊâÄÊúâÊé•Âè£ËØ∑Ê±ÇÈÉΩÂè™ÁªèËøá‰Ω†Ëá™Â∑±ÁöÑÊúçÂä°Âô®ÔºåÁõ¥ËøûAnthropic API</li><li>‚ö° : Â∞±‰Ω†‰ª¨Âá†‰∏™‰∫∫Áî®ÔºåMax 200ÂàÄÂ•óÈ§êÂü∫Êú¨‰∏äÂèØ‰ª•ÁàΩÁî®Opus</li><li>üí∞ : Áî®‰∫ÜÂ§öÂ∞ëtoken‰∏ÄÁõÆ‰∫ÜÁÑ∂ÔºåÊåâÂÆòÊñπ‰ª∑Ê†ºÊç¢ÁÆó‰∫ÜÂÖ∑‰ΩìË¥πÁî®</li></ul><ul><li>‚úÖ : ÂèØ‰ª•Ê∑ªÂä†Â§ö‰∏™ClaudeË¥¶Êà∑Ëá™Âä®ËΩÆÊç¢</li><li>‚úÖ : ÁªôÊØè‰∏™‰∫∫ÂàÜÈÖçÁã¨Á´ãÁöÑKey</li></ul><ul></ul><ul><li>: ËÉΩËÆøÈóÆÂà∞Anthropic APIÔºàÂª∫ËÆÆ‰ΩøÁî®USÂú∞Âå∫ÁöÑÊú∫Âô®Ôºâ</li><li>: 2Ê†∏4GÁöÑÂü∫Êú¨Â§ü‰∫ÜÔºåÁΩëÁªúÂ∞ΩÈáèÈÄâÂõûÂõΩÁ∫øË∑ØÂø´‰∏ÄÁÇπÁöÑÔºà‰∏∫‰∫ÜÊèêÈ´òÈÄüÂ∫¶ÔºåÂª∫ËÆÆ‰∏çË¶ÅÂºÄ‰ª£ÁêÜÊàñËÄÖËÆæÁΩÆÊúçÂä°Âô®ÁöÑIPÁõ¥ËøûÔºâ</li><li>: ÈòøÈáå‰∫ë„ÄÅËÖæËÆØ‰∫ëÁöÑÊµ∑Â§ñ‰∏ªÊú∫ÁªèÊµãËØï‰ºöË¢´CloudflareÊã¶Êà™ÔºåÊó†Ê≥ïÁõ¥Êé•ËÆøÈóÆclaude api</li></ul><ul></ul><ul></ul><p>Êé®Ëçê‰ΩøÁî®ÁÆ°ÁêÜËÑöÊú¨ËøõË°å‰∏ÄÈîÆÈÉ®ÁΩ≤ÔºåÁÆÄÂçïÂø´Êç∑ÔºåËá™Âä®Â§ÑÁêÜÊâÄÊúâ‰æùËµñÂíåÈÖçÁΩÆ„ÄÇ</p><pre><code>curl -fsSL https://pincc.ai/manage.sh -o manage.sh &amp;&amp; chmod +x manage.sh &amp;&amp; ./manage.sh install\n</code></pre><ul><li>‚úÖ : Ëá™Âä®Ê£ÄÊµãÁ≥ªÁªüÁéØÂ¢ÉÔºåÂÆâË£Ö Node.js 18+„ÄÅRedis Á≠â‰æùËµñ</li><li>‚úÖ : ÂèãÂ•ΩÁöÑÈÖçÁΩÆÂêëÂØºÔºåËÆæÁΩÆÁ´ØÂè£„ÄÅRedis ËøûÊé•Á≠â</li><li>‚úÖ : ÂÆâË£ÖÂÆåÊàêÂêéËá™Âä®ÂêØÂä®ÊúçÂä°Âπ∂ÊòæÁ§∫ËÆøÈóÆÂú∞ÂùÄ</li></ul><pre><code>crs install   # ÂÆâË£ÖÊúçÂä°\ncrs start     # ÂêØÂä®ÊúçÂä°\ncrs stop      # ÂÅúÊ≠¢ÊúçÂä°\ncrs restart   # ÈáçÂêØÊúçÂä°\ncrs status    # Êü•ÁúãÁä∂ÊÄÅ\ncrs update    # Êõ¥Êñ∞ÊúçÂä°\ncrs uninstall # Âç∏ËΩΩÊúçÂä°\n</code></pre><pre><code>$ crs install\n\n# ‰ºö‰æùÊ¨°ËØ¢ÈóÆÔºö\nÂÆâË£ÖÁõÆÂΩï (ÈªòËÆ§: ~/claude-relay-service):\nÊúçÂä°Á´ØÂè£ (ÈªòËÆ§: 3000): 8080\nRedis Âú∞ÂùÄ (ÈªòËÆ§: localhost):\nRedis Á´ØÂè£ (ÈªòËÆ§: 6379):\nRedis ÂØÜÁ†Å (ÈªòËÆ§: Êó†ÂØÜÁ†Å):\n\n# ÂÆâË£ÖÂÆåÊàêÂêéËá™Âä®ÂêØÂä®Âπ∂ÊòæÁ§∫Ôºö\nÊúçÂä°Â∑≤ÊàêÂäüÂÆâË£ÖÂπ∂ÂêØÂä®ÔºÅ\n\nËÆøÈóÆÂú∞ÂùÄÔºö\n  Êú¨Âú∞ Web: http://localhost:8080/web\n  ÂÖ¨ÁΩë Web: http://YOUR_IP:8080/web\n\nÁÆ°ÁêÜÂëòË¥¶Âè∑‰ø°ÊÅØÂ∑≤‰øùÂ≠òÂà∞: data/init.json\n</code></pre><ul><li>ÊîØÊåÅÁ≥ªÁªü: Ubuntu/Debian„ÄÅCentOS/RedHat„ÄÅArch Linux„ÄÅmacOS</li></ul><pre><code># ÂÆâË£ÖNode.js\ncurl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\nsudo apt-get install -y nodejs\n\n# ÂÆâË£ÖRedis\nsudo apt update\nsudo apt install redis-server\nsudo systemctl start redis-server\n</code></pre><pre><code># ÂÆâË£ÖNode.js\ncurl -fsSL https://rpm.nodesource.com/setup_18.x | sudo bash -\nsudo yum install -y nodejs\n\n# ÂÆâË£ÖRedis\nsudo yum install redis\nsudo systemctl start redis\n</code></pre><pre><code># ‰∏ãËΩΩÈ°πÁõÆ\ngit clone https://github.com/Wei-Shaw//claude-relay-service.git\ncd claude-relay-service\n\n# ÂÆâË£Ö‰æùËµñ\nnpm install\n\n# Â§çÂà∂ÈÖçÁΩÆÊñá‰ª∂ÔºàÈáçË¶ÅÔºÅÔºâ\ncp config/config.example.js config/config.js\ncp .env.example .env\n</code></pre><pre><code># Ëøô‰∏§‰∏™ÂØÜÈí•Èöè‰æøÁîüÊàêÔºå‰ΩÜË¶ÅËÆ∞‰Ωè\nJWT_SECRET=‰Ω†ÁöÑË∂ÖÁ∫ßÁßòÂØÜÂØÜÈí•\nENCRYPTION_KEY=32‰ΩçÁöÑÂä†ÂØÜÂØÜÈí•Èöè‰æøÂÜô\n\n# RedisÈÖçÁΩÆ\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=\n\n</code></pre><pre><code>module.exports = {\n  server: {\n    port: 3000, // ÊúçÂä°Á´ØÂè£ÔºåÂèØ‰ª•Êîπ\n    host: '0.0.0.0' // ‰∏çÁî®Êîπ\n  },\n  redis: {\n    host: '127.0.0.1', // RedisÂú∞ÂùÄ\n    port: 6379 // RedisÁ´ØÂè£\n  }\n  // ÂÖ∂‰ªñÈÖçÁΩÆ‰øùÊåÅÈªòËÆ§Â∞±Ë°å\n}\n</code></pre><pre><code># ÂÆâË£ÖÂâçÁ´Ø‰æùËµñ\nnpm run install:web\n\n# ÊûÑÂª∫ÂâçÁ´ØÔºàÁîüÊàê dist ÁõÆÂΩïÔºâ\nnpm run build:web\n</code></pre><pre><code># ÂàùÂßãÂåñ\nnpm run setup # ‰ºöÈöèÊú∫ÁîüÊàêÂêéÂè∞Ë¥¶Âè∑ÂØÜÁ†Å‰ø°ÊÅØÔºåÂ≠òÂÇ®Âú® data/init.json\n# ÊàñËÄÖÈÄöËøáÁéØÂ¢ÉÂèòÈáèÈ¢ÑËÆæÁÆ°ÁêÜÂëòÂá≠ÊçÆÔºö\n# export ADMIN_USERNAME=cr_admin_custom\n# export ADMIN_PASSWORD=your-secure-password\n\n# ÂêØÂä®ÊúçÂä°\nnpm run service:start:daemon   # ÂêéÂè∞ËøêË°å\n\n# Êü•ÁúãÁä∂ÊÄÅ\nnpm run service:status\n</code></pre><h4>Á¨¨‰∏ÄÊ≠•Ôºö‰∏ãËΩΩÊûÑÂª∫docker-compose.ymlÊñá‰ª∂ÁöÑËÑöÊú¨Âπ∂ÊâßË°å</h4><pre><code>curl -fsSL https://pincc.ai/crs-compose.sh -o crs-compose.sh &amp;&amp; chmod +x crs-compose.sh &amp;&amp; ./crs-compose.sh\n</code></pre><ul></ul><ul><li>: JWTÂØÜÈí•ÔºåËá≥Â∞ë32‰∏™Â≠óÁ¨¶</li><li>: Âä†ÂØÜÂØÜÈí•ÔºåÂøÖÈ°ªÊòØ32‰∏™Â≠óÁ¨¶</li></ul><ul><li>: ÁÆ°ÁêÜÂëòÁî®Êà∑ÂêçÔºà‰∏çËÆæÁΩÆÂàôËá™Âä®ÁîüÊàêÔºâ</li><li>: ÁÆ°ÁêÜÂëòÂØÜÁ†ÅÔºà‰∏çËÆæÁΩÆÂàôËá™Âä®ÁîüÊàêÔºâ</li></ul><ol><li><pre><code>docker logs claude-relay-service\n</code></pre></li><li><pre><code># Âú® .env Êñá‰ª∂‰∏≠ËÆæÁΩÆ\nADMIN_USERNAME=cr_admin_custom\nADMIN_PASSWORD=your-secure-password\n</code></pre></li></ol><p>ÊµèËßàÂô®ËÆøÈóÆÔºö</p><ul><li>ÁéØÂ¢ÉÂèòÈáèÈ¢ÑËÆæÔºöÈÄöËøá ADMIN_USERNAME Âíå ADMIN_PASSWORD ËÆæÁΩÆ</li><li>Docker ÈÉ®ÁΩ≤ÔºöÊü•ÁúãÂÆπÂô®Êó•Âøó <code>docker logs claude-relay-service</code></li></ul><ol><li>Â¶ÇÊûú‰Ω†ÊãÖÂøÉÂ§ö‰∏™Ë¥¶Âè∑ÂÖ±Áî®1‰∏™IPÊÄïË¢´Â∞ÅÁ¶ÅÔºåÂèØ‰ª•ÈÄâÊã©ËÆæÁΩÆÈùôÊÄÅ‰ª£ÁêÜIPÔºàÂèØÈÄâÔºâ</li></ol><ol><li>ËÆæÁΩÆ‰ΩøÁî®ÈôêÂà∂ÔºàÂèØÈÄâÔºâÔºö \n  <ul><li>: ÈôêÂà∂ÊØè‰∏™Êó∂Èó¥Á™óÂè£ÁöÑËØ∑Ê±ÇÊ¨°Êï∞ÂíåToken‰ΩøÁî®Èáè</li><li>: ÈôêÂà∂Âè™ÂÖÅËÆ∏ÁâπÂÆöÂÆ¢Êà∑Á´Ø‰ΩøÁî®ÔºàÂ¶ÇClaudeCode„ÄÅGemini-CLIÁ≠âÔºâ</li></ul></li></ol><h3>4. ÂºÄÂßã‰ΩøÁî® Claude Code Âíå Gemini CLI</h3><pre><code>export ANTHROPIC_BASE_URL=\"http://127.0.0.1:3000/api/\" # Ê†πÊçÆÂÆûÈôÖÂ°´ÂÜô‰Ω†ÊúçÂä°Âô®ÁöÑipÂú∞ÂùÄÊàñËÄÖÂüüÂêç\nexport ANTHROPIC_AUTH_TOKEN=\"ÂêéÂè∞ÂàõÂª∫ÁöÑAPIÂØÜÈí•\"\n</code></pre><p>ÈÄÇÁî®‰∫éÈÄöËøá Antigravity Ê∏†ÈÅì‰ΩøÁî® Claude Ê®°ÂûãÔºàÂ¶Ç  Á≠âÔºâ„ÄÇ</p><pre><code># 1. ËÆæÁΩÆ Base URL ‰∏∫ Antigravity ‰∏ìÁî®Ë∑ØÂæÑ\nexport ANTHROPIC_BASE_URL=\"http://127.0.0.1:3000/antigravity/api/\"\n\n# 2. ËÆæÁΩÆ API KeyÔºàÂú®ÂêéÂè∞ÂàõÂª∫ÔºåÊùÉÈôêÈúÄÂåÖÂê´ 'all' Êàñ 'gemini'Ôºâ\nexport ANTHROPIC_AUTH_TOKEN=\"ÂêéÂè∞ÂàõÂª∫ÁöÑAPIÂØÜÈí•\"\n\n# 3. ÊåáÂÆöÊ®°ÂûãÂêçÁß∞ÔºàÁõ¥Êé•‰ΩøÁî®Áü≠ÂêçÔºåÊó†ÈúÄÂâçÁºÄÔºÅÔºâ\nexport ANTHROPIC_MODEL=\"claude-opus-4-5\"\n\n# 4. ÂêØÂä®\nclaude\n</code></pre><p>Â¶ÇÊûú‰ΩøÁî® VSCode ÁöÑ Claude Êèí‰ª∂ÔºåÈúÄË¶ÅÂú®  Êñá‰ª∂‰∏≠ÈÖçÁΩÆÔºö</p><pre><code>{\n    \"primaryApiKey\": \"crs\"\n}\n</code></pre><p>Â¶ÇÊûúËØ•Êñá‰ª∂‰∏çÂ≠òÂú®ÔºåËØ∑ÊâãÂä®ÂàõÂª∫„ÄÇWindows Áî®Êà∑Ë∑ØÂæÑ‰∏∫ <code>C:\\Users\\‰Ω†ÁöÑÁî®Êà∑Âêç\\.claude\\config.json</code>„ÄÇ</p><blockquote><p>üí° Ôºö<a href=\"https://github.com/touwaeriol/claude-code-plus\">Claude Code Plus</a> - Â∞Ü Claude Code Áõ¥Êé•ÈõÜÊàêÂà∞ IDEÔºåÊîØÊåÅ‰ª£Á†ÅÁêÜËß£„ÄÅÊñá‰ª∂ËØªÂÜô„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÇÊèí‰ª∂Â∏ÇÂú∫ÊêúÁ¥¢  Âç≥ÂèØÂÆâË£Ö„ÄÇ</p></blockquote><p><strong>ÊñπÂºè‰∏ÄÔºàÊé®ËçêÔºâÔºöÈÄöËøá Gemini Assist API ÊñπÂºèËÆøÈóÆ</strong></p><pre><code>CODE_ASSIST_ENDPOINT=\"http://127.0.0.1:3000/gemini\"  # Ê†πÊçÆÂÆûÈôÖÂ°´ÂÜô‰Ω†ÊúçÂä°Âô®ÁöÑipÂú∞ÂùÄÊàñËÄÖÂüüÂêç\nGOOGLE_CLOUD_ACCESS_TOKEN=\"ÂêéÂè∞ÂàõÂª∫ÁöÑAPIÂØÜÈí•\"\nGOOGLE_GENAI_USE_GCA=\"true\"\nGEMINI_MODEL=\"gemini-2.5-pro\" # Â¶ÇÊûú‰Ω†Êúâgemini3ÊùÉÈôêÂèØ‰ª•Â°´Ôºö gemini-3-pro-preview\n</code></pre><blockquote><p>ÔºöÂè™ËÉΩÈÄâ  ËøõË°åËÆ§ËØÅÔºåÂ¶ÇÊûúË∑≥ GoogleËØ∑Âà†Èô§  ÂêéÂÜçÂ∞ùËØïÂêØÂä®„ÄÇÔºögemini-cli ÊéßÂà∂Âè∞‰ºöÊèêÁ§∫ <code>Failed to fetch user info: 401 Unauthorized</code>Ôºå‰ΩÜ‰ΩøÁî®‰∏çÂèó‰ªª‰ΩïÂΩ±Âìç„ÄÇ</p></blockquote><pre><code>GOOGLE_GEMINI_BASE_URL=\"http://127.0.0.1:3000/gemini\"  # Ê†πÊçÆÂÆûÈôÖÂ°´ÂÜô‰Ω†ÊúçÂä°Âô®ÁöÑipÂú∞ÂùÄÊàñËÄÖÂüüÂêç\nGEMINI_API_KEY=\"ÂêéÂè∞ÂàõÂª∫ÁöÑAPIÂØÜÈí•\"\nGEMINI_MODEL=\"gemini-2.5-pro\" # Â¶ÇÊûú‰Ω†Êúâgemini3ÊùÉÈôêÂèØ‰ª•Â°´Ôºö gemini-3-pro-preview\n</code></pre><blockquote><p>ÔºöÂè™ËÉΩÈÄâ  ËøõË°åËÆ§ËØÅÔºåÂ¶ÇÊûúÊèêÁ§∫  ËØ∑Áõ¥Êé•ÁïôÁ©∫ÊåâÂõûËΩ¶„ÄÇÂ¶ÇÊûú‰∏ÄÊâìÂºÄÂ∞±Ë∑≥ GoogleËØ∑Âà†Èô§  ÂêéÂÜçÂ∞ùËØïÂêØÂä®„ÄÇ</p></blockquote><pre><code>gemini  # ÊàñÂÖ∂‰ªñ Gemini CLI ÂëΩ‰ª§\n</code></pre><p>Âú®  Êñá‰ª∂Ê∑ªÂä†‰ª•‰∏ãÈÖçÁΩÆÔºö</p><pre><code>model_provider = \"crs\"\nmodel = \"gpt-5.1-codex-max\"\nmodel_reasoning_effort = \"high\"\ndisable_response_storage = true\npreferred_auth_method = \"apikey\"\n\n[model_providers.crs]\nname = \"crs\"\nbase_url = \"http://127.0.0.1:3000/openai\"  # Ê†πÊçÆÂÆûÈôÖÂ°´ÂÜô‰Ω†ÊúçÂä°Âô®ÁöÑipÂú∞ÂùÄÊàñËÄÖÂüüÂêç\nwire_api = \"responses\"\nrequires_openai_auth = true\n</code></pre><p>Âú®  Êñá‰ª∂‰∏≠ÈÖçÁΩÆAPIÂØÜÈí•‰∏∫ nullÔºö</p><pre><code>{\n    \"OPENAI_API_KEY\": \"ÂêéÂè∞ÂàõÂª∫ÁöÑAPIÂØÜÈí•\"  \n}\n</code></pre><blockquote><p>‚ö†Ô∏è Âú®ÈÄöËøá Nginx ÂèçÂêë‰ª£ÁêÜ CRS ÊúçÂä°Âπ∂‰ΩøÁî® Codex CLI Êó∂ÔºåÈúÄË¶ÅÂú® http Âùó‰∏≠Ê∑ªÂä† underscores_in_headers on;„ÄÇÂõ†‰∏∫ Nginx ÈªòËÆ§‰ºöÁßªÈô§Â∏¶‰∏ãÂàíÁ∫øÁöÑËØ∑Ê±ÇÂ§¥ÔºàÂ¶Ç session_idÔºâÔºå‰∏ÄÊó¶ËØ•Â§¥Ë¢´‰∏¢ÂºÉÔºåÂ§öË¥¶Âè∑ÁéØÂ¢É‰∏ãÁöÑÁ≤òÊÄß‰ºöËØùÂäüËÉΩÂ∞ÜÂ§±Êïà„ÄÇ</p></blockquote><p>Droid CLI ËØªÂèñ „ÄÇÂèØ‰ª•Âú®ËØ•Êñá‰ª∂‰∏≠Ê∑ªÂä†Ëá™ÂÆö‰πâÊ®°Âûã‰ª•ÊåáÂêëÊú¨ÊúçÂä°ÁöÑÊñ∞Á´ØÁÇπÔºö</p><pre><code>{\n  \"custom_models\": [\n    {\n      \"model_display_name\": \"Opus 4.5 [crs]\",\n      \"model\": \"claude-opus-4-5-20251101\",\n      \"base_url\": \"http://127.0.0.1:3000/droid/claude\",\n      \"api_key\": \"ÂêéÂè∞ÂàõÂª∫ÁöÑAPIÂØÜÈí•\",\n      \"provider\": \"anthropic\",\n      \"max_tokens\": 64000\n    },\n    {\n      \"model_display_name\": \"GPT5-Codex [crs]\",\n      \"model\": \"gpt-5-codex\",\n      \"base_url\": \"http://127.0.0.1:3000/droid/openai\",\n      \"api_key\": \"ÂêéÂè∞ÂàõÂª∫ÁöÑAPIÂØÜÈí•\",\n      \"provider\": \"openai\",\n      \"max_tokens\": 16384\n    },\n    {\n      \"model_display_name\": \"Gemini-3-Pro [crs]\",\n      \"model\": \"gemini-3-pro-preview\",\n      \"base_url\": \"http://127.0.0.1:3000/droid/comm/v1/\",\n      \"api_key\": \"ÂêéÂè∞ÂàõÂª∫ÁöÑAPIÂØÜÈí•\",\n      \"provider\": \"generic-chat-completion-api\",\n      \"max_tokens\": 65535\n    },\n    {\n      \"model_display_name\": \"GLM-4.6 [crs]\",\n      \"model\": \"glm-4.6\",\n      \"base_url\": \"http://127.0.0.1:3000/droid/comm/v1/\",\n      \"api_key\": \"ÂêéÂè∞ÂàõÂª∫ÁöÑAPIÂØÜÈí•\",\n      \"provider\": \"generic-chat-completion-api\",\n      \"max_tokens\": 202800\n    }\n  ]\n}\n</code></pre><blockquote><p>üí° Â∞ÜÁ§∫‰æã‰∏≠ÁöÑ  ÊõøÊç¢‰∏∫‰Ω†ÁöÑÊúçÂä°ÂüüÂêçÊàñÂÖ¨ÁΩëÂú∞ÂùÄÔºåÂπ∂ÂÜôÂÖ•ÂêéÂè∞ÁîüÊàêÁöÑ API ÂØÜÈí•Ôºàcr_ ÂºÄÂ§¥Ôºâ„ÄÇ</p></blockquote><p>Êú¨ÊúçÂä°ÊîØÊåÅÂ§öÁßçAPIÁ´ØÁÇπÊ†ºÂºèÔºåÊñπ‰æøÊé•ÂÖ•‰∏çÂêåÁöÑÁ¨¨‰∏âÊñπÂ∑•ÂÖ∑ÔºàÂ¶ÇCherry StudioÁ≠âÔºâ„ÄÇ</p><p>Cherry StudioÊîØÊåÅÂ§öÁßçAIÊúçÂä°ÁöÑÊé•ÂÖ•Ôºå‰∏ãÈù¢ÊòØ‰∏çÂêåË¥¶Âè∑Á±ªÂûãÁöÑËØ¶ÁªÜÈÖçÁΩÆÔºö</p><pre><code># APIÂú∞ÂùÄ\nhttp://‰Ω†ÁöÑÊúçÂä°Âô®:3000/claude\n\n# Ê®°ÂûãIDÁ§∫‰æã\nclaude-sonnet-4-5-20250929 # Claude Sonnet 4.5\nclaude-opus-4-20250514     # Claude Opus 4\n</code></pre><ul><li>APIÂú∞ÂùÄÂ°´ÂÖ•Ôºö</li><li>API KeyÂ°´ÂÖ•ÔºöÂêéÂè∞ÂàõÂª∫ÁöÑAPIÂØÜÈí•Ôºàcr_ÂºÄÂ§¥Ôºâ</li></ul><pre><code># APIÂú∞ÂùÄ\nhttp://‰Ω†ÁöÑÊúçÂä°Âô®:3000/gemini\n\n# Ê®°ÂûãIDÁ§∫‰æã\ngemini-2.5-pro             # Gemini 2.5 Pro\n</code></pre><ul><li>APIÂú∞ÂùÄÂ°´ÂÖ•Ôºö</li><li>API KeyÂ°´ÂÖ•ÔºöÂêéÂè∞ÂàõÂª∫ÁöÑAPIÂØÜÈí•Ôºàcr_ÂºÄÂ§¥Ôºâ</li></ul><pre><code># APIÂú∞ÂùÄ\nhttp://‰Ω†ÁöÑÊúçÂä°Âô®:3000/openai\n\n# Ê®°ÂûãIDÔºàÂõ∫ÂÆöÔºâ\ngpt-5                      # Codex‰ΩøÁî®Âõ∫ÂÆöÊ®°ÂûãID\n</code></pre><ul><li>APIÂú∞ÂùÄÂ°´ÂÖ•Ôºö</li><li>API KeyÂ°´ÂÖ•ÔºöÂêéÂè∞ÂàõÂª∫ÁöÑAPIÂØÜÈí•Ôºàcr_ÂºÄÂ§¥Ôºâ</li><li>ÔºöCodexÂè™ÊîØÊåÅOpenai-ResponseÊ†áÂáÜ</li></ul><ul><li>‚úÖ ÔºöÔºà‰∏çÂä†ÁªìÂ∞æ ÔºåËÆ© Cherry Studio Ëá™Âä®Âä†‰∏ä v1Ôºâ</li><li>‚úÖ Ôºö<code>http://‰Ω†ÁöÑÊúçÂä°Âô®:3000/claude/v1/</code>ÔºàÊâãÂä®ÊåáÂÆö v1 Âπ∂Âä†ÁªìÂ∞æ Ôºâ</li><li>üí° ÔºöËøô‰∏§ÁßçÊ†ºÂºèÂú® Cherry Studio ‰∏≠ÊòØÂÆåÂÖ®Á≠âÊïàÁöÑ</li><li>‚ùå Ôºö<code>http://‰Ω†ÁöÑÊúçÂä°Âô®:3000/claude/</code>ÔºàÂçïÁã¨ÁöÑ  ÁªìÂ∞æ‰ºöË¢´ Cherry Studio ÂøΩÁï• v1 ÁâàÊú¨Ôºâ</li></ul><ul><li>ÊâÄÊúâË¥¶Âè∑Á±ªÂûãÈÉΩ‰ΩøÁî®Áõ∏ÂêåÁöÑAPIÂØÜÈí•ÔºàÂú®ÂêéÂè∞Áªü‰∏ÄÂàõÂª∫Ôºâ</li><li> - ‰ΩøÁî®AntigravityË¥¶Âè∑Ê±†ÔºàÊé®ËçêÁî®‰∫éClaude CodeÔºâ</li><li> - ‰ΩøÁî®DroidÁ±ªÂûãClaudeË¥¶Âè∑Ê±†ÔºàÂè™Âª∫ËÆÆapiË∞ÉÁî®ÊàñDroid Cli‰∏≠‰ΩøÁî®Ôºâ</li><li> - ‰ΩøÁî®CodexË¥¶Âè∑ÔºàÂè™ÊîØÊåÅOpenai-ResponseÊ†ºÂºèÔºâ</li><li> - ‰ΩøÁî®DroidÁ±ªÂûãOpenAIÂÖºÂÆπË¥¶Âè∑Ê±†ÔºàÂè™Âª∫ËÆÆapiË∞ÉÁî®ÊàñDroid Cli‰∏≠‰ΩøÁî®Ôºâ</li><li>ÊîØÊåÅÊâÄÊúâÊ†áÂáÜAPIÁ´ØÁÇπÔºàmessages„ÄÅmodelsÁ≠âÔºâ</li></ul><ul><li>Á°Æ‰øùÂú®ÂêéÂè∞Â∑≤Ê∑ªÂä†ÂØπÂ∫îÁ±ªÂûãÁöÑË¥¶Âè∑ÔºàClaude/Gemini/CodexÔºâ</li><li>APIÂØÜÈí•ÂèØ‰ª•ÈÄöÁî®ÔºåÁ≥ªÁªü‰ºöÊ†πÊçÆË∑ØÁî±Ëá™Âä®ÈÄâÊã©Ë¥¶Âè∑Á±ªÂûã</li></ul><pre><code># Êü•ÁúãÊúçÂä°Áä∂ÊÄÅ\nnpm run service:status\n\n# Êü•ÁúãÊó•Âøó\nnpm run service:logs\n\n# ÈáçÂêØÊúçÂä°\nnpm run service:restart:daemon\n\n# ÂÅúÊ≠¢ÊúçÂä°\nnpm run service:stop\n</code></pre><ul><li>:  - Êü•Áúã‰ΩøÁî®ÁªüËÆ°</li><li>:  - Á°ÆËÆ§ÊúçÂä°Ê≠£Â∏∏</li></ul><pre><code># 1. ËøõÂÖ•È°πÁõÆÁõÆÂΩï\ncd claude-relay-service\n\n# 2. ÊãâÂèñÊúÄÊñ∞‰ª£Á†Å\ngit pull origin main\n\n# Â¶ÇÊûúÈÅáÂà∞ package-lock.json ÂÜ≤Á™ÅÔºå‰ΩøÁî®ËøúÁ®ãÁâàÊú¨\ngit checkout --theirs package-lock.json\ngit add package-lock.json\n\n# 3. ÂÆâË£ÖÊñ∞ÁöÑ‰æùËµñÔºàÂ¶ÇÊûúÊúâÔºâ\nnpm install\n\n# 4. ÂÆâË£ÖÂπ∂ÊûÑÂª∫ÂâçÁ´Ø\nnpm run install:web\nnpm run build:web\n\n# 5. ÈáçÂêØÊúçÂä°\nnpm run service:restart:daemon\n\n# 6. Ê£ÄÊü•ÊúçÂä°Áä∂ÊÄÅ\nnpm run service:status\n</code></pre><ul><li>ÂçáÁ∫ßÂâçÂª∫ËÆÆÂ§á‰ªΩÈáçË¶ÅÈÖçÁΩÆÊñá‰ª∂Ôºà.env, config/config.jsÔºâ</li></ul><p>ÂÆ¢Êà∑Á´ØÈôêÂà∂ÂäüËÉΩÂÖÅËÆ∏‰Ω†ÊéßÂà∂ÊØè‰∏™API KeyÂèØ‰ª•Ë¢´Âì™‰∫õÂÆ¢Êà∑Á´Ø‰ΩøÁî®ÔºåÈÄöËøáUser-AgentËØÜÂà´ÂÆ¢Êà∑Á´ØÔºåÊèêÈ´òAPIÁöÑÂÆâÂÖ®ÊÄß„ÄÇ</p><ol><li><ul></ul></li><li><ul><li>: ÂÆòÊñπClaude CLIÔºàÂåπÈÖç <code>claude-cli/x.x.x (external, cli)</code> Ê†ºÂºèÔºâ</li><li>: GeminiÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑ÔºàÂåπÈÖç <code>GeminiCLI/vx.x.x (platform; arch)</code> Ê†ºÂºèÔºâ</li></ul></li><li><ul><li>ÈÄöËøáÊó•ÂøóÂèØ‰ª•Êü•ÁúãÂÆûÈôÖÁöÑUser-AgentÊ†ºÂºèÔºåÊñπ‰æøÈÖçÁΩÆËá™ÂÆö‰πâÂÆ¢Êà∑Á´Ø</li></ul></li></ol><pre><code>üîì Authenticated request from key: ÊµãËØïKey (key-id) in 5ms\n   User-Agent: \"claude-cli/1.0.58 (external, cli)\"\n</code></pre><pre><code>üîç Checking client restriction for key: key-id (ÊµãËØïKey)\n   User-Agent: \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n   Allowed clients: claude_code, gemini_cli\nüö´ Client restriction failed for key: key-id (ÊµãËØïKey) from 127.0.0.1, User-Agent: Mozilla/5.0...\n</code></pre><pre><code># Ê£ÄÊü•RedisÊòØÂê¶ÂêØÂä®\nredis-cli ping\n\n# Â∫îËØ•ËøîÂõû PONG\n</code></pre><ul></ul><ul></ul><p>Âú®Áîü‰∫ßÁéØÂ¢É‰∏≠ÔºåÂª∫ËÆÆÈÄöËøáÂèçÂêë‰ª£ÁêÜËøõË°åËøûÊé•Ôºå‰ª•‰æø‰ΩøÁî®Ëá™Âä® HTTPS„ÄÅÂÆâÂÖ®Â§¥ÈÉ®ÂíåÊÄßËÉΩ‰ºòÂåñ„ÄÇ‰∏ãÈù¢Êèê‰æõ‰∏§ÁßçÂ∏∏Áî®ÊñπÊ°àÔºö  Âíå <strong>Nginx Proxy Manager (NPM)</strong>„ÄÇ</p><p>Caddy ÊòØ‰∏ÄÊ¨æËá™Âä®ÁÆ°ÁêÜ HTTPS ËØÅ‰π¶ÁöÑ Web ÊúçÂä°Âô®ÔºåÈÖçÁΩÆÁÆÄÂçï„ÄÅÊÄßËÉΩ‰ºòÁßÄÔºåÂæàÈÄÇÂêà‰∏çÈúÄË¶Å Docker ÁéØÂ¢ÉÁöÑÈÉ®ÁΩ≤ÊñπÊ°à„ÄÇ</p><pre><code># Ubuntu/Debian\nsudo apt install -y debian-keyring debian-archive-keyring apt-transport-https\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/gpg.key' | sudo gpg --dearmor -o /usr/share/keyrings/caddy-stable-archive-keyring.gpg\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/debian.deb.txt' | sudo tee /etc/apt/sources.list.d/caddy-stable.list\nsudo apt update\nsudo apt install caddy\n\n# CentOS/RHEL/Fedora\nsudo yum install yum-plugin-copr\nsudo yum copr enable @caddy/caddy\nsudo yum install caddy\n</code></pre><pre><code>your-domain.com {\n    # ÂèçÂêë‰ª£ÁêÜÂà∞Êú¨Âú∞ÊúçÂä°\n    reverse_proxy 127.0.0.1:3000 {\n        # ÊîØÊåÅÊµÅÂºèÂìçÂ∫îÊàñ SSE\n        flush_interval -1\n\n        # ‰º†ÈÄíÁúüÂÆû IP\n        header_up X-Real-IP {remote_host}\n        header_up X-Forwarded-For {remote_host}\n        header_up X-Forwarded-Proto {scheme}\n\n        # ÈïøËØª/ÂÜôË∂ÖÊó∂ÈÖçÁΩÆ\n        transport http {\n            read_timeout 300s\n            write_timeout 300s\n            dial_timeout 30s\n        }\n    }\n\n    # ÂÆâÂÖ®Â§¥ÈÉ®\n    header {\n        Strict-Transport-Security \"max-age=31536000; includeSubDomains\"\n        X-Frame-Options \"DENY\"\n        X-Content-Type-Options \"nosniff\"\n        -Server\n    }\n}\n</code></pre><pre><code>sudo caddy validate --config /etc/caddy/Caddyfile\nsudo systemctl start caddy\nsudo systemctl enable caddy\nsudo systemctl status caddy\n</code></pre><p>Caddy ‰ºöËá™Âä®ÁÆ°ÁêÜ HTTPSÔºåÂõ†Ê≠§ÂèØ‰ª•Â∞ÜÊúçÂä°ÈôêÂà∂Âú®Êú¨Âú∞ËøõË°åÁõëÂê¨Ôºö</p><pre><code>// config/config.js\nmodule.exports = {\n  server: {\n    port: 3000,\n    host: '127.0.0.1' // Âè™ÁõëÂê¨Êú¨Âú∞\n  }\n}\n</code></pre><ul></ul><h2>Nginx Proxy Manager (NPM) ÊñπÊ°à</h2><p>Nginx Proxy Manager ÈÄöËøáÂõæÂΩ¢ÂåñÁïåÈù¢ÁÆ°ÁêÜÂèçÂêë‰ª£ÁêÜÂíå HTTPS ËØÅ‰π¶Ôºå‰∏¶‰ª• Docker ÂÆπÂô®ÈÉ®ÁΩ≤„ÄÇ</p><table><tbody><tr></tr><tr><td>192.168.0.1 (docker Êú∫Âô® IP)</td></tr><tr></tr></tbody></table><blockquote><ul><li>ËØ∑Á°Æ‰øù Claude Relay Service <strong>ÁõëÂê¨ host ‰∏∫  „ÄÅÂÆπÂô® IP ÊàñÊú¨Êú∫ IP</strong>Ôºå‰ª•‰æø NPM ÂÆûÁé∞ÂÜÖÁΩëËøûÊé•„ÄÇ</li><li><strong>Websockets Support Âíå Cache Assets ÂøÖÈ°ªÂÖ≥Èó≠</strong>ÔºåÂê¶Âàô‰ºöÂØºËá¥ SSE / ÊµÅÂºèÂìçÂ∫îÂ§±Ë¥•„ÄÇ</li></ul></blockquote><ul><li>: Request a new SSL Certificate (Let's Encrypt) ÊàñÂ∑≤ÊúâËØÅ‰π¶</li></ul><p>Custom Nginx Configuration ‰∏≠Ê∑ªÂä†‰ª•‰∏ãÂÜÖÂÆπÔºö</p><pre><code># ‰º†ÈÄíÁúüÂÆûÁî®Êà∑ IP\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header X-Forwarded-Proto $scheme;\n\n# ÊîØÊåÅ WebSocket / SSE Á≠âÊµÅÂºèÈÄö‰ø°\nproxy_http_version 1.1;\nproxy_set_header Upgrade $http_upgrade;\nproxy_set_header Connection \"upgrade\";\nproxy_buffering off;\n\n# ÈïøËøûÊé• / Ë∂ÖÊó∂ËÆæÁΩÆÔºàÈÄÇÂêà AI ËÅäÂ§©ÊµÅÂºè‰º†ËæìÔºâ\nproxy_read_timeout 300s;\nproxy_send_timeout 300s;\nproxy_connect_timeout 30s;\n\n# ---- ÂÆâÂÖ®ÊÄßËÆæÁΩÆ ----\n# ‰∏•Ê†º HTTPS Á≠ñÁï• (HSTS)\nadd_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n\n# ÈòªÊå°ÁÇπÂáªÂä´ÊåÅ‰∏éÂÜÖÂÆπÂóÖÊé¢\nadd_header X-Frame-Options \"DENY\" always;\nadd_header X-Content-Type-Options \"nosniff\" always;\n\n# Referrer / Permissions ÈôêÂà∂Á≠ñÁï•\nadd_header Referrer-Policy \"no-referrer-when-downgrade\" always;\nadd_header Permissions-Policy \"camera=(), microphone=(), geolocation=()\" always;\n\n# ÈöêËóèÊúçÂä°Âô®‰ø°ÊÅØÔºàÁ≠âÊïà‰∫é Caddy ÁöÑ `-Server`Ôºâ\nproxy_hide_header Server;\n\n# ---- ÊÄßËÉΩÂæÆË∞É ----\n# ÂÖ≥Èó≠‰ª£ÁêÜÁ´ØÁºìÂ≠òÔºåÁ°Æ‰øùÂç≥Êó∂ÂìçÂ∫îÔºàSSE / StreamingÔºâ\nproxy_cache_bypass $http_upgrade;\nproxy_no_cache $http_upgrade;\nproxy_request_buffering off;\n</code></pre><ul><li>‰øùÂ≠òÂêéÁ≠âÂæÖ NPM Ëá™Âä®Áî≥ËØ∑ Let's Encrypt ËØÅ‰π¶ÔºàÂ¶ÇÊûúÊúâÔºâ„ÄÇ</li><li>Dashboard ‰∏≠Êü•Áúã Proxy Host Áä∂ÊÄÅÔºåÁ°Æ‰øùÊòæÁ§∫‰∏∫ \"Online\"„ÄÇ</li><li>ËÆøÈóÆ <code>https://relay.example.com</code>ÔºåÂ¶ÇÊûúÊòæÁ§∫ÁªøËâ≤ÈîÅÂõæÊ†áÂç≥Ë°®Á§∫ HTTPS Ê≠£Â∏∏„ÄÇ</li></ul><ul></ul><ul><li>: ÂèØ‰ª•Áªô‰∏çÂêåÁöÑ‰∫∫ÂàÜÈÖç‰∏çÂêåÁöÑapikeyÔºåÂèØ‰ª•Ê†πÊçÆ‰∏çÂêåÁöÑapikeyÊù•ÂàÜÊûêÁî®Èáè</li></ul><ul><li>: Âº∫ÁÉàÂª∫ËÆÆ‰ΩøÁî®CaddyÂèçÂêë‰ª£ÁêÜÔºàËá™Âä®HTTPSÔºâÔºåÁ°Æ‰øùÊï∞ÊçÆ‰º†ËæìÂÆâÂÖ®</li><li>: Âè™ÂºÄÊîæÂøÖË¶ÅÁöÑÁ´ØÂè£Ôºà80, 443ÔºâÔºåÈöêËóèÁõ¥Êé•ÊúçÂä°Á´ØÂè£</li></ul><ol></ol><ul></ul><div align=\"center\"><p><strong>‚≠ê ËßâÂæóÊúâÁî®ÁöÑËØùÁªô‰∏™StarÂëóÔºåËøôÊòØÂØπ‰ΩúËÄÖÊúÄÂ§ßÁöÑÈºìÂä±ÔºÅ</strong></p></div>",
      "contentLength": 16762,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/4640fc9af8d47520d53673d5d22d9bbb5fc6b8a57ded6838383d4029010c23f5/Wei-Shaw/claude-relay-service",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "moeru-ai/airi",
      "url": "https://github.com/moeru-ai/airi",
      "date": 1772247153,
      "author": "",
      "guid": 49043,
      "unread": true,
      "content": "<p>üíñüß∏ Self hosted, you-owned Grok Companion, a container of souls of waifu, cyber livings to bring them into our worlds, wishing to achieve Neuro-sama's altitude. Capable of realtime voice chat, Minecraft, Factorio playing. Web / macOS / Windows supported.</p><p align=\"center\">Re-creating Neuro-sama, a soul container of AI waifu / virtual characters to bring them into our world.</p><blockquote><p>[!WARNING]  We  have any officially minted cryptocurrency or token associated with this project. Please check the information and proceed with caution.</p></blockquote><blockquote><p>We've got a whole dedicated organization <a href=\"https://github.com/proj-airi\">@proj-airi</a> for all the sub-projects born from Project AIRI. Check it out!</p><p>RAG, memory system, embedded database, icons, Live2D utilities, and more!</p></blockquote><blockquote><p>[!TIP] We have a translation project on <a href=\"https://crowdin.com/project/proj-airi\">Crowdin</a>. If you find any inaccurate translations, feel free to contribute improvements there. <a href=\"https://crowdin.com/project/proj-airi\" target=\"_blank\" rel=\"nofollow\"><img src=\"https://badges.crowdin.net/badge/light/crowdin-on-dark.png\" srcset=\"https://badges.crowdin.net/badge/light/crowdin-on-dark.png 1x, https://badges.crowdin.net/badge/light/crowdin-on-dark@2x.png 2x\" alt=\"Crowdin | Agile localization for tech companies\" width=\"140\" height=\"40\"></a></p></blockquote><p>Have you dreamed about having a cyber living being (cyber waifu, digital pet) or digital companion that could play with and talk to you?</p><p>With the power of modern large language models like <a href=\"https://chatgpt.com\">ChatGPT</a> and famous <a href=\"https://claude.ai\">Claude</a>, asking a virtual being to roleplay and chat with us is already easy enough for everyone. Platforms like <a href=\"https://character.ai\">Character.ai (a.k.a. c.ai)</a> and <a href=\"https://janitorai.com/\">JanitorAI</a> as well as local playgrounds like <a href=\"https://github.com/SillyTavern/SillyTavern\">SillyTavern</a> are already good-enough solutions for a chat based or visual adventure game like experience.</p><blockquote><p>But, what about the abilities to play games? And see what you are coding at? Chatting while playing games, watching videos, and is capable of doing many other things.</p></blockquote><p>Perhaps you know <a href=\"https://www.youtube.com/@Neurosama\">Neuro-sama</a> already. She is currently the best virtual streamer capable of playing games, chatting, and interacting with you and the participants. Some also call this kind of being \"digital human.\" <strong>Sadly, as it's not open sourced, you cannot interact with her after her live streams go offline</strong>.</p><p>Therefore, this project, AIRI, offers another possibility here: <strong>let you own your digital life, cyber living, easily, anywhere, anytime</strong>.</p><h2>DevLogs We Posted &amp; Recent Updates</h2><h2>What's So Special About This Project?</h2><blockquote><p>[!TIP] Worrying about the performance drop since we are using Web related technologies?</p><p>Don't worry, while Web browser version is meant to give an insight about how much we can push and do inside browsers, and webviews, we will never fully rely on this, the desktop version of AIRI is capable of using native <a href=\"https://developer.nvidia.com/cuda-toolkit\">NVIDIA CUDA</a> and <a href=\"https://developer.apple.com/metal/\">Apple Metal</a> by default (thanks to HuggingFace &amp; beloved <a href=\"https://github.com/huggingface/candle\">candle</a> project), without any complex dependency managements, considering the tradeoff, it was partially powered by Web technologies for graphics, layouts, animations, and the WIP plugin systems for everyone to integrate things.</p></blockquote><p>This means that <strong>„Ç¢„Ç§„É™ is capable of running on modern browsers and devices</strong> and even on mobile devices (already done with PWA support). This brings a lot of possibilities for us (the developers) to build and extend the power of „Ç¢„Ç§„É™ VTuber to the next level, while still leaving the flexibilities for users to enable features that requires TCP connections or other non-Web technologies such as connecting to a Discord voice channel or playing Minecraft and Factorio with friends.</p><blockquote><p>We are still in the early stage of development where we are seeking out talented developers to join us and help us to make „Ç¢„Ç§„É™ a reality.</p><p>It's ok if you are not familiar with Vue.js, TypeScript, and devtools required for this project, you can join us as an artist, designer, or even help us to launch our first live stream.</p><p>Even if you are a big fan of React, Svelte or even Solid, we welcome you. You can open a sub-directory to add features that you want to see in „Ç¢„Ç§„É™, or would like to experiment with.</p><p>Fields (and related projects) that we are looking for:</p><ul></ul></blockquote><blockquote><p>[!NOTE] By default,  will start the development server for the Stage Web (browser version). If you would like to try developing the desktop version, please make sure you read <a href=\"https://raw.githubusercontent.com/moeru-ai/airi/main/.github/CONTRIBUTING.md\">CONTRIBUTING.md</a> to setup the environment correctly.</p></blockquote><h3>Stage Tamagotchi (Desktop Version)</h3><p>A Nix package for Tamagotchi is included. To run airi with Nix, first make sure to enable flakes, then run:</p><pre><code>nix run github:moeru-ai/airi\n</code></pre><h3>Stage Pocket (Mobile Version)</h3><p>Start the development server for the capacitor web version:</p><p>Check your IP address in the output of the command above:</p><pre><code>  ROLLDOWN-VITE v7.3.0  ready in 1073 ms\n\n  ‚ûú  Local:   https://localhost:5273/\n  ‚ûú  Network: https://&lt;ip-will-be-here&gt;:5273/\n  ‚ûú  Vue DevTools: Open https://localhost:5273/__devtools__/ as a separate window\n  ‚ûú  Vue DevTools: Press Option(‚å•)+Shift(‚áß)+D in App to toggle the Vue DevTools\n  ‚ûú  UnoCSS Inspector: https://localhost:5273/__unocss/\n</code></pre><pre><code>CAPACITOR_DEV_SERVER_URL=https://&lt;your-ip-address&gt;:5273 pnpm open:ios\n</code></pre><p>Then Xcode will open and you can click the \"Run\" button to run the app on your iPhone.</p><p>If you need to connect server channel on pocket in wireless mode, you need to start tamagotchi as root:</p><p>Then enable secure websocket in tamagotchi .</p><p>Please update the version in  after running :</p><pre><code>npx bumpp --no-commit --no-tag\n</code></pre><h2>Support of LLM API Providers (powered by <a href=\"https://github.com/moeru-ai/xsai\">xsai</a>)</h2><h2>Sub-projects Born from This Project</h2><pre><code>%%{ init: { 'flowchart': { 'curve': 'catmullRom' } } }%%\n\nflowchart TD\n  Core(\"Core\")\n  Unspeech(\"unspeech\")\n  DBDriver(\"@proj-airi/drizzle-duckdb-wasm\")\n  MemoryDriver(\"[WIP] Memory Alaya\")\n  DB1(\"@proj-airi/duckdb-wasm\")\n  SVRT(\"@proj-airi/server-runtime\")\n  Memory(\"Memory\")\n  STT(\"STT\")\n  Stage(\"Stage\")\n  StageUI(\"@proj-airi/stage-ui\")\n  UI(\"@proj-airi/ui\")\n\n  subgraph AIRI\n    DB1 --&gt; DBDriver --&gt; MemoryDriver --&gt; Memory --&gt; Core\n    UI --&gt; StageUI --&gt; Stage --&gt; Core\n    Core --&gt; STT\n    Core --&gt; SVRT\n  end\n\n  subgraph UI_Components\n    UI --&gt; StageUI\n    UITransitions(\"@proj-airi/ui-transitions\") --&gt; StageUI\n    UILoadingScreens(\"@proj-airi/ui-loading-screens\") --&gt; StageUI\n    FontCJK(\"@proj-airi/font-cjkfonts-allseto\") --&gt; StageUI\n    FontXiaolai(\"@proj-airi/font-xiaolai\") --&gt; StageUI\n  end\n\n  subgraph Apps\n    Stage --&gt; StageWeb(\"@proj-airi/stage-web\")\n    Stage --&gt; StageTamagotchi(\"@proj-airi/stage-tamagotchi\")\n    Core --&gt; RealtimeAudio(\"@proj-airi/realtime-audio\")\n    Core --&gt; PromptEngineering(\"@proj-airi/playground-prompt-engineering\")\n  end\n\n  subgraph Server_Components\n    Core --&gt; ServerSDK(\"@proj-airi/server-sdk\")\n    ServerShared(\"@proj-airi/server-shared\") --&gt; SVRT\n    ServerShared --&gt; ServerSDK\n  end\n\n  STT --&gt;|Speaking| Unspeech\n  SVRT --&gt;|Playing Factorio| F_AGENT\n  SVRT --&gt;|Playing Minecraft| MC_AGENT\n\n  subgraph Factorio_Agent\n    F_AGENT(\"Factorio Agent\")\n    F_API(\"Factorio RCON API\")\n    factorio-server(\"factorio-server\")\n    F_MOD1(\"autorio\")\n\n    F_AGENT --&gt; F_API -.-&gt; factorio-server\n    F_MOD1 -.-&gt; factorio-server\n  end\n\n  subgraph Minecraft_Agent\n    MC_AGENT(\"Minecraft Agent\")\n    Mineflayer(\"Mineflayer\")\n    minecraft-server(\"minecraft-server\")\n\n    MC_AGENT --&gt; Mineflayer -.-&gt; minecraft-server\n  end\n\n  XSAI(\"xsAI\") --&gt; Core\n  XSAI --&gt; F_AGENT\n  XSAI --&gt; MC_AGENT\n\n  Core --&gt; TauriMCP(\"@proj-airi/tauri-plugin-mcp\")\n  Memory_PGVector(\"@proj-airi/memory-pgvector\") --&gt; Memory\n\n  style Core fill:#f9d4d4,stroke:#333,stroke-width:1px\n  style AIRI fill:#fcf7f7,stroke:#333,stroke-width:1px\n  style UI fill:#d4f9d4,stroke:#333,stroke-width:1px\n  style Stage fill:#d4f9d4,stroke:#333,stroke-width:1px\n  style UI_Components fill:#d4f9d4,stroke:#333,stroke-width:1px\n  style Server_Components fill:#d4e6f9,stroke:#333,stroke-width:1px\n  style Apps fill:#d4d4f9,stroke:#333,stroke-width:1px\n  style Factorio_Agent fill:#f9d4f2,stroke:#333,stroke-width:1px\n  style Minecraft_Agent fill:#f9d4f2,stroke:#333,stroke-width:1px\n\n  style DBDriver fill:#f9f9d4,stroke:#333,stroke-width:1px\n  style MemoryDriver fill:#f9f9d4,stroke:#333,stroke-width:1px\n  style DB1 fill:#f9f9d4,stroke:#333,stroke-width:1px\n  style Memory fill:#f9f9d4,stroke:#333,stroke-width:1px\n  style Memory_PGVector fill:#f9f9d4,stroke:#333,stroke-width:1px\n</code></pre>",
      "contentLength": 7812,
      "flags": null,
      "enclosureUrl": "https://repository-images.githubusercontent.com/896924279/ab484a48-4af4-4127-af24-c589fd6da072",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "alibaba/OpenSandbox",
      "url": "https://github.com/alibaba/OpenSandbox",
      "date": 1772247153,
      "author": "",
      "guid": 49044,
      "unread": true,
      "content": "<p>OpenSandbox is a general-purpose sandbox platform for AI applications, offering multi-language SDKs, unified sandbox APIs, and Docker/Kubernetes runtimes for scenarios like Coding Agents, GUI Agents, Agent Evaluation, AI Code Execution, and RL Training.</p><p>OpenSandbox is a <strong>general-purpose sandbox platform</strong> for AI applications, offering multi-language SDKs, unified sandbox APIs, and Docker/Kubernetes runtimes for scenarios like Coding Agents, GUI Agents, Agent Evaluation, AI Code Execution, and RL Training.</p><ul><li>: Provides sandbox SDKs in Python, Java/Kotlin, JavaScript/TypeScript, C#/.NET, Go (Roadmap), and more.</li><li>: Defines sandbox lifecycle management APIs and sandbox execution APIs so you can extend custom sandbox runtimes.</li><li>: Built-in Command, Filesystem, and Code Interpreter implementations. Examples cover Coding Agents (e.g., Claude Code), browser automation (Chrome, Playwright), and desktop environments (VNC, VS Code).</li></ul><ul><li>Docker (required for local execution)</li><li>Python 3.10+ (recommended for examples and local runtime)</li></ul><h4>1. Install and Configure the Sandbox Server</h4><pre><code>uv pip install opensandbox-server\nopensandbox-server init-config ~/.sandbox.toml --example docker\n</code></pre><blockquote><p>If you prefer working from source, you can still clone the repo for development, but server startup no longer requires it.</p><pre><code>git clone https://github.com/alibaba/OpenSandbox.git\ncd OpenSandbox/server\nuv sync\ncp example.config.toml ~/.sandbox.toml # Copy configuration file\nuv run python -m src.main # Start the service\n</code></pre></blockquote><h4>2. Start the Sandbox Server</h4><pre><code>opensandbox-server\n\n# Show help\nopensandbox-server -h\n</code></pre><h4>3. Create a Code Interpreter and Execute Commands</h4><p>Install the Code Interpreter SDK</p><pre><code>uv pip install opensandbox-code-interpreter\n</code></pre><p>Create a sandbox and execute commands</p><pre><code>import asyncio\nfrom datetime import timedelta\n\nfrom code_interpreter import CodeInterpreter, SupportedLanguage\nfrom opensandbox import Sandbox\nfrom opensandbox.models import WriteEntry\n\nasync def main() -&gt; None:\n    # 1. Create a sandbox\n    sandbox = await Sandbox.create(\n        \"opensandbox/code-interpreter:v1.0.1\",\n        entrypoint=[\"/opt/opensandbox/code-interpreter.sh\"],\n        env={\"PYTHON_VERSION\": \"3.11\"},\n        timeout=timedelta(minutes=10),\n    )\n\n    async with sandbox:\n\n        # 2. Execute a shell command\n        execution = await sandbox.commands.run(\"echo 'Hello OpenSandbox!'\")\n        print(execution.logs.stdout[0].text)\n\n        # 3. Write a file\n        await sandbox.files.write_files([\n            WriteEntry(path=\"/tmp/hello.txt\", data=\"Hello World\", mode=644)\n        ])\n\n        # 4. Read a file\n        content = await sandbox.files.read_file(\"/tmp/hello.txt\")\n        print(f\"Content: {content}\") # Content: Hello World\n\n        # 5. Create a code interpreter\n        interpreter = await CodeInterpreter.create(sandbox)\n\n        # 6. Execute Python code (single-run, pass language directly)\n        result = await interpreter.codes.run(\n              \"\"\"\n                  import sys\n                  print(sys.version)\n                  result = 2 + 2\n                  result\n              \"\"\",\n              language=SupportedLanguage.PYTHON,\n        )\n\n        print(result.result[0].text) # 4\n        print(result.logs.stdout[0].text) # 3.11.14\n\n    # 7. Cleanup the sandbox\n    await sandbox.kill()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre><p>OpenSandbox provides rich examples demonstrating sandbox usage in different scenarios. All example code is located in the  directory.</p><h4>ü§ñ Coding Agent Integrations</h4><ul><li> - Run Google Gemini CLI inside OpenSandbox.</li><li> - Run OpenAI Codex CLI inside OpenSandbox.</li><li> - LangGraph state-machine workflow that creates/runs a sandbox job with fallback retry.</li><li> - Google ADK agent using OpenSandbox tools to write/read files and run commands.</li><li> - Launch an OpenClaw Gateway inside a sandbox.</li></ul><h4>üåê Browser and Desktop Environments</h4><ul><li> - Headless Chromium with VNC and DevTools access for automation/debugging.</li><li> - Playwright + Chromium headless scraping and testing example.</li><li> - Full desktop environment in a sandbox with VNC access.</li><li> - code-server (VS Code Web) running inside a sandbox for remote dev.</li></ul><ul><li> - DQN CartPole training in a sandbox with checkpoints and summary output.</li></ul><p>For more details, please refer to <a href=\"https://raw.githubusercontent.com/alibaba/OpenSandbox/main/examples/README.md\">examples</a> and the README files in each example directory.</p><table></table><ul><li>Issues: Submit bugs, feature requests, or design discussions through GitHub Issues</li></ul>",
      "contentLength": 4327,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/b09d686d82a79aa57a0846f6dd093405a3693aea3acb9512e2eff1f6f38428c8/alibaba/OpenSandbox",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "tukaani-project/xz",
      "url": "https://github.com/tukaani-project/xz",
      "date": 1772247153,
      "author": "",
      "guid": 49045,
      "unread": true,
      "content": "<pre><code>0. Overview\n1. Documentation\n   1.1. Overall documentation\n   1.2. Documentation for command-line tools\n   1.3. Documentation for liblzma\n2. Version numbering\n3. Reporting bugs\n4. Translations\n   4.1. Testing translations\n5. Other implementations of the .xz format\n6. Contact information\n</code></pre><pre><code>XZ Utils provide a general-purpose data-compression library plus\ncommand-line tools. The native file format is the .xz format, but\nalso the legacy .lzma format is supported. The .xz format supports\nmultiple compression algorithms, which are called \"filters\" in the\ncontext of XZ Utils. The primary filter is currently LZMA2. With\ntypical files, XZ Utils create about 30 % smaller files than gzip.\n\nTo ease adapting support for the .xz format into existing applications\nand scripts, the API of liblzma is somewhat similar to the API of the\npopular zlib library. For the same reason, the command-line tool xz\nhas a command-line syntax similar to that of gzip.\n\nWhen aiming for the highest compression ratio, the LZMA2 encoder uses\na lot of CPU time and may use, depending on the settings, even\nhundreds of megabytes of RAM. However, in fast modes, the LZMA2 encoder\ncompetes with bzip2 in compression speed, RAM usage, and compression\nratio.\n\nLZMA2 is reasonably fast to decompress. It is a little slower than\ngzip, but a lot faster than bzip2. Being fast to decompress means\nthat the .xz format is especially nice when the same file will be\ndecompressed very many times (usually on different computers), which\nis the case e.g. when distributing software packages. In such\nsituations, it's not too bad if the compression takes some time,\nsince that needs to be done only once to benefit many people.\n\nWith some file types, combining (or \"chaining\") LZMA2 with an\nadditional filter can improve the compression ratio. A filter chain may\ncontain up to four filters, although usually only one or two are used.\nFor example, putting a BCJ (Branch/Call/Jump) filter before LZMA2\nin the filter chain can improve compression ratio of executable files.\n\nSince the .xz format allows adding new filter IDs, it is possible that\nsome day there will be a filter that is, for example, much faster to\ncompress than LZMA2 (but probably with worse compression ratio).\nSimilarly, it is possible that some day there is a filter that will\ncompress better than LZMA2.\n\nXZ Utils supports multithreaded compression. XZ Utils doesn't support\nmultithreaded decompression yet. It has been planned though and taken\ninto account when designing the .xz file format. In the future, files\nthat were created in threaded mode can be decompressed in threaded\nmode too.\n</code></pre><p>1.1. Overall documentation</p><pre><code>README                This file\n\nINSTALL.generic       Generic install instructions for those not\n                      familiar with packages using GNU Autotools\nINSTALL               Installation instructions specific to XZ Utils\nPACKAGERS             Information to packagers of XZ Utils\n\nCOPYING               XZ Utils copyright and license information\nCOPYING.0BSD          BSD Zero Clause License\nCOPYING.GPLv2         GNU General Public License version 2\nCOPYING.GPLv3         GNU General Public License version 3\nCOPYING.LGPLv2.1      GNU Lesser General Public License version 2.1\n\nAUTHORS               The main authors of XZ Utils\nTHANKS                Incomplete list of people who have helped making\n                      this software\nNEWS                  User-visible changes between XZ Utils releases\nChangeLog             Detailed list of changes (commit log)\nTODO                  Known bugs and some sort of to-do list\n\nNote that only some of the above files are included in binary\npackages.\n</code></pre><p>1.2. Documentation for command-line tools</p><pre><code>The command-line tools are documented as man pages. In source code\nreleases (and possibly also in some binary packages), the man pages\nare also provided in plain text (ASCII only) format in the directory\n\"doc/man\" to make the man pages more accessible to those whose\noperating system doesn't provide an easy way to view man pages.\n</code></pre><p>1.3. Documentation for liblzma</p><pre><code>The liblzma API headers include short docs about each function\nand data type as Doxygen tags. These docs should be quite OK as\na quick reference.\n\nThere are a few example/tutorial programs that should help in\ngetting started with liblzma. In the source package the examples\nare in \"doc/examples\" and in binary packages they may be under\n\"examples\" in the same directory as this README.\n\nSince the liblzma API has similarities to the zlib API, some people\nmay find it useful to read the zlib docs and tutorial too:\n\n    https://zlib.net/manual.html\n    https://zlib.net/zlib_how.html\n</code></pre><pre><code>The version number format of XZ Utils is X.Y.ZS:\n\n  - X is the major version. When this is incremented, the library\n    API and ABI break.\n\n  - Y is the minor version. It is incremented when new features\n    are added without breaking the existing API or ABI. An even Y\n    indicates a stable release and an odd Y indicates unstable\n    (alpha or beta version).\n\n  - Z is the revision. This has a different meaning for stable and\n    unstable releases:\n\n      * Stable: Z is incremented when bugs get fixed without adding\n        any new features. This is intended to be convenient for\n        downstream distributors that want bug fixes but don't want\n        any new features to minimize the risk of introducing new bugs.\n\n      * Unstable: Z is just a counter. API or ABI of features added\n        in earlier unstable releases having the same X.Y may break.\n\n  - S indicates stability of the release. It is missing from the\n    stable releases, where Y is an even number. When Y is odd, S\n    is either \"alpha\" or \"beta\" to make it very clear that such\n    versions are not stable releases. The same X.Y.Z combination is\n    not used for more than one stability level, i.e. after X.Y.Zalpha,\n    the next version can be X.Y.(Z+1)beta but not X.Y.Zbeta.\n</code></pre><pre><code>Naturally it is easiest for me if you already know what causes the\nunexpected behavior. Even better if you have a patch to propose.\nHowever, quite often the reason for unexpected behavior is unknown,\nso here are a few things to do before sending a bug report:\n\n  1. Try to create a small example how to reproduce the issue.\n\n  2. Compile XZ Utils with debugging code using configure switches\n     --enable-debug and, if possible, --disable-shared. If you are\n     using GCC, use CFLAGS='-O0 -ggdb3'. Don't strip the resulting\n     binaries.\n\n  3. Turn on core dumps. The exact command depends on your shell;\n     for example in GNU bash it is done with \"ulimit -c unlimited\",\n     and in tcsh with \"limit coredumpsize unlimited\".\n\n  4. Try to reproduce the suspected bug. If you get \"assertion failed\"\n     message, be sure to include the complete message in your bug\n     report. If the application leaves a coredump, get a backtrace\n     using gdb:\n       $ gdb /path/to/app-binary   # Load the app to the debugger.\n       (gdb) core core   # Open the coredump.\n       (gdb) bt   # Print the backtrace. Copy &amp; paste to bug report.\n       (gdb) quit   # Quit gdb.\n\nReport your bug via email or IRC (see Contact information below).\nDon't send core dump files or any executables. If you have a small\nexample file(s) (total size less than 256 KiB), please include\nit/them as an attachment. If you have bigger test files, put them\nonline somewhere and include a URL to the file(s) in the bug report.\n\nAlways include the exact version number of XZ Utils in the bug report.\nIf you are using a snapshot from the git repository, use \"git describe\"\nto get the exact snapshot version. If you are using XZ Utils shipped\nin an operating system distribution, mention the distribution name,\ndistribution version, and exact xz package version; if you cannot\nrepeat the bug with the code compiled from unpatched source code,\nyou probably need to report a bug to your distribution's bug tracking\nsystem.\n</code></pre><pre><code>The xz command line tool and all man pages can be translated.\nThe translations are handled via the Translation Project. If you\nwish to help translating xz, please join the Translation Project:\n\n    https://translationproject.org/html/translators.html\n\nUpdates to translations won't be accepted by methods that bypass\nthe Translation Project because there is a risk of duplicate work:\ntranslation updates made in the xz repository aren't seen by the\ntranslators in the Translation Project. If you have found bugs in\na translation, please report them to the Language-Team address\nwhich can be found near the beginning of the PO file.\n\nIf you find language problems in the original English strings,\nfeel free to suggest improvements. Ask if something is unclear.\n</code></pre><p>4.1. Testing translations</p><pre><code>Testing can be done by installing xz into a temporary directory.\n\nIf building from Git repository (not tarball), generate the\nAutotools files:\n\n    ./autogen.sh\n\nCreate a subdirectory for the build files. The tmp-build directory\ncan be deleted after testing.\n\n    mkdir tmp-build\n    cd tmp-build\n    ../configure --disable-shared --enable-debug --prefix=$PWD/inst\n\nEdit the .po file in the po directory. Then build and install to\nthe \"tmp-build/inst\" directory, and use translations.bash to see\nhow some of the messages look. Repeat these  steps if needed:\n\n    make -C po update-po\n    make -j\"$(nproc)\" install\n    bash ../debug/translation.bash | less\n    bash ../debug/translation.bash | less -S  # For --list outputs\n\nTo test other languages, set the LANGUAGE environment variable\nbefore running translations.bash. The value should match the PO file\nname without the .po suffix. Example:\n\n    export LANGUAGE=fi\n</code></pre><ol start=\"5\"><li>Other implementations of the .xz format</li></ol><pre><code>7-Zip and the p7zip port of 7-Zip support the .xz format starting\nfrom the version 9.00alpha.\n\n    https://7-zip.org/\n    https://p7zip.sourceforge.net/\n\nXZ Embedded is a limited implementation written for use in the Linux\nkernel, but it is also suitable for other embedded use.\n\n    https://tukaani.org/xz/embedded.html\n\nXZ for Java is a complete implementation written in pure Java.\n\n    https://tukaani.org/xz/java.html\n</code></pre><pre><code>XZ Utils in general:\n  - Home page: https://tukaani.org/xz/\n  - Email to maintainer(s): xz@tukaani.org\n  - IRC: #tukaani on Libera Chat\n  - GitHub: https://github.com/tukaani-project/xz\n\nLead maintainer:\n  - Email: Lasse Collin &lt;lasse.collin@tukaani.org&gt;\n  - IRC: Larhzu on Libera Chat\n</code></pre>",
      "contentLength": 10336,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/87075d8e69c75bd25f94db01055dbdc51fa6220f7f00a60feb181d56dc4ccb7e/tukaani-project/xz",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "steipete/CodexBar",
      "url": "https://github.com/steipete/CodexBar",
      "date": 1772247153,
      "author": "",
      "guid": 49046,
      "unread": true,
      "content": "<p>Show usage stats for OpenAI Codex and Claude Code, without having to login.</p><p>Tiny macOS 14+ menu bar app that keeps your Codex, Claude, Cursor, Gemini, Antigravity, Droid (Factory), Copilot, z.ai, Kiro, Vertex AI, Augment, Amp, JetBrains AI, and OpenRouter limits visible (session + weekly where available) and shows when each window resets. One status item per provider (or Merge Icons mode with a provider switcher and optional Overview tab); enable what you use from Settings. No Dock icon, minimal UI, dynamic bar icons in the menu bar.</p><img src=\"https://raw.githubusercontent.com/steipete/CodexBar/main/codexbar.png\" alt=\"CodexBar menu screenshot\" width=\"520\"><pre><code>brew install --cask steipete/tap/codexbar\n</code></pre><pre><code>brew install steipete/tap/codexbar\n</code></pre><p>Or download <code>CodexBarCLI-v&lt;tag&gt;-linux-&lt;arch&gt;.tar.gz</code> from GitHub Releases. Linux support via Omarchy: community Waybar module and TUI, driven by the  executable.</p><ul><li>Open Settings ‚Üí Providers and enable what you use.</li><li>Install/sign in to the provider sources you rely on (e.g. , , , browser cookies, or OAuth; Antigravity requires the Antigravity app running).</li><li>Optional: Settings ‚Üí Providers ‚Üí Codex ‚Üí OpenAI cookies (Automatic or Manual) to add dashboard extras.</li></ul><ul><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/codex.md\">Codex</a> ‚Äî Local Codex CLI RPC (+ PTY fallback) and optional OpenAI web dashboard extras.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/claude.md\">Claude</a> ‚Äî OAuth API or browser cookies (+ CLI PTY fallback); session + weekly usage.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/cursor.md\">Cursor</a> ‚Äî Browser session cookies for plan + usage + billing resets.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/gemini.md\">Gemini</a> ‚Äî OAuth-backed quota API using Gemini CLI credentials (no browser cookies).</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/antigravity.md\">Antigravity</a> ‚Äî Local language server probe (experimental); no external auth.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/factory.md\">Droid</a> ‚Äî Browser cookies + WorkOS token flows for Factory usage + billing.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/copilot.md\">Copilot</a> ‚Äî GitHub device flow + Copilot internal usage API.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/zai.md\">z.ai</a> ‚Äî API token (Keychain) for quota + MCP windows.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/kimi.md\">Kimi</a> ‚Äî Auth token (JWT from  cookie) for weekly quota + 5‚Äëhour rate limit.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/kimi-k2.md\">Kimi K2</a> ‚Äî API key for credit-based usage totals.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/kiro.md\">Kiro</a> ‚Äî CLI-based usage via  command; monthly credits + bonus credits.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/vertexai.md\">Vertex AI</a> ‚Äî Google Cloud gcloud OAuth with token cost tracking from local Claude logs.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/augment.md\">Augment</a> ‚Äî Browser cookie-based authentication with automatic session keepalive; credits tracking and usage monitoring.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/amp.md\">Amp</a> ‚Äî Browser cookie-based authentication with Amp Free usage tracking.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/jetbrains.md\">JetBrains AI</a> ‚Äî Local XML-based quota from JetBrains IDE configuration; monthly credits tracking.</li><li><a href=\"https://raw.githubusercontent.com/steipete/CodexBar/main/docs/openrouter.md\">OpenRouter</a> ‚Äî API token for credit-based usage tracking across multiple AI providers.</li></ul><p>The menu bar icon is a tiny two-bar meter:</p><ul><li>Top bar: 5‚Äëhour/session window. If weekly is missing/exhausted and credits are available, it becomes a thicker credits bar.</li><li>Bottom bar: weekly window (hairline).</li><li>Errors/stale data dim the icon; status overlays indicate incidents.</li></ul><ul><li>Multi-provider menu bar with per-provider toggles (Settings ‚Üí Providers).</li><li>Session + weekly meters with reset countdowns.</li><li>Optional Codex web dashboard enrichments (code review remaining, usage breakdown, credits history).</li><li>Local cost-usage scan for Codex + Claude (last 30 days).</li><li>Provider status polling with incident badges in the menu and icon overlay.</li><li>Merge Icons mode to combine providers into one status item + switcher, with an optional Overview tab for up to three providers.</li><li>Refresh cadence presets (manual, 1m, 2m, 5m, 15m).</li><li>Bundled CLI () for scripts and CI (including <code>codexbar cost --provider codex|claude</code> for local cost usage); Linux CLI builds available.</li><li>WidgetKit widget mirrors the menu card snapshot.</li><li>Privacy-first: on-device parsing by default; browser cookies are opt-in and reused (no passwords stored).</li></ul><p>Wondering if CodexBar scans your disk? It doesn‚Äôt crawl your filesystem; it reads a small set of known locations (browser cookies/local storage, local JSONL logs) when the related features are enabled. See the discussion and audit notes in <a href=\"https://github.com/steipete/CodexBar/issues/12\">issue #12</a>.</p><h2>macOS permissions (why they‚Äôre needed)</h2><ul><li>Clone the repo and open it in Xcode or run the scripts directly.</li><li>Launch once, then toggle providers in Settings ‚Üí Providers.</li><li>Install/sign in to provider sources you rely on (CLIs, browser cookies, or OAuth).</li><li>Optional: set OpenAI cookies (Automatic or Manual) for Codex dashboard extras.</li></ul><pre><code>swift build -c release          # or debug for development\n./Scripts/package_app.sh        # builds CodexBar.app in-place\nCODEXBAR_SIGNING=adhoc ./Scripts/package_app.sh  # ad-hoc signing (no Apple Developer account)\nopen CodexBar.app\n</code></pre><pre><code>./Scripts/compile_and_run.sh\n</code></pre><ul><li>‚úÇÔ∏è <a href=\"https://github.com/steipete/Trimmy\">Trimmy</a> ‚Äî ‚ÄúPaste once, run once.‚Äù Flatten multi-line shell snippets so they paste and run.</li><li>üß≥ <a href=\"https://mcporter.dev\">MCPorter</a> ‚Äî TypeScript toolkit + CLI for Model Context Protocol servers.</li><li>üßø <a href=\"https://askoracle.dev\">oracle</a> ‚Äî Ask the oracle when you're stuck. Invoke GPT-5 Pro with a custom context and files.</li></ul><h2>Looking for a Windows version?</h2><p>Inspired by <a href=\"https://github.com/ryoppippi/ccusage\">ccusage</a> (MIT), specifically the cost usage tracking.</p>",
      "contentLength": 4629,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/962ac3b44edbfe94daefb31dc99e5ef7d2d7ec7a3281f4301c080c54509ac9bf/steipete/CodexBar",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ruvnet/claude-flow",
      "url": "https://github.com/ruvnet/claude-flow",
      "date": 1772161389,
      "author": "",
      "guid": 48720,
      "unread": true,
      "content": "<p>üåä The leading agent orchestration platform for Claude. Deploy intelligent multi-agent swarms, coordinate autonomous workflows, and build conversational AI systems. Features enterprise-grade architecture, distributed swarm intelligence, RAG integration, and native Claude Code / Codex Integration</p><p>Ruflo is a comprehensive AI agent orchestration framework that transforms Claude Code into a powerful multi-agent development platform. It enables teams to deploy, coordinate, and optimize specialized AI agents working together on complex software engineering tasks.</p><h3>Self-Learning/Self-Optimizing Agent Architecture</h3><pre><code>User ‚Üí Ruflo (CLI/MCP) ‚Üí Router ‚Üí Swarm ‚Üí Agents ‚Üí Memory ‚Üí LLM Providers\n                       ‚Üë                          ‚Üì\n                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ Learning Loop ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</code></pre><pre><code># One-line install (recommended)\ncurl -fsSL https://cdn.jsdelivr.net/gh/ruvnet/claude-flow@main/scripts/install.sh | bash\n\n# Or full setup with MCP + diagnostics\ncurl -fsSL https://cdn.jsdelivr.net/gh/ruvnet/claude-flow@main/scripts/install.sh | bash -s -- --full\n\n# Or via npx\nnpx ruflo@alpha init --wizard\n</code></pre><p>ü§ñ  - Ready-to-use AI agents for coding, code review, testing, security audits, documentation, and DevOps. Each agent is optimized for its specific role.</p><p>üêù  - Run unlimited agents simultaneously in organized swarms. Agents spawn sub-workers, communicate, share context, and divide work automatically using hierarchical (queen/workers) or mesh (peer-to-peer) patterns.</p><p>üß† <strong>Learns From Your Workflow</strong> - The system remembers what works. Successful patterns are stored and reused, routing similar tasks to the best-performing agents. Gets smarter over time.</p><p>üîå  - Switch between Claude, GPT, Gemini, Cohere, or local models like Llama. Automatic failover if one provider is unavailable. Smart routing picks the cheapest option that meets quality requirements.</p><p>‚ö°  - Native integration via MCP (Model Context Protocol). Use ruflo commands directly in your Claude Code sessions with full tool access.</p><p>üîí <strong>Production-Ready Security</strong> - Built-in protection against prompt injection, input validation, path traversal prevention, command injection blocking, and safe credential handling.</p><p>üß©  - Add custom capabilities with the plugin SDK. Create workers, hooks, providers, and security modules. Share plugins via the decentralized IPFS marketplace.</p><h3>A multi-purpose Agent Tool Kit</h3><h3>Claude Code: With vs Without Ruflo</h3><table><thead><tr></tr></thead><tbody><tr><td>Agents work in isolation, no shared context</td><td>Agents collaborate via swarms with shared memory and consensus</td></tr><tr><td>Manual orchestration between tasks</td><td>Queen-led hierarchy with 5 consensus algorithms (Raft, Byzantine, Gossip)</td></tr><tr><td>üêù Queen-led swarms with collective intelligence, 3 queen types, 8 worker types</td></tr><tr><td>‚õî No multi-agent decisions</td><td>Byzantine fault-tolerant voting (f &lt; n/3), weighted, majority</td></tr><tr><td>Session-only, no persistence</td><td>HNSW vector memory with 150x-12,500x faster retrieval + knowledge graph</td></tr><tr><td>üêò RuVector PostgreSQL with 77+ SQL functions, ~61¬µs search, 16,400 QPS</td></tr><tr><td>PageRank + community detection identifies influential insights (ADR-049)</td></tr><tr><td>Shared knowledge base with LRU cache, SQLite persistence, 8 memory types</td></tr><tr><td>Static behavior, no adaptation</td><td>SONA self-learning with &lt;0.05ms adaptation, LearningBridge for insights</td></tr><tr><td>3-scope agent memory (project/local/user) with cross-agent transfer</td></tr><tr><td>You decide which agent to use</td><td>Intelligent routing based on learned patterns (89% accuracy)</td></tr><tr><td>Manual breakdown required</td><td>Automatic decomposition across 5 domains (Security, Core, Integration, Support)</td></tr><tr><td>Nothing runs automatically</td><td>12 context-triggered workers auto-dispatch on file changes, patterns, sessions</td></tr><tr><td>6 providers with automatic failover and cost-based routing (85% savings)</td></tr><tr><td>CVE-hardened with bcrypt, input validation, path traversal prevention</td></tr><tr><td>2.8-4.4x faster tasks, 10-20x faster swarm spawning, 84.8% SWE-Bench</td></tr></tbody></table><ul><li> /  /  package manager</li></ul><p>: Claude Code must be installed first:</p><pre><code># 1. Install Claude Code globally\nnpm install -g @anthropic-ai/claude-code\n\n# 2. (Optional) Skip permissions check for faster setup\nclaude --dangerously-skip-permissions\n</code></pre><h4>One-Line Install (Recommended)</h4><pre><code># curl-style installer with progress display\ncurl -fsSL https://cdn.jsdelivr.net/gh/ruvnet/claude-flow@main/scripts/install.sh | bash\n\n# Full setup (global + MCP + diagnostics)\ncurl -fsSL https://cdn.jsdelivr.net/gh/ruvnet/claude-flow@main/scripts/install.sh | bash -s -- --full\n</code></pre><pre><code># Quick start (no install needed)\nnpx ruflo@alpha init\n\n# Or install globally\nnpm install -g ruflo@alpha\nruflo init\n\n# With Bun (faster)\nbunx ruflo@alpha init\n</code></pre><table><tbody><tr></tr><tr><td>Full install with ML/embeddings</td></tr></tbody></table><pre><code># Minimal install (skip ML/embeddings)\nnpm install -g ruflo@alpha --omit=optional\n</code></pre><pre><code># Initialize project\nnpx ruflo@alpha init\n\n# Start MCP server for Claude Code integration\nnpx ruflo@alpha mcp start\n\n# Run a task with agents\nnpx ruflo@alpha --agent coder --task \"Implement user authentication\"\n\n# List available agents\nnpx ruflo@alpha --list\n</code></pre><pre><code># Update helpers and statusline (preserves your data)\nnpx ruflo@v3alpha init upgrade\n\n# Update AND add any missing skills/agents/commands\nnpx ruflo@v3alpha init upgrade --add-missing\n</code></pre><p>The  flag automatically detects and installs new skills, agents, and commands that were added in newer versions, without overwriting your existing customizations.</p><h3>Claude Code MCP Integration</h3><p>Add ruflo as an MCP server for seamless integration:</p><pre><code># Add ruflo MCP server to Claude Code\nclaude mcp add ruflo -- npx -y ruflo@latest mcp start\n\n# Verify installation\nclaude mcp list\n</code></pre><p>Once added, Claude Code can use all 175+ ruflo MCP tools directly:</p><ul><li> - Initialize agent swarms</li><li> - Spawn specialized agents</li><li> - Search patterns with HNSW (150x faster)</li><li> - Intelligent task routing</li></ul><h2>What is it exactly? Agents that learn, build and work perpetually.</h2><p>Connect Ruflo to your development environment.</p><p>Comprehensive capabilities for enterprise-grade AI agent orchestration.</p><p>Real-world scenarios and pre-built workflows for common tasks.</p><h2>üß† Infinite Context &amp; Memory Optimization</h2><p>Ruflo eliminates Claude Code's context window ceiling with a real-time memory management system that archives, optimizes, and restores conversation context automatically.</p><h2>üß† Intelligence &amp; Learning</h2><p>Self-learning hooks, pattern recognition, and intelligent task routing.</p><p>Scripts, coordination systems, and collaborative development features.</p><p>Use Ruflo packages directly in your applications.</p><h2>üîó Ecosystem &amp; Integrations</h2><p>Core infrastructure packages powering Ruflo's intelligence layer.</p><p>Cloud platform integration and deployment tools.</p><p>AI manipulation defense, threat detection, and input validation.</p><h2>üèóÔ∏è Architecture &amp; Modules</h2><p>Domain-driven design, performance benchmarks, and testing framework.</p><h2>‚öôÔ∏è Configuration &amp; Reference</h2><p>Environment setup, configuration options, and platform support.</p><p>Troubleshooting, migration guides, and documentation links.</p>",
      "contentLength": 6765,
      "flags": null,
      "enclosureUrl": "https://repository-images.githubusercontent.com/995029641/b9acbe16-0f49-420d-804f-468ba2a73ace",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "farion1231/cc-switch",
      "url": "https://github.com/farion1231/cc-switch",
      "date": 1772161389,
      "author": "",
      "guid": 48721,
      "unread": true,
      "content": "<p>A cross-platform desktop All-in-One assistant tool for Claude Code, Codex, OpenCode &amp; Gemini CLI.</p><p>MiniMax-M2.5 is a SOTA large language model designed for real-world productivity. Trained in a diverse range of complex real-world digital working environments, M2.5 builds upon the coding expertise of M2.1 to extend into general office work, reaching fluency in generating and operating Word, Excel, and Powerpoint files, context switching between diverse software environments, and working across different agent and human teams. Scoring 80.2% on SWE-Bench Verified, 51.3% on Multi-SWE-Bench, and 76.3% on BrowseComp, M2.5 is also more token efficient than previous generations, having been trained to optimize its actions and output through planning.</p><p><a href=\"https://platform.minimax.io/subscribe/coding-plan?code=ClLhgxr2je&amp;source=link\">Click</a> to get an exclusive 12% off the MiniMax Coding Plan!</p><p><strong>v3.8.0 Major Update (2025-11-28)</strong></p><p><strong>Persistence Architecture Upgrade &amp; Brand New UI</strong></p><ul><li><p><strong>SQLite + JSON Dual-layer Architecture</strong></p><ul><li>Migrated from JSON file storage to SQLite + JSON dual-layer structure</li><li>Syncable data (providers, MCP, Prompts, Skills) stored in SQLite</li><li>Device-level data (window state, local paths) stored in JSON</li><li>Lays the foundation for future cloud sync functionality</li><li>Schema version management for database migrations</li></ul></li><li><ul><li>Completely redesigned interface layout</li><li>Unified component styles and smoother animations</li><li>Optimized visual hierarchy</li><li>Tailwind CSS downgraded from v4 to v3.4 for better browser compatibility</li></ul></li><li><p><strong>Japanese Language Support</strong></p><ul><li>Added Japanese interface support (now supports Chinese/English/Japanese)</li></ul></li><li><ul><li>One-click enable/disable in settings</li><li>Platform-native APIs (Registry/LaunchAgent/XDG autostart)</li></ul></li><li><p><strong>Skills Recursive Scanning</strong></p><ul><li>Support for multi-level directory structures</li><li>Allow same-named skills from different repositories</li></ul></li><li><ul><li>Fixed custom endpoints lost when updating providers</li><li>Fixed Gemini configuration write issues</li><li>Fixed Linux WebKitGTK rendering issues</li></ul></li></ul><p><strong>Six Core Features, 18,000+ Lines of New Code</strong></p><ul><li><ul><li>Third supported AI CLI (Claude Code / Codex / Gemini)</li><li>Dual-file configuration support ( + )</li><li>Complete MCP server management</li><li>Presets: Google Official (OAuth) / PackyCode / Custom</li></ul></li><li><p><strong>Claude Skills Management System</strong></p><ul><li>Auto-scan skills from GitHub repositories (3 pre-configured curated repos)</li><li>One-click install/uninstall to </li><li>Custom repository support + subdirectory scanning</li><li>Complete lifecycle management (discover/install/update)</li></ul></li><li><p><strong>Prompts Management System</strong></p><ul><li>Multi-preset system prompt management (unlimited presets, quick switching)</li><li>Cross-app support (Claude:  / Codex:  / Gemini: )</li><li>Markdown editor (CodeMirror 6 + real-time preview)</li><li>Smart backfill protection, preserves manual modifications</li></ul></li><li><p><strong>MCP v3.7.0 Unified Architecture</strong></p><ul><li>Single panel manages MCP servers across three applications</li><li>New SSE (Server-Sent Events) transport type</li><li>Smart JSON parser + Codex TOML format auto-correction</li><li>Unified import/export + bidirectional sync</li></ul></li><li><ul><li> protocol registration (all platforms)</li><li>One-click import provider configs via shared links</li><li>Security validation + lifecycle integration</li></ul></li><li><p><strong>Environment Variable Conflict Detection</strong></p><ul><li>Auto-detect cross-app configuration conflicts (Claude/Codex/Gemini/MCP)</li><li>Visual conflict indicators + resolution suggestions</li><li>Override warnings + backup before changes</li></ul></li></ul><ul><li>: One-click switching between Claude Code, Codex, and Gemini API configurations</li><li>: Built-in AWS Bedrock provider presets with AKSK and API Key authentication, cross-region inference support (global/us/eu/apac), covering Claude Code and OpenCode</li><li>: Measure API endpoint latency with visual quality indicators</li><li>: Backup and restore configs with auto-rotation (keep 10 most recent)</li><li>: Complete Chinese/English localization (UI, errors, tray)</li><li>: One-click apply/restore Claude plugin configurations</li></ul><ul><li>Provider duplication &amp; drag-and-drop sorting</li><li>Multi-endpoint management &amp; custom config directory (cloud sync ready)</li><li>Granular model configuration (4-tier: Haiku/Sonnet/Opus/Custom)</li><li>WSL environment support with auto-sync on directory change</li><li>100% hooks test coverage &amp; complete architecture refactoring</li></ul><ul><li>System tray with quick switching</li><li>Atomic writes with rollback protection</li></ul><ul><li>: Windows 10 and above</li><li>: macOS 10.15 (Catalina) and above</li><li>: Ubuntu 22.04+ / Debian 11+ / Fedora 34+ and other mainstream distributions</li></ul><p>Download the latest <code>CC-Switch-v{version}-Windows.msi</code> installer or <code>CC-Switch-v{version}-Windows-Portable.zip</code> portable version from the <a href=\"https://raw.githubusercontent.com/farion1231/releases\">Releases</a> page.</p><p><strong>Method 1: Install via Homebrew (Recommended)</strong></p><pre><code>brew tap farion1231/ccswitch\nbrew install --cask cc-switch\n</code></pre><pre><code>brew upgrade --cask cc-switch\n</code></pre><p><strong>Method 2: Manual Download</strong></p><p>Download <code>CC-Switch-v{version}-macOS.zip</code> from the <a href=\"https://raw.githubusercontent.com/farion1231/releases\">Releases</a> page and extract to use.</p><blockquote><p>: Since the author doesn't have an Apple Developer account, you may see an \"unidentified developer\" warning on first launch. Please close it first, then go to \"System Settings\" ‚Üí \"Privacy &amp; Security\" ‚Üí click \"Open Anyway\", and you'll be able to open it normally afterwards.</p></blockquote><p><strong>Install via paru (Recommended)</strong></p><p>Download the latest Linux build from the <a href=\"https://raw.githubusercontent.com/farion1231/releases\">Releases</a> page:</p><ul><li><code>CC-Switch-v{version}-Linux.deb</code> (Debian/Ubuntu)</li><li><code>CC-Switch-v{version}-Linux.rpm</code> (Fedora/RHEL/openSUSE)</li><li><code>CC-Switch-v{version}-Linux.AppImage</code> (Universal)</li><li><code>CC-Switch-v{version}-Linux.flatpak</code> (Flatpak)</li></ul><pre><code>flatpak install --user ./CC-Switch-v{version}-Linux.flatpak\nflatpak run com.ccswitch.desktop\n</code></pre><ol><li>: Click \"Add Provider\" ‚Üí Choose preset or create custom configuration</li><li>: \n  <ul><li>Main UI: Select provider ‚Üí Click \"Enable\"</li><li>System Tray: Click provider name directly (instant effect)</li></ul></li><li>: Restart your terminal or Claude Code / Codex / Gemini clients to apply changes</li><li>: Select the \"Official Login\" preset (Claude/Codex) or \"Google Official\" preset (Gemini), restart the corresponding client, then follow its login/OAuth flow</li></ol><ul><li>: Click \"MCP\" button in top-right corner</li><li>: \n  <ul><li>Use built-in templates (mcp-fetch, mcp-filesystem, etc.)</li><li>Support stdio / http / sse transport types</li><li>Configure independent MCP servers for different apps</li></ul></li><li>: Toggle switches to control which servers sync to live config</li><li>: Enabled servers auto-sync to each app's live files</li><li>: Import existing MCP servers from Claude/Codex/Gemini config files</li></ul><h3>Skills Management (v3.7.0 New)</h3><ul><li>: Click \"Skills\" button in top-right corner</li><li>: \n  <ul><li>Auto-scan pre-configured GitHub repositories (Anthropic official, ComposioHQ, community, etc.)</li><li>Add custom repositories (supports subdirectory scanning)</li></ul></li><li>: Click \"Install\" to one-click install to </li><li>: Click \"Uninstall\" to safely remove and clean up state</li><li>: Add/remove custom GitHub repositories</li></ul><h3>Prompts Management (v3.7.0 New)</h3><ul><li>: Click \"Prompts\" button in top-right corner</li><li>: \n  <ul><li>Create unlimited system prompt presets</li><li>Use Markdown editor to write prompts (syntax highlighting + real-time preview)</li></ul></li><li>: Select preset ‚Üí Click \"Activate\" to apply immediately</li><li>: \n  <ul><li>Claude: </li><li>Gemini: </li></ul></li><li>: Auto-save current prompt content before switching, preserves manual modifications</li></ul><ul><li>Live config:  (or )</li><li>API key field:  or </li><li>MCP servers:  ‚Üí </li></ul><ul><li>Live config:  (required) +  (optional)</li><li>API key field:  in </li><li>MCP servers:  ‚Üí  tables</li></ul><ul><li>Live config:  (API key) +  (auth mode)</li><li>API key field:  or  in </li><li>Environment variables: Support , , etc.</li><li>MCP servers:  ‚Üí </li><li>Tray quick switch: Each provider switch rewrites , no need to restart Gemini CLI</li></ul><p><strong>CC Switch Storage (v3.8.0 New Architecture)</strong></p><ul><li>Database (SSOT): <code>~/.cc-switch/cc-switch.db</code> (SQLite, stores providers, MCP, Prompts, Skills)</li><li>Local settings: <code>~/.cc-switch/settings.json</code> (device-level settings)</li><li>Backups:  (auto-rotate, keep 10)</li></ul><ol><li>Go to Settings ‚Üí \"Custom Configuration Directory\"</li><li>Choose your cloud sync folder (Dropbox, OneDrive, iCloud, etc.)</li><li>Repeat on other devices to enable cross-device sync</li></ol><blockquote><p>: First launch auto-imports existing Claude/Codex configs as default provider.</p></blockquote><pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                    Frontend (React + TS)                    ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n‚îÇ  ‚îÇ Components  ‚îÇ  ‚îÇ    Hooks     ‚îÇ  ‚îÇ  TanStack Query  ‚îÇ    ‚îÇ\n‚îÇ  ‚îÇ   (UI)      ‚îÇ‚îÄ‚îÄ‚îÇ (Bus. Logic) ‚îÇ‚îÄ‚îÄ‚îÇ   (Cache/Sync)   ‚îÇ    ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                         ‚îÇ Tauri IPC\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                  Backend (Tauri + Rust)                     ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n‚îÇ  ‚îÇ  Commands   ‚îÇ  ‚îÇ   Services   ‚îÇ  ‚îÇ  Models/Config   ‚îÇ    ‚îÇ\n‚îÇ  ‚îÇ (API Layer) ‚îÇ‚îÄ‚îÄ‚îÇ (Bus. Layer) ‚îÇ‚îÄ‚îÄ‚îÇ     (Data)       ‚îÇ    ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</code></pre><ul><li> (Single Source of Truth): All data stored in <code>~/.cc-switch/cc-switch.db</code> (SQLite)</li><li>: SQLite for syncable data, JSON for device-level settings</li><li>: Write to live files on switch, backfill from live when editing active provider</li><li>: Temp file + rename pattern prevents config corruption</li><li>: Mutex-protected database connection avoids race conditions</li><li>: Clear separation (Commands ‚Üí Services ‚Üí DAO ‚Üí Database)</li></ul><ul><li>: Provider CRUD, switching, backfill, sorting</li><li>: MCP server management, import/export, live file sync</li><li>: Config import/export, backup rotation</li><li>: API endpoint latency measurement</li></ul><ul><li>Backend: 5-phase refactoring (error handling ‚Üí command split ‚Üí tests ‚Üí services ‚Üí concurrency)</li><li>Frontend: 4-stage refactoring (test infra ‚Üí hooks ‚Üí components ‚Üí cleanup)</li><li>Testing: 100% hooks coverage + integration tests (vitest + MSW)</li></ul><ul></ul><pre><code># Install dependencies\npnpm install\n\n# Dev mode (hot reload)\npnpm dev\n\n# Type check\npnpm typecheck\n\n# Format code\npnpm format\n\n# Check code format\npnpm format:check\n\n# Run frontend unit tests\npnpm test:unit\n\n# Run tests in watch mode (recommended for development)\npnpm test:unit:watch\n\n# Build application\npnpm build\n\n# Build debug version\npnpm tauri build --debug\n</code></pre><pre><code>cd src-tauri\n\n# Format Rust code\ncargo fmt\n\n# Run clippy checks\ncargo clippy\n\n# Run backend tests\ncargo test\n\n# Run specific tests\ncargo test test_name\n\n# Run tests with test-hooks feature\ncargo test --features test-hooks\n</code></pre><ul><li>Uses  as test framework</li><li>Uses <strong>MSW (Mock Service Worker)</strong> to mock Tauri API calls</li><li>Uses  for component testing</li></ul><ul><li>Hooks unit tests (100% coverage) \n  <ul><li> - Provider operations</li><li> - MCP management</li><li> series - Settings management</li><li> - Import/export</li></ul></li><li>Integration tests \n  <ul><li>App main application flow</li><li>SettingsDialog complete interaction</li></ul></li></ul><pre><code># Run all tests\npnpm test:unit\n\n# Watch mode (auto re-run)\npnpm test:unit:watch\n\n# With coverage report\npnpm test:unit --coverage\n</code></pre><p>: React 18 ¬∑ TypeScript ¬∑ Vite ¬∑ TailwindCSS 4 ¬∑ TanStack Query v5 ¬∑ react-i18next ¬∑ react-hook-form ¬∑ zod ¬∑ shadcn/ui ¬∑ @dnd-kit</p><p>: Tauri 2.8 ¬∑ Rust ¬∑ serde ¬∑ tokio ¬∑ thiserror ¬∑ tauri-plugin-updater/process/dialog/store/log</p><p>: vitest ¬∑ MSW ¬∑ @testing-library/react</p><pre><code>‚îú‚îÄ‚îÄ src/                      # Frontend (React + TypeScript)\n‚îÇ   ‚îú‚îÄ‚îÄ components/           # UI components (providers/settings/mcp/ui)\n‚îÇ   ‚îú‚îÄ‚îÄ hooks/                # Custom hooks (business logic)\n‚îÇ   ‚îú‚îÄ‚îÄ lib/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/              # Tauri API wrapper (type-safe)\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ query/            # TanStack Query config\n‚îÇ   ‚îú‚îÄ‚îÄ i18n/locales/         # Translations (zh/en)\n‚îÇ   ‚îú‚îÄ‚îÄ config/               # Presets (providers/mcp)\n‚îÇ   ‚îî‚îÄ‚îÄ types/                # TypeScript definitions\n‚îú‚îÄ‚îÄ src-tauri/                # Backend (Rust)\n‚îÇ   ‚îî‚îÄ‚îÄ src/\n‚îÇ       ‚îú‚îÄ‚îÄ commands/         # Tauri command layer (by domain)\n‚îÇ       ‚îú‚îÄ‚îÄ services/         # Business logic layer\n‚îÇ       ‚îú‚îÄ‚îÄ app_config.rs     # Config data models\n‚îÇ       ‚îú‚îÄ‚îÄ provider.rs       # Provider domain models\n‚îÇ       ‚îú‚îÄ‚îÄ mcp.rs            # MCP sync &amp; validation\n‚îÇ       ‚îî‚îÄ‚îÄ lib.rs            # App entry &amp; tray menu\n‚îú‚îÄ‚îÄ tests/                    # Frontend tests\n‚îÇ   ‚îú‚îÄ‚îÄ hooks/                # Unit tests\n‚îÇ   ‚îî‚îÄ‚îÄ components/           # Integration tests\n‚îî‚îÄ‚îÄ assets/                   # Screenshots &amp; partner resources\n</code></pre><p><a href=\"https://raw.githubusercontent.com/farion1231/releases\">Releases</a> retains v2.0.3 legacy Electron version</p><p>If you need legacy Electron code, you can pull the electron-legacy branch</p><p>Issues and suggestions are welcome!</p><p>Before submitting PRs, please ensure:</p><ul><li>Pass type check: </li><li>Pass format check: </li><li>Pass unit tests: </li><li>üí° For new features, please open an issue for discussion before submitting a PR</li></ul>",
      "contentLength": 13089,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/ccc2d68cb35065d47042aa6f2f21a11c9237caae11a0e0fdd86c074f71a92b85/farion1231/cc-switch",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "clockworklabs/SpacetimeDB",
      "url": "https://github.com/clockworklabs/SpacetimeDB",
      "date": 1772161389,
      "author": "",
      "guid": 48722,
      "unread": true,
      "content": "<p>Development at the speed of light</p><h3 align=\"center\"> Development at the speed of light. </h3><p>You can think of SpacetimeDB as both a database and server combined into one.</p><p>It is a relational database system that lets you upload your application logic directly into the database by way of fancy stored procedures called \"modules.\"</p><p>Instead of deploying a web or game server that sits in between your clients and your database, your clients connect directly to the database and execute your application logic inside the database itself. You can write all of your permission and authorization logic right inside your module just as you would in a normal server.</p><p>This means that you can write your entire application in a single language, Rust, and deploy it as a single binary. No more microservices, no more containers, no more Kubernetes, no more Docker, no more VMs, no more DevOps, no more infrastructure, no more ops, no more servers.</p><figure><img src=\"https://raw.githubusercontent.com/clockworklabs/SpacetimeDB/master/images/basic-architecture-diagram.png\" alt=\"SpacetimeDB Architecture\"><figcaption align=\"center\"><p align=\"center\"><b>SpacetimeDB application architecture</b><sup><sub>(elements in white are provided by SpacetimeDB)</sub></sup></p></figcaption></figure><p>It's actually similar to the idea of smart contracts, except that SpacetimeDB is a database, has nothing to do with blockchain, and is orders of magnitude faster than any smart contract system.</p><p>So fast, in fact, that the entire backend of our MMORPG <a href=\"https://bitcraftonline.com\">BitCraft Online</a> is just a SpacetimeDB module. We don't have any other servers or services running, which means that everything in the game, all of the chat messages, items, resources, terrain, and even the locations of the players are stored and processed by the database before being synchronized out to all of the clients in real-time.</p><p>SpacetimeDB is optimized for maximum speed and minimum latency rather than batch processing or OLAP workloads. It is designed to be used for real-time applications like games, chat, and collaboration tools.</p><p>This speed and latency is achieved by holding all of application state in memory, while persisting the data in a write-ahead-log (WAL) which is used to recover application state.</p><p>You can run SpacetimeDB as a standalone database server via the  CLI tool. Install instructions for supported platforms are outlined below. The same install instructions can be found on our website at <a href=\"https://spacetimedb.com/install\">https://spacetimedb.com/install</a>.</p><p>Installing on macOS is as simple as running our install script. After that you can use the spacetime command to manage versions.</p><pre><code>curl -sSf https://install.spacetimedb.com | sh\n</code></pre><p>Installing on Linux is as simple as running our install script. After that you can use the spacetime command to manage versions.</p><pre><code>curl -sSf https://install.spacetimedb.com | sh\n</code></pre><p>Installing on Windows is as simple as pasting the snippet below into PowerShell. If you would like to use WSL instead, please follow the Linux install instructions.</p><pre><code>iwr https://windows.spacetimedb.com -useb | iex\n</code></pre><p>A quick note on installing from source: we recommend that you don't install from source unless there is a feature that is available in  that hasn't been released yet, otherwise follow the official installation instructions.</p><p>Installing on macOS + Linux is pretty straightforward. First we are going to build all of the binaries that we need:</p><pre><code># Install rustup, you can skip this step if you have cargo and the wasm32-unknown-unknown target already installed.\ncurl https://sh.rustup.rs -sSf | sh\n# Clone SpacetimeDB\ngit clone https://github.com/clockworklabs/SpacetimeDB\n# Build and install the CLI\ncd SpacetimeDB\ncargo build --locked --release -p spacetimedb-standalone -p spacetimedb-update -p spacetimedb-cli\n\n# Create directories\nmkdir -p ~/.local/bin\nexport STDB_VERSION=\"$(./target/release/spacetimedb-cli --version | sed -n 's/.*spacetimedb tool version \\([0-9.]*\\);.*/\\1/p')\"\nmkdir -p ~/.local/share/spacetime/bin/$STDB_VERSION\n\n# Install the update binary\ncp target/release/spacetimedb-update ~/.local/bin/spacetime\ncp target/release/spacetimedb-cli ~/.local/share/spacetime/bin/$STDB_VERSION\ncp target/release/spacetimedb-standalone ~/.local/share/spacetime/bin/$STDB_VERSION\n</code></pre><p>At this stage you'll need to add ~/.local/bin to your path if you haven't already.</p><pre><code># Please add the following line to your shell configuration and open a new shell session:\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n</code></pre><p>Then finally set your SpacetimeDB version:</p><pre><code>\n# Then, in a new shell, set the current version:\nspacetime version use $STDB_VERSION\n\n# If STDB_VERSION is not set anymore then you can use the following command to list your versions:\nspacetime version list\n</code></pre><p>You can verify that the correct version has been installed via .</p><p>Building on windows is a bit more complicated. You'll need a slightly different version of perl compared to what comes pre-bundled in most Windows terminals. We recommend <a href=\"https://strawberryperl.com/\">Strawberry Perl</a>. You may also need access to an  binary which actually comes pre-installed with <a href=\"https://git-scm.com/downloads/win\">Git for Windows</a>. Also, you'll need to install <a href=\"https://rustup.rs/\">rustup</a> for Windows.</p><p>In a Git for Windows shell you should have something that looks like this:</p><pre><code>$ which perl\n/c/Strawberry/perl/bin/perl\n$ which openssl\n/mingw64/bin/openssl\n$ which cargo \n/c/Users/&lt;user&gt;/.cargo/bin/cargo\n</code></pre><p>If that looks correct then you're ready to proceed!</p><pre><code># Clone SpacetimeDB\ngit clone https://github.com/clockworklabs/SpacetimeDB\n\n# Build and install the CLI\ncd SpacetimeDB\ncargo build --locked --release -p spacetimedb-standalone -p spacetimedb-update -p spacetimedb-cli\n\n# Create directories\n$stdbDir = \"$HOME\\AppData\\Local\\SpacetimeDB\"\n$stdbVersion = &amp; \".\\target\\release\\spacetimedb-cli\" --version | Select-String -Pattern 'spacetimedb tool version ([0-9.]+);' | ForEach-Object { $_.Matches.Groups[1].Value }\nNew-Item -ItemType Directory -Path \"$stdbDir\\bin\\$stdbVersion\" -Force | Out-Null\n\n# Install the update binary\nCopy-Item \"target\\release\\spacetimedb-update.exe\" \"$stdbDir\\spacetime.exe\"\nCopy-Item \"target\\release\\spacetimedb-cli.exe\" \"$stdbDir\\bin\\$stdbVersion\\\"\nCopy-Item \"target\\release\\spacetimedb-standalone.exe\" \"$stdbDir\\bin\\$stdbVersion\\\"\n\n</code></pre><p>Now add the directory we just created to your path. We recommend adding it to the system path because then it will be available to all of your applications (including Unity3D). After you do this, restart your shell!</p><pre><code>%USERPROFILE%\\AppData\\Local\\SpacetimeDB\n</code></pre><p>Then finally, open a new shell and use the installed SpacetimeDB version:</p><pre><code>spacetime version use $stdbVersion\n\n# If stdbVersion is no longer set, list versions using the following command:\nspacetime version list\n</code></pre><p>You can verify that the correct version has been installed via .</p><p>If you're using Git for Windows you can follow these instructions instead:</p><pre><code># Clone SpacetimeDB\ngit clone https://github.com/clockworklabs/SpacetimeDB\n# Build and install the CLI\ncd SpacetimeDB\n# Build the CLI binaries - this takes a while on windows so go grab a coffee :)\ncargo build --locked --release -p spacetimedb-standalone -p spacetimedb-update -p spacetimedb-cli\n\n# Create directories\nexport STDB_VERSION=\"$(./target/release/spacetimedb-cli --version | sed -n 's/.*spacetimedb tool version \\([0-9.]*\\);.*/\\1/p')\"\nmkdir -p ~/AppData/Local/SpacetimeDB/bin/$STDB_VERSION\n\n# Install the update binary\ncp target/release/spacetimedb-update ~/AppData/Local/SpacetimeDB/spacetime\ncp target/release/spacetimedb-cli ~/AppData/Local/SpacetimeDB/bin/$STDB_VERSION\ncp target/release/spacetimedb-standalone ~/AppData/Local/SpacetimeDB/bin/$STDB_VERSION\n\n# Now add the directory we just created to your path. We recommend adding it to the system path because then it will be available to all of your applications (including Unity3D). After you do this, restart your shell!\n# %USERPROFILE%\\AppData\\Local\\SpacetimeDB\n\n# Set the current version\nspacetime version use $STDB_VERSION\n</code></pre><p>You can verify that the correct version has been installed via .</p><p>If you prefer to run Spacetime in a container, you can use the following command to start a new instance.</p><pre><code>docker run --rm --pull always -p 3000:3000 clockworklabs/spacetime start\n</code></pre><p>For more information about SpacetimeDB, getting started guides, game development guides, and reference material please see our <a href=\"https://spacetimedb.com/docs\">documentation</a>.</p><p>We've prepared several getting started guides in each of our supported languages to help you get up and running with SpacetimeDB as quickly as possible. You can find them on our <a href=\"https://spacetimedb.com/docs\">docs page</a>.</p><p>In summary there are only 4 steps to getting started with SpacetimeDB.</p><ol><li>Install the  CLI tool.</li><li>Start a SpacetimeDB standalone node with .</li><li>Write and upload a module in one of our supported module languages.</li><li>Connect to the database with one of our client libraries.</li></ol><p>You can see a summary of the supported languages below with a link to the getting started guide for each.</p><p>You can write SpacetimeDB modules in several popular languages, with more to come in the future!</p><p>SpacetimeDB is licensed under the BSL 1.1 license. This is not an open source or free software license, however, it converts to the AGPL v3.0 license with a linking exception after a few years.</p><p>Note that the AGPL v3.0 does not typically include a linking exception. We have added a custom linking exception to the AGPL license for SpacetimeDB. Our motivation for choosing a free software license is to ensure that contributions made to SpacetimeDB are propagated back to the community. We are expressly not interested in forcing users of SpacetimeDB to open source their own code if they link with SpacetimeDB, so we needed to include a linking exception.</p>",
      "contentLength": 9229,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/9b875bc2bc929d7eebebb5a6bbc677c99cfe3c22cbe0e2bacea77198bd68d28e/clockworklabs/SpacetimeDB",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "NVIDIA/Megatron-LM",
      "url": "https://github.com/NVIDIA/Megatron-LM",
      "date": 1772074985,
      "author": "",
      "guid": 48389,
      "unread": true,
      "content": "<p>This repository contains two components:  and .</p><p> is a reference example that includes Megatron Core plus pre-configured training scripts. Best for research teams, learning distributed training, and quick experimentation.</p><p> is a composable library with GPU-optimized building blocks for custom training frameworks. It provides transformer building blocks, advanced parallelism strategies (TP, PP, DP, EP, CP), mixed precision support (FP16, BF16, FP8, FP4), and model architectures. Best for framework developers and ML engineers building custom training pipelines.</p><p> provides bidirectional Hugging Face ‚Üî Megatron checkpoint conversion with production-ready recipes.</p><p>Install Megatron Core with pip:</p><ol><li><p>Install Megatron Core with required dependencies:</p><pre><code>pip install --no-build-isolation megatron-core[mlm,dev]\n</code></pre></li><li><p>Clone repository for examples:</p><pre><code>git clone https://github.com/NVIDIA/Megatron-LM.git\ncd Megatron-LM\npip install --no-build-isolation .[mlm,dev]\n</code></pre></li></ol><ul><li><strong>Megatron Core development has moved to GitHub!</strong> All development and CI now happens in the open. We welcome community contributions.</li><li> - Bidirectional converter for interoperability between Hugging Face and Megatron checkpoints, featuring production-ready recipes for popular models.</li><li> - Comprehensive roadmap for MoE features including DeepSeek-V3, Qwen3, advanced parallelism strategies, FP8 optimizations, and Blackwell performance enhancements.</li><li> - Advanced features including YaRN RoPE scaling, attention sinks, and custom activation functions are being integrated into Megatron Core.</li><li> - Best practices and optimized configurations for training DeepSeek-V3, Mixtral, and Qwen3 MoE models with performance benchmarking and checkpoint conversion tools.</li><li> Megatron Core v0.11.0 brings new capabilities for multi-data center LLM training (<a href=\"https://developer.nvidia.com/blog/turbocharge-llm-training-across-long-haul-data-center-networks-with-nvidia-nemo-framework/\">blog</a>).</li></ul><pre><code>Megatron-LM/\n‚îú‚îÄ‚îÄ megatron/\n‚îÇ   ‚îú‚îÄ‚îÄ core/                    # Megatron Core (kernels, parallelism, building blocks)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/              # Transformer models\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformer/         # Transformer building blocks\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tensor_parallel/     # Tensor parallelism\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline_parallel/   # Pipeline parallelism\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ distributed/         # Distributed training (FSDP, DDP)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ optimizer/           # Optimizers\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasets/            # Dataset loaders\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference/           # Inference engines and server\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ export/              # Model export (e.g. TensorRT-LLM)\n‚îÇ   ‚îú‚îÄ‚îÄ training/                # Training scripts\n‚îÇ   ‚îú‚îÄ‚îÄ legacy/                  # Legacy components\n‚îÇ   ‚îú‚îÄ‚îÄ post_training/           # Post-training (quantization, distillation, pruning, etc.)\n‚îÇ   ‚îî‚îÄ‚îÄ rl/                      # Reinforcement learning (RLHF, etc.)\n‚îú‚îÄ‚îÄ examples/                    # Ready-to-use training examples\n‚îú‚îÄ‚îÄ tools/                       # Utility tools\n‚îú‚îÄ‚îÄ tests/                       # Comprehensive test suite\n‚îî‚îÄ‚îÄ docs/                        # Documentation\n</code></pre><p>Our codebase efficiently trains models from 2B to 462B parameters across thousands of GPUs, achieving up to <strong>47% Model FLOP Utilization (MFU)</strong> on H100 clusters.</p><ul><li>: 131,072 tokens</li><li>: 4096 tokens</li><li>: Varied hidden size, attention heads, and layers to achieve target parameter counts</li><li><strong>Communication optimizations</strong>: Fine-grained overlapping with DP (, ), TP (), and PP (enabled by default)</li></ul><ul><li>: Successfully benchmarked 462B parameter model training</li><li>: MFU increases from 41% to 47-48% with model size</li><li>: Throughputs include all operations (data loading, optimizer steps, communication, logging)</li><li>: Full training pipeline with checkpointing and fault tolerance</li><li><em>Note: Performance results measured without training to convergence</em></li></ul><p>Our weak scaled results show superlinear scaling (MFU increases from 41% for the smallest model considered to 47-48% for the largest models); this is because larger GEMMs have higher arithmetic intensity and are consequently more efficient to execute.</p><p>We also strong scaled the standard GPT-3 model (our version has slightly more than 175 billion parameters due to larger vocabulary size) from 96 H100 GPUs to 4608 GPUs, using the same batch size of 1152 sequences throughout. Communication becomes more exposed at larger scale, leading to a reduction in MFU from 47% to 42%.</p><ul><li> - DeepSeek-V3, Qwen3, advanced parallelism, FP8 optimizations, and Blackwell enhancements</li></ul><p>We ‚ù§Ô∏è contributions! Ways to contribute:</p><ul><li>üêõ  - Help us improve reliability</li><li>üí°  - Shape the future of Megatron Core</li><li>üìù  - Make Megatron Core more accessible</li><li>üîß  - Contribute code improvements</li></ul><p>If you use Megatron in your research or project, we appreciate that you use the following citations:</p><pre><code>@article{megatron-lm,\n  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},\n  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},\n  journal={arXiv preprint arXiv:1909.08053},\n  year={2019}\n}\n</code></pre>",
      "contentLength": 5038,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/f93c361837c260f3a59acf1e129a086fedbe0fb877f0d405599470ebf0efa230/NVIDIA/Megatron-LM",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "katanemo/plano",
      "url": "https://github.com/katanemo/plano",
      "date": 1772074985,
      "author": "",
      "guid": 48390,
      "unread": true,
      "content": "<p>Delivery infrastructure for agentic apps - Plano is an AI-native proxy and data plane that offloads plumbing work, so you stay focused on your agent's core logic (via any AI framework).</p><div align=\"center\"><p><em>The AI-native proxy server and data plane for agentic apps.</em><p> Plano pulls out the rote plumbing work and decouples you from brittle framework abstractions, centralizing what shouldn‚Äôt be bespoke in every codebase - like agent routing and orchestration, rich agentic signals and traces for continuous improvement, guardrail filters for safety and moderation, and smart LLM routing APIs for model agility. Use any language or AI framework, and deliver agents faster to production.</p></p><p>Star ‚≠êÔ∏è the repo if you found Plano useful ‚Äî new releases and updates land here first.</p></div><p>Building agentic demos is easy. Shipping agentic applications safely, reliably, and repeatably to production is hard. After the thrill of a quick hack, you end up building the ‚Äúhidden middleware‚Äù to reach production: routing logic to reach the right agent, guardrail hooks for safety and moderation, evaluation and observability glue for continuous learning, and model/provider quirks scattered across frameworks and application code.</p><p>Plano solves this by moving core delivery concerns into a unified, out-of-process dataplane.</p><p>Plano pulls rote plumbing out of your framework so you can stay focused on what matters most: the core product logic of your agentic applications. Plano is backed by <a href=\"https://planoai.dev/research\">industry-leading LLM research</a> and built on <a href=\"https://envoyproxy.io\">Envoy</a> by its core contributors, who built critical infrastructure at scale for modern worklaods.</p><p> to learn how you can use Plano to improve the speed, safety and obervability of your agentic applications.</p><blockquote><p>[!IMPORTANT] Plano and the Arch family of LLMs (like Plano-Orchestrator-4B, Arch-Router, etc) are hosted free of charge in the US-central region to give you a great first-run developer experience of Plano. To scale and run in production, you can either run these LLMs locally or contact us on <a href=\"https://discord.gg/pGZf2gcwEc\">Discord</a> for API keys.</p></blockquote><h2>Build Agentic Apps with Plano</h2><p>Plano handles <strong>orchestration, model management, and observability</strong> as modular building blocks - letting you configure only what you need (edge proxying for agentic orchestration and guardrails, or LLM routing from your services, or both together) to fit cleanly into existing architectures. Below is a simple multi-agent travel agent built with Plano that showcases all three core capabilities</p><h3>1. Define Your Agents in YAML</h3><pre><code># config.yaml\nversion: v0.3.0\n\n# What you declare: Agent URLs and natural language descriptions\n# What you don't write: Intent classifiers, routing logic, model fallbacks, provider adapters, or tracing instrumentation\n\nagents:\n  - id: weather_agent\n    url: http://localhost:10510\n  - id: flight_agent\n    url: http://localhost:10520\n\nmodel_providers:\n  - model: openai/gpt-4o\n    access_key: $OPENAI_API_KEY\n    default: true\n  - model: anthropic/claude-3-5-sonnet\n    access_key: $ANTHROPIC_API_KEY\n\nlisteners:\n  - type: agent\n    name: travel_assistant\n    port: 8001\n    router: plano_orchestrator_v1  # Powered by our 4B-parameter routing model. You can change this to different models\n    agents:\n      - id: weather_agent\n        description: |\n          Gets real-time weather and forecasts for any city worldwide.\n          Handles: \"What's the weather in Paris?\", \"Will it rain in Tokyo?\"\n\n      - id: flight_agent\n        description: |\n          Searches flights between airports with live status and schedules.\n          Handles: \"Flights from NYC to LA\", \"Show me flights to Seattle\"\n\ntracing:\n  random_sampling: 100  # Auto-capture traces for evaluation\n</code></pre><h3>2. Write Simple Agent Code</h3><p>Your agents are just HTTP servers that implement the OpenAI-compatible chat completions endpoint. Use any language or framework:</p><pre><code># weather_agent.py\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse\nfrom openai import AsyncOpenAI\n\napp = FastAPI()\n\n# Point to Plano's LLM gateway - it handles model routing for you\nllm = AsyncOpenAI(base_url=\"http://localhost:12001/v1\", api_key=\"EMPTY\")\n\n@app.post(\"/v1/chat/completions\")\nasync def chat(request: Request):\n    body = await request.json()\n    messages = body.get(\"messages\", [])\n    days = 7\n\n    # Your agent logic: fetch data, call APIs, run tools\n    # See demos/agent_orchestration/travel_agents/ for the full implementation\n    weather_data = await get_weather_data(request, messages, days)\n\n    # Stream the response back through Plano\n    async def generate():\n        stream = await llm.chat.completions.create(\n            model=\"openai/gpt-4o\",\n            messages=[{\"role\": \"system\", \"content\": f\"Weather: {weather_data}\"}, *messages],\n            stream=True\n        )\n        async for chunk in stream:\n            yield f\"data: {chunk.model_dump_json()}\\n\\n\"\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n</code></pre><h3>3. Start Plano &amp; Query Your Agents</h3><pre><code># Start Plano\nplanoai up config.yaml\n...\n\n# Query - Plano intelligently routes to both agents in a single conversation\ncurl http://localhost:8001/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"I want to travel from NYC to Paris next week. What is the weather like there, and can you find me some flights?\"}\n    ]\n  }'\n# ‚Üí Plano routes to weather_agent for Paris weather ‚úì\n# ‚Üí Then routes to flight_agent for NYC ‚Üí Paris flights ‚úì\n# ‚Üí Returns a complete travel plan with both weather info and flight options\n</code></pre><h3>4. Get Observability and Model Agility for Free</h3><p>Every request is traced end-to-end with OpenTelemetry - no instrumentation code needed.</p><h3>What You Didn't Have to Build</h3><table><thead><tr></tr></thead><tbody><tr><td>Write intent classifier + routing logic</td><td>Declare agent descriptions in YAML</td></tr><tr><td>Handle each provider's API quirks</td><td>Unified LLM APIs with state management</td></tr><tr><td>Instrument every service with OTEL</td><td>Automatic end-to-end traces and logs</td></tr><tr><td>Build pipeline to capture/export spans</td><td>Zero-code agentic signals</td></tr><tr><td>Update routing code, test, redeploy</td></tr></tbody></table><p> Plano uses purpose-built, lightweight LLMs (like our 4B-parameter orchestrator) instead of heavyweight frameworks or GPT-4 for routing - giving you production-grade routing at a fraction of the cost and latency.</p><p>To get in touch with us, please join our <a href=\"https://discord.gg/pGZf2gcwEc\">discord server</a>. We actively monitor that and offer support there.</p><p>Ready to try Plano? Check out our comprehensive documentation:</p><p>We would love feedback on our <a href=\"https://github.com/orgs/katanemo/projects/1\">Roadmap</a> and we welcome contributions to ! Whether you're fixing bugs, adding new features, improving documentation, or creating tutorials, your help is much appreciated. Please visit our <a href=\"https://raw.githubusercontent.com/katanemo/plano/main/CONTRIBUTING.md\">Contribution Guide</a> for more details</p><p>Star ‚≠êÔ∏è the repo if you found Plano useful ‚Äî new releases and updates land here first.</p>",
      "contentLength": 6736,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/5065011867d6724d4d4787e09a46c104d0e69953278e0622362e354e485f8e27/katanemo/plano",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "shareAI-lab/learn-claude-code",
      "url": "https://github.com/shareAI-lab/learn-claude-code",
      "date": 1772074985,
      "author": "",
      "guid": 48391,
      "unread": true,
      "content": "<p>Bash is all you need - A nano Claude Code‚Äìlike agent, built from 0 to 1</p><pre><code>                    THE AGENT PATTERN\n                    =================\n\n    User --&gt; messages[] --&gt; LLM --&gt; response\n                                      |\n                            stop_reason == \"tool_use\"?\n                           /                          \\\n                         yes                           no\n                          |                             |\n                    execute tools                    return text\n                    append results\n                    loop back -----------------&gt; messages[]\n\n\n    That's the minimal loop. Every AI coding agent needs this loop.\n    Production agents add policy, permissions, and lifecycle layers.\n</code></pre><p><strong>12 progressive sessions, from a simple loop to isolated autonomous execution.</strong><strong>Each session adds one mechanism. Each mechanism has one motto.</strong></p><blockquote><p> ‚Äî one tool + one loop = an agent</p><p> ‚Äî adding tools means adding handlers, not rewriting the loop</p><p> ‚Äî visible plans improve task completion</p><p><em>\"Process isolation = context isolation\"</em> ‚Äî fresh messages[] per subagent</p><p><em>\"Load on demand, not upfront\"</em> ‚Äî inject knowledge via tool_result, not system prompt</p><p> ‚Äî forget old context to enable infinite sessions</p><p><em>\"State survives /compact\"</em> ‚Äî file-based state outlives context compression</p><p> ‚Äî non-blocking threads + notification queue</p><p><em>\"Append to send, drain to read\"</em> ‚Äî async mailboxes for persistent teammates</p><p><em>\"Same request_id, two protocols\"</em> ‚Äî one FSM pattern powers shutdown + plan approval</p><p><em>\"Poll, claim, work, repeat\"</em> ‚Äî no coordinator needed, agents self-organize</p><p><em>\"Isolate by directory, coordinate by task ID\"</em> ‚Äî task board + optional worktree lanes</p></blockquote><pre><code>def agent_loop(messages):\n    while True:\n        response = client.messages.create(\n            model=MODEL, system=SYSTEM,\n            messages=messages, tools=TOOLS,\n        )\n        messages.append({\"role\": \"assistant\",\n                         \"content\": response.content})\n\n        if response.stop_reason != \"tool_use\":\n            return\n\n        results = []\n        for block in response.content:\n            if block.type == \"tool_use\":\n                output = TOOL_HANDLERS[block.name](**block.input)\n                results.append({\n                    \"type\": \"tool_result\",\n                    \"tool_use_id\": block.id,\n                    \"content\": output,\n                })\n        messages.append({\"role\": \"user\", \"content\": results})\n</code></pre><p>Every session layers one mechanism on top of this loop -- without changing the loop itself.</p><p>This repository is a 0-&gt;1 learning project for building a nano Claude Code-like agent. It intentionally simplifies or omits several production mechanisms:</p><ul><li>Full event/hook buses (for example PreToolUse, SessionStart/End, ConfigChange). s12 includes only a minimal append-only lifecycle event stream for teaching.</li><li>Rule-based permission governance and trust workflows</li><li>Session lifecycle controls (resume/fork) and advanced worktree lifecycle controls</li><li>Full MCP runtime details (transport/OAuth/resource subscribe/polling)</li></ul><p>Treat the team JSONL mailbox protocol in this repo as a teaching implementation, not a claim about any specific production internals.</p><pre><code>git clone https://github.com/shareAI-lab/learn-claude-code\ncd learn-claude-code\npip install -r requirements.txt\ncp .env.example .env   # Edit .env with your ANTHROPIC_API_KEY\n\npython agents/s01_agent_loop.py       # Start here\npython agents/s11_autonomous_agents.py  # Full autonomous team\npython agents/s12_worktree_task_isolation.py  # Task-aware worktree isolation\n</code></pre><p>Interactive visualizations, step-through diagrams, source viewer, and documentation.</p><pre><code>cd web &amp;&amp; npm install &amp;&amp; npm run dev   # http://localhost:3000\n</code></pre><pre><code>Phase 1: THE LOOP                    Phase 2: PLANNING &amp; KNOWLEDGE\n==================                   ==============================\ns01  The Agent Loop          [1]     s03  TodoWrite               [5]\n     while + stop_reason                  TodoManager + nag reminder\n     |                                    |\n     +-&gt; s02  Tools              [4]     s04  Subagents            [5]\n              dispatch map: name-&gt;handler     fresh messages[] per child\n                                              |\n                                         s05  Skills               [5]\n                                              SKILL.md via tool_result\n                                              |\n                                         s06  Compact              [5]\n                                              3-layer compression\n\nPhase 3: PERSISTENCE                 Phase 4: TEAMS\n==================                   =====================\ns07  Tasks                   [8]     s09  Agent Teams             [9]\n     file-based CRUD + deps graph         teammates + JSONL mailboxes\n     |                                    |\ns08  Background Tasks        [6]     s10  Team Protocols          [12]\n     daemon threads + notify queue        shutdown + plan approval FSM\n                                          |\n                                     s11  Autonomous Agents       [14]\n                                          idle cycle + auto-claim\n                                     |\n                                     s12  Worktree Isolation      [16]\n                                          task coordination + optional isolated execution lanes\n\n                                     [N] = number of tools\n</code></pre><pre><code>learn-claude-code/\n|\n|-- agents/                        # Python reference implementations (s01-s12 + full)\n|-- docs/{en,zh,ja}/               # Mental-model-first documentation (3 languages)\n|-- web/                           # Interactive learning platform (Next.js)\n|-- skills/                        # Skill files for s05\n+-- .github/workflows/ci.yml      # CI: typecheck + build\n</code></pre><p>Mental-model-first: problem, solution, ASCII diagram, minimal code. Available in <a href=\"https://raw.githubusercontent.com/shareAI-lab/learn-claude-code/main/docs/en/\">English</a> | <a href=\"https://raw.githubusercontent.com/shareAI-lab/learn-claude-code/main/docs/zh/\">‰∏≠Êñá</a> | <a href=\"https://raw.githubusercontent.com/shareAI-lab/learn-claude-code/main/docs/ja/\">Êó•Êú¨Ë™û</a>.</p><table><tbody><tr></tr><tr></tr><tr></tr><tr><td><em>Process isolation = context isolation</em></td></tr><tr><td><em>Load on demand, not upfront</em></td></tr><tr></tr><tr></tr><tr></tr><tr><td><em>Append to send, drain to read</em></td></tr><tr><td><em>Same request_id, two protocols</em></td></tr><tr><td><em>Poll, claim, work, repeat</em></td></tr><tr><td>Worktree + Task Isolation</td><td><em>Isolate by directory, coordinate by task ID</em></td></tr></tbody></table><p><strong>The model is the agent. Our job is to give it tools and stay out of the way.</strong></p>",
      "contentLength": 6206,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/e052ff8a97ef6e2c1c726e3709265726552488956cf6af1b8ea102ed483b3cd3/shareAI-lab/learn-claude-code",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "bytedance/deer-flow",
      "url": "https://github.com/bytedance/deer-flow",
      "date": 1772074985,
      "author": "",
      "guid": 48392,
      "unread": true,
      "content": "<p>An open-source SuperAgent harness that researches, codes, and creates. With the help of sandboxes, memories, tools, skills and subagents, it handles different levels of tasks that could take minutes to hours.</p><blockquote><p>On February 28th, 2026, DeerFlow claimed the üèÜ #1 spot on GitHub Trending following the launch of version 2. Thanks a million to our incredible community ‚Äî you made this happen! üí™üî•</p></blockquote><p>DeerFlow (eep xploration and fficient esearch ) is an open-source  that orchestrates , , and  to do almost anything ‚Äî powered by .</p><blockquote><p>[!NOTE] <strong>DeerFlow 2.0 is a ground-up rewrite.</strong> It shares no code with v1. If you're looking for the original Deep Research framework, it's maintained on the <a href=\"https://github.com/bytedance/deer-flow/tree/main-1.x\"> branch</a> ‚Äî contributions there are still welcome. Active development has moved to 2.0.</p></blockquote><p>Learn more and see  on our official website.</p><ol><li><p><strong>Clone the DeerFlow repository</strong></p><pre><code>git clone https://github.com/bytedance/deer-flow.git\ncd deer-flow\n</code></pre></li><li><p><strong>Generate local configuration files</strong></p><p>From the project root directory (), run:</p><p>This command creates local configuration files based on the provided example templates.</p></li><li><p><strong>Configure your preferred model(s)</strong></p><p>Edit  and define at least one model:</p><pre><code>models:\n  - name: gpt-4                       # Internal identifier\n    display_name: GPT-4               # Human-readable name\n    use: langchain_openai:ChatOpenAI  # LangChain class path\n    model: gpt-4                      # Model identifier for API\n    api_key: $OPENAI_API_KEY          # API key (recommended: use env var)\n    max_tokens: 4096                  # Maximum tokens per request\n    temperature: 0.7                  # Sampling temperature\n</code></pre></li><li><p><strong>Set API keys for your configured model(s)</strong></p><p>Choose one of the following methods:</p></li></ol><ul><li><p>Option A: Edit the  file in the project root (Recommended)</p><pre><code>TAVILY_API_KEY=your-tavily-api-key\nOPENAI_API_KEY=your-openai-api-key\n# Add other provider keys as needed\n</code></pre></li><li><p>Option B: Export environment variables in your shell</p><pre><code>export OPENAI_API_KEY=your-openai-api-key\n</code></pre></li><li><p>Option C: Edit  directly (Not recommended for production)</p><pre><code>models:\n  - name: gpt-4\n    api_key: your-actual-api-key-here  # Replace placeholder\n</code></pre></li></ul><h4>Option 1: Docker (Recommended)</h4><p>The fastest way to get started with a consistent environment:</p><ol><li><pre><code>make docker-init    # Pull sandbox image (Only once or when image updates)\nmake docker-start   # Start services (auto-detects sandbox mode from config.yaml)\n</code></pre><p> now starts  only when  uses provisioner mode (<code>sandbox.use: src.community.aio_sandbox:AioSandboxProvider</code> with ).</p></li></ol><h4>Option 2: Local Development</h4><p>If you prefer running services locally:</p><ol><li><pre><code>make check  # Verifies Node.js 22+, pnpm, uv, nginx\n</code></pre></li><li><p><strong>(Optional) Pre-pull sandbox image</strong>:</p><pre><code># Recommended if using Docker/Container-based sandbox\nmake setup-sandbox\n</code></pre></li></ol><p>DeerFlow supports multiple sandbox execution modes:</p><ul><li> (runs sandbox code directly on the host machine)</li><li> (runs sandbox code in isolated Docker containers)</li><li><strong>Docker Execution with Kubernetes</strong> (runs sandbox code in Kubernetes pods via provisioner service)</li></ul><p>For Docker development, service startup follows  sandbox mode. In Local/Docker modes,  is not started.</p><p>DeerFlow supports configurable MCP servers and skills to extend its capabilities. See the <a href=\"https://raw.githubusercontent.com/bytedance/deer-flow/main/backend/docs/MCP_SERVER.md\">MCP Server Guide</a> for detailed instructions.</p><h2>From Deep Research to Super Agent Harness</h2><p>DeerFlow started as a Deep Research framework ‚Äî and the community ran with it. Since launch, developers have pushed it far beyond research: building data pipelines, generating slide decks, spinning up dashboards, automating content workflows. Things we never anticipated.</p><p>That told us something important: DeerFlow wasn't just a research tool. It was a  ‚Äî a runtime that gives agents the infrastructure to actually get work done.</p><p>So we rebuilt it from scratch.</p><p>DeerFlow 2.0 is no longer a framework you wire together. It's a super agent harness ‚Äî batteries included, fully extensible. Built on LangGraph and LangChain, it ships with everything an agent needs out of the box: a filesystem, memory, skills, sandboxed execution, and the ability to plan and spawn sub-agents for complex, multi-step tasks.</p><p>Use it as-is. Or tear it apart and make it yours.</p><p>Skills are what make DeerFlow do .</p><p>A standard Agent Skill is a structured capability module ‚Äî a Markdown file that defines a workflow, best practices, and references to supporting resources. DeerFlow ships with built-in skills for research, report generation, slide creation, web pages, image and video generation, and more. But the real power is extensibility: add your own skills, replace the built-in ones, or combine them into compound workflows.</p><p>Skills are loaded progressively ‚Äî only when the task needs them, not all at once. This keeps the context window lean and makes DeerFlow work well even with token-sensitive models.</p><p>Tools follow the same philosophy. DeerFlow comes with a core toolset ‚Äî web search, web fetch, file operations, bash execution ‚Äî and supports custom tools via MCP servers and Python functions. Swap anything. Add anything.</p><pre><code># Paths inside the sandbox container\n/mnt/skills/public\n‚îú‚îÄ‚îÄ research/SKILL.md\n‚îú‚îÄ‚îÄ report-generation/SKILL.md\n‚îú‚îÄ‚îÄ slide-creation/SKILL.md\n‚îú‚îÄ‚îÄ web-page/SKILL.md\n‚îî‚îÄ‚îÄ image-generation/SKILL.md\n\n/mnt/skills/custom\n‚îî‚îÄ‚îÄ your-custom-skill/SKILL.md      ‚Üê yours\n</code></pre><p>Complex tasks rarely fit in a single pass. DeerFlow decomposes them.</p><p>The lead agent can spawn sub-agents on the fly ‚Äî each with its own scoped context, tools, and termination conditions. Sub-agents run in parallel when possible, report back structured results, and the lead agent synthesizes everything into a coherent output.</p><p>This is how DeerFlow handles tasks that take minutes to hours: a research task might fan out into a dozen sub-agents, each exploring a different angle, then converge into a single report ‚Äî or a website ‚Äî or a slide deck with generated visuals. One harness, many hands.</p><p>DeerFlow doesn't just  about doing things. It has its own computer.</p><p>Each task runs inside an isolated Docker container with a full filesystem ‚Äî skills, workspace, uploads, outputs. The agent reads, writes, and edits files. It executes bash commands and codes. It views images. All sandboxed, all auditable, zero contamination between sessions.</p><p>This is the difference between a chatbot with tool access and an agent with an actual execution environment.</p><pre><code># Paths inside the sandbox container\n/mnt/user-data/\n‚îú‚îÄ‚îÄ uploads/          ‚Üê your files\n‚îú‚îÄ‚îÄ workspace/        ‚Üê agents' working directory\n‚îî‚îÄ‚îÄ outputs/          ‚Üê final deliverables\n</code></pre><p><strong>Isolated Sub-Agent Context</strong>: Each sub-agent runs in its own isolated context. This means that the sub-agent will not be able to see the context of the main agent or other sub-agents. This is important to ensure that the sub-agent is able to focus on the task at hand and not be distracted by the context of the main agent or other sub-agents.</p><p>: Within a session, DeerFlow manages context aggressively ‚Äî summarizing completed sub-tasks, offloading intermediate results to the filesystem, compressing what's no longer immediately relevant. This lets it stay sharp across long, multi-step tasks without blowing the context window.</p><p>Most agents forget everything the moment a conversation ends. DeerFlow remembers.</p><p>Across sessions, DeerFlow builds a persistent memory of your profile, preferences, and accumulated knowledge. The more you use it, the better it knows you ‚Äî your writing style, your technical stack, your recurring workflows. Memory is stored locally and stays under your control.</p><p>DeerFlow is model-agnostic ‚Äî it works with any LLM that implements the OpenAI-compatible API. That said, it performs best with models that support:</p><ul><li> (100k+ tokens) for deep research and multi-step tasks</li><li> for adaptive planning and complex decomposition</li><li> for image understanding and video comprehension</li><li> for reliable function calling and structured outputs</li></ul><p>DeerFlow can be used as an embedded Python library without running the full HTTP services. The  provides direct in-process access to all agent and Gateway capabilities, returning the same response schemas as the HTTP Gateway API:</p><pre><code>from src.client import DeerFlowClient\n\nclient = DeerFlowClient()\n\n# Chat\nresponse = client.chat(\"Analyze this paper for me\", thread_id=\"my-thread\")\n\n# Streaming (LangGraph SSE protocol: values, messages-tuple, end)\nfor event in client.stream(\"hello\"):\n    if event.type == \"messages-tuple\" and event.data.get(\"type\") == \"ai\":\n        print(event.data[\"content\"])\n\n# Configuration &amp; management ‚Äî returns Gateway-aligned dicts\nmodels = client.list_models()        # {\"models\": [...]}\nskills = client.list_skills()        # {\"skills\": [...]}\nclient.update_skill(\"web-search\", enabled=True)\nclient.upload_files(\"thread-1\", [\"./report.pdf\"])  # {\"success\": True, \"files\": [...]}\n</code></pre><p>All dict-returning methods are validated against Gateway Pydantic response models in CI (), ensuring the embedded client stays in sync with the HTTP API schemas. See  for full API documentation.</p><p>We welcome contributions! Please see <a href=\"https://raw.githubusercontent.com/bytedance/deer-flow/main/CONTRIBUTING.md\">CONTRIBUTING.md</a> for development setup, workflow, and guidelines.</p><p>Regression coverage includes Docker sandbox mode detection and provisioner kubeconfig-path handling tests in .</p><p>This project is open source and available under the <a href=\"https://raw.githubusercontent.com/bytedance/deer-flow/main/LICENSE\">MIT License</a>.</p><p>DeerFlow is built upon the incredible work of the open-source community. We are deeply grateful to all the projects and contributors whose efforts have made DeerFlow possible. Truly, we stand on the shoulders of giants.</p><p>We would like to extend our sincere appreciation to the following projects for their invaluable contributions:</p><ul><li>: Their exceptional framework powers our LLM interactions and chains, enabling seamless integration and functionality.</li><li>: Their innovative approach to multi-agent orchestration has been instrumental in enabling DeerFlow's sophisticated workflows.</li></ul><p>These projects exemplify the transformative power of open-source collaboration, and we are proud to build upon their foundations.</p><p>A heartfelt thank you goes out to the core authors of , whose vision, passion, and dedication have brought this project to life:</p><p>Your unwavering commitment and expertise have been the driving force behind DeerFlow's success. We are honored to have you at the helm of this journey.</p>",
      "contentLength": 10134,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/a1ddda11cfcf83094b926e22b12578f7b4b1563b0f512f0a20a0dd84e8fd8f62/bytedance/deer-flow",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "datawhalechina/hello-agents",
      "url": "https://github.com/datawhalechina/hello-agents",
      "date": 1772074985,
      "author": "",
      "guid": 48393,
      "unread": true,
      "content": "<p>üìö „Ää‰ªéÈõ∂ÂºÄÂßãÊûÑÂª∫Êô∫ËÉΩ‰Ωì„Äã‚Äî‚Äî‰ªéÈõ∂ÂºÄÂßãÁöÑÊô∫ËÉΩ‰ΩìÂéüÁêÜ‰∏éÂÆûË∑µÊïôÁ®ã</p><p>‚ÄÉ‚ÄÉÂ¶ÇÊûúËØ¥ 2024 Âπ¥ÊòØ\"ÁôæÊ®°Â§ßÊàò\"ÁöÑÂÖÉÂπ¥ÔºåÈÇ£‰πà 2025 Âπ¥Êó†ÁñëÂºÄÂêØ‰∫Ü\"Agent ÂÖÉÂπ¥\"„ÄÇÊäÄÊúØÁöÑÁÑ¶ÁÇπÊ≠£‰ªéËÆ≠ÁªÉÊõ¥Â§ßÁöÑÂü∫Á°ÄÊ®°ÂûãÔºåËΩ¨ÂêëÊûÑÂª∫Êõ¥ËÅ™ÊòéÁöÑÊô∫ËÉΩ‰ΩìÂ∫îÁî®„ÄÇÁÑ∂ËÄåÔºåÂΩìÂâçÁ≥ªÁªüÊÄß„ÄÅÈáçÂÆûË∑µÁöÑÊïôÁ®ãÂç¥ÊûÅÂ∫¶ÂåÆ‰πè„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÂèëËµ∑‰∫Ü Hello-Agents È°πÁõÆÔºåÂ∏åÊúõËÉΩ‰∏∫Á§æÂå∫Êèê‰æõ‰∏ÄÊú¨‰ªéÈõ∂ÂºÄÂßã„ÄÅÁêÜËÆ∫‰∏éÂÆûÊàòÂπ∂ÈáçÁöÑÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÊûÑÂª∫ÊåáÂçó„ÄÇ</p><p>‚ÄÉ‚ÄÉHello-Agents ÊòØ Datawhale Á§æÂå∫ÁöÑ„ÄÇÂ¶Ç‰ªä Agent ÊûÑÂª∫‰∏ªË¶ÅÂàÜ‰∏∫‰∏§Ê¥æÔºå‰∏ÄÊ¥æÊòØ DifyÔºåCozeÔºån8n ËøôÁ±ªËΩØ‰ª∂Â∑•Á®ãÁ±ª AgentÔºåÂÖ∂Êú¨Ë¥®ÊòØÊµÅÁ®ãÈ©±Âä®ÁöÑËΩØ‰ª∂ÂºÄÂèëÔºåLLM ‰Ωú‰∏∫Êï∞ÊçÆÂ§ÑÁêÜÁöÑÂêéÁ´ØÔºõÂè¶‰∏ÄÊ¥æÂàôÊòØ AI ÂéüÁîüÁöÑ AgentÔºåÂç≥ÁúüÊ≠£‰ª• AI È©±Âä®ÁöÑ Agent„ÄÇÊú¨ÊïôÁ®ãÊó®Âú®Â∏¶È¢ÜÂ§ßÂÆ∂Ê∑±ÂÖ•ÁêÜËß£Âπ∂ÊûÑÂª∫ÂêéËÄÖ‚Äî‚ÄîÁúüÊ≠£ÁöÑ AI Native Agent„ÄÇÊïôÁ®ãÂ∞ÜÂ∏¶È¢Ü‰Ω†Á©øÈÄèÊ°ÜÊû∂Ë°®Ë±°Ôºå‰ªéÊô∫ËÉΩ‰ΩìÁöÑÊ†∏ÂøÉÂéüÁêÜÂá∫ÂèëÔºåÊ∑±ÂÖ•ÂÖ∂Ê†∏ÂøÉÊû∂ÊûÑÔºåÁêÜËß£ÂÖ∂ÁªèÂÖ∏ËåÉÂºèÔºåÂπ∂ÊúÄÁªà‰∫≤ÊâãÊûÑÂª∫Ëµ∑Â±û‰∫éËá™Â∑±ÁöÑÂ§öÊô∫ËÉΩ‰ΩìÂ∫îÁî®„ÄÇÊàë‰ª¨Áõ∏‰ø°ÔºåÊúÄÂ•ΩÁöÑÂ≠¶‰π†ÊñπÂºèÂ∞±ÊòØÂä®ÊâãÂÆûË∑µ„ÄÇÂ∏åÊúõËøôÊú¨ÊïôÁ®ãËÉΩÊàê‰∏∫‰Ω†Êé¢Á¥¢Êô∫ËÉΩ‰Ωì‰∏ñÁïåÁöÑËµ∑ÁÇπÔºåËÉΩÂ§ü‰ªé‰∏ÄÂêçÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑ\"‰ΩøÁî®ËÄÖ\"ÔºåËúïÂèò‰∏∫‰∏ÄÂêçÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑ\"ÊûÑÂª∫ËÄÖ\"„ÄÇ</p><p>Â¶ÇÊûúÊÇ®Â∏åÊúõÂú®Êú¨Âú∞ÈòÖËØªÊàñË¥°ÁåÆÂÜÖÂÆπÔºåËØ∑ÂèÇËÄÉ‰∏ãÊñπÁöÑÂ≠¶‰π†ÊåáÂçó„ÄÇ</p><ul><li>üìñ  ÂÆåÂÖ®ÂÖçË¥πÂ≠¶‰π†Êú¨È°πÁõÆÊâÄÊúâÂÜÖÂÆπÔºå‰∏éÁ§æÂå∫ÂÖ±ÂêåÊàêÈïø</li><li>üîç  Ê∑±ÂÖ•ÁêÜËß£Êô∫ËÉΩ‰ΩìÁöÑÊ¶ÇÂøµ„ÄÅÂéÜÂè≤‰∏éÁªèÂÖ∏ËåÉÂºè</li><li>üèóÔ∏è  ÊéåÊè°ÁÉ≠Èó®‰Ωé‰ª£Á†ÅÂπ≥Âè∞ÂíåÊô∫ËÉΩ‰Ωì‰ª£Á†ÅÊ°ÜÊû∂ÁöÑ‰ΩøÁî®</li><li>‚öôÔ∏è  ‰∏ÄÊ≠•Ê≠•ÂÆûÁé∞‰∏ä‰∏ãÊñáÂ∑•Á®ã„ÄÅMemory„ÄÅÂçèËÆÆ„ÄÅËØÑ‰º∞Á≠âÁ≥ªÁªüÊÄßÊäÄÊúØ</li><li>ü§ù  ÊéåÊè° Agentic RLÔºå‰ªé SFT Âà∞ GRPO ÁöÑÂÖ®ÊµÅÁ®ãÂÆûÊàòËÆ≠ÁªÉ LLM</li><li>üöÄ  ÂÆûÊàòÂºÄÂèëÊô∫ËÉΩÊóÖË°åÂä©Êâã„ÄÅËµõÂçöÂ∞èÈïáÁ≠âÁªºÂêàÈ°πÁõÆ</li></ul><p>‚ÄÉ‚ÄÉÊ¨¢ËøéÂ§ßÂÆ∂Â∞ÜÂú®Â≠¶‰π† Hello-Agents Êàñ Agent Áõ∏ÂÖ≥ÊäÄÊúØ‰∏≠ÁöÑÁã¨Âà∞ËßÅËß£„ÄÅÂÆûË∑µÊÄªÁªìÔºå‰ª• PR ÁöÑÂΩ¢ÂºèË¥°ÁåÆÂà∞Á§æÂå∫Á≤æÈÄâ„ÄÇÂ¶ÇÊûúÊòØÁã¨Á´ã‰∫éÊ≠£ÊñáÁöÑÂÜÖÂÆπÔºå‰πüÂèØ‰ª•ÊäïÁ®øËá≥ Extra-ChapterÔºÅ</p><p><em><strong>Êú¨ Hello-Agents PDF ÊïôÁ®ãÂÆåÂÖ®ÂºÄÊ∫êÂÖçË¥π„ÄÇ‰∏∫Èò≤Ê≠¢ÂêÑÁ±ªËê•ÈîÄÂè∑Âä†Ê∞¥Âç∞ÂêéË¥©ÂçñÁªôÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÂàùÂ≠¶ËÄÖÔºåÊàë‰ª¨ÁâπÂú∞Âú® PDF Êñá‰ª∂‰∏≠È¢ÑÂÖàÊ∑ªÂä†‰∫Ü‰∏çÂΩ±ÂìçÈòÖËØªÁöÑ Datawhale ÂºÄÊ∫êÊ†áÂøóÊ∞¥Âç∞ÔºåÊï¨ËØ∑Ë∞ÖËß£ÔΩû</strong></em></p><p>‚ÄÉ‚ÄÉÊ¨¢Ëøé‰Ω†ÔºåÊú™Êù•ÁöÑÊô∫ËÉΩÁ≥ªÁªüÊûÑÂª∫ËÄÖÔºÅÂú®ÂºÄÂêØËøôÊÆµÊøÄÂä®‰∫∫ÂøÉÁöÑÊóÖÁ®ã‰πãÂâçÔºåËØ∑ÂÖÅËÆ∏Êàë‰ª¨Áªô‰Ω†‰∏Ä‰∫õÊ∏ÖÊô∞ÁöÑÊåáÂºï„ÄÇ</p><p>‚ÄÉ‚ÄÉÊú¨È°πÁõÆÂÜÖÂÆπÂÖºÈ°æÁêÜËÆ∫‰∏éÂÆûÊàòÔºåÊó®Âú®Â∏ÆÂä©‰Ω†Á≥ªÁªüÊÄßÂú∞ÊéåÊè°‰ªéÂçï‰∏™Êô∫ËÉΩ‰ΩìÂà∞Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑËÆæËÆ°‰∏éÂºÄÂèëÂÖ®ÊµÅÁ®ã„ÄÇÂõ†Ê≠§ÔºåÂ∞§ÂÖ∂ÈÄÇÂêàÊúâ‰∏ÄÂÆöÁºñÁ®ãÂü∫Á°ÄÁöÑ  ‰ª•ÂèäÂØπÂâçÊ≤ø AI ÊäÄÊúØÊä±ÊúâÊµìÂéöÂÖ¥Ë∂£ÁöÑ „ÄÇÂú®Â≠¶‰π†Êú¨È°πÁõÆ‰πãÂâçÔºåÊàë‰ª¨Â∏åÊúõ‰Ω†ÂÖ∑Â§áÂü∫Á°ÄÁöÑ Python ÁºñÁ®ãËÉΩÂäõÔºåÂπ∂ÂØπÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊúâÂü∫Êú¨ÁöÑÊ¶ÇÂøµÊÄß‰∫ÜËß£Ôºà‰æãÂ¶ÇÔºåÁü•ÈÅìÂ¶Ç‰ΩïÈÄöËøá API Ë∞ÉÁî®‰∏Ä‰∏™ LLMÔºâ„ÄÇÈ°πÁõÆÁöÑÈáçÁÇπÊòØÂ∫îÁî®‰∏éÊûÑÂª∫ÔºåÂõ†Ê≠§‰Ω†Êó†ÈúÄÂÖ∑Â§áÊ∑±ÂéöÁöÑÁÆóÊ≥ïÊàñÊ®°ÂûãËÆ≠ÁªÉËÉåÊôØ„ÄÇ</p><p>‚ÄÉ‚ÄÉÈ°πÁõÆÂàÜ‰∏∫‰∫îÂ§ßÈÉ®ÂàÜÔºåÊØè‰∏ÄÈÉ®ÂàÜÈÉΩÊòØÈÄöÂæÄ‰∏ã‰∏ÄÈò∂ÊÆµÁöÑÂùöÂÆûÈò∂Ê¢ØÔºö</p><ul><li><p>ÔºàÁ¨¨‰∏ÄÁ´†ÔΩûÁ¨¨‰∏âÁ´†ÔºâÔºåÊàë‰ª¨Â∞Ü‰ªéÊô∫ËÉΩ‰ΩìÁöÑÂÆö‰πâ„ÄÅÁ±ªÂûã‰∏éÂèëÂ±ïÂéÜÂè≤ËÆ≤Ëµ∑Ôºå‰∏∫‰Ω†Ê¢≥ÁêÜ\"Êô∫ËÉΩ‰Ωì\"Ëøô‰∏ÄÊ¶ÇÂøµÁöÑÊù•ÈæôÂéªËÑâ„ÄÇÈöèÂêéÔºåÊàë‰ª¨‰ºöÂø´ÈÄüÂ∑©Âõ∫Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ†∏ÂøÉÁü•ËØÜÔºå‰∏∫‰Ω†ÁöÑÂÆûË∑µ‰πãÊóÖÊâì‰∏ãÂùöÂÆûÁöÑÁêÜËÆ∫Âú∞Âü∫„ÄÇ</p></li><li><p>ÔºàÁ¨¨ÂõõÁ´†ÔΩûÁ¨¨‰∏ÉÁ´†ÔºâÔºåËøôÊòØ‰Ω†Âä®ÊâãÂÆûË∑µÁöÑËµ∑ÁÇπ„ÄÇ‰Ω†Â∞Ü‰∫≤ÊâãÂÆûÁé∞ ReAct Á≠âÁªèÂÖ∏ËåÉÂºèÔºå‰ΩìÈ™å Coze Á≠â‰Ωé‰ª£Á†ÅÂπ≥Âè∞ÁöÑ‰æøÊç∑ÔºåÂπ∂ÊéåÊè° Langgraph Á≠â‰∏ªÊµÅÊ°ÜÊû∂ÁöÑÂ∫îÁî®„ÄÇÊúÄÁªàÔºåÊàë‰ª¨Ëøò‰ºöÂ∏¶‰Ω†‰ªéÈõ∂ÂºÄÂßãÊûÑÂª∫‰∏Ä‰∏™Â±û‰∫éËá™Â∑±ÁöÑÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåËÆ©‰Ω†ÂÖºÂÖ∑‚ÄúÁî®ËΩÆÂ≠ê‚Äù‰∏é‚ÄúÈÄ†ËΩÆÂ≠ê‚ÄùÁöÑËÉΩÂäõ„ÄÇ</p></li><li><p>ÔºàÁ¨¨ÂÖ´Á´†ÔΩûÁ¨¨ÂçÅ‰∫åÁ´†ÔºâÔºåÂú®Ëøô‰∏ÄÈÉ®ÂàÜÔºå‰Ω†ÁöÑÊô∫ËÉΩ‰ΩìÂ∞Ü‚ÄúÂ≠¶‰ºö‚ÄùÊÄùËÄÉ‰∏éÂçè‰Ωú„ÄÇÊàë‰ª¨Â∞Ü‰ΩøÁî®Á¨¨‰∫åÈÉ®ÂàÜÁöÑËá™Á†îÊ°ÜÊû∂ÔºåÊ∑±ÂÖ•Êé¢Á¥¢ËÆ∞ÂøÜ‰∏éÊ£ÄÁ¥¢„ÄÅ‰∏ä‰∏ãÊñáÂ∑•Á®ã„ÄÅAgent ËÆ≠ÁªÉÁ≠âÊ†∏ÂøÉÊäÄÊúØÔºåÂπ∂Â≠¶‰π†Â§öÊô∫ËÉΩ‰ΩìÈó¥ÁöÑÈÄö‰ø°ÂçèËÆÆ„ÄÇÊúÄÁªàÔºå‰Ω†Â∞ÜÊéåÊè°ËØÑ‰º∞Êô∫ËÉΩ‰ΩìÁ≥ªÁªüÊÄßËÉΩÁöÑ‰∏ì‰∏öÊñπÊ≥ï„ÄÇ</p></li><li><p>ÔºàÁ¨¨ÂçÅ‰∏âÁ´†ÔΩûÁ¨¨ÂçÅ‰∫îÁ´†ÔºâÔºåËøôÈáåÊòØÁêÜËÆ∫‰∏éÂÆûË∑µÁöÑ‰∫§Ê±áÁÇπ„ÄÇ‰Ω†Â∞ÜÊääÊâÄÂ≠¶Ëûç‰ºöË¥ØÈÄöÔºå‰∫≤ÊâãÊâìÈÄ†Êô∫ËÉΩÊóÖË°åÂä©Êâã„ÄÅËá™Âä®ÂåñÊ∑±Â∫¶Á†îÁ©∂Êô∫ËÉΩ‰ΩìÔºå‰πÉËá≥‰∏Ä‰∏™Ê®°ÊãüÁ§æ‰ºöÂä®ÊÄÅÁöÑËµõÂçöÂ∞èÈïáÔºåÂú®ÁúüÂÆûÊúâË∂£ÁöÑÈ°πÁõÆ‰∏≠Ê∑¨ÁÇº‰Ω†ÁöÑÊûÑÂª∫ËÉΩÂäõ„ÄÇ</p></li><li><p>ÔºàÁ¨¨ÂçÅÂÖ≠Á´†ÔºâÔºåÂú®ÊóÖÁ®ãÁöÑÁªàÁÇπÔºå‰Ω†Â∞ÜËøéÊù•‰∏Ä‰∏™ÊØï‰∏öËÆæËÆ°ÔºåÊûÑÂª∫‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ„ÄÅÂ±û‰∫é‰Ω†Ëá™Â∑±ÁöÑÂ§öÊô∫ËÉΩ‰ΩìÂ∫îÁî®ÔºåÂÖ®Èù¢Ê£ÄÈ™å‰Ω†ÁöÑÂ≠¶‰π†ÊàêÊûú„ÄÇÊàë‰ª¨ËøòÂ∞Ü‰∏é‰Ω†‰∏ÄÂêåÂ±ïÊúõÊô∫ËÉΩ‰ΩìÁöÑÊú™Êù•ÔºåÊé¢Á¥¢ÊøÄÂä®‰∫∫ÂøÉÁöÑÂâçÊ≤øÊñπÂêë„ÄÇ</p></li></ul><p>‚ÄÉ‚ÄÉÊô∫ËÉΩ‰ΩìÊòØ‰∏Ä‰∏™È£ûÈÄüÂèëÂ±ï‰∏îÊûÅÂ∫¶‰æùËµñÂÆûË∑µÁöÑÈ¢ÜÂüü„ÄÇ‰∏∫‰∫ÜËé∑ÂæóÊúÄ‰Ω≥ÁöÑÂ≠¶‰π†ÊïàÊûúÔºåÊàë‰ª¨Âú®È°πÁõÆÁöÑÊñá‰ª∂Â§πÂÜÖÊèê‰æõ‰∫ÜÈÖçÂ•óÁöÑÂÖ®ÈÉ®‰ª£Á†ÅÔºåÂº∫ÁÉàÂª∫ËÆÆ‰Ω†„ÄÇËØ∑Âä°ÂøÖ‰∫≤ÊâãËøêË°å„ÄÅË∞ÉËØïÁîöËá≥‰øÆÊîπÈ°πÁõÆÈáåÊèê‰æõÁöÑÊØè‰∏Ä‰ªΩ‰ª£Á†Å„ÄÇÊ¨¢Ëøé‰Ω†ÈöèÊó∂ÂÖ≥Ê≥® Datawhale ‰ª•ÂèäÂÖ∂‰ªñ Agent Áõ∏ÂÖ≥Á§æÂå∫ÔºåÂΩìÈÅáÂà∞ÈóÆÈ¢òÊó∂Ôºå‰Ω†ÂèØ‰ª•ÈöèÊó∂Âú®Êú¨È°πÁõÆÁöÑ issue Âå∫ÊèêÈóÆ„ÄÇ</p><p>‚ÄÉ‚ÄÉÁé∞Âú®ÔºåÂáÜÂ§áÂ•ΩËøõÂÖ•Êô∫ËÉΩ‰ΩìÁöÑÂ•áÂ¶ô‰∏ñÁïå‰∫ÜÂêóÔºüËÆ©Êàë‰ª¨Âç≥ÂàªÂêØÁ®ãÔºÅ</p><ul><li>ËßÜÈ¢ëËØæÁ®ãÈôÜÁª≠ÊîæÂá∫ÔºàÂ∞Ü‰ºöÊõ¥Âä†ÁªÜËá¥ÔºåÂÆûË∑µËØæÂ∏¶È¢ÜÂ§ßÂÆ∂‰ªéËÆæËÆ°ÊÄùË∑ØÂà∞ÂÆûÊñΩÔºåÊéà‰∫∫‰ª•È±º‰πüÊéà‰∫∫‰ª•Ê∏îÔºâ</li><li>ÂÆåÂñÑHelloAgentsÊ°ÜÊû∂ÔºåÂºÄÂ±ïDevÂàÜÊîØÁªßÁª≠Áª¥Êä§ÔºåÂÖºÂÆπÂ≠¶‰π†ÁâàÊú¨„ÄÇ</li><li>ÊÑüË∞¢Â§ßÂÆ∂Âä©Âäõ2W Star! ËææÂà∞3W StarÂ∞Ü‰ºöÊõ¥Êñ∞Áª≠‰ΩúÔºå„Ää‰ªéÈõ∂ÂºÄÂßãËÆ≠ÁªÉÊô∫ËÉΩ‰Ωì„ÄãÔºåÂ∏ÆÂä©ÊØè‰∏Ä‰∏™Â≠¶‰π†ËÄÖÊéåÊè°‰ªéÈõ∂Âà∞‰∏ÄËÆ≠ÁªÉËá™ÂÆö‰πâÂú∫ÊôØÊô∫ËÉΩ‰ΩìÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇ</li></ul><ul><li>üêõ  - ÂèëÁé∞ÂÜÖÂÆπÊàñ‰ª£Á†ÅÈóÆÈ¢òÔºåËØ∑Êèê‰∫§ Issue</li><li>üìù  - Â∏ÆÂä©ÊîπËøõÊïôÁ®ãÔºåÊèê‰∫§‰Ω†ÁöÑ Pull Request</li><li>‚úçÔ∏è  - Âú®\"Á§æÂå∫Ë¥°ÁåÆÁ≤æÈÄâ\"‰∏≠ÂàÜ‰∫´‰Ω†ÁöÑÂ≠¶‰π†Á¨îËÆ∞ÂíåÈ°πÁõÆ</li></ul><ul></ul><div align=\"center\"><p>‚≠ê Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåËØ∑ÁªôÊàë‰ª¨‰∏Ä‰∏™ StarÔºÅ</p></div><div align=\"center\"><img src=\"https://github.com/datawhalechina/%E8%AF%BB%E8%80%85%E7%BE%A4%E4%BA%8C%E7%BB%B4%E7%A0%81.png\" alt=\"ËØªËÄÖÁæ§‰∫åÁª¥Á†Å\" width=\"30%\"></div><div align=\"center\"><img src=\"https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/datawhale.png\" alt=\"Datawhale\" width=\"30%\"><p>Êâ´Êèè‰∫åÁª¥Á†ÅÂÖ≥Ê≥® Datawhale ÂÖ¨‰ºóÂè∑ÔºåËé∑ÂèñÊõ¥Â§ö‰ºòË¥®ÂºÄÊ∫êÂÜÖÂÆπ</p></div>",
      "contentLength": 5613,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/e826f77b9ef368fc9eeac1b877d0dd8aeee0a9cd77f131c79cd60cebb8a6f5b8/datawhalechina/hello-agents",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "liyupi/ai-guide",
      "url": "https://github.com/liyupi/ai-guide",
      "date": 1772074985,
      "author": "",
      "guid": 48394,
      "unread": true,
      "content": "<p>Á®ãÂ∫èÂëòÈ±ºÁöÆÁöÑ AI ËµÑÊ∫êÂ§ßÂÖ® + Vibe Coding Èõ∂Âü∫Á°ÄÊïôÁ®ãÔºåÂàÜ‰∫´Â§ßÊ®°ÂûãÈÄâÊã©ÊåáÂçóÔºàDeepSeek / GPT / Gemini / ClaudeÔºâ„ÄÅÊúÄÊñ∞ AI ËµÑËÆØ„ÄÅPrompt ÊèêÁ§∫ËØçÂ§ßÂÖ®„ÄÅAI Áü•ËØÜÁôæÁßëÔºàRAG / MCP / A2AÔºâ„ÄÅAI ÁºñÁ®ãÊïôÁ®ã„ÄÅAI Â∑•ÂÖ∑Áî®Ê≥ïÔºàCursor / Claude Code / OpenClaw / TRAE / Lovable / Agent SkillsÔºâ„ÄÅAI ÂºÄÂèëÊ°ÜÊû∂ÊïôÁ®ãÔºàSpring AI / LangChainÔºâ„ÄÅAI ‰∫ßÂìÅÂèòÁé∞ÊåáÂçóÔºåÂ∏Æ‰Ω†Âø´ÈÄüÊéåÊè° AI ÊäÄÊúØÔºåËµ∞Âú®Êó∂‰ª£ÂâçÊ≤ø„ÄÇÊú¨È°πÁõÆ‰∏∫ÂºÄÊ∫êÊñáÊ°£ÁâàÊú¨ÔºåÂ∑≤ÂçáÁ∫ß‰∏∫È±ºÁöÆ AI ÂØºËà™ÁΩëÁ´ô</p><p align=\"center\"><b>ÂÆåÂÖ®ÂÖçË¥πÂºÄÊîæÁöÑ AI Áü•ËØÜÂÖ±‰∫´Âπ≥Âè∞ | ÂáèÂ∞ë‰ø°ÊÅØÂ∑ÆÔºåËÆ©ÊØè‰∏™‰∫∫ÈÉΩËÉΩ‰∫´ÂèóÊäÄÊúØÁ∫¢Âà©</b></p><p>ËøôÊòØ‰∏Ä‰∏™  ÁöÑ AI Áü•ËØÜÂÖ±‰∫´Âπ≥Âè∞ÔºåÊ±áÊÄªÊï¥ÂêàÁõÆÂâçÁÉ≠Èó®ÁöÑ AI Â∑•ÂÖ∑Áõ∏ÂÖ≥‰ø°ÊÅØÔºåÂåÖÊã¨‰∫ßÂìÅ‰ªãÁªç„ÄÅ‰ΩøÁî®ÊåáÂçó„ÄÅÂ∑•ÂÖ∑ÊµãËØÑ„ÄÅÊäÄÂ∑ßÂàÜ‰∫´„ÄÅÂ∫îÁî®Âú∫ÊôØ„ÄÅAI ÂèòÁé∞„ÄÅË°å‰∏öËµÑËÆØ„ÄÅÊïôÁ®ãËµÑÊ∫êÁ≠â‰∏ÄÁ≥ªÂàóÂÜÖÂÆπ„ÄÇ</p><p>È±ºÁöÆÂ∏åÊúõÂ∏¶È¢ÜÂ§ßÂÆ∂ÊâìÁ†¥ AI ÊäÄÊúØÁöÑ‰ø°ÊÅØÂ£ÅÂûíÔºåËÆ©ÊØè‰∏™‰∫∫ÈÉΩËÉΩÂπ≥Á≠âËé∑Âèñ AI Êó∂‰ª£ÁöÑÂ∑•ÂÖ∑‰∏éËÆ§Áü•ÔºåÂà©Áî®ÁßëÊäÄËÆ©ÁîüÊ¥ªÊõ¥ÁæéÂ•Ω„ÄÇ</p><h2>üî• È±ºÁöÆÁöÑ Vibe Coding Èõ∂Âü∫Á°ÄÂÖ•Èó®ÊïôÁ®ã</h2><p>Â¶Ç‰ªä  Â∑≤ÁªèÁÅ´ÈÅçÂÖ®ÁΩëÔºÅ‰∏ç‰ªÖÊòØÁ®ãÂ∫èÂëòÔºåËøûËÆæËÆ°Â∏à„ÄÅ‰∫ßÂìÅËøêËê•„ÄÅÁîöËá≥ÂÆåÂÖ®‰∏çÊáÇÊäÄÊúØÁöÑ‰∫∫ÈÉΩÂºÄÂßãÁî® Vibe Coding ÂÆûÁé∞Ëá™Â∑±ÁöÑÊÉ≥Ê≥ïÔºåÁî® AI ÂÅöÂá∫‰∫ÜËá™Â∑±ÁöÑ‰∫ßÂìÅÂπ∂ÁõàÂà©ÂèòÁé∞„ÄÇ</p><p><strong>Â∏ÆÂä©‰ªª‰Ωï‰∫∫Âø´ÈÄüÊéåÊè° Vibe CodingÔºåÂì™ÊÄïÈõ∂Âü∫Á°ÄÔºå‰πüËÉΩÂø´ÈÄüÂºÄÂèë‰∏äÁ∫øËá™Â∑±ÁöÑ‰∫ßÂìÅÂπ∂ÁõàÂà©„ÄÇ</strong></p><p>Ëá≠‰∏çË¶ÅËÑ∏‰∏Ä‰∏ãÔºåÊàëÊï¢ËØ¥ËøôÂ•óÂÖçË¥πÊïôÁ®ãÂêäÊâì 90% ÁöÑ‰ªòË¥π Vibe Coding ÂÜÖÂÆπ„ÄÇ</p><p>ÊàëÁ≤æÂøÉÊ¢≥ÁêÜ‰∫ÜÂÜÖÂÆπÁªìÊûÑÔºåËÆ©‰Ω†ËÉΩÂ§ü‰∏ÄÊù°ÈæôÂ≠¶‰π†ÔºåÊàñËÄÖÂø´ÈÄüÊâæÂà∞ÈÄÇÂêàËá™Â∑±ÈòÖËØªÁöÑÂÜÖÂÆπ„ÄÇ</p><ul><li>Âü∫Á°ÄÂøÖËØªÔºöÂ∏Æ‰Ω†Âø´ÈÄüÁêÜËß£ Vibe Coding Âπ∂‰∏äÊâãÂÆûË∑µÔºå10 ÂàÜÈíüÂÅöÂá∫Á¨¨‰∏Ä‰∏™‰ΩúÂìÅ</li><li>ÁºñÁ®ãÂ∑•ÂÖ∑ÔºöÂ∏Æ‰Ω†ÈÄâÊã©ÈÄÇÂêàËá™Â∑±ÁöÑ AI ÁºñÁ®ãÂ∑•ÂÖ∑ÔºåÂåÖÊã¨ AI Ê®°ÂûãÈÄâÊã©„ÄÅAI Èõ∂‰ª£Á†ÅÂπ≥Âè∞„ÄÅAI Êô∫ËÉΩ‰ΩìÂπ≥Âè∞„ÄÅAI ‰ª£Á†ÅÁºñËæëÂô®„ÄÅAI ÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑„ÄÅIDE Êèí‰ª∂Á≠â</li><li>È°πÁõÆÂÆûÊàòÔºöÊâãÊääÊâãÂ∏¶‰Ω†‰ªé 0 Âà∞ 1 ÂÅöÂá∫ÁúüÂÆûÂèØÁî®ÁöÑ‰∫ßÂìÅÔºåË¶ÜÁõñ‰∏™‰∫∫Â∑•ÂÖ∑„ÄÅAI Â∫îÁî®„ÄÅÂÖ®Ê†àÂ∫îÁî®„ÄÅÂ∞èÁ®ãÂ∫èÁ≠âÂ§öÁßçÁ±ªÂûã</li><li>ÁªèÈ™åÊäÄÂ∑ßÔºöÂ∏Æ‰Ω†ÊèêÂçá Vibe Coding ÊïàÁéáÂíåË¥®ÈáèÔºåÂåÖÊã¨Ê†∏ÂøÉÂøÉÊ≥ï„ÄÅÂØπËØùÂ∑•Á®ã„ÄÅ‰∏ä‰∏ãÊñáÁÆ°ÁêÜ„ÄÅÂπªËßâÂ§ÑÁêÜ„ÄÅ‰ª£Á†ÅË¥®Èáè‰øùÈöúÁ≠â</li><li>‰∫ßÂìÅÂèòÁé∞ÔºöÊïô‰Ω†Â¶Ç‰ΩïËÆ©‰∫ßÂìÅ‰∫ßÁîü‰ª∑ÂÄºÔºåÊ∂µÁõñÈúÄÊ±ÇÂàÜÊûê„ÄÅÊäÄÊúØÈÄâÂûã„ÄÅÊû∂ÊûÑËÆæËÆ°„ÄÅÁõàÂà©Ê®°Âºè„ÄÅSEO ‰ºòÂåñ„ÄÅËá™Â™í‰ΩìËøêËê•Á≠â</li><li>ÁºñÁ®ãÂ≠¶‰π†Ôºö‰∏∫ÊÉ≥Ê∑±ÂÖ•Â≠¶‰π†ÁºñÁ®ãÁöÑÂêåÂ≠¶ÂáÜÂ§áÁöÑËøõÈò∂ÂÜÖÂÆπÔºåÂåÖÊã¨Â≠¶‰π†Ë∑ØÁ∫ø„ÄÅÁü•ËØÜÁôæÁßë„ÄÅËµÑÊ∫êÂ§ßÂÖ®„ÄÅMCP ÂºÄÂèë„ÄÅÈù¢ËØïÂà∑È¢òÁ≠â</li><li>ËµÑÊ∫êÂÆùÂ∫ìÔºöÊ±áÈõÜÂêÑÁßçÂÆûÁî®ËµÑÊ∫êÔºåÂåÖÊã¨Â∑•ÂÖ∑Â§ßÂÖ®„ÄÅÊèêÁ§∫ËØçÊ®°Êùø„ÄÅAI Ê¶ÇÂøµÂ§ßÂÖ®„ÄÅVibe Coding Â∏∏ËßÅÈóÆÈ¢òÁ≠â</li></ul><ul><li>Á¨¨ 1 Â§©ÔºöËØªÂÆåÂü∫Á°ÄÂøÖËØªÔºåÁêÜËß£ Vibe Coding Âπ∂ÂÅöÂá∫Á¨¨‰∏Ä‰∏™‰ΩúÂìÅ</li><li>Á¨¨ 1-2 Âë®ÔºöÂ≠¶‰π† AI ÁºñÁ®ãÂ∑•ÂÖ∑ + ÂÅöÂá†‰∏™ÁÆÄÂçïÈ°πÁõÆ</li></ul><ul><li>Á¨¨ 1 Âë®ÔºöÂ≠¶‰π†‰∏ªÊµÅ AI ÁºñÁ®ãÂ∑•ÂÖ∑ÔºåÂ∞ùËØïÈáçÊûÑ‰πãÂâçÁöÑÈ°πÁõÆ</li></ul><pre><code>ai-guide/\n‚îú‚îÄ‚îÄ üî• Vibe Coding Èõ∂Âü∫Á°ÄÊïôÁ®ã/     # ÈáçÁ£ÖÊïôÁ®ãÔºåÂº∫ÁÉàÊé®Ëçê\n‚îÇ   ‚îú‚îÄ‚îÄ 00 Vibe Coding ÁÆÄ‰ªã\n‚îÇ   ‚îú‚îÄ‚îÄ 01 Âø´ÈÄü‰∏äÊâã Vibe Coding\n‚îÇ   ‚îú‚îÄ‚îÄ 10 ÁºñÁ®ãÂ∑•ÂÖ∑/\n‚îÇ   ‚îú‚îÄ‚îÄ 20 È°πÁõÆÂÆûÊàò/\n‚îÇ   ‚îú‚îÄ‚îÄ 30 ÁªèÈ™åÊäÄÂ∑ß/\n‚îÇ   ‚îú‚îÄ‚îÄ 40 ÁºñÁ®ãÂ≠¶‰π†/\n‚îÇ   ‚îú‚îÄ‚îÄ 50 ‰∫ßÂìÅÂèòÁé∞/\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ AI/\n‚îÇ   ‚îú‚îÄ‚îÄ È±ºÁöÆÁöÑ AI ÊåáÂçó/            # AI Ê†∏ÂøÉÊ¶ÇÂøµ„ÄÅÂ∑•ÂÖ∑„ÄÅÊäÄÂ∑ß\n‚îÇ   ‚îú‚îÄ‚îÄ ÂÖ≥‰∫é DeepSeek/             # DeepSeek Âü∫Á°ÄÁü•ËØÜ\n‚îÇ   ‚îú‚îÄ‚îÄ DeepSeek ‰ΩøÁî®ÊåáÂçó/         # ÂÆâË£Ö„ÄÅ‰ΩøÁî®„ÄÅÊäÄÂ∑ßÂ§ßÂÖ®\n‚îÇ   ‚îú‚îÄ‚îÄ DeepSeek ÊäÄÊúØËß£Êûê/         # Ê∑±Â∫¶ÊäÄÊúØËß£ËØª\n‚îÇ   ‚îú‚îÄ‚îÄ DeepSeek ËµÑÊ∫êÊ±áÊÄª/         # ËµÑÊ∫ê„ÄÅÊïôÁ®ã„ÄÅÂºÄÊ∫êÈ°πÁõÆ\n‚îÇ   ‚îú‚îÄ‚îÄ AI Â∫îÁî®Âú∫ÊôØ/               # ÂàõÊÑèËÆæËÆ°„ÄÅÊïàÁéáÊèêÂçá„ÄÅÁºñÁ®ãÂºÄÂèë\n‚îÇ   ‚îú‚îÄ‚îÄ AI È°πÁõÆÊïôÁ®ã/               # ÂÆûÊàòÈ°πÁõÆÊïôÁ®ã\n‚îÇ   ‚îî‚îÄ‚îÄ AI Ë°å‰∏öËµÑËÆØ/               # ÊúÄÊñ∞Ë°å‰∏öÂä®ÊÄÅ\n‚îî‚îÄ‚îÄ ‰∫ßÂìÅÊúçÂä°/                      # È±ºÁöÆÁöÑ‰∫ßÂìÅÂíåÊúçÂä°\n</code></pre><p>Ê¨¢ËøéÂä†ÂÖ•Êàë‰ª¨ÁöÑ AI ‰∫§ÊµÅÁæ§ÔºåÂÖ≥Ê≥®ÂÖ¨‰ºóÂè∑ÔºöÔºåËé∑ÂèñÊõ¥Â§öÊúÄ‰∏ÄÊâã AI ËµÑËÆØÔºå‰∏ÄËµ∑Êé¢ËÆ® AI Â∫îÁî®ÂÆûË∑µ„ÄÇ</p><p>Â¶ÇÊûú‰Ω†‰πüÊòØ AI Êé¢Á¥¢ËÄÖ„ÄÅÁà±Â•ΩËÄÖÔºåÂπ∂‰∏î‰πê‰∫éÂàÜ‰∫´ÂíåÊ≤âÊ∑Ä‰Ω†ÁöÑÁü•ËØÜÂíåÂ•áÊÄùÂ¶ôÊÉ≥ÔºåÊ¨¢ËøéÂä†ÂÖ•ËøõÊù•ÂèÇ‰∏éÁü•ËØÜÂ∫ìÂÖ±Âª∫Ôºå‰∏ÄËµ∑ÊûÑÂª∫Â±û‰∫éÊâÄÊúâ‰∫∫ÁöÑ AI Áü•ËØÜÂÆùËóèÔºÅ</p><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏Ä‰∏™  ‚≠êÔ∏è ÊîØÊåÅ‰∏Ä‰∏ãÔºÅ</p><p>ËøôÂ•óÊïôÁ®ãÂÆåÂÖ®ÂÖçË¥πÂºÄÊ∫êÔºåÂ∏åÊúõËÉΩÂ∏ÆÊõ¥Â§ö‰∫∫ÊâìÂºÄ Vibe Coding ÁöÑÂ§ßÈó®„ÄÇ</p><p>‰ΩÜÊØïÁ´üÊòØ‰∏Ä‰∏™‰∫∫ÁºñÂÜôÁöÑÔºå‰ºöÊúâ‰∏çË∂≥ÁöÑÂú∞ÊñπÔºåÊàë‰ºöÊåÅÁª≠Êõ¥Êñ∞ÂíåÂÆåÂñÑÂÜÖÂÆπ„ÄÇ</p><p><strong>Â¶ÇÊûúËøôÂ•óÊïôÁ®ãÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÁöÑËØùÔºåÂ∏åÊúõËÉΩÁÇπËµûÊàñËÄÖ Star ‚≠êÔ∏è ÊîØÊåÅ‰∏Ä‰∏ãÔºÅ</strong></p><p>Âà´ÁäπË±´ÔºåÁé∞Âú®Â∞±ÊâìÂºÄÊïôÁ®ãÔºå10 ÂàÜÈíüÂêé‰Ω†Â∞±ËÉΩÂÅöÂá∫Á¨¨‰∏Ä‰∏™‰ΩúÂìÅÔºåË∑üÁùÄÈ±ºÁöÆ‰∏ÄËµ∑ÂºÄÂêØ Vibe Coding ‰πãÊóÖÂêßÔºÅüõ´</p>",
      "contentLength": 4611,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/384c7cb40b76df20002586d01fa7d3e12faf3209db2613273ff7f59f1f2966b2/liyupi/ai-guide",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ruvnet/ruvector",
      "url": "https://github.com/ruvnet/ruvector",
      "date": 1771988741,
      "author": "",
      "guid": 48065,
      "unread": true,
      "content": "<p>RuVector is a High Performance, Real-Time, Self-Learning, Vector Graph Neural Network, and Database built in Rust.</p><h3><strong>The self-learning, self-optimizing vector database ‚Äî with graph intelligence, local AI, and PostgreSQL built in.</strong></h3><blockquote><p>Created by <a href=\"https://ruv.io\">rUv</a> and powering <a href=\"https://cognitum.one\">Cognitum</a>, a üèÖ <strong>CES 2026 Innovation Awards Honoree</strong> ‚Äî the world's first Agentic Chip designed to be always running for AI agents. Tens of thousands of agents, near-zero power, learns from every signal. <a href=\"https://cognitum.one\">Learn more ‚Üí</a></p></blockquote><h4>Most vector databases store your data and search it ‚Äî the same way, every time.</h4><h4> is fundamentally different. It watches how you use it and gets smarter: search results improve automatically, the system tunes itself to your workload, and it runs AI models right on your hardware ‚Äî no cloud APIs, no per-query bills, GPUs optional, CPUs preferred. It drops into PostgreSQL, runs in browsers, and ships as a single file.</h4><p>Open source. ‚ù§Ô∏è Free forever.</p><pre><code>User Query ‚Üí [SONA Engine] ‚Üí Model Response ‚Üí User Feedback\n                  ‚Üë                                 ‚îÇ\n                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Learning Signal ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                         (&lt; 1ms adaptation)\n</code></pre><h3>A Complete Agentic AI Operating System</h3><p>RuVector isn't a database you add to your stack ‚Äî it's the entire stack. Self-learning, self-optimizing, and self-deploying. Everything an AI application needs to run, from bare metal hardware up to the application layer, in one package:</p><table><thead><tr></tr></thead><tbody><tr><td>SONA adapts in &lt;1 ms ‚Äî LoRA fine-tuning + EWC++ memory on every request</td></tr><tr><td>Manual tuning, config files</td><td>Auto-tunes routing, ranking, compression, and index parameters</td></tr><tr><td>OpenAI API, Cohere, static models</td><td>Contrastive training, triplet loss, real-time fine-tuning ‚Äî embeddings improve as you use them</td></tr><tr><td>Formal proofs + statistical tests on every training step ‚Äî gradients only apply if invariants pass</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>Pinecone, Weaviate, Qdrant</td><td>Self-learning HNSW ‚Äî GNN improves results from every query</td></tr><tr><td>Separate database + cache</td><td>Vector store, graph DB, key-value cache ‚Äî unified engine</td></tr><tr><td>Drop-in replacement ‚Äî 230+ SQL functions, same interface but search gets smarter over time</td></tr><tr><td>Cypher, W3C SPARQL 1.1, hyperedges ‚Äî all built in</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>ruvllm ‚Äî GGUF models, MicroLoRA (&lt;1 ms), speculative decoding, continuous batching, WASM</td></tr><tr><td>46 attention types, 8 graph transformers, spiking networks, sparse inference, sublinear solvers</td></tr><tr><td>Manual testing, guardrails</td><td>Min-cut finds the weakest links in any network ‚Äî detects AI drift, prunes wasted compute (50% reduction), keeps agents in sync</td></tr><tr><td>Genomics (DNA variant calling), physics simulation, economic modeling, biological networks</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>CUDA toolkit, driver configs</td><td>Sparse/spiking CPU (AVX-512, NEON) ‚Äî GPU for bursts (Metal, CUDA, ANE, WebGPU, FPGA)</td></tr><tr><td> file boots its own kernel in 125 ms ‚Äî eBPF accelerates hot paths</td></tr><tr><td>Raft consensus, multi-master replication, CRDT delta sync, auto-sharding</td></tr><tr><td>One  file = your entire service ‚Äî servers, browsers, phones, IoT, bare metal</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>API gateways, LLM routers</td><td>Semantic routing (Tiny Dancer), MCP protocol gateway, agent-to-agent discovery</td></tr><tr><td>Latency/power/memory profiling, coherence scoring, real-time metrics</td></tr><tr><td>Manual review, guardrails</td><td>Cognitum Gate ‚Äî 256-tile WASM fabric, Permit/Defer/Deny in &lt;1 ms, witness receipts</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>Post-quantum (ML-DSA-65, Ed25519), SHAKE-256, witness chains, hardware attestation</td></tr><tr><td>Every operation recorded in a tamper-proof chain ‚Äî full provenance from creation to deployment</td></tr></tbody></table><p>The <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">RVF cognitive container</a> ties it all together: a single file that packages your vectors, models, data, and a bootable kernel. Drop it on any machine and it starts serving in 125 ms ‚Äî no install, no dependencies. It branches like Git (only changes are copied), logs every operation in a tamper-proof chain, and runs anywhere from a browser to bare metal.</p><p>Traditional vector search:</p><pre><code>Query ‚Üí HNSW Index ‚Üí Top K Results\n</code></pre><pre><code>Query ‚Üí HNSW Index ‚Üí GNN Layer ‚Üí Enhanced Results\n                ‚Üë                      ‚îÇ\n                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ learns from ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</code></pre><ol><li>Takes your query and its nearest neighbors</li><li>Applies multi-head attention to weigh which neighbors matter</li><li>Updates representations based on graph structure</li><li>Returns better-ranked results ‚Äî all in under 1ms</li></ol><p>This is  ‚Äî the system learns from the sequence and timing of queries, not just their content. A query asked right after another carries context. Patterns that repeat get reinforced. Paths that lead to good results get stronger over time. The result: search gets faster and more accurate the more you use it, adapting in real time without retraining.</p><pre><code># Interactive installer - lists all packages\nnpx ruvector install\n\n# Or install directly\nnpm install ruvector\nnpx ruvector\n\n# Self-learning hooks for Claude Code\nnpx @ruvector/cli hooks init\nnpx @ruvector/cli hooks install\n\n# LLM runtime (SONA learning, HNSW memory)\nnpm install @ruvector/ruvllm\n</code></pre><pre><code># Install\nnpm install ruvector\n\n# Or try instantly\nnpx ruvector\n</code></pre><h3>Ecosystem: AI Agent Orchestration</h3><p>RuVector powers two major AI orchestration platforms:</p><p>See how RuVector stacks up against popular vector databases across 40+ features ‚Äî from latency and graph queries to self-learning, cognitive containers, and PostgreSQL integration.</p><p>Everything RuVector can do ‚Äî organized by category. Vector search, graph queries, LLM inference, distributed systems, deployment targets, and the self-learning stack that ties it all together.</p><p>Run RuVector wherever your application lives ‚Äî as a server, a PostgreSQL extension, a browser library, an edge database, or a self-booting container.</p><p>Real numbers from real benchmarks ‚Äî measured on Apple M4 Pro (48GB RAM) with Criterion.rs statistical sampling.</p><p>RuVector automatically manages memory like a CPU cache ‚Äî hot data stays at full precision, cold data compresses in the background. No manual tuning required.</p><p>RuVector runs on Node.js, Rust, browsers, PostgreSQL, and Docker. Pick the package that fits your stack.</p><table><tbody><tr><td><code>npm install @ruvector/sona</code></td></tr><tr><td><code>npm install @ruvector/rvdna</code></td></tr><tr><td><code>npm install @ruvector/rvf</code></td></tr><tr><td><code>npm install ruvector-wasm</code></td></tr><tr><td><code>cargo add ruvector-core ruvector-graph ruvector-gnn</code></td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td><code>npx @ruvector/rvf-mcp-server --transport stdio</code></td></tr></tbody></table><pre><code># Run tests\ncargo test --workspace\n\n# Run benchmarks\ncargo bench --workspace\n\n# Build WASM\ncargo build -p ruvector-gnn-wasm --target wasm32-unknown-unknown\n</code></pre><p>MIT License ‚Äî free for commercial and personal use.</p>",
      "contentLength": 6279,
      "flags": null,
      "enclosureUrl": "https://repository-images.githubusercontent.com/1099547803/948d2495-1db9-47f6-9ea1-f7f977343e5f",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "GVCLab/PersonaLive",
      "url": "https://github.com/GVCLab/PersonaLive",
      "date": 1771988741,
      "author": "",
      "guid": 48066,
      "unread": true,
      "content": "<p>[CVPR 2026] PersonaLive! : Expressive Portrait Image Animation for Live Streaming</p><img src=\"https://raw.githubusercontent.com/GVCLab/PersonaLive/main/assets/overview.png\" alt=\"Image 1\" width=\"100%\"><p>We present PersonaLive, a  and  diffusion framework capable of generating  portrait animations.</p><pre><code># clone this repo\ngit clone https://github.com/GVCLab/PersonaLive\ncd PersonaLive\n\n# Create conda environment\nconda create -n personalive python=3.10\nconda activate personalive\n\n# Install packages with pip\npip install -r requirements_base.txt\n</code></pre><pre><code>python tools/download_weights.py\n</code></pre><p>Option 2: Download pre-trained weights into the  folder from one of the below URLs:</p><p>Finally, these weights should be organized as follows:</p><pre><code>pretrained_weights\n‚îú‚îÄ‚îÄ onnx\n‚îÇ   ‚îú‚îÄ‚îÄ unet_opt\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ unet_opt.onnx\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ unet_opt.onnx.data\n‚îÇ   ‚îî‚îÄ‚îÄ unet\n‚îú‚îÄ‚îÄ personalive\n‚îÇ   ‚îú‚îÄ‚îÄ denoising_unet.pth\n‚îÇ   ‚îú‚îÄ‚îÄ motion_encoder.pth\n‚îÇ   ‚îú‚îÄ‚îÄ motion_extractor.pth\n‚îÇ   ‚îú‚îÄ‚îÄ pose_guider.pth\n‚îÇ   ‚îú‚îÄ‚îÄ reference_unet.pth\n‚îÇ   ‚îî‚îÄ‚îÄ temporal_module.pth\n‚îú‚îÄ‚îÄ sd-vae-ft-mse\n‚îÇ   ‚îú‚îÄ‚îÄ diffusion_pytorch_model.bin\n‚îÇ   ‚îî‚îÄ‚îÄ config.json\n‚îú‚îÄ‚îÄ sd-image-variations-diffusers\n‚îÇ   ‚îú‚îÄ‚îÄ image_encoder\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pytorch_model.bin\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.json\n‚îÇ   ‚îú‚îÄ‚îÄ unet\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ diffusion_pytorch_model.bin\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.json\n‚îÇ   ‚îî‚îÄ‚îÄ model_index.json\n‚îî‚îÄ‚îÄ tensorrt\n    ‚îî‚îÄ‚îÄ unet_work.engine\n</code></pre><p>Run offline inference with the default configuration:</p><pre><code>python inference_offline.py\n</code></pre><ul><li>: Max number of frames to generate. (Default: 100)</li><li>: Enable xFormers memory efficient attention. (Default: True)</li><li>: Enable streaming generation strategy. (Default: True)</li><li>: Path to a specific reference image. Overrides settings in config.</li><li>: Path to a specific driving video. Overrides settings in config.</li></ul><p>‚ö†Ô∏è Note for RTX 50-Series (Blackwell) Users: xformers is not yet fully compatible with the new architecture. To avoid crashes, please disable it by running:</p><pre><code>python inference_offline.py --use_xformers False\n</code></pre><pre><code># install Node.js 18+\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash\nnvm install 18\n\nsource web_start.sh\n</code></pre><h4>üèéÔ∏è Acceleration (Optional)</h4><p>Converting the model to TensorRT can significantly speed up inference (~ 2x ‚ö°Ô∏è). Building the engine may take about  depending on your device. Note that TensorRT optimizations may lead to slight variations or a small drop in output quality.</p><pre><code># Install packages with pip\npip install -r requirements_trt.txt\n\n# Converting the model to TensorRT\npython torch2trt.py\n</code></pre><p>üí° <strong>PyCUDA Installation Issues</strong>: If you encounter a \"Failed to build wheel for pycuda\" error during the installation above, please follow these steps:</p><pre><code># Install PyCUDA manually using Conda (avoids compilation issues):\nconda install -c conda-forge pycuda \"numpy&lt;2.0\"\n\n# Open requirements_trt.txt and comment out or remove the line \"pycuda==2024.1.2\"\n\n# Install other packages with pip\npip install -r requirements_trt.txt\n\n# Converting the model to TensorRT\npython torch2trt.py\n</code></pre><p>‚ö†Ô∏è The provided TensorRT model is from an . We recommend  (including H100 users) re-run  locally to ensure best compatibility.</p><pre><code>python inference_online.py --acceleration none (for RTX 50-Series) or xformers or tensorrt\n</code></pre><p>Then open  in your browser. (*If  does not work well, try )</p><p>: Upload Image ‚û°Ô∏è Fuse Reference ‚û°Ô∏è Start Animation ‚û°Ô∏è Enjoy! üéâ</p><p>: Latency varies depending on your device's computing power. You can try the following methods to optimize it:</p><p>Special thanks to the community for providing helpful setups! ü•Ç</p><ul><li><p><strong>Windows + RTX 50-Series Guide</strong>: Thanks to <a href=\"https://github.com/dknos\">@dknos</a> for providing a <a href=\"https://github.com/GVCLab/PersonaLive/issues/10#issuecomment-3662785532\">detailed guide</a> on running this project on Windows with Blackwell GPUs.</p></li><li><p>: Thanks to <a href=\"https://github.com/suruoxi\">@suruoxi</a> for implementing , and to <a href=\"https://github.com/andchir\">@andchir</a> for adding audio merging functionality.</p></li></ul><p>If you find PersonaLive useful for your research, welcome to cite our work using the following BibTeX:</p><pre><code>@article{li2025personalive,\n  title={PersonaLive! Expressive Portrait Image Animation for Live Streaming},\n  author={Li, Zhiyuan and Pun, Chi-Man and Fang, Chen and Wang, Jue and Cun, Xiaodong},\n  journal={arXiv preprint arXiv:2512.11253},\n  year={2025}\n}\n</code></pre>",
      "contentLength": 4122,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/d4680994dab18c7857754af72ccab22cbbcd3bf0883ebeb978c4f888af54e215/GVCLab/PersonaLive",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "openemr/openemr",
      "url": "https://github.com/openemr/openemr",
      "date": 1771988741,
      "author": "",
      "guid": 48067,
      "unread": true,
      "content": "<p>The most popular open source electronic health records and medical practice management solution.</p><p><a href=\"https://open-emr.org\">OpenEMR</a> is a Free and Open Source electronic health records and medical practice management application. It features fully integrated electronic health records, practice management, scheduling, electronic billing, internationalization, free support, a vibrant community, and a whole lot more. It runs on Windows, Linux, Mac OS X, and many other platforms.</p><p>OpenEMR is a leader in healthcare open source software and comprises a large and diverse community of software developers, medical providers and educators with a very healthy mix of both volunteers and professionals. <a href=\"https://open-emr.org/wiki/index.php/FAQ#How_do_I_begin_to_volunteer_for_the_OpenEMR_project.3F\">Join us and learn how to start contributing today!</a></p><blockquote><p>Already comfortable with git? Check out <a href=\"https://raw.githubusercontent.com/openemr/openemr/master/CONTRIBUTING.md\">CONTRIBUTING.md</a> for quick setup instructions and requirements for contributing to OpenEMR by resolving a bug or adding an awesome feature üòä.</p></blockquote><p>Community and Professional support can be found <a href=\"https://open-emr.org/wiki/index.php/OpenEMR_Support_Guide\">here</a>.</p><p>Extensive documentation and forums can be found on the <a href=\"https://open-emr.org\">OpenEMR website</a> that can help you to become more familiar about the project üìñ.</p><h3>Reporting Issues and Bugs</h3><p>Report these on the <a href=\"https://github.com/openemr/openemr/issues\">Issue Tracker</a>. If you are unsure if it is an issue/bug, then always feel free to use the <a href=\"https://community.open-emr.org/\">Forum</a> and <a href=\"https://www.open-emr.org/chat/\">Chat</a> to discuss about the issue ü™≤.</p><h3>Reporting Security Vulnerabilities</h3><p>If using OpenEMR directly from the code repository, then the following commands will build OpenEMR (Node.js version 22.* is required) :</p><pre><code>composer install --no-dev\nnpm install\nnpm run build\ncomposer dump-autoload -o\n</code></pre><p>This project exists thanks to all the people who have contributed. <a href=\"https://raw.githubusercontent.com/openemr/openemr/master/CONTRIBUTING.md\">[Contribute]</a>. <a href=\"https://github.com/openemr/openemr/graphs/contributors\"><img src=\"https://opencollective.com/openemr/contributors.svg?width=890\"></a></p>",
      "contentLength": 1599,
      "flags": null,
      "enclosureUrl": "https://repository-images.githubusercontent.com/679584/58435691-1a0f-4345-8f7c-d6c15875a814",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "HunxByts/GhostTrack",
      "url": "https://github.com/HunxByts/GhostTrack",
      "date": 1771988741,
      "author": "",
      "guid": 48068,
      "unread": true,
      "content": "<p>Useful tool to track location or mobile number</p><p>Useful tool to track location or mobile number, so this tool can be called osint or also information gathering</p><img src=\"https://github.com/HunxByts/GhostTrack/raw/main/asset/bn.png\"><h3>Instalation on Linux (deb)</h3><pre><code>sudo apt-get install git\nsudo apt-get install python3\n</code></pre><pre><code>pkg install git\npkg install python3\n</code></pre><pre><code>git clone https://github.com/HunxByts/GhostTrack.git\ncd GhostTrack\npip3 install -r requirements.txt\npython3 GhostTR.py\n</code></pre><p>Display on the menu </p><img src=\"https://github.com/HunxByts/GhostTrack/blob/main/asset/ip.png \"><p>on the IP Track menu, you can combo with the seeker tool to get the target IP</p><p>Display on the menu </p><img src=\"https://github.com/HunxByts/GhostTrack/raw/main/asset/phone.png\"><p>on this menu you can search for information from the target phone number</p><p>Display on the menu </p><img src=\"https://github.com/HunxByts/GhostTrack/raw/main/asset/User.png\"> on this menu you can search for information from the target username on social media \n",
      "contentLength": 687,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/d771e74d0edc4fa1a3b8241107169b3a8e331edd43bf9e55c164830d98833350/HunxByts/GhostTrack",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "D4Vinci/Scrapling",
      "url": "https://github.com/D4Vinci/Scrapling",
      "date": 1771988741,
      "author": "",
      "guid": 48069,
      "unread": true,
      "content": "<p>üï∑Ô∏è An adaptive Web Scraping framework that handles everything from a single request to a full-scale crawl!</p><p>Scrapling is an adaptive Web Scraping framework that handles everything from a single request to a full-scale crawl.</p><p>Its parser learns from website changes and automatically relocates your elements when pages update. Its fetchers bypass anti-bot systems like Cloudflare Turnstile out of the box. And its spider framework lets you scale up to concurrent, multi-session crawls with pause/resume and automatic proxy rotation ‚Äî all in a few lines of Python. One library, zero compromises.</p><p>Blazing fast crawls with real-time stats and streaming. Built by Web Scrapers for Web Scrapers and regular users, there's something for everyone.</p><pre><code>from scrapling.fetchers import Fetcher, AsyncFetcher, StealthyFetcher, DynamicFetcher\nStealthyFetcher.adaptive = True\np = StealthyFetcher.fetch('https://example.com', headless=True, network_idle=True)  # Fetch website under the radar!\nproducts = p.css('.product', auto_save=True)                                        # Scrape data that survives website design changes!\nproducts = p.css('.product', adaptive=True)                                         # Later, if the website structure changes, pass `adaptive=True` to find them!\n</code></pre><p>Or scale up to full crawls</p><pre><code>from scrapling.spiders import Spider, Response\n\nclass MySpider(Spider):\n  name = \"demo\"\n  start_urls = [\"https://example.com/\"]\n\n  async def parse(self, response: Response):\n      for item in response.css('.product'):\n          yield {\"title\": item.css('h2::text').get()}\n\nMySpider().start()\n</code></pre><p><i><sub>Do you want to be the first company to show up here? Click <a href=\"https://github.com/sponsors/D4Vinci/sponsorships?tier_id=586646\">here</a></sub></i></p><p><i><sub>Do you want to show your ad here? Click <a href=\"https://github.com/sponsors/D4Vinci\">here</a> and choose the tier that suites you!</sub></i></p><h3>Spiders ‚Äî A Full Crawling Framework</h3><ul><li>üï∑Ô∏è : Define spiders with , async  callbacks, and / objects.</li><li>‚ö° : Configurable concurrency limits, per-domain throttling, and download delays.</li><li>üîÑ : Unified interface for HTTP requests, and stealthy headless browsers in a single spider ‚Äî route requests to different sessions by ID.</li><li>üíæ : Checkpoint-based crawl persistence. Press Ctrl+C for a graceful shutdown; restart to resume from where you left off.</li><li>üì° : Stream scraped items as they arrive via <code>async for item in spider.stream()</code> with real-time stats ‚Äî ideal for UI, pipelines, and long-running crawls.</li><li>üõ°Ô∏è <strong>Blocked Request Detection</strong>: Automatic detection and retry of blocked requests with customizable logic.</li><li>üì¶ : Export results through hooks and your own pipeline or the built-in JSON/JSONL with  /  respectively.</li></ul><h3>Advanced Websites Fetching with Session Support</h3><ul><li>: Fast and stealthy HTTP requests with the  class. Can impersonate browsers' TLS fingerprint, headers, and use HTTP/3.</li><li>: Fetch dynamic websites with full browser automation through the  class supporting Playwright's Chromium and Google's Chrome.</li><li>: Advanced stealth capabilities with  and fingerprint spoofing. Can easily bypass all types of Cloudflare's Turnstile/Interstitial with automation.</li><li>: Persistent session support with , , and  classes for cookie and state management across requests.</li><li>: Built-in  with cyclic or custom rotation strategies across all session types, plus per-request proxy overrides.</li><li>: Block requests to specific domains (and their subdomains) in browser-based fetchers.</li><li>: Complete async support across all fetchers and dedicated async session classes.</li></ul><h3>Adaptive Scraping &amp; AI Integration</h3><ul><li>üîÑ : Relocate elements after website changes using intelligent similarity algorithms.</li><li>üéØ : CSS selectors, XPath selectors, filter-based search, text search, regex search, and more.</li><li>üîç : Automatically locate elements similar to found elements.</li><li>ü§ñ <strong>MCP Server to be used with AI</strong>: Built-in MCP server for AI-assisted Web Scraping and data extraction. The MCP server features powerful, custom capabilities that leverage Scrapling to extract targeted content before passing it to the AI (Claude/Cursor/etc), thereby speeding up operations and reducing costs by minimizing token usage. (<a href=\"https://www.youtube.com/watch?v=qyFk3ZNwOxE\">demo video</a>)</li></ul><h3>High-Performance &amp; battle-tested Architecture</h3><ul><li>üöÄ : Optimized performance outperforming most Python scraping libraries.</li><li>üîã : Optimized data structures and lazy loading for a minimal memory footprint.</li><li>‚ö° : 10x faster than the standard library.</li><li>üèóÔ∏è : Not only does Scrapling have 92% test coverage and full type hints coverage, but it has been used daily by hundreds of Web Scrapers over the past year.</li></ul><h3>Developer/Web Scraper Friendly Experience</h3><ul><li>üéØ <strong>Interactive Web Scraping Shell</strong>: Optional built-in IPython shell with Scrapling integration, shortcuts, and new tools to speed up Web Scraping scripts development, like converting curl requests to Scrapling requests and viewing requests results in your browser.</li><li>üöÄ <strong>Use it directly from the Terminal</strong>: Optionally, you can use Scrapling to scrape a URL without writing a single line of code!</li><li>üõ†Ô∏è : Advanced DOM traversal with parent, sibling, and child navigation methods.</li><li>üß¨ : Built-in regex, cleaning methods, and optimized string operations.</li><li>üìù : Generate robust CSS/XPath selectors for any element.</li><li>üîå : Similar to Scrapy/BeautifulSoup with the same pseudo-elements used in Scrapy/Parsel.</li><li>üìò : Full type hints for excellent IDE support and code completion. The entire codebase is automatically scanned with  and  with each change.</li><li>üîã : With each release, a Docker image containing all browsers is automatically built and pushed.</li></ul><p>Let's give you a quick glimpse of what Scrapling can do without deep diving.</p><p>HTTP requests with session support</p><pre><code>from scrapling.fetchers import Fetcher, FetcherSession\n\nwith FetcherSession(impersonate='chrome') as session:  # Use latest version of Chrome's TLS fingerprint\n    page = session.get('https://quotes.toscrape.com/', stealthy_headers=True)\n    quotes = page.css('.quote .text::text').getall()\n\n# Or use one-off requests\npage = Fetcher.get('https://quotes.toscrape.com/')\nquotes = page.css('.quote .text::text').getall()\n</code></pre><pre><code>from scrapling.fetchers import StealthyFetcher, StealthySession\n\nwith StealthySession(headless=True, solve_cloudflare=True) as session:  # Keep the browser open until you finish\n    page = session.fetch('https://nopecha.com/demo/cloudflare', google_search=False)\n    data = page.css('#padded_content a').getall()\n\n# Or use one-off request style, it opens the browser for this request, then closes it after finishing\npage = StealthyFetcher.fetch('https://nopecha.com/demo/cloudflare')\ndata = page.css('#padded_content a').getall()\n</code></pre><pre><code>from scrapling.fetchers import DynamicFetcher, DynamicSession\n\nwith DynamicSession(headless=True, disable_resources=False, network_idle=True) as session:  # Keep the browser open until you finish\n    page = session.fetch('https://quotes.toscrape.com/', load_dom=False)\n    data = page.xpath('//span[@class=\"text\"]/text()').getall()  # XPath selector if you prefer it\n\n# Or use one-off request style, it opens the browser for this request, then closes it after finishing\npage = DynamicFetcher.fetch('https://quotes.toscrape.com/')\ndata = page.css('.quote .text::text').getall()\n</code></pre><p>Build full crawlers with concurrent requests, multiple session types, and pause/resume:</p><pre><code>from scrapling.spiders import Spider, Request, Response\n\nclass QuotesSpider(Spider):\n    name = \"quotes\"\n    start_urls = [\"https://quotes.toscrape.com/\"]\n    concurrent_requests = 10\n    \n    async def parse(self, response: Response):\n        for quote in response.css('.quote'):\n            yield {\n                \"text\": quote.css('.text::text').get(),\n                \"author\": quote.css('.author::text').get(),\n            }\n            \n        next_page = response.css('.next a')\n        if next_page:\n            yield response.follow(next_page[0].attrib['href'])\n\nresult = QuotesSpider().start()\nprint(f\"Scraped {len(result.items)} quotes\")\nresult.items.to_json(\"quotes.json\")\n</code></pre><p>Use multiple session types in a single spider:</p><pre><code>from scrapling.spiders import Spider, Request, Response\nfrom scrapling.fetchers import FetcherSession, AsyncStealthySession\n\nclass MultiSessionSpider(Spider):\n    name = \"multi\"\n    start_urls = [\"https://example.com/\"]\n    \n    def configure_sessions(self, manager):\n        manager.add(\"fast\", FetcherSession(impersonate=\"chrome\"))\n        manager.add(\"stealth\", AsyncStealthySession(headless=True), lazy=True)\n    \n    async def parse(self, response: Response):\n        for link in response.css('a::attr(href)').getall():\n            # Route protected pages through the stealth session\n            if \"protected\" in link:\n                yield Request(link, sid=\"stealth\")\n            else:\n                yield Request(link, sid=\"fast\", callback=self.parse)  # explicit callback\n</code></pre><p>Pause and resume long crawls with checkpoints by running the spider like this:</p><pre><code>QuotesSpider(crawldir=\"./crawl_data\").start()\n</code></pre><p>Press Ctrl+C to pause gracefully ‚Äî progress is saved automatically. Later, when you start the spider again, pass the same , and it will resume from where it stopped.</p><h3>Advanced Parsing &amp; Navigation</h3><pre><code>from scrapling.fetchers import Fetcher\n\n# Rich element selection and navigation\npage = Fetcher.get('https://quotes.toscrape.com/')\n\n# Get quotes with multiple selection methods\nquotes = page.css('.quote')  # CSS selector\nquotes = page.xpath('//div[@class=\"quote\"]')  # XPath\nquotes = page.find_all('div', {'class': 'quote'})  # BeautifulSoup-style\n# Same as\nquotes = page.find_all('div', class_='quote')\nquotes = page.find_all(['div'], class_='quote')\nquotes = page.find_all(class_='quote')  # and so on...\n# Find element by text content\nquotes = page.find_by_text('quote', tag='div')\n\n# Advanced navigation\nquote_text = page.css('.quote')[0].css('.text::text').get()\nquote_text = page.css('.quote').css('.text::text').getall()  # Chained selectors\nfirst_quote = page.css('.quote')[0]\nauthor = first_quote.next_sibling.css('.author::text')\nparent_container = first_quote.parent\n\n# Element relationships and similarity\nsimilar_elements = first_quote.find_similar()\nbelow_elements = first_quote.below_elements()\n</code></pre><p>You can use the parser right away if you don't want to fetch websites like below:</p><pre><code>from scrapling.parser import Selector\n\npage = Selector(\"&lt;html&gt;...&lt;/html&gt;\")\n</code></pre><p>And it works precisely the same way!</p><h3>Async Session Management Examples</h3><pre><code>import asyncio\nfrom scrapling.fetchers import FetcherSession, AsyncStealthySession, AsyncDynamicSession\n\nasync with FetcherSession(http3=True) as session:  # `FetcherSession` is context-aware and can work in both sync/async patterns\n    page1 = session.get('https://quotes.toscrape.com/')\n    page2 = session.get('https://quotes.toscrape.com/', impersonate='firefox135')\n\n# Async session usage\nasync with AsyncStealthySession(max_pages=2) as session:\n    tasks = []\n    urls = ['https://example.com/page1', 'https://example.com/page2']\n    \n    for url in urls:\n        task = session.fetch(url)\n        tasks.append(task)\n    \n    print(session.get_pool_stats())  # Optional - The status of the browser tabs pool (busy/free/error)\n    results = await asyncio.gather(*tasks)\n    print(session.get_pool_stats())\n</code></pre><p>Scrapling includes a powerful command-line interface:</p><p>Launch the interactive Web Scraping shell</p><p>Extract pages to a file directly without programming (Extracts the content inside the  tag by default). If the output file ends with , then the text content of the target will be extracted. If it ends in , it will be a Markdown representation of the HTML content; if it ends in , it will be the HTML content itself.</p><pre><code>scrapling extract get 'https://example.com' content.md\nscrapling extract get 'https://example.com' content.txt --css-selector '#fromSkipToProducts' --impersonate 'chrome'  # All elements matching the CSS selector '#fromSkipToProducts'\nscrapling extract fetch 'https://example.com' content.md --css-selector '#fromSkipToProducts' --no-headless\nscrapling extract stealthy-fetch 'https://nopecha.com/demo/cloudflare' captchas.html --css-selector '#padded_content a' --solve-cloudflare\n</code></pre><blockquote><p>[!NOTE] There are many additional features, but we want to keep this page concise, including the MCP server and the interactive Web Scraping Shell. Check out the full documentation <a href=\"https://scrapling.readthedocs.io/en/latest/\">here</a></p></blockquote><p>Scrapling isn't just powerful‚Äîit's also blazing fast. The following benchmarks compare Scrapling's parser with the latest versions of other popular libraries.</p><h3>Text Extraction Speed Test (5000 nested elements)</h3><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><h3>Element Similarity &amp; Text Search Performance</h3><p>Scrapling's adaptive element finding capabilities significantly outperform alternatives:</p><table><thead><tr></tr></thead><tbody></tbody></table><blockquote><p>All benchmarks represent averages of 100+ runs. See <a href=\"https://github.com/D4Vinci/Scrapling/raw/main/benchmarks.py\">benchmarks.py</a> for methodology.</p></blockquote><p>Scrapling requires Python 3.10 or higher:</p><p>This installation only includes the parser engine and its dependencies, without any fetchers or commandline dependencies.</p><ol><li><p>If you are going to use any of the extra features below, the fetchers, or their classes, you will need to install fetchers' dependencies and their browser dependencies as follows:</p><pre><code>pip install \"scrapling[fetchers]\"\n\nscrapling install           # normal install\nscrapling install  --force  # force reinstall\n</code></pre><p>This downloads all browsers, along with their system dependencies and fingerprint manipulation dependencies.</p><p>Or you can install them from the code instead of running a command like this:</p><pre><code>from scrapling.cli import install\n\ninstall([], standalone_mode=False)          # normal install\ninstall([\"--force\"], standalone_mode=False) # force reinstall\n</code></pre></li><li><ul><li>Install the MCP server feature: <pre><code>pip install \"scrapling[ai]\"\n</code></pre></li><li>Install shell features (Web Scraping shell and the  command): <pre><code>pip install \"scrapling[shell]\"\n</code></pre></li><li>Install everything: <pre><code>pip install \"scrapling[all]\"\n</code></pre></li></ul><p>Remember that you need to install the browser dependencies with  after any of these extras (if you didn't already)</p></li></ol><p>You can also install a Docker image with all extras and browsers with the following command from DockerHub:</p><pre><code>docker pull pyd4vinci/scrapling\n</code></pre><p>Or download it from the GitHub registry:</p><pre><code>docker pull ghcr.io/d4vinci/scrapling:latest\n</code></pre><p>This image is automatically built and pushed using GitHub Actions and the repository's main branch.</p><blockquote><p>[!CAUTION] This library is provided for educational and research purposes only. By using this library, you agree to comply with local and international data scraping and privacy laws. The authors and contributors are not responsible for any misuse of this software. Always respect the terms of service of websites and robots.txt files.</p></blockquote><p>This work is licensed under the BSD-3-Clause License.</p><p>This project includes code adapted from:</p><div align=\"center\"><small>Designed &amp; crafted with ‚ù§Ô∏è by Karim Shoair.</small></div>",
      "contentLength": 14416,
      "flags": null,
      "enclosureUrl": "https://repository-images.githubusercontent.com/872119017/7b056203-a221-4d0c-a666-40944ec669a9",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "LadybirdBrowser/ladybird",
      "url": "https://github.com/LadybirdBrowser/ladybird",
      "date": 1771988741,
      "author": "",
      "guid": 48070,
      "unread": true,
      "content": "<p>Truly independent web browser</p><p><a href=\"https://ladybird.org\">Ladybird</a> is a truly independent web browser, using a novel engine based on web standards.</p><blockquote><p>[!IMPORTANT] Ladybird is in a pre-alpha state, and only suitable for use by developers</p></blockquote><p>We aim to build a complete, usable browser for the modern web.</p><p>Ladybird uses a multi-process architecture with a main UI process, several WebContent renderer processes, an ImageDecoder process, and a RequestServer process.</p><p>Image decoding and network connections are done out of process to be more robust against malicious content. Each tab has its own renderer process, which is sandboxed from the rest of the system.</p><p>At the moment, many core library support components are inherited from SerenityOS:</p><ul><li>LibWeb: Web rendering engine</li><li>LibWasm: WebAssembly implementation</li><li>LibCrypto/LibTLS: Cryptography primitives and Transport Layer Security</li><li>LibGfx: 2D Graphics Library, Image Decoding and Rendering</li><li>LibUnicode: Unicode and locale support</li><li>LibMedia: Audio and video playback</li><li>LibCore: Event loop, OS abstraction layer</li><li>LibIPC: Inter-process communication</li></ul><h2>How do I build and run this?</h2><p>Ladybird runs on Linux, macOS, Windows (with WSL2), and many other *Nixes.</p><h2>How do I read the documentation?</h2><p>Code-related documentation can be found in the <a href=\"https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/\">documentation</a> folder.</p><h2>Get in touch and participate!</h2><p>Ladybird is licensed under a 2-clause BSD license.</p>",
      "contentLength": 1320,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/5ca22384a10d538a1b3d9fce5e47a73a5c1611f15a792f37235b865eca8568aa/LadybirdBrowser/ladybird",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "obra/superpowers",
      "url": "https://github.com/obra/superpowers",
      "date": 1771988741,
      "author": "",
      "guid": 48071,
      "unread": true,
      "content": "<p>An agentic skills framework &amp; software development methodology that works.</p><p>Superpowers is a complete software development workflow for your coding agents, built on top of a set of composable \"skills\" and some initial instructions that make sure your agent uses them.</p><p>It starts from the moment you fire up your coding agent. As soon as it sees that you're building something, it  just jump into trying to write code. Instead, it steps back and asks you what you're really trying to do.</p><p>Once it's teased a spec out of the conversation, it shows it to you in chunks short enough to actually read and digest.</p><p>After you've signed off on the design, your agent puts together an implementation plan that's clear enough for an enthusiastic junior engineer with poor taste, no judgement, no project context, and an aversion to testing to follow. It emphasizes true red/green TDD, YAGNI (You Aren't Gonna Need It), and DRY.</p><p>Next up, once you say \"go\", it launches a <em>subagent-driven-development</em> process, having agents work through each engineering task, inspecting and reviewing their work, and continuing forward. It's not uncommon for Claude to be able to work autonomously for a couple hours at a time without deviating from the plan you put together.</p><p>There's a bunch more to it, but that's the core of the system. And because the skills trigger automatically, you don't need to do anything special. Your coding agent just has Superpowers.</p><p>If Superpowers has helped you do stuff that makes money and you are so inclined, I'd greatly appreciate it if you'd consider <a href=\"https://github.com/sponsors/obra\">sponsoring my opensource work</a>.</p><p> Installation differs by platform. Claude Code or Cursor have built-in plugin marketplaces. Codex and OpenCode require manual setup.</p><h3>Claude Code (via Plugin Marketplace)</h3><p>In Claude Code, register the marketplace first:</p><pre><code>/plugin marketplace add obra/superpowers-marketplace\n</code></pre><p>Then install the plugin from this marketplace:</p><pre><code>/plugin install superpowers@superpowers-marketplace\n</code></pre><h3>Cursor (via Plugin Marketplace)</h3><p>In Cursor Agent chat, install from marketplace:</p><pre><code>Fetch and follow instructions from https://raw.githubusercontent.com/obra/superpowers/refs/heads/main/.codex/INSTALL.md\n</code></pre><pre><code>Fetch and follow instructions from https://raw.githubusercontent.com/obra/superpowers/refs/heads/main/.opencode/INSTALL.md\n</code></pre><p>Start a new session in your chosen platform and ask for something that should trigger a skill (for example, \"help me plan this feature\" or \"let's debug this issue\"). The agent should automatically invoke the relevant superpowers skill.</p><ol><li><p> - Activates before writing code. Refines rough ideas through questions, explores alternatives, presents design in sections for validation. Saves design document.</p></li><li><p> - Activates after design approval. Creates isolated workspace on new branch, runs project setup, verifies clean test baseline.</p></li><li><p> - Activates with approved design. Breaks work into bite-sized tasks (2-5 minutes each). Every task has exact file paths, complete code, verification steps.</p></li><li><p><strong>subagent-driven-development</strong> or  - Activates with plan. Dispatches fresh subagent per task with two-stage review (spec compliance, then code quality), or executes in batches with human checkpoints.</p></li><li><p> - Activates during implementation. Enforces RED-GREEN-REFACTOR: write failing test, watch it fail, write minimal code, watch it pass, commit. Deletes code written before tests.</p></li><li><p> - Activates between tasks. Reviews against plan, reports issues by severity. Critical issues block progress.</p></li><li><p><strong>finishing-a-development-branch</strong> - Activates when tasks complete. Verifies tests, presents options (merge/PR/keep/discard), cleans up worktree.</p></li></ol><p><strong>The agent checks for relevant skills before any task.</strong> Mandatory workflows, not suggestions.</p><ul><li> - RED-GREEN-REFACTOR cycle (includes testing anti-patterns reference)</li></ul><ul><li> - 4-phase root cause process (includes root-cause-tracing, defense-in-depth, condition-based-waiting techniques)</li><li><strong>verification-before-completion</strong> - Ensure it's actually fixed</li></ul><ul><li> - Socratic design refinement</li><li> - Detailed implementation plans</li><li> - Batch execution with checkpoints</li><li><strong>dispatching-parallel-agents</strong> - Concurrent subagent workflows</li><li> - Pre-review checklist</li><li> - Responding to feedback</li><li> - Parallel development branches</li><li><strong>finishing-a-development-branch</strong> - Merge/PR decision workflow</li><li><strong>subagent-driven-development</strong> - Fast iteration with two-stage review (spec compliance, then code quality)</li></ul><ul><li> - Create new skills following best practices (includes testing methodology)</li><li> - Introduction to the skills system</li></ul><ul><li> - Write tests first, always</li><li> - Process over guessing</li><li> - Simplicity as primary goal</li><li> - Verify before declaring success</li></ul><p>Skills live directly in this repository. To contribute:</p><ol><li>Create a branch for your skill</li><li>Follow the  skill for creating and testing new skills</li></ol><p>See <code>skills/writing-skills/SKILL.md</code> for the complete guide.</p><p>Skills update automatically when you update the plugin:</p><pre><code>/plugin update superpowers\n</code></pre><p>MIT License - see LICENSE file for details</p>",
      "contentLength": 4847,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/a175fe95ad52d95b9d33514221aa78fe5b2f18deaede86ec57055aeea0e2917c/obra/superpowers",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "siteboon/claudecodeui",
      "url": "https://github.com/siteboon/claudecodeui",
      "date": 1771902315,
      "author": "",
      "guid": 47743,
      "unread": true,
      "content": "<p>Use Claude Code, Cursor CLI or Codex on mobile and web with CloudCLI (aka Claude Code UI). CloudCLI is a free open source webui/GUI that helps you manage your Claude Code session and projects remotely</p><p>A desktop and mobile UI for <a href=\"https://docs.anthropic.com/en/docs/claude-code\">Claude Code</a>, <a href=\"https://docs.cursor.com/en/cli/overview\">Cursor CLI</a> and <a href=\"https://developers.openai.com/codex\">Codex</a>. You can use it locally or remotely to view your active projects and sessions in Claude Code, Cursor, or Codex and make changes to them from everywhere (mobile or desktop). This gives you a proper interface that works everywhere.</p><ul><li> - Works seamlessly across desktop, tablet, and mobile so you can also use Claude Code, Cursor, or Codex from mobile</li><li><strong>Interactive Chat Interface</strong> - Built-in chat interface for seamless communication with Claude Code, Cursor, or Codex</li><li><strong>Integrated Shell Terminal</strong> - Direct access to Claude Code, Cursor CLI, or Codex through built-in shell functionality</li><li> - Interactive file tree with syntax highlighting and live editing</li><li> - View, stage and commit your changes. You can also switch branches</li><li> - Resume conversations, manage multiple sessions, and track history</li><li><strong>TaskMaster AI Integration</strong> - Advanced project management with AI-powered task planning, PRD parsing, and workflow automation</li><li> - Works with Claude Sonnet 4.5, Opus 4.5, and GPT-5.2</li></ul><h3>One-click Operation (Recommended)</h3><p>No installation required, direct operation:</p><pre><code>npx @siteboon/claude-code-ui\n</code></pre><p>The server will start and be accessible at  (or your configured PORT).</p><p>: Simply run the same  command again after stopping the server</p><h3>Global Installation (For Regular Use)</h3><p>For frequent use, install globally once:</p><pre><code>npm install -g @siteboon/claude-code-ui\n</code></pre><p>Then start with a simple command:</p><p>: Stop with Ctrl+C and run  again.</p><p>After global installation, you have access to both  and  commands:</p><table><thead><tr></tr></thead><tbody><tr><td>Start the server (default)</td></tr><tr><td>Start the server explicitly</td></tr><tr><td>Show configuration and data locations</td></tr><tr><td>Update to the latest version</td></tr><tr></tr><tr></tr><tr><td>Set server port (default: 3001)</td></tr><tr><td>Set custom database location</td></tr></tbody></table><pre><code>cloudcli                          # Start with defaults\ncloudcli -p 8080              # Start on custom port\ncloudcli status                   # Show current configuration\n</code></pre><h3>Run as Background Service (Recommended for Production)</h3><p>For production use, run Claude Code UI as a background service using PM2 (Process Manager 2):</p><h4>Start as Background Service</h4><pre><code># Start the server in background\npm2 start claude-code-ui --name \"claude-code-ui\"\n\n# Or using the shorter alias\npm2 start cloudcli --name \"claude-code-ui\"\n\n# Start on a custom port\npm2 start cloudcli --name \"claude-code-ui\" -- --port 8080\n</code></pre><h4>Auto-Start on System Boot</h4><p>To make Claude Code UI start automatically when your system boots:</p><pre><code># Generate startup script for your platform\npm2 startup\n\n# Save current process list\npm2 save\n</code></pre><h3>Local Development Installation</h3><pre><code>git clone https://github.com/siteboon/claudecodeui.git\ncd claudecodeui\n</code></pre><pre><code>cp .env.example .env\n# Edit .env with your preferred settings\n</code></pre><pre><code># Development mode (with hot reload)\nnpm run dev\n\n</code></pre><p>The application will start at the port you specified in your .env</p><ol start=\"5\"><li><ul><li>Development: </li></ul></li></ol><h2>Security &amp; Tools Configuration</h2><p>: All Claude Code tools are . This prevents potentially harmful operations from running automatically.</p><p>To use Claude Code's full functionality, you'll need to manually enable tools:</p><ol><li> - Click the gear icon in the sidebar</li><li> - Turn on only the tools you need</li><li> - Your preferences are saved locally</li></ol><p>: Start with basic tools enabled and add more as needed. You can always adjust these settings later.</p><h2>TaskMaster AI Integration </h2><p>Claude Code UI supports  (aka claude-task-master) integration for advanced project management and AI-powered task planning.</p><ul><li>AI-powered task generation from PRDs (Product Requirements Documents)</li><li>Smart task breakdown and dependency management</li><li>Visual task boards and progress tracking</li></ul><p>: Visit the <a href=\"https://github.com/eyaltoledano/claude-task-master\">TaskMaster AI GitHub repository</a> for installation instructions, configuration guides, and usage examples. After installing it you should be able to enable it from the Settings</p><p>It automatically discovers Claude Code, Cursor or Codex sessions when available and groups them together into projects session counts</p><ul><li> - Rename, delete, and organize projects</li><li> - Quick access to recent projects and sessions</li><li> - Add your own MCP servers through the UI</li></ul><ul><li><strong>Use responsive chat or Claude Code/Cursor CLI/Codex CLI</strong> - You can either use the adapted chat interface or use the shell button to connect to your selected CLI.</li><li> - Stream responses from your selected CLI (Claude Code/Cursor/Codex) with WebSocket connection</li><li> - Resume previous conversations or start fresh sessions</li><li> - Complete conversation history with timestamps and metadata</li><li> - Text, code blocks, and file references</li></ul><ul><li> - Browse project structure with expand/collapse navigation</li><li> - Read, modify, and save files directly in the interface</li><li> - Support for multiple programming languages</li><li> - Create, rename, delete files and directories</li></ul><h4>TaskMaster AI Integration </h4><ul><li> - Kanban-style interface for managing development tasks</li><li> - Create Product Requirements Documents and parse them into structured tasks</li><li> - Real-time status updates and completion tracking</li></ul><ul><li> - All conversations automatically saved</li><li> - Group sessions by project and timestamp</li><li> - Rename, delete, and export conversation history</li><li> - Access sessions from any device</li></ul><ul><li> - Optimized for all screen sizes</li><li> - Swipe gestures and touch navigation</li><li> - Bottom tab bar for easy thumb navigation</li><li> - Collapsible sidebar and smart content prioritization</li><li><strong>Add shortcut to Home Screen</strong> - Add a shortcut to your home screen and the app will behave like a PWA</li></ul><pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Frontend      ‚îÇ    ‚îÇ   Backend       ‚îÇ    ‚îÇ  Agent     ‚îÇ\n‚îÇ   (React/Vite)  ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ (Express/WS)    ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ  Integration    ‚îÇ\n‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</code></pre><h3>Backend (Node.js + Express)</h3><ul><li> - RESTful API with static file serving</li><li> - Communication for chats and project refresh</li><li><strong>Agent Integration (Claude Code / Cursor CLI / Codex)</strong> - Process spawning and management</li><li> - Exposing file browser for projects</li></ul><ul><li> - Modern component architecture with hooks</li><li> - Advanced code editor with syntax highlighting</li></ul><p>We welcome contributions! Please see our <a href=\"https://raw.githubusercontent.com/siteboon/claudecodeui/main/CONTRIBUTING.md\">Contributing Guide</a> for details on commit conventions, development workflow, and release process.</p><h3>Common Issues &amp; Solutions</h3><h4>\"No Claude projects found\"</h4><p>: The UI shows no projects or empty project list :</p><ul><li>Run  command in at least one project directory to initialize</li><li>Verify  directory exists and has proper permissions</li></ul><p>: Files not loading, permission errors, empty directories :</p><ul><li>Check project directory permissions ( in terminal)</li><li>Verify the project path exists and is accessible</li><li>Review server console logs for detailed error messages</li><li>Ensure you're not trying to access system directories outside project scope</li></ul><p>GNU General Public License v3.0 - see <a href=\"https://raw.githubusercontent.com/siteboon/claudecodeui/main/LICENSE\">LICENSE</a> file for details.</p><p>This project is open source and free to use, modify, and distribute under the GPL v3 license.</p><ul><li> this repository to show support</li><li> for updates and new releases</li><li> the project for announcements</li></ul><div align=\"center\"><strong>Made with care for the Claude Code, Cursor and Codex community.</strong></div>",
      "contentLength": 7250,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/5fceb106b2663a05cdffa26485450de38c1e0fe2de5807b8881e9a8d1798c5dd/siteboon/claudecodeui",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "VectifyAI/PageIndex",
      "url": "https://github.com/VectifyAI/PageIndex",
      "date": 1771902315,
      "author": "",
      "guid": 47744,
      "unread": true,
      "content": "<p>üìë PageIndex: Document Index for Vectorless, Reasoning-based RAG</p><p>Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic  rather than true . But  ‚Äî what we truly need in retrieval is , and that requires . When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.</p><p>Inspired by AlphaGo, we propose  ‚Äî a ,  system that builds a  from long documents and uses LLMs to  for <strong>agentic, context-aware retrieval</strong>. It simulates how  navigate and extract knowledge from complex documents through , enabling LLMs to  and  their way to the most relevant document sections. PageIndex performs retrieval in two steps:</p><ol><li>Generate a ‚ÄúTable-of-Contents‚Äù  of documents</li><li>Perform reasoning-based retrieval through </li></ol><p>Compared to traditional vector-based RAG,  features:</p><ul><li>: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.</li><li>: Documents are organized into natural sections, not artificial chunks.</li><li>: Simulates how human experts navigate and extract knowledge from complex documents.</li><li><strong>Better Explainability and Traceability</strong>: Retrieval is based on reasoning ‚Äî traceable and interpretable, with page and section references. No more opaque, approximate vector search (‚Äúvibe retrieval‚Äù).</li></ul><p>PageIndex powers a reasoning-based RAG system that achieved <a href=\"https://github.com/VectifyAI/Mafin2.5-FinanceBench\">98.7% accuracy</a> on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our <a href=\"https://vectify.ai/blog/Mafin2.5\">blog post</a> for details).</p><p>The PageIndex service is available as a ChatGPT-style <a href=\"https://chat.pageindex.ai\">chat platform</a>, or can be integrated via <a href=\"https://pageindex.ai/mcp\">MCP</a> or <a href=\"https://docs.pageindex.ai/quickstart\">API</a>.</p><ul><li>Self-host ‚Äî run locally with this open-source repo.</li></ul><ul><li>Try the <a href=\"https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb\"></a> notebook ‚Äî a , hands-on example of reasoning-based RAG using PageIndex.</li></ul><p>PageIndex can transform lengthy PDF documents into a semantic , similar to a  but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.</p><pre><code>...\n{\n  \"title\": \"Financial Stability\",\n  \"node_id\": \"0006\",\n  \"start_index\": 21,\n  \"end_index\": 22,\n  \"summary\": \"The Federal Reserve ...\",\n  \"nodes\": [\n    {\n      \"title\": \"Monitoring Financial Vulnerabilities\",\n      \"node_id\": \"0007\",\n      \"start_index\": 22,\n      \"end_index\": 28,\n      \"summary\": \"The Federal Reserve's monitoring ...\"\n    },\n    {\n      \"title\": \"Domestic and International Cooperation and Coordination\",\n      \"node_id\": \"0008\",\n      \"start_index\": 28,\n      \"end_index\": 31,\n      \"summary\": \"In 2023, the Federal Reserve collaborated ...\"\n    }\n  ]\n}\n...\n</code></pre><p>You can generate the PageIndex tree structure with this open-source repo, or use our <a href=\"https://docs.pageindex.ai/quickstart\">API</a></p><p>You can follow these steps to generate a PageIndex tree from a PDF document.</p><pre><code>pip3 install --upgrade -r requirements.txt\n</code></pre><h3>2. Set your OpenAI API key</h3><p>Create a  file in the root directory and add your API key:</p><pre><code>CHATGPT_API_KEY=your_openai_key_here\n</code></pre><h3>3. Run PageIndex on your PDF</h3><pre><code>python3 run_pageindex.py --pdf_path /path/to/your/document.pdf\n</code></pre><p><a href=\"https://vectify.ai/mafin\">Mafin 2.5</a> is a reasoning-based RAG system for financial document analysis, powered by . It achieved a state-of-the-art <a href=\"https://vectify.ai/blog/Mafin2.5\"></a> on the <a href=\"https://arxiv.org/abs/2311.11944\">FinanceBench</a> benchmark, significantly outperforming traditional vector-based RAG systems.</p><p>PageIndex's hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.</p><ul><li>üß™ <a href=\"https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex\">Cookbooks</a>: hands-on, runnable examples and advanced use cases.</li><li>üìñ <a href=\"https://docs.pageindex.ai/doc-search\">Tutorials</a>: practical guides and strategies, including  and .</li><li>üìù <a href=\"https://pageindex.ai/blog\">Blog</a>: technical articles, research insights, and product updates.</li></ul><p>Please cite this work as:</p><pre><code>Mingtian Zhang, Yu Tang and PageIndex Team,\n\"PageIndex: Next-Generation Vectorless, Reasoning-based RAG\",\nPageIndex Blog, Sep 2025.\n</code></pre><p>Or use the BibTeX citation:</p><pre><code>@article{zhang2025pageindex,\n  author = {Mingtian Zhang and Yu Tang and PageIndex Team},\n  title = {PageIndex: Next-Generation Vectorless, Reasoning-based RAG},\n  journal = {PageIndex Blog},\n  year = {2025},\n  month = {September},\n  note = {https://pageindex.ai/blog/pageindex-intro},\n}\n</code></pre><p>Leave us a star üåü if you like our project. Thank you!</p>",
      "contentLength": 4261,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/1f94f8866183ff4b5c6789eaf5237e4ba222fc34aafc3653c28a7726f63a8866/VectifyAI/PageIndex",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "CompVis/stable-diffusion",
      "url": "https://github.com/CompVis/stable-diffusion",
      "date": 1771902315,
      "author": "",
      "guid": 47745,
      "unread": true,
      "content": "<p>A latent text-to-image diffusion model</p><p><em>Stable Diffusion was made possible thanks to a collaboration with <a href=\"https://stability.ai/\">Stability AI</a> and <a href=\"https://runwayml.com/\">Runway</a> and builds upon our previous work:</em></p><p><img src=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/merged-0006.png\" alt=\"txt2img-stable2\"><a href=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#stable-diffusion-v1\">Stable Diffusion</a> is a latent text-to-image diffusion model. Thanks to a generous compute donation from <a href=\"https://stability.ai/\">Stability AI</a> and support from <a href=\"https://laion.ai/\">LAION</a>, we were able to train a Latent Diffusion Model on 512x512 images from a subset of the <a href=\"https://laion.ai/blog/laion-5b/\">LAION-5B</a> database. Similar to Google's <a href=\"https://arxiv.org/abs/2205.11487\">Imagen</a>, this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See <a href=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#stable-diffusion-v1\">this section</a> below and the <a href=\"https://huggingface.co/CompVis/stable-diffusion\">model card</a>.</p><p>A suitable <a href=\"https://conda.io/\">conda</a> environment named  can be created and activated with:</p><pre><code>conda env create -f environment.yaml\nconda activate ldm\n</code></pre><pre><code>conda install pytorch torchvision -c pytorch\npip install transformers==4.19.2 diffusers invisible-watermark\npip install -e .\n</code></pre><p>Stable Diffusion v1 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 860M UNet and CLIP ViT-L/14 text encoder for the diffusion model. The model was pretrained on 256x256 images and then finetuned on 512x512 images.</p><p><em>Note: Stable Diffusion v1 is a general text-to-image diffusion model and therefore mirrors biases and (mis-)conceptions that are present in its training data. Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding <a href=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/Stable_Diffusion_v1_Model_Card.md\">model card</a>.</em></p><p>We currently provide the following checkpoints:</p><ul><li>: 237k steps at resolution  on <a href=\"https://huggingface.co/datasets/laion/laion2B-en\">laion2B-en</a>. 194k steps at resolution  on <a href=\"https://huggingface.co/datasets/laion/laion-high-resolution\">laion-high-resolution</a> (170M examples from LAION-5B with resolution ).</li><li>: Resumed from . 515k steps at resolution  on <a href=\"https://laion.ai/blog/laion-aesthetics/\">laion-aesthetics v2 5+</a> (a subset of laion2B-en with estimated aesthetics score , and additionally filtered to images with an original size , and an estimated watermark probability . The watermark estimate is from the <a href=\"https://laion.ai/blog/laion-5b/\">LAION-5B</a> metadata, the aesthetics score is estimated using the <a href=\"https://github.com/christophschuhmann/improved-aesthetic-predictor\">LAION-Aesthetics Predictor V2</a>).</li><li>: Resumed from . 195k steps at resolution  on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve <a href=\"https://arxiv.org/abs/2207.12598\">classifier-free guidance sampling</a>.</li><li>: Resumed from . 225k steps at resolution  on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve <a href=\"https://arxiv.org/abs/2207.12598\">classifier-free guidance sampling</a>.</li></ul><p>Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling steps show the relative improvements of the checkpoints: <img src=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/v1-variants-scores.jpg\" alt=\"sd evaluation results\"></p><h3>Text-to-Image with Stable Diffusion</h3><p>Stable Diffusion is a latent diffusion model conditioned on the (non-pooled) text embeddings of a CLIP ViT-L/14 text encoder. We provide a <a href=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#reference-sampling-script\">reference script for sampling</a>, but there also exists a <a href=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#diffusers-integration\">diffusers integration</a>, which we expect to see more active community development.</p><h4>Reference Sampling Script</h4><p>We provide a reference sampling script, which incorporates</p><pre><code>mkdir -p models/ldm/stable-diffusion-v1/\nln -s &lt;path/to/model.ckpt&gt; models/ldm/stable-diffusion-v1/model.ckpt \n</code></pre><pre><code>python scripts/txt2img.py --prompt \"a photograph of an astronaut riding a horse\" --plms \n</code></pre><p>By default, this uses a guidance scale of , <a href=\"https://github.com/CompVis/latent-diffusion/pull/51\">Katherine Crowson's implementation</a> of the <a href=\"https://arxiv.org/abs/2202.09778\">PLMS</a> sampler, and renders images of size 512x512 (which it was trained on) in 50 steps. All supported arguments are listed below (type <code>python scripts/txt2img.py --help</code>).</p><pre></pre><p>Note: The inference config for all v1 versions is designed to be used with EMA-only checkpoints. For this reason  is set in the configuration, otherwise the code will try to switch from non-EMA to EMA weights. If you want to examine the effect of EMA vs no EMA, we provide \"full\" checkpoints which contain both types of weights. For these,  will load and use the non-EMA weights.</p><p>A simple way to download and sample Stable Diffusion is by using the <a href=\"https://github.com/huggingface/diffusers/tree/main#new--stable-diffusion-is-now-fully-compatible-with-diffusers\">diffusers library</a>:</p><pre><code># make sure you're logged in with `huggingface-cli login`\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n\t\"CompVis/stable-diffusion-v1-4\", \n\tuse_auth_token=True\n).to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt)[\"sample\"][0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n</code></pre><h3>Image Modification with Stable Diffusion</h3><p>By using a diffusion-denoising mechanism as first proposed by <a href=\"https://arxiv.org/abs/2108.01073\">SDEdit</a>, the model can be used for different tasks such as text-guided image-to-image translation and upscaling. Similar to the txt2img sampling script, we provide a script to perform image modification with Stable Diffusion.</p><p>The following describes an example where a rough sketch made in <a href=\"https://www.pinta-project.com/\">Pinta</a> is converted into a detailed artwork.</p><pre><code>python scripts/img2img.py --prompt \"A fantasy landscape, trending on artstation\" --init-img &lt;path-to-img.jpg&gt; --strength 0.8\n</code></pre><p>Here, strength is a value between 0.0 and 1.0, that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input. See the following example.</p><p>This procedure can, for example, also be used to upscale samples from the base model.</p><pre><code>@misc{rombach2021highresolution,\n      title={High-Resolution Image Synthesis with Latent Diffusion Models}, \n      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bj√∂rn Ommer},\n      year={2021},\n      eprint={2112.10752},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>",
      "contentLength": 5519,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "muratcankoylan/Agent-Skills-for-Context-Engineering",
      "url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering",
      "date": 1771902315,
      "author": "",
      "guid": 47746,
      "unread": true,
      "content": "<p>A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.</p><p>A comprehensive, open collection of Agent Skills focused on context engineering principles for building production-grade AI agent systems. These skills teach the art and science of curating context to maximize agent effectiveness across any agent platform.</p><h2>What is Context Engineering?</h2><p>Context engineering is the discipline of managing the language model's context window. Unlike prompt engineering, which focuses on crafting effective instructions, context engineering addresses the holistic curation of all information that enters the model's limited attention budget: system prompts, tool definitions, retrieved documents, message history, and tool outputs.</p><p>The fundamental challenge is that context windows are constrained not by raw token capacity but by attention mechanics. As context length increases, models exhibit predictable degradation patterns: the \"lost-in-the-middle\" phenomenon, U-shaped attention curves, and attention scarcity. Effective context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.</p><p>This repository is cited in academic research as foundational work on static skill architecture:</p><blockquote><p>\"While static skills are well-recognized [Anthropic, 2025b; Muratcan Koylan, 2025], MCE is among the first to dynamically evolve them, bridging manual skill engineering and autonomous self-improvement.\"</p></blockquote><p>These skills establish the foundational understanding required for all subsequent context engineering work.</p><table><tbody><tr><td>Understand what context is, why it matters, and the anatomy of context in agent systems</td></tr><tr><td>Recognize patterns of context failure: lost-in-middle, poisoning, distraction, and clash</td></tr></tbody></table><p>These skills cover the patterns and structures for building effective agent systems.</p><table><tbody><tr><td>Design short-term, long-term, and graph-based memory architectures</td></tr><tr><td>Use filesystems for dynamic context discovery, tool output offloading, and plan persistence</td></tr><tr><td> Build background coding agents with sandboxed VMs, pre-built images, multiplayer support, and multi-client interfaces</td></tr></tbody></table><p>These skills address the ongoing operation and optimization of agent systems.</p><table><tbody><tr><td>Build evaluation frameworks for agent systems</td></tr><tr><td>Master LLM-as-a-Judge techniques: direct scoring, pairwise comparison, rubric generation, and bias mitigation</td></tr></tbody></table><p>These skills cover the meta-level practices for building LLM-powered projects.</p><table><tbody><tr><td>Design and build LLM projects from ideation through deployment, including task-model fit analysis, pipeline architecture, and structured output design</td></tr></tbody></table><h3>Cognitive Architecture Skills</h3><p>These skills cover formal cognitive modeling for rational agent systems.</p><table><tbody><tr><td> Transform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns for deliberative reasoning and explainability</td></tr></tbody></table><p>Each skill is structured for efficient context use. At startup, agents load only skill names and descriptions. Full content loads only when a skill is activated for relevant tasks.</p><p>These skills focus on transferable principles rather than vendor-specific implementations. The patterns work across Claude Code, Cursor, and any agent platform that supports skills or allows custom instructions.</p><h3>Conceptual Foundation with Practical Examples</h3><p>Scripts and examples demonstrate concepts using Python pseudocode that works across environments without requiring specific dependency installations.</p><p>This repository is a <strong>Claude Code Plugin Marketplace</strong> containing context engineering skills that Claude automatically discovers and activates based on your task context.</p><p><strong>Step 1: Add the Marketplace</strong></p><p>Run this command in Claude Code to register this repository as a plugin source:</p><pre><code>/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering\n</code></pre><p><strong>Step 2: Browse and Install</strong></p><p>Option A - Browse available plugins:</p><ol><li>Select <code>Browse and install plugins</code></li><li>Select <code>context-engineering-marketplace</code></li><li>Choose a plugin (e.g., <code>context-engineering-fundamentals</code>, )</li></ol><p>Option B - Direct install via command:</p><pre><code>/plugin install context-engineering-fundamentals@context-engineering-marketplace\n/plugin install agent-architecture@context-engineering-marketplace\n/plugin install agent-evaluation@context-engineering-marketplace\n/plugin install agent-development@context-engineering-marketplace\n/plugin install cognitive-architecture@context-engineering-marketplace\n</code></pre><table><tbody><tr><td><code>context-engineering-fundamentals</code></td><td>context-fundamentals, context-degradation, context-compression, context-optimization</td></tr><tr><td>multi-agent-patterns, memory-systems, tool-design, filesystem-context, hosted-agents</td></tr><tr><td>evaluation, advanced-evaluation</td></tr><tr></tr><tr></tr></tbody></table><table><tbody><tr><td>\"understand context\", \"explain context windows\", \"design agent architecture\"</td></tr><tr><td>\"diagnose context problems\", \"fix lost-in-middle\", \"debug agent failures\"</td></tr><tr><td>\"compress context\", \"summarize conversation\", \"reduce token usage\"</td></tr><tr><td>\"optimize context\", \"reduce token costs\", \"implement KV-cache\"</td></tr><tr><td>\"design multi-agent system\", \"implement supervisor pattern\"</td></tr><tr><td>\"implement agent memory\", \"build knowledge graph\", \"track entities\"</td></tr><tr><td>\"design agent tools\", \"reduce tool complexity\", \"implement MCP tools\"</td></tr><tr><td>\"offload context to files\", \"dynamic context discovery\", \"agent scratch pad\", \"file-based context\"</td></tr><tr><td>\"build background agent\", \"create hosted coding agent\", \"sandboxed execution\", \"multiplayer agent\", \"Modal sandboxes\"</td></tr><tr><td>\"evaluate agent performance\", \"build test framework\", \"measure quality\"</td></tr><tr><td>\"implement LLM-as-judge\", \"compare model outputs\", \"mitigate bias\"</td></tr><tr><td>\"start LLM project\", \"design batch pipeline\", \"evaluate task-model fit\"</td></tr><tr><td>\"model agent mental states\", \"implement BDI architecture\", \"transform RDF to beliefs\", \"build cognitive agent\"</td></tr></tbody></table><img width=\"1014\" height=\"894\" alt=\"Screenshot 2025-12-26 at 12 34 47‚ÄØPM\" src=\"https://github.com/user-attachments/assets/f79aaf03-fd2d-4c71-a630-7027adeb9bfe\"><p>Copy skill content into  or create project-specific Skills folders. The skills provide the context and guidelines that agent needs for effective context engineering and agent design.</p><h3>For Custom Implementations</h3><p>Extract the principles and patterns from any skill and implement them in your agent framework. The skills are deliberately platform-agnostic.</p><p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/\">examples</a> folder contains complete system designs that demonstrate how multiple skills work together in practice.</p><table><thead><tr></tr></thead><tbody><tr><td> Personal operating system for founders and creators. Complete Claude Code skill with 6 modules, 4 automation scripts</td><td>context-fundamentals, context-optimization, memory-systems, tool-design, multi-agent-patterns, evaluation, project-development</td></tr><tr><td>Multi-agent system that monitors X accounts and generates daily synthesized books</td><td>multi-agent-patterns, memory-systems, context-optimization, tool-design, evaluation</td></tr><tr><td>Production-ready LLM evaluation tools with TypeScript implementation, 19 passing tests</td><td>advanced-evaluation, tool-design, context-fundamentals, evaluation</td></tr><tr><td>Train models to write in any author's style. Includes Gertrude Stein case study with 70% human score on Pangram, $2 total cost</td><td>project-development, context-compression, multi-agent-patterns, evaluation</td></tr></tbody></table><ul><li>Complete PRD with architecture decisions</li><li>Skills mapping showing which concepts informed each decision</li></ul><h3>Digital Brain Skill Example</h3><p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/digital-brain-skill/\">digital-brain-skill</a> example is a complete personal operating system demonstrating comprehensive skills application:</p><ul><li>: 3-level loading (SKILL.md ‚Üí MODULE.md ‚Üí data files)</li><li>: 6 independent modules (identity, content, knowledge, network, operations, agents)</li><li>: JSONL files with schema-first lines for agent-friendly parsing</li><li>: 4 consolidated tools (weekly_review, content_ideas, stale_contacts, idea_to_draft)</li></ul><p>Includes detailed traceability in <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/digital-brain-skill/HOW-SKILLS-BUILT-THIS.md\">HOW-SKILLS-BUILT-THIS.md</a> mapping every architectural decision to specific skill principles.</p><h3>LLM-as-Judge Skills Example</h3><ul><li>: Evaluate responses against weighted criteria with rubric support</li><li>: Compare responses with position bias mitigation</li><li>: Create domain-specific evaluation standards</li><li>: High-level agent combining all evaluation capabilities</li></ul><h3>Book SFT Pipeline Example</h3><p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/book-sft-pipeline/\">book-sft-pipeline</a> example demonstrates training small models (8B) to write in any author's style:</p><ul><li>: Two-tier chunking with overlap for maximum training examples</li><li>: 15+ templates to prevent memorization and force style learning</li><li>: Complete LoRA training workflow with $2 total cost</li><li>: Modern scenario testing proves style transfer vs content memorization</li></ul><p>Integrates with context engineering skills: project-development, context-compression, multi-agent-patterns, evaluation.</p><img width=\"3664\" height=\"2648\" alt=\"star-history-2026224\" src=\"https://github.com/user-attachments/assets/b3bdbf23-4b6a-4774-ae85-42ef4d9b2d79\"><p>Each skill follows the Agent Skills specification:</p><pre><code>skill-name/\n‚îú‚îÄ‚îÄ SKILL.md              # Required: instructions + metadata\n‚îú‚îÄ‚îÄ scripts/              # Optional: executable code demonstrating concepts\n‚îî‚îÄ‚îÄ references/           # Optional: additional documentation and resources\n</code></pre><p>See the <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/template/\">template</a> folder for the canonical skill structure.</p><p>This repository follows the Agent Skills open development model. Contributions are welcome from the broader ecosystem. When contributing:</p><ol><li>Follow the skill template structure</li><li>Provide clear, actionable instructions</li><li>Include working examples where appropriate</li><li>Document trade-offs and potential issues</li><li>Keep SKILL.md under 500 lines for optimal performance</li></ol><p>Feel free to contact <a href=\"https://x.com/koylanai\">Muratcan Koylan</a> for collaboration opportunities or any inquiries.</p><p>MIT License - see LICENSE file for details.</p><p>The principles in these skills are derived from research and production experience at leading AI labs and framework developers. Each skill includes references to the underlying research and case studies that inform its recommendations.</p>",
      "contentLength": 9409,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/8ff8f9cab2a8d3be596da57264131d4de47f25ff6b06d3ba699eec85bab53f28/muratcankoylan/Agent-Skills-for-Context-Engineering",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "clash-verge-rev/clash-verge-rev",
      "url": "https://github.com/clash-verge-rev/clash-verge-rev",
      "date": 1771902315,
      "author": "",
      "guid": 47747,
      "unread": true,
      "content": "<p>A modern GUI client based on Tauri, designed to run in Windows, macOS and Linux for tailored proxy experience</p><h3 align=\"center\"> A Clash Meta GUI based on <a href=\"https://github.com/tauri-apps/tauri\">Tauri</a>. </h3><p>ËØ∑Âà∞ÂèëÂ∏ÉÈ°µÈù¢‰∏ãËΩΩÂØπÂ∫îÁöÑÂÆâË£ÖÂåÖÔºö<a href=\"https://github.com/clash-verge-rev/clash-verge-rev/releases\">Release page</a> Go to the <a href=\"https://github.com/clash-verge-rev/clash-verge-rev/releases\">Release page</a> to download the corresponding installation package Supports Windows (x64/x86), Linux (x64/arm64) and macOS 10.15+ (intel/apple).</p><p>üöÄ È´òÊÄßËÉΩÊµ∑Â§ñÊäÄÊúØÊµÅÊú∫Âú∫ÔºåÊîØÊåÅÂÖçË¥πËØïÁî®‰∏é‰ºòÊÉ†Â•óÈ§êÔºåÂÖ®Èù¢Ëß£ÈîÅÊµÅÂ™í‰ΩìÂèä AI ÊúçÂä°ÔºåÂÖ®ÁêÉÈ¶ñÂÆ∂ÈááÁî® „ÄÇ</p><p>üéÅ ‰ΩøÁî®  Ê≥®ÂÜåÂç≥ÈÄÅ ÔºåÊØèÊó• Ôºöüëâ <a href=\"https://verge.dginv.click/#/register?code=oaxsAGo6\">ÁÇπÊ≠§Ê≥®ÂÜå</a></p><ul><li>üì± Ëá™Á†î iOS ÂÆ¢Êà∑Á´ØÔºà‰∏öÂÜÖ\"ÂîØ‰∏Ä\"ÔºâÊäÄÊúØÁªèÂæóËµ∑ËÄÉÈ™åÔºåÊûÅÂ§ßÊäïÂÖ•</li><li>üßë‚Äçüíª (È°∫Â∏¶Ëß£ÂÜ≥ Clash Verge ‰ΩøÁî®ÈóÆÈ¢ò)</li><li>üí∞ ‰ºòÊÉ†Â•óÈ§êÊØèÊúà</li><li>‚öôÔ∏è ËÆæËÆ°ÔºåÔºåÈ´òÈÄü‰∏ìÁ∫ø(ÂÖºÂÆπËÄÅÂÆ¢Êà∑Á´Ø)ÔºåÊûÅ‰ΩéÂª∂ËøüÔºåÊó†ËßÜÊôöÈ´òÂ≥∞Ôºå4K ÁßíÂºÄ</li><li>‚ö° ÂÖ®ÁêÉÈ¶ñÂÆ∂ÔºåÁé∞Â∑≤‰∏äÁ∫øÊõ¥Âø´ÁöÑ Tuic ÂçèËÆÆ(Clash Verge ÂÆ¢Êà∑Á´ØÊúÄ‰Ω≥Êê≠ÈÖç)</li></ul><ul><li>Âü∫‰∫éÊÄßËÉΩÂº∫Âä≤ÁöÑ Rust Âíå Tauri 2 Ê°ÜÊû∂</li><li>ÁÆÄÊ¥ÅÁæéËßÇÁöÑÁî®Êà∑ÁïåÈù¢ÔºåÊîØÊåÅËá™ÂÆö‰πâ‰∏ªÈ¢òÈ¢úËâ≤„ÄÅ‰ª£ÁêÜÁªÑ/ÊâòÁõòÂõæÊ†á‰ª•Âèä „ÄÇ</li><li>ÈÖçÁΩÆÊñá‰ª∂ÁÆ°ÁêÜÂíåÂ¢ûÂº∫ÔºàMerge Âíå ScriptÔºâÔºåÈÖçÁΩÆÊñá‰ª∂ËØ≠Ê≥ïÊèêÁ§∫„ÄÇ</li></ul><p>To run the development server, execute the following commands after all prerequisites for  are installed:</p><pre><code>pnpm i\npnpm run prebuild\npnpm dev\n</code></pre><p>Clash Verge rev was based on or inspired by these projects and so on:</p>",
      "contentLength": 1303,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "f/prompts.chat",
      "url": "https://github.com/f/prompts.chat",
      "date": 1771902315,
      "author": "",
      "guid": 47748,
      "unread": true,
      "content": "<p>a.k.a. Awesome ChatGPT Prompts. Share, discover, and collect prompts from the community. Free and open source ‚Äî self-host for your organization with complete privacy.</p><p align=\"center\"><strong>The world's largest open-source prompt library for AI</strong><sub>Works with ChatGPT, Claude, Gemini, Llama, Mistral, and more</sub></p><p align=\"center\"><sub>formerly known as Awesome ChatGPT Prompts</sub></p><p>A curated collection of  for AI chat models. Originally created for ChatGPT, these prompts work great with any modern AI assistant.</p><h2>üìñ The Interactive Book of Prompting</h2><p>Learn prompt engineering with our  ‚Äî 25+ chapters covering everything from basics to advanced techniques like chain-of-thought reasoning, few-shot learning, and AI agents.</p><p>An interactive, game-based adventure to teach children (ages 8-14) how to communicate with AI through fun puzzles and stories.</p><p>Deploy your own private prompt library with custom branding, themes, and authentication.</p><pre><code>npx prompts.chat new my-prompt-library\ncd my-prompt-library\n</code></pre><pre><code>git clone https://github.com/f/prompts.chat.git\ncd prompts.chat\nnpm install &amp;&amp; npm run setup\n</code></pre><p>The setup wizard configures branding, theme, authentication (GitHub/Google/Azure AD), and features.</p><pre><code>/plugin marketplace add f/prompts.chat\n/plugin install prompts.chat@prompts.chat\n</code></pre><p>Use prompts.chat as an MCP server in your AI tools.</p><pre><code>{\n  \"mcpServers\": {\n    \"prompts.chat\": {\n      \"url\": \"https://prompts.chat/api/mcp\"\n    }\n  }\n}\n</code></pre><pre><code>{\n  \"mcpServers\": {\n    \"prompts.chat\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"prompts.chat\", \"mcp\"]\n    }\n  }\n}\n</code></pre><a href=\"https://github.com/f/prompts.chat/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=f/prompts.chat\"></a>",
      "contentLength": 1482,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "NevaMind-AI/memU",
      "url": "https://github.com/NevaMind-AI/memU",
      "date": 1771902315,
      "author": "",
      "guid": 47749,
      "unread": true,
      "content": "<p>Memory for 24/7 proactive agents like openclaw (moltbot, clawdbot).</p><p>memU is a memory framework built for . It is designed for long-running use and greatly <strong>reduces the LLM token cost</strong> of keeping agents always online, making always-on, evolving agents practical in production systems. memU <strong>continuously captures and understands user intent</strong>. Even without a command, the agent can tell what you are about to do and act on it by itself.</p><img width=\"100%\" src=\"https://github.com/NevaMind-AI/memU/raw/main/assets/memUbot.png\"><ul><li><strong>Download-and-use and simple</strong> to get started.</li><li>Builds long-term memory to  and act proactively.</li><li> with smaller context.</li></ul><h2>üóÉÔ∏è Memory as File System, File System as Memory</h2><p>memU treats <strong>memory like a file system</strong>‚Äîstructured, hierarchical, and instantly accessible.</p><table><tbody><tr><td>üè∑Ô∏è Categories (auto-organized topics)</td></tr><tr><td>üß† Memory Items (extracted facts, preferences, skills)</td></tr><tr><td>üîÑ Cross-references (related memories linked)</td></tr><tr><td>üì• Resources (conversations, documents, images)</td></tr></tbody></table><ul><li> like browsing directories‚Äîdrill down from broad categories to specific facts</li><li> instantly‚Äîconversations and documents become queryable memory</li><li>‚Äîmemories reference each other, building a connected knowledge graph</li><li>‚Äîexport, backup, and transfer memory like files</li></ul><pre><code>memory/\n‚îú‚îÄ‚îÄ preferences/\n‚îÇ   ‚îú‚îÄ‚îÄ communication_style.md\n‚îÇ   ‚îî‚îÄ‚îÄ topic_interests.md\n‚îú‚îÄ‚îÄ relationships/\n‚îÇ   ‚îú‚îÄ‚îÄ contacts/\n‚îÇ   ‚îî‚îÄ‚îÄ interaction_history/\n‚îú‚îÄ‚îÄ knowledge/\n‚îÇ   ‚îú‚îÄ‚îÄ domain_expertise/\n‚îÇ   ‚îî‚îÄ‚îÄ learned_skills/\n‚îî‚îÄ‚îÄ context/\n    ‚îú‚îÄ‚îÄ recent_conversations/\n    ‚îî‚îÄ‚îÄ pending_tasks/\n</code></pre><p>Just as a file system turns raw bytes into organized data, memU transforms raw interactions into <strong>structured, searchable, proactive intelligence</strong>.</p><img width=\"100%\" src=\"https://github.com/NevaMind-AI/memU/raw/main/assets/star.gif\"> If you find memU useful or interesting, a GitHub Star ‚≠êÔ∏è would be greatly appreciated. \n<table><tbody><tr><td>Always-on memory agent that works continuously in the background‚Äînever sleeps, never forgets</td></tr><tr><td>Understands and remembers user goals, preferences, and context across sessions automatically</td></tr><tr><td>Reduces long-running token costs by caching insights and avoiding redundant LLM calls</td></tr></tbody></table><h2>üîÑ How Proactive Memory Works</h2><pre><code>\ncd examples/proactive\npython proactive.py\n\n</code></pre><h3>Proactive Memory Lifecycle</h3><pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                                         USER QUERY                                               ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                 ‚îÇ                                                           ‚îÇ\n                 ‚ñº                                                           ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         ü§ñ MAIN AGENT                  ‚îÇ         ‚îÇ              üß† MEMU BOT                       ‚îÇ\n‚îÇ                                        ‚îÇ         ‚îÇ                                                ‚îÇ\n‚îÇ  Handle user queries &amp; execute tasks   ‚îÇ  ‚óÑ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îÇ  Monitor, memorize &amp; proactive intelligence   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                        ‚îÇ         ‚îÇ                                                ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ  1. RECEIVE USER INPUT           ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ  1. MONITOR INPUT/OUTPUT                 ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ     Parse query, understand      ‚îÇ  ‚îÇ   ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îÇ  ‚îÇ     Observe agent interactions           ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ     context and intent           ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ     Track conversation flow              ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ                 ‚îÇ                      ‚îÇ         ‚îÇ                    ‚îÇ                           ‚îÇ\n‚îÇ                 ‚ñº                      ‚îÇ         ‚îÇ                    ‚ñº                           ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ  2. PLAN &amp; EXECUTE               ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ  2. MEMORIZE &amp; EXTRACT                   ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ     Break down tasks             ‚îÇ  ‚îÇ   ‚óÑ‚îÄ‚îÄ‚îÄ  ‚îÇ  ‚îÇ     Store insights, facts, preferences   ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ     Call tools, retrieve data    ‚îÇ  ‚îÇ  inject ‚îÇ  ‚îÇ     Extract skills &amp; knowledge           ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ     Generate responses           ‚îÇ  ‚îÇ  memory ‚îÇ  ‚îÇ     Update user profile                  ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ                 ‚îÇ                      ‚îÇ         ‚îÇ                    ‚îÇ                           ‚îÇ\n‚îÇ                 ‚ñº                      ‚îÇ         ‚îÇ                    ‚ñº                           ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ  3. RESPOND TO USER              ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ  3. PREDICT USER INTENT                  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ     Deliver answer/result        ‚îÇ  ‚îÇ   ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îÇ  ‚îÇ     Anticipate next steps                ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ     Continue conversation        ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ     Identify upcoming needs              ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ                 ‚îÇ                      ‚îÇ         ‚îÇ                    ‚îÇ                           ‚îÇ\n‚îÇ                 ‚ñº                      ‚îÇ         ‚îÇ                    ‚ñº                           ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ  4. LOOP                         ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ  4. RUN PROACTIVE TASKS                  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ     Wait for next user input     ‚îÇ  ‚îÇ   ‚óÑ‚îÄ‚îÄ‚îÄ  ‚îÇ  ‚îÇ     Pre-fetch relevant context           ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ     or proactive suggestions     ‚îÇ  ‚îÇ  suggest‚îÇ  ‚îÇ     Prepare recommendations              ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ         ‚îÇ  ‚îÇ     Update todolist autonomously         ‚îÇ  ‚îÇ\n‚îÇ                                        ‚îÇ         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                 ‚îÇ                                                           ‚îÇ\n                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                             ‚ñº\n                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                              ‚îÇ     CONTINUOUS SYNC LOOP     ‚îÇ\n                              ‚îÇ  Agent ‚óÑ‚îÄ‚îÄ‚ñ∫ MemU Bot ‚óÑ‚îÄ‚îÄ‚ñ∫ DB ‚îÇ\n                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</code></pre><h3>1. <strong>Information Recommendation</strong></h3><p><em>Agent monitors interests and proactively surfaces relevant content</em></p><pre><code># User has been researching AI topics\nMemU tracks: reading history, saved articles, search queries\n\n# When new content arrives:\nAgent: \"I found 3 new papers on RAG optimization that align with\n        your recent research on retrieval systems. One author\n        (Dr. Chen) you've cited before published yesterday.\"\n\n# Proactive behaviors:\n- Learns topic preferences from browsing patterns\n- Tracks author/source credibility preferences\n- Filters noise based on engagement history\n- Times recommendations for optimal attention\n</code></pre><p><em>Agent learns communication patterns and handles routine correspondence</em></p><pre><code># MemU observes email patterns over time:\n- Response templates for common scenarios\n- Priority contacts and urgent keywords\n- Scheduling preferences and availability\n- Writing style and tone variations\n\n# Proactive email assistance:\nAgent: \"You have 12 new emails. I've drafted responses for 3 routine\n        requests and flagged 2 urgent items from your priority contacts.\n        Should I also reschedule tomorrow's meeting based on the\n        conflict John mentioned?\"\n\n# Autonomous actions:\n‚úì Draft context-aware replies\n‚úì Categorize and prioritize inbox\n‚úì Detect scheduling conflicts\n‚úì Summarize long threads with key decisions\n</code></pre><h3>3. <strong>Trading &amp; Financial Monitoring</strong></h3><p><em>Agent tracks market context and user investment behavior</em></p><pre><code># MemU learns trading preferences:\n- Risk tolerance from historical decisions\n- Preferred sectors and asset classes\n- Response patterns to market events\n- Portfolio rebalancing triggers\n\n# Proactive alerts:\nAgent: \"NVDA dropped 5% in after-hours trading. Based on your past\n        behavior, you typically buy tech dips above 3%. Your current\n        allocation allows for $2,000 additional exposure while\n        maintaining your 70/30 equity-bond target.\"\n\n# Continuous monitoring:\n- Track price alerts tied to user-defined thresholds\n- Correlate news events with portfolio impact\n- Learn from executed vs. ignored recommendations\n- Anticipate tax-loss harvesting opportunities\n</code></pre><h2>üóÇÔ∏è Hierarchical Memory Architecture</h2><p>MemU's three-layer system enables both  and <strong>proactive context loading</strong>:</p><img width=\"100%\" alt=\"structure\" src=\"https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/structure.png\"><table><thead><tr></tr></thead><tbody><tr><td>Direct access to original data</td><td>Background monitoring for new patterns</td></tr><tr><td>Real-time extraction from ongoing interactions</td></tr><tr><td>Automatic context assembly for anticipation</td></tr></tbody></table><ul><li>: New memories self-organize into topics</li><li>: System identifies recurring themes</li><li>: Anticipates what information will be needed next</li></ul><p>Experience proactive memory instantly:</p><p>üëâ  - Hosted service with 7√ó24 continuous learning</p><p>For enterprise deployment with custom proactive workflows, contact </p><table><thead><tr></tr></thead><tbody><tr><td><code>Authorization: Bearer YOUR_API_KEY</code></td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>Register continuous learning task</td></tr><tr><td><code>/api/v3/memory/memorize/status/{task_id}</code></td><td>Check real-time processing status</td></tr><tr><td><code>/api/v3/memory/categories</code></td><td>List auto-generated categories</td></tr><tr><td>Query memory (supports proactive context loading)</td></tr></tbody></table><blockquote><p>: Python 3.13+ and an OpenAI API key</p></blockquote><p> (in-memory):</p><pre><code>export OPENAI_API_KEY=your_api_key\ncd tests\npython test_inmemory.py\n</code></pre><p><strong>Test with Persistent Storage</strong> (PostgreSQL):</p><pre><code># Start PostgreSQL with pgvector\ndocker run -d \\\n  --name memu-postgres \\\n  -e POSTGRES_USER=postgres \\\n  -e POSTGRES_PASSWORD=postgres \\\n  -e POSTGRES_DB=memu \\\n  -p 5432:5432 \\\n  pgvector/pgvector:pg16\n\n# Run continuous learning test\nexport OPENAI_API_KEY=your_api_key\ncd tests\npython test_postgres.py\n</code></pre><p>Both examples demonstrate <strong>proactive memory workflows</strong>:</p><ol><li>: Process multiple files sequentially</li><li>: Immediate memory creation</li><li>: Context-aware memory surfacing</li></ol><h3>Custom LLM and Embedding Providers</h3><p>MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via :</p><pre><code>from memu import MemUService\n\nservice = MemUService(\n    llm_profiles={\n        # Default profile for LLM operations\n        \"default\": {\n            \"base_url\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n            \"api_key\": \"your_api_key\",\n            \"chat_model\": \"qwen3-max\",\n            \"client_backend\": \"sdk\"  # \"sdk\" or \"http\"\n        },\n        # Separate profile for embeddings\n        \"embedding\": {\n            \"base_url\": \"https://api.voyageai.com/v1\",\n            \"api_key\": \"your_voyage_api_key\",\n            \"embed_model\": \"voyage-3.5-lite\"\n        }\n    },\n    # ... other configuration\n)\n</code></pre><p>MemU supports <a href=\"https://openrouter.ai\">OpenRouter</a> as a model provider, giving you access to multiple LLM providers through a single API.</p><pre><code>from memu import MemoryService\n\nservice = MemoryService(\n    llm_profiles={\n        \"default\": {\n            \"provider\": \"openrouter\",\n            \"client_backend\": \"httpx\",\n            \"base_url\": \"https://openrouter.ai\",\n            \"api_key\": \"your_openrouter_api_key\",\n            \"chat_model\": \"anthropic/claude-3.5-sonnet\",  # Any OpenRouter model\n            \"embed_model\": \"openai/text-embedding-3-small\",  # Embedding model\n        },\n    },\n    database_config={\n        \"metadata_store\": {\"provider\": \"inmemory\"},\n    },\n)\n</code></pre><table><tbody><tr><td>Works with any OpenRouter chat model</td></tr><tr><td>Use OpenAI embedding models via OpenRouter</td></tr><tr><td>Use vision-capable models (e.g., )</td></tr></tbody></table><pre><code>export OPENROUTER_API_KEY=your_api_key\n\n# Full workflow test (memorize + retrieve)\npython tests/test_openrouter.py\n\n# Embedding-specific tests\npython tests/test_openrouter_embedding.py\n\n# Vision-specific tests\npython tests/test_openrouter_vision.py\n</code></pre><h3> - Continuous Learning Pipeline</h3><p>Processes inputs in real-time and immediately updates memory:</p><img width=\"100%\" alt=\"memorize\" src=\"https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/memorize.png\"><pre><code>result = await service.memorize(\n    resource_url=\"path/to/file.json\",  # File path or URL\n    modality=\"conversation\",            # conversation | document | image | video | audio\n    user={\"user_id\": \"123\"}             # Optional: scope to a user\n)\n\n# Returns immediately with extracted memory:\n{\n    \"resource\": {...},      # Stored resource metadata\n    \"items\": [...],         # Extracted memory items (available instantly)\n    \"categories\": [...]     # Auto-updated category structure\n}\n</code></pre><ul><li>Zero-delay processing‚Äîmemories available immediately</li><li>Automatic categorization without manual tagging</li><li>Cross-reference with existing memories for pattern detection</li></ul><h3> - Dual-Mode Intelligence</h3><p>MemU supports both <strong>proactive context loading</strong> and :</p><img width=\"100%\" alt=\"retrieve\" src=\"https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/retrieve.png\"><h4>RAG-based Retrieval ()</h4><p>Fast <strong>proactive context assembly</strong> using embeddings:</p><ul><li>‚úÖ : Sub-second memory surfacing</li><li>‚úÖ : Can run continuously without LLM costs</li><li>‚úÖ : Identifies most relevant memories automatically</li></ul><h4>LLM-based Retrieval ()</h4><p>Deep  for complex contexts:</p><ul><li>‚úÖ : LLM infers what user needs before they ask</li><li>‚úÖ : Automatically refines search as context develops</li><li>‚úÖ : Stops when sufficient context is gathered</li></ul><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr><td>Triggered context loading</td></tr><tr></tr></tbody></table><pre><code># Proactive retrieval with context history\nresult = await service.retrieve(\n    queries=[\n        {\"role\": \"user\", \"content\": {\"text\": \"What are their preferences?\"}},\n        {\"role\": \"user\", \"content\": {\"text\": \"Tell me about work habits\"}}\n    ],\n    where={\"user_id\": \"123\"},  # Optional: scope filter\n    method=\"rag\"  # or \"llm\" for deeper reasoning\n)\n\n# Returns context-aware results:\n{\n    \"categories\": [...],     # Relevant topic areas (auto-prioritized)\n    \"items\": [...],          # Specific memory facts\n    \"resources\": [...],      # Original sources for traceability\n    \"next_step_query\": \"...\" # Predicted follow-up context\n}\n</code></pre><p>: Use  to scope continuous monitoring:</p><ul><li> - User-specific context</li><li><code>where={\"agent_id__in\": [\"1\", \"2\"]}</code> - Multi-agent coordination</li><li>Omit  for global context awareness</li></ul><h3>Example 1: Always-Learning Assistant</h3><p>Continuously learns from every interaction without explicit memory commands:</p><pre><code>export OPENAI_API_KEY=your_api_key\npython examples/example_1_conversation_memory.py\n</code></pre><ul><li>Automatically extracts preferences from casual mentions</li><li>Builds relationship models from interaction patterns</li><li>Surfaces relevant context in future conversations</li><li>Adapts communication style based on learned preferences</li></ul><p> Personal AI assistants, customer support that remembers, social chatbots</p><h3>Example 2: Self-Improving Agent</h3><p>Learns from execution logs and proactively suggests optimizations:</p><pre><code>export OPENAI_API_KEY=your_api_key\npython examples/example_2_skill_extraction.py\n</code></pre><ul><li>Monitors agent actions and outcomes continuously</li><li>Identifies patterns in successes and failures</li><li>Auto-generates skill guides from experience</li><li>Proactively suggests strategies for similar future tasks</li></ul><p> DevOps automation, agent self-improvement, knowledge capture</p><h3>Example 3: Multimodal Context Builder</h3><p>Unifies memory across different input types for comprehensive context:</p><pre><code>export OPENAI_API_KEY=your_api_key\npython examples/example_3_multimodal_memory.py\n</code></pre><ul><li>Cross-references text, images, and documents automatically</li><li>Builds unified understanding across modalities</li><li>Surfaces visual context when discussing related topics</li><li>Anticipates information needs by combining multiple sources</li></ul><p> Documentation systems, learning platforms, research assistants</p><p>MemU achieves  on the Locomo benchmark across all reasoning tasks, demonstrating reliable proactive memory operations.</p><img width=\"100%\" alt=\"benchmark\" src=\"https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9\"><table><thead><tr></tr></thead><tbody><tr><td>Core proactive memory engine</td><td>7√ó24 learning pipeline, auto-categorization</td></tr><tr><td>Backend with continuous sync</td><td>Real-time memory updates, webhook triggers</td></tr><tr><td>Live memory evolution monitoring</td></tr></tbody></table><p>We welcome contributions from the community! Whether you're fixing bugs, adding features, or improving documentation, your help is appreciated.</p><p>To start contributing to MemU, you'll need to set up your development environment:</p><ul><li><a href=\"https://github.com/astral-sh/uv\">uv</a> (Python package manager)</li></ul><h4>Setup Development Environment</h4><pre><code># 1. Fork and clone the repository\ngit clone https://github.com/YOUR_USERNAME/memU.git\ncd memU\n\n# 2. Install development dependencies\nmake install\n</code></pre><p>The  command will:</p><ul><li>Create a virtual environment using </li><li>Install all project dependencies</li><li>Set up pre-commit hooks for code quality checks</li></ul><p>Before submitting your contribution, ensure your code passes all quality checks:</p><p>The  command runs:</p><ul><li>: Ensures  consistency</li><li>: Lints code with Ruff, formats with Black</li><li>: Runs  for static type analysis</li><li>: Uses  to find obsolete dependencies</li></ul><p>For detailed contribution guidelines, code standards, and development practices, please see <a href=\"https://raw.githubusercontent.com/NevaMind-AI/memU/main/CONTRIBUTING.md\">CONTRIBUTING.md</a>.</p><ul><li>Create a new branch for each feature or bug fix</li><li>Write clear commit messages</li><li>Add tests for new functionality</li><li>Update documentation as needed</li><li>Run  before pushing</li></ul><div align=\"center\"><p>‚≠ê  to get notified about new releases!</p></div>",
      "contentLength": 19624,
      "flags": null,
      "enclosureUrl": "https://repository-images.githubusercontent.com/1028070615/af79b134-ac3f-4efb-9f95-5de0dc77a63d",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "x1xhlol/system-prompts-and-models-of-ai-tools",
      "url": "https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools",
      "date": 1771816069,
      "author": "",
      "guid": 47435,
      "unread": true,
      "content": "<p>FULL Augment Code, Claude Code, Cluely, CodeBuddy, Comet, Cursor, Devin AI, Junie, Kiro, Leap.new, Lovable, Manus, NotionAI, Orchids.app, Perplexity, Poke, Qoder, Replit, Same.dev, Trae, Traycer AI, VSCode Agent, Warp.dev, Windsurf, Xcode, Z.ai Code, Dia &amp; v0. (And other Open Sourced) System Prompts, Internal Tools &amp; AI Models</p><p align=\"center\">Official CA: DEffWzJyaFRNyA4ogUox631hfHuv3KLeCcpBh2ipBAGS (on Solana)</p><a href=\"https://discord.gg/NwzrWErdMU\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1402660735833604126?label=LeaksLab%20Discord&amp;logo=discord&amp;style=for-the-badge\" alt=\"LeaksLab Discord\"></a><p>üìú Over  of insights into their structure and functionality.</p><p>If you find this collection valuable and appreciate the effort involved in obtaining and sharing these insights, please consider supporting the project.</p><p>You can show your support via:</p><p>üôè Thank you for your support!</p><p>Sponsor the most comprehensive repository of AI system prompts and reach thousands of developers.</p><h2>üõ°Ô∏è Security Notice for AI Startups</h2><blockquote><p>‚ö†Ô∏è  If you're an AI startup, make sure your data is secure. Exposed prompts or AI models can easily become a target for hackers.</p></blockquote><blockquote><p>üîê  Interested in securing your AI systems? Check out , a service designed to help startups  leaks in system instructions, internal tools, and model configurations. <strong>Get a free AI security audit</strong> to ensure your AI is protected from vulnerabilities.</p></blockquote><a href=\"https://www.star-history.com/#x1xhlol/system-prompts-and-models-of-ai-tools&amp;Date\"></a><p>‚≠ê <strong>Drop a star if you find this useful!</strong></p>",
      "contentLength": 1229,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/19ec439540a875a44f7c4e121887b417d68d3285eabda315bb5b169cdd9e8327/x1xhlol/system-prompts-and-models-of-ai-tools",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "OpenBB-finance/OpenBB",
      "url": "https://github.com/OpenBB-finance/OpenBB",
      "date": 1771816069,
      "author": "",
      "guid": 47436,
      "unread": true,
      "content": "<p>Financial data platform for analysts, quants and AI agents.</p><img src=\"https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-light.svg?raw=true#gh-light-mode-only\" alt=\"Open Data Platform by OpenBB logo\" width=\"600\"><img src=\"https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only\" alt=\"Open Data Platform by OpenBB logo\" width=\"600\"><p>Open Data Platform by OpenBB (ODP) is the open-source toolset that helps data engineers integrate proprietary, licensed, and public data sources into downstream applications like AI copilots and research dashboards.</p><p>ODP operates as the \"connect once, consume everywhere\" infrastructure layer that consolidates and exposes data to multiple surfaces at once: Python environments for quants, OpenBB Workspace and Excel for analysts, MCP servers for AI agents, and REST APIs for other applications.</p><a href=\"https://pro.openbb.co\"></a><p>Get started with: </p><pre><code>from openbb import obb\noutput = obb.equity.price.historical(\"AAPL\")\ndf = output.to_dataframe()\n</code></pre><p>While the Open Data Platform provides the open-source data integration foundation,  offers the enterprise UI for analysts to visualize datasets and leverage AI agents. The platform's \"connect once, consume everywhere\" architecture enables seamless integration between the two.</p><a href=\"https://pro.openbb.co\"></a><h3>Integrating Open Data Platform to the OpenBB Workspace</h3><p>Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.</p><pre><code>pip install \"openbb[all]\"\n</code></pre><ul><li>Start the API server over localhost.</li></ul><p>This will launch a FastAPI server, via Uvicorn, at .</p><h4>Integrate the ODP Backend to OpenBB Workspace</h4><ol><li>Click on \"Connect backend\"</li><li>Click on \"Test\". You should get a \"Test successful\" with the number of apps found.</li></ol><p>The ODP Python Package can be installed from <a href=\"https://pypi.org/project/openbb/\">PyPI package</a> by running </p><p>or by cloning the repository directly with <code>git clone https://github.com/OpenBB-finance/OpenBB.git</code>.</p><p>The ODP CLI is a command-line interface that allows you to access the ODP directly from your command line.</p><p>It can be installed by running </p><p>or by cloning the repository directly with <code>git clone https://github.com/OpenBB-finance/OpenBB.git</code>.</p><p>There are three main ways of contributing to this project. (Hopefully you have starred the project by now ‚≠êÔ∏è)</p><p>Distributed under the AGPLv3 License. See <a href=\"https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE\">LICENSE</a> for more information.</p><p>Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.</p><p>Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.</p><p>The data contained in the Open Data Platform is not necessarily accurate.</p><p>OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.</p><p>All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.</p><p>Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.</p><p>If you have any questions about the platform or anything OpenBB, feel free to email us at </p><p>If you want to say hi, or are interested in partnering with us, feel free to reach us at </p><p>This is a proxy of our growth and that we are just getting started.</p><p>OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.</p><a href=\"https://github.com/OpenBB-finance/OpenBB/graphs/contributors\"><img src=\"https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB\" width=\"800\"></a>",
      "contentLength": 3503,
      "flags": null,
      "enclosureUrl": "https://repository-images.githubusercontent.com/323048702/4659bbdb-ae11-4f51-8a16-860fa9dfc551",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "hiddify/hiddify-app",
      "url": "https://github.com/hiddify/hiddify-app",
      "date": 1771729583,
      "author": "",
      "guid": 47261,
      "unread": true,
      "content": "<p>Multi-platform auto-proxy client, supporting Sing-box, X-ray, TUIC, Hysteria, Reality, Trojan, SSH etc. It‚Äôs an open-source, secure and ad-free.</p><p dir=\"ltr\">A multi-platform proxy client based on <a href=\"https://github.com/SagerNet/sing-box\">Sing-box</a> universal proxy tool-chain. Hiddify offers a wide range of capabilities, like automatic node selection, TUN mode, remote profiles etc. Hiddify is ad-free and open-source. With support for a wide range of protocols, it provides a secure and private way for accessing free internet.</p><p>‚úàÔ∏è Multi-platform: Android, iOS, Windows, macOS and Linux</p><p>‚≠ê Intuitive and accessible UI</p><p>üîç Delay based node selection</p><p>üü° Wide range of protocols: Vless, Vmess, Reality, TUIC, Hysteria, Wireguard, SSH etc.</p><p>üü° Subscription link and configuration formats: Sing-box, V2ray, Clash, Clash meta</p><p>üîÑ Automatic subscription update</p><p>üîé Display profile information including remaining days and traffic usage</p><p>üõ° Open source, secure and community driven</p><p>‚öô Compatible with all proxy management panels</p><p>‚≠ê Appropriate configuration for Iran, China, Russia and other countries</p><p>üì± Available on official stores</p><h2>‚öôÔ∏è Installation and tutorials</h2><p><strong>Find tutorial information on our wiki page by clicking on image below.</strong></p><p>Improve existing languages or add new ones by manually editing the JSON files located in  or by using the <a href=\"https://fink.inlang.com/github.com/hiddify/hiddify-next\">Inlang online editor</a>.</p><p>We would like to express our sincere appreciation to the contributors of the following projects, whose robust foundation and innovative features have significantly enhanced the success and functionality of this project.</p><p>The easiest way to support us is to click on the star (‚≠ê) at the top of this page.</p><a href=\"https://next.ossinsight.io/widgets/official/analyze-repo-stars-history?repo_id=643504282\" target=\"_blank\" align=\"center\"></a><p>We also need financial support for our services. All of our activities are done voluntarily and financial support will be spent on the development of the project. You can view our support addresses <a href=\"https://github.com/hiddify/hiddify-server/wiki/support\">here</a>.</p><h2>üë©‚Äçüè´ Collaboration and Contact Information</h2><p>Hiddify is a community driven project. If you're interested in contributing, please read the <a href=\"https://raw.githubusercontent.com/hiddify/hiddify-app/main/CONTRIBUTING.md\">contribution guidelines</a>. We would specially appreciate any help we can get in these areas: <strong>Flutter, Go, iOS development (Swift), Android development (Kotlin).</strong></p><p align=\"center\"> We appreciate all people who are participating in this project. Some people here and many many more outside of Github. It means a lot to us. ‚ô• </p>",
      "contentLength": 2258,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Stremio/stremio-web",
      "url": "https://github.com/Stremio/stremio-web",
      "date": 1771729583,
      "author": "",
      "guid": 47262,
      "unread": true,
      "content": "<p>Stremio - Freedom to Stream</p><p>Stremio is a modern media center that's a one-stop solution for your video entertainment. You discover, watch and organize video content from easy to install addons.</p><ul></ul><pre><code>docker build -t stremio-web .\ndocker run -p 8080:8080 stremio-web\n</code></pre><p>Stremio is copyright 2017-2023 Smart code and available under GPLv2 license. See the <a href=\"https://raw.githubusercontent.com/Stremio/stremio-web/development/LICENSE.md\">LICENSE</a> file in the project for more information.</p>",
      "contentLength": 392,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ggml-org/ggml",
      "url": "https://github.com/ggml-org/ggml",
      "date": 1771729583,
      "author": "",
      "guid": 47263,
      "unread": true,
      "content": "<p>Tensor library for machine learning</p><p>Tensor library for machine learning</p><p><em><strong>Note that this project is under active development.  Some of the development is currently happening in the <a href=\"https://github.com/ggerganov/llama.cpp\">llama.cpp</a> and <a href=\"https://github.com/ggerganov/whisper.cpp\">whisper.cpp</a> repos</strong></em></p><ul><li>Low-level cross-platform implementation</li><li>Integer quantization support</li><li>Automatic differentiation</li><li>ADAM and L-BFGS optimizers</li><li>No third-party dependencies</li><li>Zero memory allocations during runtime</li></ul><pre><code>git clone https://github.com/ggml-org/ggml\ncd ggml\n\n# install python dependencies in a virtual environment\npython3.10 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n\n# build the examples\nmkdir build &amp;&amp; cd build\ncmake ..\ncmake --build . --config Release -j 8\n</code></pre><pre><code># run the GPT-2 small 117M model\n../examples/gpt-2/download-ggml-model.sh 117M\n./bin/gpt-2-backend -m models/gpt-2-117M/ggml-model.bin -p \"This is an example\"\n</code></pre><p>For more information, checkout the corresponding programs in the <a href=\"https://raw.githubusercontent.com/ggml-org/ggml/master/examples\">examples</a> folder.</p><pre><code># fix the path to point to your CUDA compiler\ncmake -DGGML_CUDA=ON -DCMAKE_CUDA_COMPILER=/usr/local/cuda-12.1/bin/nvcc ..\n</code></pre><pre><code>cmake -DCMAKE_C_COMPILER=\"$(hipconfig -l)/clang\" -DCMAKE_CXX_COMPILER=\"$(hipconfig -l)/clang++\" -DGGML_HIP=ON\n</code></pre><pre><code># linux\nsource /opt/intel/oneapi/setvars.sh\ncmake -G \"Ninja\" -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL=ON ..\n\n# windows\n\"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat\"\ncmake -G \"Ninja\" -DCMAKE_C_COMPILER=cl -DCMAKE_CXX_COMPILER=icx -DGGML_SYCL=ON ..\n</code></pre><p>Download and unzip the NDK from this download <a href=\"https://developer.android.com/ndk/downloads\">page</a>. Set the NDK_ROOT_PATH environment variable or provide the absolute path to the CMAKE_ANDROID_NDK in the command below.</p><pre><code>cmake .. \\\n   -DCMAKE_SYSTEM_NAME=Android \\\n   -DCMAKE_SYSTEM_VERSION=33 \\\n   -DCMAKE_ANDROID_ARCH_ABI=arm64-v8a \\\n   -DCMAKE_ANDROID_NDK=$NDK_ROOT_PATH\n   -DCMAKE_ANDROID_STL_TYPE=c++_shared\n</code></pre><pre><code># create directories\nadb shell 'mkdir /data/local/tmp/bin'\nadb shell 'mkdir /data/local/tmp/models'\n\n# push the compiled binaries to the folder\nadb push bin/* /data/local/tmp/bin/\n\n# push the ggml library\nadb push src/libggml.so /data/local/tmp/\n\n# push model files\nadb push models/gpt-2-117M/ggml-model.bin /data/local/tmp/models/\n\nadb shell\ncd /data/local/tmp\nexport LD_LIBRARY_PATH=/data/local/tmp\n./bin/gpt-2-backend -m models/ggml-model.bin -p \"this is an example\"\n</code></pre>",
      "contentLength": 2249,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "abhigyanpatwari/GitNexus",
      "url": "https://github.com/abhigyanpatwari/GitNexus",
      "date": 1771729583,
      "author": "",
      "guid": 47264,
      "unread": true,
      "content": "<p>GitNexus: The Zero-Server Code Intelligence Engine - GitNexus is a client-side knowledge graph creator that runs entirely in your browser. Drop in a GitHub repo or ZIP file, and get an interactive knowledge graph wit a built in Graph RAG Agent. Perfect for code exploration</p><p><strong>Building git for agent context.</strong></p><p>Indexes any codebase into a knowledge graph ‚Äî every dependency, call chain, cluster, and execution flow ‚Äî then exposes it through smart tools so AI agents never miss code.</p><blockquote><p><em>Like DeepWiki, but deeper.</em> DeepWiki helps you  code. GitNexus lets you  it ‚Äî because a knowledge graph tracks every relationship, not just descriptions.</p></blockquote><p> The  is a quick way to chat with any repo. The  is how you make your AI agent actually reliable ‚Äî it gives Cursor, Claude Code, and friends a deep architectural view of your codebase so they stop missing dependencies, breaking call chains, and shipping blind edits. Even smaller models get full architectural clarity, making it compete with goliath models.</p><table><tbody><tr><td>Index repos locally, connect AI agents via MCP</td><td>Visual graph explorer + AI chat in browser</td></tr><tr><td>Daily development with Cursor, Claude Code, Windsurf, OpenCode</td><td>Quick exploration, demos, one-off analysis</td></tr><tr><td>Limited by browser memory (~5k files), or unlimited via backend mode</td></tr><tr><td>KuzuDB native (fast, persistent)</td><td>KuzuDB WASM (in-memory, per session)</td></tr><tr><td>Tree-sitter native bindings</td></tr><tr><td>Everything local, no network</td><td>Everything in-browser, no server</td></tr></tbody></table><blockquote><p> connects the two ‚Äî the web UI auto-detects the local server and can browse all your CLI-indexed repos without re-uploading or re-indexing.</p></blockquote><p>The CLI indexes your repository and runs an MCP server that gives AI agents deep codebase awareness.</p><pre><code># Index your repo (run from repo root)\nnpx gitnexus analyze\n</code></pre><p>That's it. This indexes the codebase, installs agent skills, registers Claude Code hooks, and creates  /  context files ‚Äî all in one command.</p><p>To configure MCP for your editor, run  once ‚Äî or set it up manually below.</p><p> auto-detects your editors and writes the correct global MCP config. You only need to run it once.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><blockquote><p> gets the deepest integration: MCP tools + agent skills + PreToolUse hooks that automatically enrich grep/glob/bash calls with knowledge graph context.</p></blockquote><p>If you prefer manual configuration:</p><p> (full support ‚Äî MCP + skills + hooks):</p><pre><code>claude mcp add gitnexus -- npx -y gitnexus@latest mcp\n</code></pre><p> ( ‚Äî global, works for all projects):</p><pre><code>{\n  \"mcpServers\": {\n    \"gitnexus\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"gitnexus@latest\", \"mcp\"]\n    }\n  }\n}\n</code></pre><p> (<code>~/.config/opencode/config.json</code>):</p><pre><code>{\n  \"mcp\": {\n    \"gitnexus\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"gitnexus@latest\", \"mcp\"]\n    }\n  }\n}\n</code></pre><pre><code>gitnexus setup                    # Configure MCP for your editors (one-time)\ngitnexus analyze [path]           # Index a repository (or update stale index)\ngitnexus analyze --force          # Force full re-index\ngitnexus analyze --skip-embeddings  # Skip embedding generation (faster)\ngitnexus mcp                     # Start MCP server (stdio) ‚Äî serves all indexed repos\ngitnexus serve                   # Start local HTTP server (multi-repo) for web UI connection\ngitnexus list                    # List all indexed repositories\ngitnexus status                  # Show index status for current repo\ngitnexus clean                   # Delete index for current repo\ngitnexus clean --all --force     # Delete all indexes\ngitnexus wiki [path]             # Generate repository wiki from knowledge graph\ngitnexus wiki --model &lt;model&gt;    # Wiki with custom LLM model (default: gpt-4o-mini)\ngitnexus wiki --base-url &lt;url&gt;   # Wiki with custom LLM API base URL\n</code></pre><table><thead><tr></tr></thead><tbody><tr><td>Discover all indexed repositories</td></tr><tr><td>Process-grouped hybrid search (BM25 + semantic + RRF)</td></tr><tr><td>360-degree symbol view ‚Äî categorized refs, process participation</td></tr><tr><td>Blast radius analysis with depth grouping and confidence</td></tr><tr><td>Git-diff impact ‚Äî maps changed lines to affected processes</td></tr><tr><td>Multi-file coordinated rename with graph + text search</td></tr><tr></tr></tbody></table><blockquote><p>When only one repo is indexed, the  parameter is optional. With multiple repos, specify which one: <code>query({query: \"auth\", repo: \"my-app\"})</code>.</p></blockquote><p> for instant context:</p><table><tbody><tr><td>List all indexed repositories (read this first)</td></tr><tr><td><code>gitnexus://repo/{name}/context</code></td><td>Codebase stats, staleness check, and available tools</td></tr><tr><td><code>gitnexus://repo/{name}/clusters</code></td><td>All functional clusters with cohesion scores</td></tr><tr><td><code>gitnexus://repo/{name}/cluster/{name}</code></td><td>Cluster members and details</td></tr><tr><td><code>gitnexus://repo/{name}/processes</code></td></tr><tr><td><code>gitnexus://repo/{name}/process/{name}</code></td><td>Full process trace with steps</td></tr><tr><td><code>gitnexus://repo/{name}/schema</code></td><td>Graph schema for Cypher queries</td></tr></tbody></table><p> for guided workflows:</p><table><tbody><tr><td>Pre-commit change analysis ‚Äî scope, affected processes, risk level</td></tr><tr><td>Architecture documentation from the knowledge graph with mermaid diagrams</td></tr></tbody></table><p> installed to  automatically:</p><ul><li> ‚Äî Navigate unfamiliar code using the knowledge graph</li><li> ‚Äî Trace bugs through call chains</li><li> ‚Äî Analyze blast radius before changes</li><li> ‚Äî Plan safe refactors using dependency mapping</li></ul><h2>Multi-Repo MCP Architecture</h2><p>GitNexus uses a  so one MCP server can serve multiple indexed repos. No per-project MCP config needed ‚Äî set it up once and it works everywhere.</p><pre><code>flowchart TD\n    subgraph CLI [CLI Commands]\n        Setup[\"gitnexus setup\"]\n        Analyze[\"gitnexus analyze\"]\n        Clean[\"gitnexus clean\"]\n        List[\"gitnexus list\"]\n    end\n\n    subgraph Registry [\"~/.gitnexus/\"]\n        RegFile[\"registry.json\"]\n    end\n\n    subgraph Repos [Project Repos]\n        RepoA[\".gitnexus/ in repo A\"]\n        RepoB[\".gitnexus/ in repo B\"]\n    end\n\n    subgraph MCP [MCP Server]\n        Server[\"server.ts\"]\n        Backend[\"LocalBackend\"]\n        Pool[\"Connection Pool\"]\n        ConnA[\"KuzuDB conn A\"]\n        ConnB[\"KuzuDB conn B\"]\n    end\n\n    Setup --&gt;|\"writes global MCP config\"| CursorConfig[\"~/.cursor/mcp.json\"]\n    Analyze --&gt;|\"registers repo\"| RegFile\n    Analyze --&gt;|\"stores index\"| RepoA\n    Clean --&gt;|\"unregisters repo\"| RegFile\n    List --&gt;|\"reads\"| RegFile\n    Server --&gt;|\"reads registry\"| RegFile\n    Server --&gt; Backend\n    Backend --&gt; Pool\n    Pool --&gt;|\"lazy open\"| ConnA\n    Pool --&gt;|\"lazy open\"| ConnB\n    ConnA --&gt;|\"queries\"| RepoA\n    ConnB --&gt;|\"queries\"| RepoB\n</code></pre><p> Each  stores the index in  inside the repo (portable, gitignored) and registers a pointer in <code>~/.gitnexus/registry.json</code>. When an AI agent starts, the MCP server reads the registry and can serve any indexed repo. KuzuDB connections are opened lazily on first query and evicted after 5 minutes of inactivity (max 5 concurrent). If only one repo is indexed, the  parameter is optional on all tools ‚Äî agents don't need to change anything.</p><p>A fully client-side graph explorer and AI chat. No server, no install ‚Äî your code never leaves the browser.</p><img width=\"2550\" height=\"1343\" alt=\"gitnexus_img\" src=\"https://github.com/user-attachments/assets/cc5d637d-e0e5-48e6-93ff-5bcfdb929285\"><pre><code>git clone https://github.com/abhigyanpatwari/gitnexus.git\ncd gitnexus/gitnexus-web\nnpm install\nnpm run dev\n</code></pre><p>The web UI uses the same indexing pipeline as the CLI but runs entirely in WebAssembly (Tree-sitter WASM, KuzuDB WASM, in-browser embeddings). It's great for quick exploration but limited by browser memory for larger repos.</p><p> Run  and open the web UI locally ‚Äî it auto-detects the server and shows all your indexed repos, with full AI chat support. No need to re-upload or re-index. The agent's tools (Cypher queries, search, code navigation) route through the backend HTTP API automatically.</p><h2>The Problem GitNexus Solves</h2><p>Tools like , , , , and  are powerful ‚Äî but they don't truly know your codebase structure.</p><ol><li>AI edits </li><li>Doesn't know 47 functions depend on its return type</li></ol><h3>Traditional Graph RAG vs GitNexus</h3><p>Traditional approaches give the LLM raw graph edges and hope it explores enough. GitNexus <strong>precomputes structure at index time</strong> ‚Äî clustering, tracing, scoring ‚Äî so tools return complete context in one call:</p><pre><code>flowchart TB\n    subgraph Traditional[\"Traditional Graph RAG\"]\n        direction TB\n        U1[\"User: What depends on UserService?\"]\n        U1 --&gt; LLM1[\"LLM receives raw graph\"]\n        LLM1 --&gt; Q1[\"Query 1: Find callers\"]\n        Q1 --&gt; Q2[\"Query 2: What files?\"]\n        Q2 --&gt; Q3[\"Query 3: Filter tests?\"]\n        Q3 --&gt; Q4[\"Query 4: High-risk?\"]\n        Q4 --&gt; OUT1[\"Answer after 4+ queries\"]\n    end\n\n    subgraph GN[\"GitNexus Smart Tools\"]\n        direction TB\n        U2[\"User: What depends on UserService?\"]\n        U2 --&gt; TOOL[\"impact UserService upstream\"]\n        TOOL --&gt; PRECOMP[\"Pre-structured response:\n        8 callers, 3 clusters, all 90%+ confidence\"]\n        PRECOMP --&gt; OUT2[\"Complete answer, 1 query\"]\n    end\n</code></pre><p><strong>Core innovation: Precomputed Relational Intelligence</strong></p><ul><li> ‚Äî LLM can't miss context, it's already in the tool response</li><li> ‚Äî No 10-query chains to understand one function</li><li> ‚Äî Smaller LLMs work because tools do the heavy lifting</li></ul><p>GitNexus builds a complete knowledge graph of your codebase through a multi-phase indexing pipeline:</p><ol><li> ‚Äî Walks the file tree and maps folder/file relationships</li><li> ‚Äî Extracts functions, classes, methods, and interfaces using Tree-sitter ASTs</li><li> ‚Äî Resolves imports and function calls across files with language-aware logic</li><li> ‚Äî Groups related symbols into functional communities</li><li> ‚Äî Traces execution flows from entry points through call chains</li><li> ‚Äî Builds hybrid search indexes for fast retrieval</li></ol><p>TypeScript, JavaScript, Python, Java, C, C++, C#, Go, Rust</p><pre><code>impact({target: \"UserService\", direction: \"upstream\", minConfidence: 0.8})\n\nTARGET: Class UserService (src/services/user.ts)\n\nUPSTREAM (what depends on this):\n  Depth 1 (WILL BREAK):\n    handleLogin [CALLS 90%] -&gt; src/api/auth.ts:45\n    handleRegister [CALLS 90%] -&gt; src/api/auth.ts:78\n    UserController [CALLS 85%] -&gt; src/controllers/user.ts:12\n  Depth 2 (LIKELY AFFECTED):\n    authRouter [IMPORTS] -&gt; src/routes/auth.ts\n</code></pre><p>Options: , ,  (, , , ), </p><pre><code>query({query: \"authentication middleware\"})\n\nprocesses:\n  - summary: \"LoginFlow\"\n    priority: 0.042\n    symbol_count: 4\n    process_type: cross_community\n    step_count: 7\n\nprocess_symbols:\n  - name: validateUser\n    type: Function\n    filePath: src/auth/validate.ts\n    process_id: proc_login\n    step_index: 2\n\ndefinitions:\n  - name: AuthConfig\n    type: Interface\n    filePath: src/types/auth.ts\n</code></pre><h3>Context (360-degree Symbol View)</h3><pre><code>context({name: \"validateUser\"})\n\nsymbol:\n  uid: \"Function:validateUser\"\n  kind: Function\n  filePath: src/auth/validate.ts\n  startLine: 15\n\nincoming:\n  calls: [handleLogin, handleRegister, UserController]\n  imports: [authRouter]\n\noutgoing:\n  calls: [checkPassword, createSession]\n\nprocesses:\n  - name: LoginFlow (step 2/7)\n  - name: RegistrationFlow (step 3/5)\n</code></pre><h3>Detect Changes (Pre-Commit)</h3><pre><code>detect_changes({scope: \"all\"})\n\nsummary:\n  changed_count: 12\n  affected_count: 3\n  changed_files: 4\n  risk_level: medium\n\nchanged_symbols: [validateUser, AuthService, ...]\naffected_processes: [LoginFlow, RegistrationFlow, ...]\n</code></pre><pre><code>rename({symbol_name: \"validateUser\", new_name: \"verifyUser\", dry_run: true})\n\nstatus: success\nfiles_affected: 5\ntotal_edits: 8\ngraph_edits: 6     (high confidence)\ntext_search_edits: 2  (review carefully)\nchanges: [...]\n</code></pre><pre><code>-- Find what calls auth functions with high confidence\nMATCH (c:Community {heuristicLabel: 'Authentication'})&lt;-[:CodeRelation {type: 'MEMBER_OF'}]-(fn)\nMATCH (caller)-[r:CodeRelation {type: 'CALLS'}]-&gt;(fn)\nWHERE r.confidence &gt; 0.8\nRETURN caller.name, fn.name, r.confidence\nORDER BY r.confidence DESC\n</code></pre><p>Generate LLM-powered documentation from your knowledge graph:</p><pre><code># Requires an LLM API key (OPENAI_API_KEY, etc.)\ngitnexus wiki\n\n# Use a custom model or provider\ngitnexus wiki --model gpt-4o\ngitnexus wiki --base-url https://api.anthropic.com/v1\n\n# Force full regeneration\ngitnexus wiki --force\n</code></pre><p>The wiki generator reads the indexed graph structure, groups files into modules via LLM, generates per-module documentation pages, and creates an overview page ‚Äî all with cross-references to the knowledge graph.</p><table><tbody><tr></tr><tr><td>Tree-sitter native bindings</td></tr><tr></tr><tr><td>HuggingFace transformers.js (GPU/CPU)</td><td>transformers.js (WebGPU/WASM)</td></tr><tr></tr><tr></tr><tr><td>Sigma.js + Graphology (WebGL)</td></tr><tr><td>React 18, TypeScript, Vite, Tailwind v4</td></tr><tr></tr><tr></tr></tbody></table><ul><li>: Everything runs locally on your machine. No network calls. Index stored in  (gitignored). Global registry at  stores only paths and metadata.</li><li>: Everything runs in your browser. No code uploaded to any server. API keys stored in localStorage only.</li><li>Open source ‚Äî audit the code yourself.</li></ul>",
      "contentLength": 12069,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/fe951dd93c194d4d74c9c802431c78d8becf7e37d5aa624a3a97319e6cc64907/abhigyanpatwari/GitNexus",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "cloudflare/agents",
      "url": "https://github.com/cloudflare/agents",
      "date": 1771729583,
      "author": "",
      "guid": 47265,
      "unread": true,
      "content": "<p>Build and deploy AI Agents on Cloudflare</p><p>Agents are persistent, stateful execution environments for agentic workloads, powered by Cloudflare <a href=\"https://developers.cloudflare.com/durable-objects/\">Durable Objects</a>. Each agent has its own state, storage, and lifecycle ‚Äî with built-in support for real-time communication, scheduling, AI model calls, MCP, workflows, and more.</p><p>Agents hibernate when idle and wake on demand. You can run millions of them ‚Äî one per user, per session, per game room ‚Äî each costs nothing when inactive.</p><pre><code>npm create cloudflare@latest -- --template cloudflare/agents-starter\n</code></pre><p>Or add to an existing project:</p><p>A counter agent with persistent state, callable methods, and real-time sync to a React frontend:</p><pre><code>// server.ts\nimport { Agent, routeAgentRequest, callable } from \"agents\";\n\nexport type CounterState = { count: number };\n\nexport class CounterAgent extends Agent&lt;Env, CounterState&gt; {\n  initialState = { count: 0 };\n\n  @callable()\n  increment() {\n    this.setState({ count: this.state.count + 1 });\n    return this.state.count;\n  }\n\n  @callable()\n  decrement() {\n    this.setState({ count: this.state.count - 1 });\n    return this.state.count;\n  }\n}\n\nexport default {\n  async fetch(request: Request, env: Env, ctx: ExecutionContext) {\n    return (\n      (await routeAgentRequest(request, env)) ??\n      new Response(\"Not found\", { status: 404 })\n    );\n  }\n};\n</code></pre><pre><code>// client.tsx\nimport { useAgent } from \"agents/react\";\nimport { useState } from \"react\";\nimport type { CounterAgent, CounterState } from \"./server\";\n\nfunction Counter() {\n  const [count, setCount] = useState(0);\n\n  const agent = useAgent&lt;CounterAgent, CounterState&gt;({\n    agent: \"CounterAgent\",\n    onStateUpdate: (state) =&gt; setCount(state.count)\n  });\n\n  return (\n    &lt;div&gt;\n      &lt;span&gt;{count}&lt;/span&gt;\n      &lt;button onClick={() =&gt; agent.stub.increment()}&gt;+&lt;/button&gt;\n      &lt;button onClick={() =&gt; agent.stub.decrement()}&gt;-&lt;/button&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre><p>State changes sync to all connected clients automatically. Call methods like they're local functions.</p><table><tbody><tr><td>Syncs to all connected clients, survives restarts</td></tr><tr><td>Type-safe RPC via the  decorator</td></tr><tr><td>One-time, recurring, and cron-based tasks</td></tr><tr><td>Real-time bidirectional communication with lifecycle hooks</td></tr><tr><td>Message persistence, resumable streaming, server/client tool execution</td></tr><tr><td>Act as MCP servers or connect as MCP clients</td></tr><tr><td>Durable multi-step tasks with human-in-the-loop approval</td></tr><tr><td>Receive and respond via Cloudflare Email Routing</td></tr><tr><td>LLMs generate executable TypeScript instead of individual tool calls</td></tr><tr><td>Direct SQLite queries via Durable Objects</td></tr><tr><td> and  for frontend integration</td></tr><tr><td> for non-React environments</td></tr></tbody></table><p> Realtime voice agents, web browsing (headless browser), sandboxed code execution, and multi-channel communication (SMS, messengers).</p><table><tbody><tr><td>Core SDK ‚Äî Agent class, routing, state, scheduling, MCP, email, workflows</td></tr><tr><td>Higher-level AI chat ‚Äî persistent messages, resumable streaming, tool execution</td></tr><tr><td>Hono middleware for adding agents to Hono apps</td></tr></tbody></table><p>The <a href=\"https://raw.githubusercontent.com/cloudflare/agents/main/examples\"></a> directory has self-contained demos covering most SDK features ‚Äî MCP servers/clients, workflows, email agents, webhooks, tic-tac-toe, resumable streaming, and more. The <a href=\"https://raw.githubusercontent.com/cloudflare/agents/main/examples/playground\"></a> is the kitchen-sink showcase with everything in one UI.</p><pre><code>cd examples/playground\nnpm run dev\n</code></pre><p>Node 24+ required. Uses npm workspaces.</p><pre><code>npm install          # install all workspaces\nnpm run build        # build all packages\nnpm run check        # full CI check (format, lint, typecheck, exports)\nCI=true npm test     # run tests (vitest + vitest-pool-workers)\n</code></pre><p>Changes to  need a changeset:</p><p>We are not accepting external pull requests at this time ‚Äî the SDK is evolving quickly and we want to keep the surface area manageable. That said, we'd love to hear from you:</p>",
      "contentLength": 3611,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "anthropics/claude-code",
      "url": "https://github.com/anthropics/claude-code",
      "date": 1771729583,
      "author": "",
      "guid": 47266,
      "unread": true,
      "content": "<p>Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.</p><p>Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows -- all through natural language commands. Use it in your terminal, IDE, or tag @claude on Github.</p><img src=\"https://raw.githubusercontent.com/anthropics/claude-code/main/demo.gif\"><blockquote><p>[!NOTE] Installation via npm is deprecated. Use one of the recommended methods below.</p></blockquote><p>For more installation options, uninstall steps, and troubleshooting, see the <a href=\"https://code.claude.com/docs/en/setup\">setup documentation</a>.</p><ol><li><p><strong>MacOS/Linux (Recommended):</strong></p><pre><code>curl -fsSL https://claude.ai/install.sh | bash\n</code></pre><pre><code>brew install --cask claude-code\n</code></pre><pre><code>irm https://claude.ai/install.ps1 | iex\n</code></pre><pre><code>winget install Anthropic.ClaudeCode\n</code></pre><pre><code>npm install -g @anthropic-ai/claude-code\n</code></pre></li><li><p>Navigate to your project directory and run .</p></li></ol><p>This repository includes several Claude Code plugins that extend functionality with custom commands and agents. See the <a href=\"https://raw.githubusercontent.com/anthropics/claude-code/main/plugins/README.md\">plugins directory</a> for detailed documentation on available plugins.</p><p>We welcome your feedback. Use the  command to report issues directly within Claude Code, or file a <a href=\"https://github.com/anthropics/claude-code/issues\">GitHub issue</a>.</p><p>Join the <a href=\"https://anthropic.com/discord\">Claude Developers Discord</a> to connect with other developers using Claude Code. Get help, share feedback, and discuss your projects with the community.</p><h2>Data collection, usage, and retention</h2><p>When you use Claude Code, we collect feedback, which includes usage data (such as code acceptance or rejections), associated conversation data, and user feedback submitted via the  command.</p><p>We have implemented several safeguards to protect your data, including limited retention periods for sensitive information, restricted access to user session data, and clear policies against using feedback for model training.</p>",
      "contentLength": 1892,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/962d213dbd2d500b5153244b9e3b2dd832fd92600c40ea1c87bdfbcafef761ef/anthropics/claude-code",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "stan-smith/FossFLOW",
      "url": "https://github.com/stan-smith/FossFLOW",
      "date": 1771729583,
      "author": "",
      "guid": 47267,
      "unread": true,
      "content": "<p>Make beautiful isometric infrastructure diagrams</p><p> Stan here, if you've used FossFLOW and it's helped you, <b>I'd really appreciate if you could donate something small :)</b> I work full time, and finding the time to work on this project is challenging enough. If you've had a feature that I've implemented for you, or fixed a bug it'd be great if you could :) if not, that's not a problem, this software will always remain free!</p><p> If you haven't yet, please check out the underlying library this is built on by <a href=\"https://github.com/markmanx/isoflow\">@markmanx</a> I truly stand on the shoulders of a giant here ü´°</p><p align=\"center\"> Go to <b> --&gt; https://stan-smith.github.io/FossFLOW/ &lt;-- </b></p><p>FossFLOW is a powerful, open-source Progressive Web App (PWA) for creating beautiful isometric diagrams. Built with React and the <a href=\"https://github.com/markmanx/isoflow\">Isoflow</a> (Now forked and published to NPM as fossflow) library, it runs entirely in your browser with offline support.</p><h2>üê≥ Quick Deploy with Docker</h2><pre><code># Using Docker Compose (recommended - includes persistent storage)\ndocker compose up\n\n# Or run directly from Docker Hub with persistent storage\ndocker run -p 80:80 -v $(pwd)/diagrams:/data/diagrams stnsmith/fossflow:latest\n</code></pre><p>Server storage is enabled by default in Docker. Your diagrams will be saved to  on the host.</p><p>To disable server storage, set <code>ENABLE_SERVER_STORAGE=false</code>:</p><pre><code>docker run -p 80:80 -e ENABLE_SERVER_STORAGE=false stnsmith/fossflow:latest\n</code></pre><h2>Quick Start (Local Development)</h2><pre><code># Clone the repository\ngit clone https://github.com/stan-smith/FossFLOW\ncd FossFLOW\n\n# Install dependencies\nnpm install\n\n# Build the library (required first time)\nnpm run build:lib\n\n# Start development server\nnpm run dev\n</code></pre><p>This is a monorepo containing two packages:</p><ul><li> - React component library for drawing network diagrams (built with Webpack)</li><li> - Progressive Web App which wraps the lib and presents it (built with RSBuild)</li></ul><pre><code># Development\nnpm run dev          # Start app development server\nnpm run dev:lib      # Watch mode for library development\n\n# Building\nnpm run build        # Build both library and app\nnpm run build:lib    # Build library only\nnpm run build:app    # Build app only\n\n# Testing &amp; Linting\nnpm test             # Run unit tests\nnpm run lint         # Check for linting errors\n\n# E2E Tests (Selenium)\ncd e2e-tests\n./run-tests.sh       # Run end-to-end tests (requires Docker &amp; Python)\n\n# Publishing\nnpm run publish:lib  # Publish library to npm\n</code></pre><ol><li><ul><li>Press the \"+\" button on the top right menu, the library of components will appear on the left</li><li>Drag and drop components from the library onto the canvas</li><li>Or right-click on the grid and select \"Add node\"</li></ul></li><li><ul><li>Select the Connector tool (press 'C' or click connector icon)</li><li> (default): Click first node, then click second node</li><li> (optional): Click and drag from first to second node</li><li>Switch modes in Settings ‚Üí Connectors tab</li></ul></li><li><ul><li> - Saves to browser session</li><li> - Download as JSON file</li><li> - Load from JSON file</li></ul></li></ol><ul><li>: Temporary saves cleared when browser closes</li><li>: Permanent storage as JSON files</li><li>: Automatically saves changes every 5 seconds to session</li></ul>",
      "contentLength": 2948,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "PowerShell/PowerShell",
      "url": "https://github.com/PowerShell/PowerShell",
      "date": 1771729583,
      "author": "",
      "guid": 47268,
      "unread": true,
      "content": "<p>PowerShell for every system!</p><p>Welcome to the PowerShell GitHub Community! <a href=\"https://learn.microsoft.com/powershell/scripting/overview\">PowerShell</a> is a cross-platform (Windows, Linux, and macOS) automation and configuration tool/framework that works well with your existing tools and is optimized for dealing with structured data (e.g. JSON, CSV, XML, etc.), REST APIs, and object models. It includes a command-line shell, an associated scripting language, and a framework for processing cmdlets.</p><h2>Windows PowerShell vs. PowerShell 7+</h2><p>Although this repository started as a fork of the Windows PowerShell codebase, changes made in this repository are not ported back to Windows PowerShell 5.1. This also means that <a href=\"https://github.com/PowerShell/PowerShell/issues\">issues tracked here</a> are only for PowerShell 7.x and higher. Windows PowerShell specific issues should be reported with the <a href=\"https://support.microsoft.com/windows/send-feedback-to-microsoft-with-the-feedback-hub-app-f59187f8-8739-22d6-ba93-f66612949332\">Feedback Hub app</a>, by choosing \"Apps &gt; PowerShell\" in the category.</p><p>If you are new to PowerShell and want to learn more, we recommend reviewing the <a href=\"https://learn.microsoft.com/powershell/scripting/learn/more-powershell-learning\">getting started</a> documentation.</p><p>PowerShell is supported on Windows, macOS, and a variety of Linux platforms. For more information, see <a href=\"https://learn.microsoft.com/powershell/scripting/install/installing-powershell\">Installing PowerShell</a>.</p><p>For best results when upgrading, you should use the same install method you used when you first installed PowerShell. The update method is different for each platform and install method.</p><p><a href=\"https://aka.ms/PSPublicDashboard\">Dashboard</a> with visualizations for community contributions and project status using PowerShell, Azure, and PowerBI.</p><p>For more information on how and why we built this dashboard, check out this <a href=\"https://devblogs.microsoft.com/powershell/powershell-open-source-community-dashboard/\">blog post</a>.</p><p><a href=\"https://docs.github.com/discussions/quickstart\">GitHub Discussions</a> is a feature to enable free and open discussions within the community for topics that are not related to code, unlike issues.</p><p>This is an experiment we are trying in our repositories, to see if it helps move discussions out of issues so that issues remain actionable by the team or members of the community. There should be no expectation that PowerShell team members are regular participants in these discussions. Individual PowerShell team members may choose to participate in discussions, but the expectation is that community members help drive discussions so that team members can focus on issues.</p><p>Want to chat with other members of the PowerShell community?</p><p>There are dozens of topic-specific channels on our community-driven PowerShell Virtual User Group, which you can join on:</p><h2>Developing and Contributing</h2><p>Want to contribute to PowerShell? Please start with the <a href=\"https://raw.githubusercontent.com/PowerShell/PowerShell/master/.github/CONTRIBUTING.md\">Contribution Guide</a> to learn how to develop and contribute.</p><p>If you are developing .NET Core C# applications targeting PowerShell Core, <a href=\"https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/FAQ.md#where-do-i-get-the-powershell-core-sdk-package\">check out our FAQ</a> to learn more about the PowerShell SDK NuGet package.</p><p>Also, make sure to check out our <a href=\"https://github.com/powershell/powershell-rfc\">PowerShell-RFC repository</a> for request-for-comments (RFC) documents to submit and give comments on proposed and future designs.</p><p>If you have any problems building PowerShell, please start by consulting the developer <a href=\"https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/FAQ.md\">FAQ</a>.</p><h2>Downloading the Source Code</h2><p>You can clone the repository:</p><pre><code>git clone https://github.com/PowerShell/PowerShell.git\n</code></pre><blockquote><p>[!Important] The PowerShell container images are now <a href=\"https://github.com/PowerShell/Announcements/issues/75\">maintained by the .NET team</a>. The containers at <code>mcr.microsoft.com/powershell</code> are currently not maintained.</p></blockquote><p>License: By requesting and using the Container OS Image for Windows containers, you acknowledge, understand, and consent to the Supplemental License Terms available on <a href=\"https://mcr.microsoft.com/en-us/product/powershell/tags\">Microsoft Artifact Registry</a>.</p><p>Please visit our <a href=\"https://learn.microsoft.com/powershell/module/microsoft.powershell.core/about/about_telemetry\">about_Telemetry</a> topic to read details about telemetry gathered by PowerShell.</p>",
      "contentLength": 3353,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "databricks-solutions/ai-dev-kit",
      "url": "https://github.com/databricks-solutions/ai-dev-kit",
      "date": 1771642723,
      "author": "",
      "guid": 47065,
      "unread": true,
      "content": "<p>Databricks Toolkit for Coding Agents provided by Field Engineering</p>",
      "contentLength": 66,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "eslint/eslint",
      "url": "https://github.com/eslint/eslint",
      "date": 1771642723,
      "author": "",
      "guid": 47066,
      "unread": true,
      "content": "<p>Find and fix problems in your JavaScript code.</p><p>ESLint is a tool for identifying and reporting on patterns found in ECMAScript/JavaScript code. In many ways, it is similar to JSLint and JSHint with a few exceptions:</p><ul><li>ESLint uses <a href=\"https://github.com/eslint/js/tree/main/packages/espree\">Espree</a> for JavaScript parsing.</li><li>ESLint uses an AST to evaluate patterns in code.</li><li>ESLint is completely pluggable, every single rule is a plugin and you can add more at runtime.</li></ul><p>Prerequisites: <a href=\"https://nodejs.org/\">Node.js</a> (, , or ) built with SSL support. (If you are using an official Node.js distribution, SSL is always built in.)</p><p>You can install and configure ESLint using this command:</p><pre><code>npm init @eslint/config@latest\n</code></pre><p>After that, you can run ESLint on any file or directory like this:</p><p>To use ESLint with pnpm, we recommend setting up a  file with at least the following settings:</p><pre><code>auto-install-peers=true\nnode-linker=hoisted\n</code></pre><p>This ensures that pnpm installs dependencies in a way that is more compatible with npm and is less likely to produce errors.</p><p>You can configure rules in your  files as in this example:</p><pre><code>import { defineConfig } from \"eslint/config\";\n\nexport default defineConfig([\n\t{\n\t\tfiles: [\"**/*.js\", \"**/*.cjs\", \"**/*.mjs\"],\n\t\trules: {\n\t\t\t\"prefer-const\": \"warn\",\n\t\t\t\"no-constant-binary-expression\": \"error\",\n\t\t},\n\t},\n]);\n</code></pre><p>The names  and <code>\"no-constant-binary-expression\"</code> are the names of <a href=\"https://eslint.org/docs/rules\">rules</a> in ESLint. The first value is the error level of the rule and can be one of these values:</p><ul><li> or  - turn the rule off</li><li> or  - turn the rule on as a warning (doesn't affect exit code)</li><li> or  - turn the rule on as an error (exit code will be 1)</li></ul><p>The three error levels allow you fine-grained control over how ESLint applies rules (for more configuration options and details, see the <a href=\"https://eslint.org/docs/latest/use/configure\">configuration docs</a>).</p><p>The ESLint team provides ongoing support for the current version and six months of limited support for the previous version. Limited support includes critical bug fixes, security issues, and compatibility issues only.</p><p>ESLint offers commercial support for both current and previous versions through our partners, <a href=\"https://tidelift.com/funding/github/npm/eslint\">Tidelift</a> and <a href=\"https://www.herodevs.com/support/eslint-nes?utm_source=ESLintWebsite&amp;utm_medium=ESLintWebsite&amp;utm_campaign=ESLintNES&amp;utm_id=ESLintNES\">HeroDevs</a>.</p><p>Before filing an issue, please be sure to read the guidelines for what you're reporting:</p><h2>Frequently Asked Questions</h2><p>Yes, ESLint natively supports parsing JSX syntax (this must be enabled in <a href=\"https://eslint.org/docs/latest/use/configure\">configuration</a>). Please note that supporting JSX syntax  the same as supporting React. React applies specific semantics to JSX syntax that ESLint doesn't recognize. We recommend using <a href=\"https://www.npmjs.com/package/eslint-plugin-react\">eslint-plugin-react</a> if you are using React and want React semantics.</p><h3>Does Prettier replace ESLint?</h3><p>No, ESLint and Prettier have different jobs: ESLint is a linter (looking for problematic patterns) and Prettier is a code formatter. Using both tools is common, refer to <a href=\"https://prettier.io/docs/en/install#eslint-and-other-linters\">Prettier's documentation</a> to learn how to configure them to work well with each other.</p><h3>What ECMAScript versions does ESLint support?</h3><p>ESLint has full support for ECMAScript 3, 5, and every year from 2015 up until the most recent stage 4 specification (the default). You can set your desired ECMAScript syntax and other settings (like global variables) through <a href=\"https://eslint.org/docs/latest/use/configure\">configuration</a>.</p><h3>What about experimental features?</h3><p>ESLint's parser only officially supports the latest final ECMAScript standard. We will make changes to core rules in order to avoid crashes on stage 3 ECMAScript syntax proposals (as long as they are implemented using the correct experimental ESTree syntax). We may make changes to core rules to better work with language extensions (such as JSX, Flow, and TypeScript) on a case-by-case basis.</p><p>In other cases (including if rules need to warn on more or fewer cases due to new syntax, rather than just not crashing), we recommend you use other parsers and/or rule plugins. If you are using Babel, you can use <a href=\"https://www.npmjs.com/package/@babel/eslint-parser\">@babel/eslint-parser</a> and <a href=\"https://www.npmjs.com/package/@babel/eslint-plugin\">@babel/eslint-plugin</a> to use any option available in Babel.</p><p>Once a language feature has been adopted into the ECMAScript standard (stage 4 according to the <a href=\"https://tc39.github.io/process-document/\">TC39 process</a>), we will accept issues and pull requests related to the new feature, subject to our <a href=\"https://eslint.org/docs/latest/contribute\">contributing guidelines</a>. Until then, please use the appropriate parser and plugin(s) for your experimental feature.</p><h3>Which Node.js versions does ESLint support?</h3><p>ESLint updates the supported Node.js versions with each major release of ESLint. At that time, ESLint's supported Node.js versions are updated to be:</p><ol><li>The most recent maintenance release of Node.js</li><li>The lowest minor version of the Node.js LTS release that includes the features the ESLint team wants to use.</li><li>The Node.js Current release</li></ol><p>ESLint is also expected to work with Node.js versions released after the Node.js Current release.</p><p>Refer to the <a href=\"https://eslint.org/docs/latest/use/getting-started#prerequisites\">Quick Start Guide</a> for the officially supported Node.js versions for a given ESLint release.</p><h3>Why doesn't ESLint lock dependency versions?</h3><p>Lock files like  are helpful for deployed applications. They ensure that dependencies are consistent between environments and across deployments.</p><p>Packages like  that get published to the npm registry do not include lock files.  as a user will respect version constraints in ESLint's . ESLint and its dependencies will be included in the user's lock file if one exists, but ESLint's own lock file would not be used.</p><p>We intentionally don't lock dependency versions so that we have the latest compatible dependency versions in development and CI that our users get when installing ESLint in a project.</p><p>We have scheduled releases every two weeks on Friday or Saturday. You can follow a <a href=\"https://github.com/eslint/eslint/issues?q=is%3Aopen+is%3Aissue+label%3Arelease\">release issue</a> for updates about the scheduling of any particular release.</p><p>ESLint takes security seriously. We work hard to ensure that ESLint is safe for everyone and that security issues are addressed quickly and responsibly. Read the full <a href=\"https://github.com/eslint/.github/raw/master/SECURITY.md\">security policy</a>.</p><h2>Semantic Versioning Policy</h2><p>ESLint follows <a href=\"https://semver.org\">semantic versioning</a>. However, due to the nature of ESLint as a code quality tool, it's not always clear when a minor or major version bump occurs. To help clarify this for everyone, we've defined the following semantic versioning policy for ESLint:</p><ul><li>Patch release (intended to not break your lint build) \n  <ul><li>A bug fix in a rule that results in ESLint reporting fewer linting errors.</li><li>A bug fix to the CLI or core (including formatters).</li><li>Improvements to documentation.</li><li>Non-user-facing changes such as refactoring code, adding, deleting, or modifying tests, and increasing test coverage.</li><li>Re-releasing after a failed release (i.e., publishing a release that doesn't work for anyone).</li></ul></li><li>Minor release (might break your lint build) \n  <ul><li>A bug fix in a rule that results in ESLint reporting more linting errors.</li><li>A new option to an existing rule that does not result in ESLint reporting more linting errors by default.</li><li>A new addition to an existing rule to support a newly-added language feature (within the last 12 months) that will result in ESLint reporting more linting errors by default.</li><li>An existing rule is deprecated.</li><li>A new CLI capability is created.</li><li>New capabilities to the public API are added (new classes, new methods, new arguments to existing methods, etc.).</li><li>A new formatter is created.</li><li> is updated and will result in strictly fewer linting errors (e.g., rule removals).</li></ul></li><li>Major release (likely to break your lint build) \n  <ul><li> is updated and may result in new linting errors (e.g., rule additions, most rule option updates).</li><li>A new option to an existing rule that results in ESLint reporting more linting errors by default.</li><li>An existing formatter is removed.</li><li>Part of the public API is removed or changed in an incompatible way. The public API includes: \n    <ul><li>Rule, formatter, parser, plugin APIs</li></ul></li></ul></li></ul><p>According to our policy, any minor update may report more linting errors than the previous release (ex: from a bug fix). As such, we recommend using the tilde () in  e.g.  to guarantee the results of your builds.</p><p>Since ESLint is a CommonJS package, there are restrictions on which ESM-only packages can be used as dependencies.</p><p>Packages that are controlled by the ESLint team and have no external dependencies can be safely loaded synchronously using <a href=\"https://nodejs.org/api/modules.html#loading-ecmascript-modules-using-require\"></a> and therefore used in any contexts.</p><p>For external packages, we don't use  because a package could add a top-level  and thus break ESLint. We can use an external ESM-only package only in case it is needed only in asynchronous code, in which case it can be loaded using dynamic .</p><p>Copyright OpenJS Foundation and other contributors, &lt;www.openjsf.org&gt;</p><p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p><p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p><p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p><p>These folks keep the project moving and are resources for help.</p><h3>Technical Steering Committee (TSC)</h3><p>The people who manage releases, review feature requests, and meet regularly to ensure ESLint is properly maintained.</p><p>The people who review and implement new features.</p><p>The people who review and fix bugs and help triage issues.</p><p>Team members who focus specifically on eslint.org</p><p>The following companies, organizations, and individuals support ESLint's ongoing maintenance and development. <a href=\"https://eslint.org/donate\">Become a Sponsor</a> to get your logo on our READMEs and <a href=\"https://eslint.org/sponsors\">website</a>.</p> Technology sponsors allow us to use their products and services for free as part of a contribution to the open source ecosystem and our work. \n",
      "contentLength": 9918,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "PostHog/posthog",
      "url": "https://github.com/PostHog/posthog",
      "date": 1771642723,
      "author": "",
      "guid": 47067,
      "unread": true,
      "content": "<p>ü¶î PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack.</p><h2>PostHog is an all-in-one, open source platform for building successful products</h2><p><a href=\"https://posthog.com/\">PostHog</a> provides every tool you need to build a successful product including:</p><ul><li><a href=\"https://posthog.com/product-analytics\">Product Analytics</a>: Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL.</li><li><a href=\"https://posthog.com/web-analytics\">Web Analytics</a>: Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue.</li><li><a href=\"https://posthog.com/session-replay\">Session Replays</a>: Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior.</li><li><a href=\"https://posthog.com/feature-flags\">Feature Flags</a>: Safely roll out features to select users or cohorts with feature flags.</li><li><a href=\"https://posthog.com/experiments\">Experiments</a>: Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too.</li><li><a href=\"https://posthog.com/error-tracking\">Error Tracking</a>: Track errors, get alerts, and resolve issues to improve your product.</li><li><a href=\"https://posthog.com/surveys\">Surveys</a>: Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder.</li><li><a href=\"https://posthog.com/data-warehouse\">Data warehouse</a>: Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data.</li><li><a href=\"https://posthog.com/cdp\">Data pipelines</a>: Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse.</li><li><a href=\"https://posthog.com/docs/llm-analytics\">LLM analytics</a>: Capture traces, generations, latency, and cost for your LLM-powered app.</li><li><a href=\"https://posthog.com/docs/workflows\">Workflows</a>: Create workflows that automate actions or send messages to your users.</li></ul><h2>Getting started with PostHog</h2><h3>PostHog Cloud (Recommended)</h3><p>The fastest and most reliable way to get started with PostHog is signing up for free to&nbsp;<a href=\"https://us.posthog.com/signup\">PostHog Cloud</a> or <a href=\"https://eu.posthog.com/signup\">PostHog Cloud EU</a>. Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage.</p><h3>Self-hosting the open-source hobby deploy (Advanced)</h3><p>If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker (recommended 4GB memory):</p><pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)\"\n</code></pre><p>We have SDKs and libraries for popular languages and frameworks like:</p><h2>Learning more about PostHog</h2><p>We &lt;3 contributions big and small:</p><p>Need ? Check out our <a href=\"https://github.com/PostHog/posthog-foss\">posthog-foss</a> repository, which is purged of all proprietary code and features.</p><p>The pricing for our paid plan is completely transparent and available on <a href=\"https://posthog.com/pricing\">our pricing page</a>.</p><img src=\"https://res.cloudinary.com/dmukukwp6/image/upload/v1/posthog.com/src/components/Home/images/mission-control-hog\" alt=\"Hedgehog working on a Mission Control Center\" width=\"350px\"><p>Hey! If you're reading this, you've proven yourself as a dedicated README reader.</p>",
      "contentLength": 2826,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "roboflow/trackers",
      "url": "https://github.com/roboflow/trackers",
      "date": 1771642723,
      "author": "",
      "guid": 47068,
      "unread": true,
      "content": "<p>Trackers gives you clean, modular re-implementations of leading multi-object tracking algorithms released under the permissive Apache 2.0 license. You combine them with any detection model you already use.</p><p>Point at a video, webcam, RTSP stream, or image directory. Get tracked output.</p><pre><code>trackers track \\\n    --source video.mp4 \\\n    --output output.mp4 \\\n    --model rfdetr-medium \\\n    --tracker bytetrack \\\n    --show-labels \\\n    --show-trajectories\n</code></pre><p>Plug trackers into your existing detection pipeline. Works with any detector.</p><pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\nfrom trackers import ByteTrackTracker\n\nmodel = get_model(model_id=\"rfdetr-medium\")\ntracker = ByteTrackTracker()\n\nlabel_annotator = sv.LabelAnnotator()\ntrajectory_annotator = sv.TrajectoryAnnotator()\n\ncap = cv2.VideoCapture(\"video.mp4\")\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    tracked = tracker.update(detections)\n\n    frame = label_annotator.annotate(frame, tracked)\n    frame = trajectory_annotator.annotate(frame, tracked)\n</code></pre><p>Benchmark your tracker against ground truth with standard MOT metrics.</p><pre><code>trackers eval \\\n    --gt-dir data/gt \\\n    --tracker-dir data/trackers \\\n    --metrics CLEAR HOTA Identity\n</code></pre><pre><code>Sequence                        MOTA    HOTA    IDF1  IDSW\n----------------------------------------------------------\nMOT17-02-FRCNN                75.600  62.300  72.100    42\nMOT17-04-FRCNN                78.200  65.100  74.800    31\n----------------------------------------------------------\nCOMBINED                      75.033  62.400  72.033    73\n</code></pre><p>Clean, modular implementations of leading trackers. See the <a href=\"https://trackers.roboflow.com/develop/trackers/comparison/\">tracker comparison</a> for detailed benchmarks.</p>",
      "contentLength": 1787,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    }
  ],
  "tags": []
}
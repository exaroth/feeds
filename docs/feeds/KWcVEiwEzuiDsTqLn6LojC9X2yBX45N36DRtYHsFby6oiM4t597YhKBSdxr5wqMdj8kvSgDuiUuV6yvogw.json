{
  "id": "KWcVEiwEzuiDsTqLn6LojC9X2yBX45N36DRtYHsFby6oiM4t597YhKBSdxr5wqMdj8kvSgDuiUuV6yvogw",
  "title": "GitHub All Languages Daily Trending",
  "displayTitle": "Github Trending",
  "url": "https://mshibanami.github.io/GitHubTrendingRSS/daily/all.xml",
  "feedLink": "http://mshibanami.github.io/GitHubTrendingRSS",
  "isQuery": false,
  "isEmpty": false,
  "isHidden": false,
  "itemCount": 50,
  "items": [
    {
      "title": "NVIDIA/Megatron-LM",
      "url": "https://github.com/NVIDIA/Megatron-LM",
      "date": 1772074985,
      "author": "",
      "guid": 48389,
      "unread": true,
      "content": "<p>This repository contains two components:  and .</p><p> is a reference example that includes Megatron Core plus pre-configured training scripts. Best for research teams, learning distributed training, and quick experimentation.</p><p> is a composable library with GPU-optimized building blocks for custom training frameworks. It provides transformer building blocks, advanced parallelism strategies (TP, PP, DP, EP, CP), mixed precision support (FP16, BF16, FP8, FP4), and model architectures. Best for framework developers and ML engineers building custom training pipelines.</p><p> provides bidirectional Hugging Face â†” Megatron checkpoint conversion with production-ready recipes.</p><p>Install Megatron Core with pip:</p><ol><li><p>Install Megatron Core with required dependencies:</p><pre><code>pip install --no-build-isolation megatron-core[mlm,dev]\n</code></pre></li><li><p>Clone repository for examples:</p><pre><code>git clone https://github.com/NVIDIA/Megatron-LM.git\ncd Megatron-LM\npip install --no-build-isolation .[mlm,dev]\n</code></pre></li></ol><ul><li><strong>Megatron Core development has moved to GitHub!</strong> All development and CI now happens in the open. We welcome community contributions.</li><li> - Bidirectional converter for interoperability between Hugging Face and Megatron checkpoints, featuring production-ready recipes for popular models.</li><li> - Comprehensive roadmap for MoE features including DeepSeek-V3, Qwen3, advanced parallelism strategies, FP8 optimizations, and Blackwell performance enhancements.</li><li> - Advanced features including YaRN RoPE scaling, attention sinks, and custom activation functions are being integrated into Megatron Core.</li><li> - Best practices and optimized configurations for training DeepSeek-V3, Mixtral, and Qwen3 MoE models with performance benchmarking and checkpoint conversion tools.</li><li> Megatron Core v0.11.0 brings new capabilities for multi-data center LLM training (<a href=\"https://developer.nvidia.com/blog/turbocharge-llm-training-across-long-haul-data-center-networks-with-nvidia-nemo-framework/\">blog</a>).</li></ul><pre><code>Megatron-LM/\nâ”œâ”€â”€ megatron/\nâ”‚   â”œâ”€â”€ core/                    # Megatron Core (kernels, parallelism, building blocks)\nâ”‚   â”‚   â”œâ”€â”€ models/              # Transformer models\nâ”‚   â”‚   â”œâ”€â”€ transformer/         # Transformer building blocks\nâ”‚   â”‚   â”œâ”€â”€ tensor_parallel/     # Tensor parallelism\nâ”‚   â”‚   â”œâ”€â”€ pipeline_parallel/   # Pipeline parallelism\nâ”‚   â”‚   â”œâ”€â”€ distributed/         # Distributed training (FSDP, DDP)\nâ”‚   â”‚   â”œâ”€â”€ optimizer/           # Optimizers\nâ”‚   â”‚   â”œâ”€â”€ datasets/            # Dataset loaders\nâ”‚   â”‚   â”œâ”€â”€ inference/           # Inference engines and server\nâ”‚   â”‚   â””â”€â”€ export/              # Model export (e.g. TensorRT-LLM)\nâ”‚   â”œâ”€â”€ training/                # Training scripts\nâ”‚   â”œâ”€â”€ legacy/                  # Legacy components\nâ”‚   â”œâ”€â”€ post_training/           # Post-training (quantization, distillation, pruning, etc.)\nâ”‚   â””â”€â”€ rl/                      # Reinforcement learning (RLHF, etc.)\nâ”œâ”€â”€ examples/                    # Ready-to-use training examples\nâ”œâ”€â”€ tools/                       # Utility tools\nâ”œâ”€â”€ tests/                       # Comprehensive test suite\nâ””â”€â”€ docs/                        # Documentation\n</code></pre><p>Our codebase efficiently trains models from 2B to 462B parameters across thousands of GPUs, achieving up to <strong>47% Model FLOP Utilization (MFU)</strong> on H100 clusters.</p><ul><li>: 131,072 tokens</li><li>: 4096 tokens</li><li>: Varied hidden size, attention heads, and layers to achieve target parameter counts</li><li><strong>Communication optimizations</strong>: Fine-grained overlapping with DP (, ), TP (), and PP (enabled by default)</li></ul><ul><li>: Successfully benchmarked 462B parameter model training</li><li>: MFU increases from 41% to 47-48% with model size</li><li>: Throughputs include all operations (data loading, optimizer steps, communication, logging)</li><li>: Full training pipeline with checkpointing and fault tolerance</li><li><em>Note: Performance results measured without training to convergence</em></li></ul><p>Our weak scaled results show superlinear scaling (MFU increases from 41% for the smallest model considered to 47-48% for the largest models); this is because larger GEMMs have higher arithmetic intensity and are consequently more efficient to execute.</p><p>We also strong scaled the standard GPT-3 model (our version has slightly more than 175 billion parameters due to larger vocabulary size) from 96 H100 GPUs to 4608 GPUs, using the same batch size of 1152 sequences throughout. Communication becomes more exposed at larger scale, leading to a reduction in MFU from 47% to 42%.</p><ul><li> - DeepSeek-V3, Qwen3, advanced parallelism, FP8 optimizations, and Blackwell enhancements</li></ul><p>We â¤ï¸ contributions! Ways to contribute:</p><ul><li>ğŸ›  - Help us improve reliability</li><li>ğŸ’¡  - Shape the future of Megatron Core</li><li>ğŸ“  - Make Megatron Core more accessible</li><li>ğŸ”§  - Contribute code improvements</li></ul><p>If you use Megatron in your research or project, we appreciate that you use the following citations:</p><pre><code>@article{megatron-lm,\n  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},\n  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},\n  journal={arXiv preprint arXiv:1909.08053},\n  year={2019}\n}\n</code></pre>",
      "contentLength": 5038,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/f93c361837c260f3a59acf1e129a086fedbe0fb877f0d405599470ebf0efa230/NVIDIA/Megatron-LM",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "katanemo/plano",
      "url": "https://github.com/katanemo/plano",
      "date": 1772074985,
      "author": "",
      "guid": 48390,
      "unread": true,
      "content": "<p>Delivery infrastructure for agentic apps - Plano is an AI-native proxy and data plane that offloads plumbing work, so you stay focused on your agent's core logic (via any AI framework).</p><div align=\"center\"><p><em>The AI-native proxy server and data plane for agentic apps.</em><p> Plano pulls out the rote plumbing work and decouples you from brittle framework abstractions, centralizing what shouldnâ€™t be bespoke in every codebase - like agent routing and orchestration, rich agentic signals and traces for continuous improvement, guardrail filters for safety and moderation, and smart LLM routing APIs for model agility. Use any language or AI framework, and deliver agents faster to production.</p></p><p>Star â­ï¸ the repo if you found Plano useful â€” new releases and updates land here first.</p></div><p>Building agentic demos is easy. Shipping agentic applications safely, reliably, and repeatably to production is hard. After the thrill of a quick hack, you end up building the â€œhidden middlewareâ€ to reach production: routing logic to reach the right agent, guardrail hooks for safety and moderation, evaluation and observability glue for continuous learning, and model/provider quirks scattered across frameworks and application code.</p><p>Plano solves this by moving core delivery concerns into a unified, out-of-process dataplane.</p><p>Plano pulls rote plumbing out of your framework so you can stay focused on what matters most: the core product logic of your agentic applications. Plano is backed by <a href=\"https://planoai.dev/research\">industry-leading LLM research</a> and built on <a href=\"https://envoyproxy.io\">Envoy</a> by its core contributors, who built critical infrastructure at scale for modern worklaods.</p><p> to learn how you can use Plano to improve the speed, safety and obervability of your agentic applications.</p><blockquote><p>[!IMPORTANT] Plano and the Arch family of LLMs (like Plano-Orchestrator-4B, Arch-Router, etc) are hosted free of charge in the US-central region to give you a great first-run developer experience of Plano. To scale and run in production, you can either run these LLMs locally or contact us on <a href=\"https://discord.gg/pGZf2gcwEc\">Discord</a> for API keys.</p></blockquote><h2>Build Agentic Apps with Plano</h2><p>Plano handles <strong>orchestration, model management, and observability</strong> as modular building blocks - letting you configure only what you need (edge proxying for agentic orchestration and guardrails, or LLM routing from your services, or both together) to fit cleanly into existing architectures. Below is a simple multi-agent travel agent built with Plano that showcases all three core capabilities</p><h3>1. Define Your Agents in YAML</h3><pre><code># config.yaml\nversion: v0.3.0\n\n# What you declare: Agent URLs and natural language descriptions\n# What you don't write: Intent classifiers, routing logic, model fallbacks, provider adapters, or tracing instrumentation\n\nagents:\n  - id: weather_agent\n    url: http://localhost:10510\n  - id: flight_agent\n    url: http://localhost:10520\n\nmodel_providers:\n  - model: openai/gpt-4o\n    access_key: $OPENAI_API_KEY\n    default: true\n  - model: anthropic/claude-3-5-sonnet\n    access_key: $ANTHROPIC_API_KEY\n\nlisteners:\n  - type: agent\n    name: travel_assistant\n    port: 8001\n    router: plano_orchestrator_v1  # Powered by our 4B-parameter routing model. You can change this to different models\n    agents:\n      - id: weather_agent\n        description: |\n          Gets real-time weather and forecasts for any city worldwide.\n          Handles: \"What's the weather in Paris?\", \"Will it rain in Tokyo?\"\n\n      - id: flight_agent\n        description: |\n          Searches flights between airports with live status and schedules.\n          Handles: \"Flights from NYC to LA\", \"Show me flights to Seattle\"\n\ntracing:\n  random_sampling: 100  # Auto-capture traces for evaluation\n</code></pre><h3>2. Write Simple Agent Code</h3><p>Your agents are just HTTP servers that implement the OpenAI-compatible chat completions endpoint. Use any language or framework:</p><pre><code># weather_agent.py\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse\nfrom openai import AsyncOpenAI\n\napp = FastAPI()\n\n# Point to Plano's LLM gateway - it handles model routing for you\nllm = AsyncOpenAI(base_url=\"http://localhost:12001/v1\", api_key=\"EMPTY\")\n\n@app.post(\"/v1/chat/completions\")\nasync def chat(request: Request):\n    body = await request.json()\n    messages = body.get(\"messages\", [])\n    days = 7\n\n    # Your agent logic: fetch data, call APIs, run tools\n    # See demos/agent_orchestration/travel_agents/ for the full implementation\n    weather_data = await get_weather_data(request, messages, days)\n\n    # Stream the response back through Plano\n    async def generate():\n        stream = await llm.chat.completions.create(\n            model=\"openai/gpt-4o\",\n            messages=[{\"role\": \"system\", \"content\": f\"Weather: {weather_data}\"}, *messages],\n            stream=True\n        )\n        async for chunk in stream:\n            yield f\"data: {chunk.model_dump_json()}\\n\\n\"\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n</code></pre><h3>3. Start Plano &amp; Query Your Agents</h3><pre><code># Start Plano\nplanoai up config.yaml\n...\n\n# Query - Plano intelligently routes to both agents in a single conversation\ncurl http://localhost:8001/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"I want to travel from NYC to Paris next week. What is the weather like there, and can you find me some flights?\"}\n    ]\n  }'\n# â†’ Plano routes to weather_agent for Paris weather âœ“\n# â†’ Then routes to flight_agent for NYC â†’ Paris flights âœ“\n# â†’ Returns a complete travel plan with both weather info and flight options\n</code></pre><h3>4. Get Observability and Model Agility for Free</h3><p>Every request is traced end-to-end with OpenTelemetry - no instrumentation code needed.</p><h3>What You Didn't Have to Build</h3><table><thead><tr></tr></thead><tbody><tr><td>Write intent classifier + routing logic</td><td>Declare agent descriptions in YAML</td></tr><tr><td>Handle each provider's API quirks</td><td>Unified LLM APIs with state management</td></tr><tr><td>Instrument every service with OTEL</td><td>Automatic end-to-end traces and logs</td></tr><tr><td>Build pipeline to capture/export spans</td><td>Zero-code agentic signals</td></tr><tr><td>Update routing code, test, redeploy</td></tr></tbody></table><p> Plano uses purpose-built, lightweight LLMs (like our 4B-parameter orchestrator) instead of heavyweight frameworks or GPT-4 for routing - giving you production-grade routing at a fraction of the cost and latency.</p><p>To get in touch with us, please join our <a href=\"https://discord.gg/pGZf2gcwEc\">discord server</a>. We actively monitor that and offer support there.</p><p>Ready to try Plano? Check out our comprehensive documentation:</p><p>We would love feedback on our <a href=\"https://github.com/orgs/katanemo/projects/1\">Roadmap</a> and we welcome contributions to ! Whether you're fixing bugs, adding new features, improving documentation, or creating tutorials, your help is much appreciated. Please visit our <a href=\"https://raw.githubusercontent.com/katanemo/plano/main/CONTRIBUTING.md\">Contribution Guide</a> for more details</p><p>Star â­ï¸ the repo if you found Plano useful â€” new releases and updates land here first.</p>",
      "contentLength": 6736,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/5065011867d6724d4d4787e09a46c104d0e69953278e0622362e354e485f8e27/katanemo/plano",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "shareAI-lab/learn-claude-code",
      "url": "https://github.com/shareAI-lab/learn-claude-code",
      "date": 1772074985,
      "author": "",
      "guid": 48391,
      "unread": true,
      "content": "<p>Bash is all you need - A nano Claude Codeâ€“like agent, built from 0 to 1</p><pre><code>                    THE AGENT PATTERN\n                    =================\n\n    User --&gt; messages[] --&gt; LLM --&gt; response\n                                      |\n                            stop_reason == \"tool_use\"?\n                           /                          \\\n                         yes                           no\n                          |                             |\n                    execute tools                    return text\n                    append results\n                    loop back -----------------&gt; messages[]\n\n\n    That's the minimal loop. Every AI coding agent needs this loop.\n    Production agents add policy, permissions, and lifecycle layers.\n</code></pre><p><strong>12 progressive sessions, from a simple loop to isolated autonomous execution.</strong><strong>Each session adds one mechanism. Each mechanism has one motto.</strong></p><blockquote><p> â€” one tool + one loop = an agent</p><p> â€” adding tools means adding handlers, not rewriting the loop</p><p> â€” visible plans improve task completion</p><p><em>\"Process isolation = context isolation\"</em> â€” fresh messages[] per subagent</p><p><em>\"Load on demand, not upfront\"</em> â€” inject knowledge via tool_result, not system prompt</p><p> â€” forget old context to enable infinite sessions</p><p><em>\"State survives /compact\"</em> â€” file-based state outlives context compression</p><p> â€” non-blocking threads + notification queue</p><p><em>\"Append to send, drain to read\"</em> â€” async mailboxes for persistent teammates</p><p><em>\"Same request_id, two protocols\"</em> â€” one FSM pattern powers shutdown + plan approval</p><p><em>\"Poll, claim, work, repeat\"</em> â€” no coordinator needed, agents self-organize</p><p><em>\"Isolate by directory, coordinate by task ID\"</em> â€” task board + optional worktree lanes</p></blockquote><pre><code>def agent_loop(messages):\n    while True:\n        response = client.messages.create(\n            model=MODEL, system=SYSTEM,\n            messages=messages, tools=TOOLS,\n        )\n        messages.append({\"role\": \"assistant\",\n                         \"content\": response.content})\n\n        if response.stop_reason != \"tool_use\":\n            return\n\n        results = []\n        for block in response.content:\n            if block.type == \"tool_use\":\n                output = TOOL_HANDLERS[block.name](**block.input)\n                results.append({\n                    \"type\": \"tool_result\",\n                    \"tool_use_id\": block.id,\n                    \"content\": output,\n                })\n        messages.append({\"role\": \"user\", \"content\": results})\n</code></pre><p>Every session layers one mechanism on top of this loop -- without changing the loop itself.</p><p>This repository is a 0-&gt;1 learning project for building a nano Claude Code-like agent. It intentionally simplifies or omits several production mechanisms:</p><ul><li>Full event/hook buses (for example PreToolUse, SessionStart/End, ConfigChange). s12 includes only a minimal append-only lifecycle event stream for teaching.</li><li>Rule-based permission governance and trust workflows</li><li>Session lifecycle controls (resume/fork) and advanced worktree lifecycle controls</li><li>Full MCP runtime details (transport/OAuth/resource subscribe/polling)</li></ul><p>Treat the team JSONL mailbox protocol in this repo as a teaching implementation, not a claim about any specific production internals.</p><pre><code>git clone https://github.com/shareAI-lab/learn-claude-code\ncd learn-claude-code\npip install -r requirements.txt\ncp .env.example .env   # Edit .env with your ANTHROPIC_API_KEY\n\npython agents/s01_agent_loop.py       # Start here\npython agents/s11_autonomous_agents.py  # Full autonomous team\npython agents/s12_worktree_task_isolation.py  # Task-aware worktree isolation\n</code></pre><p>Interactive visualizations, step-through diagrams, source viewer, and documentation.</p><pre><code>cd web &amp;&amp; npm install &amp;&amp; npm run dev   # http://localhost:3000\n</code></pre><pre><code>Phase 1: THE LOOP                    Phase 2: PLANNING &amp; KNOWLEDGE\n==================                   ==============================\ns01  The Agent Loop          [1]     s03  TodoWrite               [5]\n     while + stop_reason                  TodoManager + nag reminder\n     |                                    |\n     +-&gt; s02  Tools              [4]     s04  Subagents            [5]\n              dispatch map: name-&gt;handler     fresh messages[] per child\n                                              |\n                                         s05  Skills               [5]\n                                              SKILL.md via tool_result\n                                              |\n                                         s06  Compact              [5]\n                                              3-layer compression\n\nPhase 3: PERSISTENCE                 Phase 4: TEAMS\n==================                   =====================\ns07  Tasks                   [8]     s09  Agent Teams             [9]\n     file-based CRUD + deps graph         teammates + JSONL mailboxes\n     |                                    |\ns08  Background Tasks        [6]     s10  Team Protocols          [12]\n     daemon threads + notify queue        shutdown + plan approval FSM\n                                          |\n                                     s11  Autonomous Agents       [14]\n                                          idle cycle + auto-claim\n                                     |\n                                     s12  Worktree Isolation      [16]\n                                          task coordination + optional isolated execution lanes\n\n                                     [N] = number of tools\n</code></pre><pre><code>learn-claude-code/\n|\n|-- agents/                        # Python reference implementations (s01-s12 + full)\n|-- docs/{en,zh,ja}/               # Mental-model-first documentation (3 languages)\n|-- web/                           # Interactive learning platform (Next.js)\n|-- skills/                        # Skill files for s05\n+-- .github/workflows/ci.yml      # CI: typecheck + build\n</code></pre><p>Mental-model-first: problem, solution, ASCII diagram, minimal code. Available in <a href=\"https://raw.githubusercontent.com/shareAI-lab/learn-claude-code/main/docs/en/\">English</a> | <a href=\"https://raw.githubusercontent.com/shareAI-lab/learn-claude-code/main/docs/zh/\">ä¸­æ–‡</a> | <a href=\"https://raw.githubusercontent.com/shareAI-lab/learn-claude-code/main/docs/ja/\">æ—¥æœ¬èª</a>.</p><table><tbody><tr></tr><tr></tr><tr></tr><tr><td><em>Process isolation = context isolation</em></td></tr><tr><td><em>Load on demand, not upfront</em></td></tr><tr></tr><tr></tr><tr></tr><tr><td><em>Append to send, drain to read</em></td></tr><tr><td><em>Same request_id, two protocols</em></td></tr><tr><td><em>Poll, claim, work, repeat</em></td></tr><tr><td>Worktree + Task Isolation</td><td><em>Isolate by directory, coordinate by task ID</em></td></tr></tbody></table><p><strong>The model is the agent. Our job is to give it tools and stay out of the way.</strong></p>",
      "contentLength": 6206,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/e052ff8a97ef6e2c1c726e3709265726552488956cf6af1b8ea102ed483b3cd3/shareAI-lab/learn-claude-code",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "bytedance/deer-flow",
      "url": "https://github.com/bytedance/deer-flow",
      "date": 1772074985,
      "author": "",
      "guid": 48392,
      "unread": true,
      "content": "<p>An open-source SuperAgent harness that researches, codes, and creates. With the help of sandboxes, memories, tools, skills and subagents, it handles different levels of tasks that could take minutes to hours.</p><p>DeerFlow (eep xploration and fficient esearch ) is an open-source  that orchestrates , , and  to do almost anything â€” powered by .</p><blockquote><p>[!NOTE] <strong>DeerFlow 2.0 is a ground-up rewrite.</strong> It shares no code with v1. If you're looking for the original Deep Research framework, it's maintained on the <a href=\"https://github.com/bytedance/deer-flow/tree/main-1.x\"> branch</a> â€” contributions there are still welcome. Active development has moved to 2.0.</p></blockquote><p>Learn more and see  on our official website.</p><ol><li><p><strong>Clone the DeerFlow repository</strong></p><pre><code>git clone https://github.com/bytedance/deer-flow.git\ncd deer-flow\n</code></pre></li><li><p><strong>Generate local configuration files</strong></p><p>From the project root directory (), run:</p><p>This command creates local configuration files based on the provided example templates.</p></li><li><p><strong>Configure your preferred model(s)</strong></p><p>Edit  and define at least one model:</p><pre><code>models:\n  - name: gpt-4                       # Internal identifier\n    display_name: GPT-4               # Human-readable name\n    use: langchain_openai:ChatOpenAI  # LangChain class path\n    model: gpt-4                      # Model identifier for API\n    api_key: $OPENAI_API_KEY          # API key (recommended: use env var)\n    max_tokens: 4096                  # Maximum tokens per request\n    temperature: 0.7                  # Sampling temperature\n</code></pre></li><li><p><strong>Set API keys for your configured model(s)</strong></p><p>Choose one of the following methods:</p></li></ol><ul><li><p>Option A: Edit the  file in the project root (Recommended)</p><pre><code>TAVILY_API_KEY=your-tavily-api-key\nOPENAI_API_KEY=your-openai-api-key\n# Add other provider keys as needed\n</code></pre></li><li><p>Option B: Export environment variables in your shell</p><pre><code>export OPENAI_API_KEY=your-openai-api-key\n</code></pre></li><li><p>Option C: Edit  directly (Not recommended for production)</p><pre><code>models:\n  - name: gpt-4\n    api_key: your-actual-api-key-here  # Replace placeholder\n</code></pre></li></ul><h4>Option 1: Docker (Recommended)</h4><p>The fastest way to get started with a consistent environment:</p><ol><li><pre><code>make docker-init    # Pull sandbox image (Only once or when image updates)\nmake docker-start   # Start services (auto-detects sandbox mode from config.yaml)\n</code></pre><p> now starts  only when  uses provisioner mode (<code>sandbox.use: src.community.aio_sandbox:AioSandboxProvider</code> with ).</p></li></ol><h4>Option 2: Local Development</h4><p>If you prefer running services locally:</p><ol><li><pre><code>make check  # Verifies Node.js 22+, pnpm, uv, nginx\n</code></pre></li><li><p><strong>(Optional) Pre-pull sandbox image</strong>:</p><pre><code># Recommended if using Docker/Container-based sandbox\nmake setup-sandbox\n</code></pre></li></ol><p>DeerFlow supports multiple sandbox execution modes:</p><ul><li> (runs sandbox code directly on the host machine)</li><li> (runs sandbox code in isolated Docker containers)</li><li><strong>Docker Execution with Kubernetes</strong> (runs sandbox code in Kubernetes pods via provisioner service)</li></ul><p>For Docker development, service startup follows  sandbox mode. In Local/Docker modes,  is not started.</p><p>DeerFlow supports configurable MCP servers and skills to extend its capabilities. See the <a href=\"https://raw.githubusercontent.com/bytedance/deer-flow/main/backend/docs/MCP_SERVER.md\">MCP Server Guide</a> for detailed instructions.</p><h2>From Deep Research to Super Agent Harness</h2><p>DeerFlow started as a Deep Research framework â€” and the community ran with it. Since launch, developers have pushed it far beyond research: building data pipelines, generating slide decks, spinning up dashboards, automating content workflows. Things we never anticipated.</p><p>That told us something important: DeerFlow wasn't just a research tool. It was a  â€” a runtime that gives agents the infrastructure to actually get work done.</p><p>So we rebuilt it from scratch.</p><p>DeerFlow 2.0 is no longer a framework you wire together. It's a super agent harness â€” batteries included, fully extensible. Built on LangGraph and LangChain, it ships with everything an agent needs out of the box: a filesystem, memory, skills, sandboxed execution, and the ability to plan and spawn sub-agents for complex, multi-step tasks.</p><p>Use it as-is. Or tear it apart and make it yours.</p><p>Skills are what make DeerFlow do .</p><p>A standard Agent Skill is a structured capability module â€” a Markdown file that defines a workflow, best practices, and references to supporting resources. DeerFlow ships with built-in skills for research, report generation, slide creation, web pages, image and video generation, and more. But the real power is extensibility: add your own skills, replace the built-in ones, or combine them into compound workflows.</p><p>Skills are loaded progressively â€” only when the task needs them, not all at once. This keeps the context window lean and makes DeerFlow work well even with token-sensitive models.</p><p>Tools follow the same philosophy. DeerFlow comes with a core toolset â€” web search, web fetch, file operations, bash execution â€” and supports custom tools via MCP servers and Python functions. Swap anything. Add anything.</p><pre><code># Paths inside the sandbox container\n/mnt/skills/public\nâ”œâ”€â”€ research/SKILL.md\nâ”œâ”€â”€ report-generation/SKILL.md\nâ”œâ”€â”€ slide-creation/SKILL.md\nâ”œâ”€â”€ web-page/SKILL.md\nâ””â”€â”€ image-generation/SKILL.md\n\n/mnt/skills/custom\nâ””â”€â”€ your-custom-skill/SKILL.md      â† yours\n</code></pre><p>Complex tasks rarely fit in a single pass. DeerFlow decomposes them.</p><p>The lead agent can spawn sub-agents on the fly â€” each with its own scoped context, tools, and termination conditions. Sub-agents run in parallel when possible, report back structured results, and the lead agent synthesizes everything into a coherent output.</p><p>This is how DeerFlow handles tasks that take minutes to hours: a research task might fan out into a dozen sub-agents, each exploring a different angle, then converge into a single report â€” or a website â€” or a slide deck with generated visuals. One harness, many hands.</p><p>DeerFlow doesn't just  about doing things. It has its own computer.</p><p>Each task runs inside an isolated Docker container with a full filesystem â€” skills, workspace, uploads, outputs. The agent reads, writes, and edits files. It executes bash commands and codes. It views images. All sandboxed, all auditable, zero contamination between sessions.</p><p>This is the difference between a chatbot with tool access and an agent with an actual execution environment.</p><pre><code># Paths inside the sandbox container\n/mnt/user-data/\nâ”œâ”€â”€ uploads/          â† your files\nâ”œâ”€â”€ workspace/        â† agents' working directory\nâ””â”€â”€ outputs/          â† final deliverables\n</code></pre><p><strong>Isolated Sub-Agent Context</strong>: Each sub-agent runs in its own isolated context. This means that the sub-agent will not be able to see the context of the main agent or other sub-agents. This is important to ensure that the sub-agent is able to focus on the task at hand and not be distracted by the context of the main agent or other sub-agents.</p><p>: Within a session, DeerFlow manages context aggressively â€” summarizing completed sub-tasks, offloading intermediate results to the filesystem, compressing what's no longer immediately relevant. This lets it stay sharp across long, multi-step tasks without blowing the context window.</p><p>Most agents forget everything the moment a conversation ends. DeerFlow remembers.</p><p>Across sessions, DeerFlow builds a persistent memory of your profile, preferences, and accumulated knowledge. The more you use it, the better it knows you â€” your writing style, your technical stack, your recurring workflows. Memory is stored locally and stays under your control.</p><p>DeerFlow is model-agnostic â€” it works with any LLM that implements the OpenAI-compatible API. That said, it performs best with models that support:</p><ul><li> (100k+ tokens) for deep research and multi-step tasks</li><li> for adaptive planning and complex decomposition</li><li> for image understanding and video comprehension</li><li> for reliable function calling and structured outputs</li></ul><p>We welcome contributions! Please see <a href=\"https://raw.githubusercontent.com/bytedance/deer-flow/main/CONTRIBUTING.md\">CONTRIBUTING.md</a> for development setup, workflow, and guidelines.</p><p>Regression coverage includes Docker sandbox mode detection and provisioner kubeconfig-path handling tests in .</p><p>This project is open source and available under the <a href=\"https://raw.githubusercontent.com/bytedance/deer-flow/main/LICENSE\">MIT License</a>.</p><p>DeerFlow is built upon the incredible work of the open-source community. We are deeply grateful to all the projects and contributors whose efforts have made DeerFlow possible. Truly, we stand on the shoulders of giants.</p><p>We would like to extend our sincere appreciation to the following projects for their invaluable contributions:</p><ul><li>: Their exceptional framework powers our LLM interactions and chains, enabling seamless integration and functionality.</li><li>: Their innovative approach to multi-agent orchestration has been instrumental in enabling DeerFlow's sophisticated workflows.</li></ul><p>These projects exemplify the transformative power of open-source collaboration, and we are proud to build upon their foundations.</p><p>A heartfelt thank you goes out to the core authors of , whose vision, passion, and dedication have brought this project to life:</p><p>Your unwavering commitment and expertise have been the driving force behind DeerFlow's success. We are honored to have you at the helm of this journey.</p>",
      "contentLength": 8844,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/a4e7dd8f8ef6613cbc0b8356b3689b6a56907cf689d4d2d9f75bcc1dd70478ad/bytedance/deer-flow",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "datawhalechina/hello-agents",
      "url": "https://github.com/datawhalechina/hello-agents",
      "date": 1772074985,
      "author": "",
      "guid": 48393,
      "unread": true,
      "content": "<p>ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹</p><p>â€ƒâ€ƒå¦‚æœè¯´ 2024 å¹´æ˜¯\"ç™¾æ¨¡å¤§æˆ˜\"çš„å…ƒå¹´ï¼Œé‚£ä¹ˆ 2025 å¹´æ— ç–‘å¼€å¯äº†\"Agent å…ƒå¹´\"ã€‚æŠ€æœ¯çš„ç„¦ç‚¹æ­£ä»è®­ç»ƒæ›´å¤§çš„åŸºç¡€æ¨¡å‹ï¼Œè½¬å‘æ„å»ºæ›´èªæ˜çš„æ™ºèƒ½ä½“åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰ç³»ç»Ÿæ€§ã€é‡å®è·µçš„æ•™ç¨‹å´æåº¦åŒ®ä¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘èµ·äº† Hello-Agents é¡¹ç›®ï¼Œå¸Œæœ›èƒ½ä¸ºç¤¾åŒºæä¾›ä¸€æœ¬ä»é›¶å¼€å§‹ã€ç†è®ºä¸å®æˆ˜å¹¶é‡çš„æ™ºèƒ½ä½“ç³»ç»Ÿæ„å»ºæŒ‡å—ã€‚</p><p>â€ƒâ€ƒHello-Agents æ˜¯ Datawhale ç¤¾åŒºçš„ã€‚å¦‚ä»Š Agent æ„å»ºä¸»è¦åˆ†ä¸ºä¸¤æ´¾ï¼Œä¸€æ´¾æ˜¯ Difyï¼ŒCozeï¼Œn8n è¿™ç±»è½¯ä»¶å·¥ç¨‹ç±» Agentï¼Œå…¶æœ¬è´¨æ˜¯æµç¨‹é©±åŠ¨çš„è½¯ä»¶å¼€å‘ï¼ŒLLM ä½œä¸ºæ•°æ®å¤„ç†çš„åç«¯ï¼›å¦ä¸€æ´¾åˆ™æ˜¯ AI åŸç”Ÿçš„ Agentï¼Œå³çœŸæ­£ä»¥ AI é©±åŠ¨çš„ Agentã€‚æœ¬æ•™ç¨‹æ—¨åœ¨å¸¦é¢†å¤§å®¶æ·±å…¥ç†è§£å¹¶æ„å»ºåè€…â€”â€”çœŸæ­£çš„ AI Native Agentã€‚æ•™ç¨‹å°†å¸¦é¢†ä½ ç©¿é€æ¡†æ¶è¡¨è±¡ï¼Œä»æ™ºèƒ½ä½“çš„æ ¸å¿ƒåŸç†å‡ºå‘ï¼Œæ·±å…¥å…¶æ ¸å¿ƒæ¶æ„ï¼Œç†è§£å…¶ç»å…¸èŒƒå¼ï¼Œå¹¶æœ€ç»ˆäº²æ‰‹æ„å»ºèµ·å±äºè‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœ€å¥½çš„å­¦ä¹ æ–¹å¼å°±æ˜¯åŠ¨æ‰‹å®è·µã€‚å¸Œæœ›è¿™æœ¬æ•™ç¨‹èƒ½æˆä¸ºä½ æ¢ç´¢æ™ºèƒ½ä½“ä¸–ç•Œçš„èµ·ç‚¹ï¼Œèƒ½å¤Ÿä»ä¸€åå¤§è¯­è¨€æ¨¡å‹çš„\"ä½¿ç”¨è€…\"ï¼Œèœ•å˜ä¸ºä¸€åæ™ºèƒ½ä½“ç³»ç»Ÿçš„\"æ„å»ºè€…\"ã€‚</p><p>å¦‚æœæ‚¨å¸Œæœ›åœ¨æœ¬åœ°é˜…è¯»æˆ–è´¡çŒ®å†…å®¹ï¼Œè¯·å‚è€ƒä¸‹æ–¹çš„å­¦ä¹ æŒ‡å—ã€‚</p><ul><li>ğŸ“–  å®Œå…¨å…è´¹å­¦ä¹ æœ¬é¡¹ç›®æ‰€æœ‰å†…å®¹ï¼Œä¸ç¤¾åŒºå…±åŒæˆé•¿</li><li>ğŸ”  æ·±å…¥ç†è§£æ™ºèƒ½ä½“çš„æ¦‚å¿µã€å†å²ä¸ç»å…¸èŒƒå¼</li><li>ğŸ—ï¸  æŒæ¡çƒ­é—¨ä½ä»£ç å¹³å°å’Œæ™ºèƒ½ä½“ä»£ç æ¡†æ¶çš„ä½¿ç”¨</li><li>âš™ï¸  ä¸€æ­¥æ­¥å®ç°ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Memoryã€åè®®ã€è¯„ä¼°ç­‰ç³»ç»Ÿæ€§æŠ€æœ¯</li><li>ğŸ¤  æŒæ¡ Agentic RLï¼Œä» SFT åˆ° GRPO çš„å…¨æµç¨‹å®æˆ˜è®­ç»ƒ LLM</li><li>ğŸš€  å®æˆ˜å¼€å‘æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€èµ›åšå°é•‡ç­‰ç»¼åˆé¡¹ç›®</li></ul><p>â€ƒâ€ƒæ¬¢è¿å¤§å®¶å°†åœ¨å­¦ä¹  Hello-Agents æˆ– Agent ç›¸å…³æŠ€æœ¯ä¸­çš„ç‹¬åˆ°è§è§£ã€å®è·µæ€»ç»“ï¼Œä»¥ PR çš„å½¢å¼è´¡çŒ®åˆ°ç¤¾åŒºç²¾é€‰ã€‚å¦‚æœæ˜¯ç‹¬ç«‹äºæ­£æ–‡çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥æŠ•ç¨¿è‡³ Extra-Chapterï¼</p><p><em><strong>æœ¬ Hello-Agents PDF æ•™ç¨‹å®Œå…¨å¼€æºå…è´¹ã€‚ä¸ºé˜²æ­¢å„ç±»è¥é”€å·åŠ æ°´å°åè´©å–ç»™å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå­¦è€…ï¼Œæˆ‘ä»¬ç‰¹åœ°åœ¨ PDF æ–‡ä»¶ä¸­é¢„å…ˆæ·»åŠ äº†ä¸å½±å“é˜…è¯»çš„ Datawhale å¼€æºæ ‡å¿—æ°´å°ï¼Œæ•¬è¯·è°…è§£ï½</strong></em></p><p>â€ƒâ€ƒæ¬¢è¿ä½ ï¼Œæœªæ¥çš„æ™ºèƒ½ç³»ç»Ÿæ„å»ºè€…ï¼åœ¨å¼€å¯è¿™æ®µæ¿€åŠ¨äººå¿ƒçš„æ—…ç¨‹ä¹‹å‰ï¼Œè¯·å…è®¸æˆ‘ä»¬ç»™ä½ ä¸€äº›æ¸…æ™°çš„æŒ‡å¼•ã€‚</p><p>â€ƒâ€ƒæœ¬é¡¹ç›®å†…å®¹å…¼é¡¾ç†è®ºä¸å®æˆ˜ï¼Œæ—¨åœ¨å¸®åŠ©ä½ ç³»ç»Ÿæ€§åœ°æŒæ¡ä»å•ä¸ªæ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å¼€å‘å…¨æµç¨‹ã€‚å› æ­¤ï¼Œå°¤å…¶é€‚åˆæœ‰ä¸€å®šç¼–ç¨‹åŸºç¡€çš„  ä»¥åŠå¯¹å‰æ²¿ AI æŠ€æœ¯æŠ±æœ‰æµ“åšå…´è¶£çš„ ã€‚åœ¨å­¦ä¹ æœ¬é¡¹ç›®ä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä½ å…·å¤‡åŸºç¡€çš„ Python ç¼–ç¨‹èƒ½åŠ›ï¼Œå¹¶å¯¹å¤§è¯­è¨€æ¨¡å‹æœ‰åŸºæœ¬çš„æ¦‚å¿µæ€§äº†è§£ï¼ˆä¾‹å¦‚ï¼ŒçŸ¥é“å¦‚ä½•é€šè¿‡ API è°ƒç”¨ä¸€ä¸ª LLMï¼‰ã€‚é¡¹ç›®çš„é‡ç‚¹æ˜¯åº”ç”¨ä¸æ„å»ºï¼Œå› æ­¤ä½ æ— éœ€å…·å¤‡æ·±åšçš„ç®—æ³•æˆ–æ¨¡å‹è®­ç»ƒèƒŒæ™¯ã€‚</p><p>â€ƒâ€ƒé¡¹ç›®åˆ†ä¸ºäº”å¤§éƒ¨åˆ†ï¼Œæ¯ä¸€éƒ¨åˆ†éƒ½æ˜¯é€šå¾€ä¸‹ä¸€é˜¶æ®µçš„åšå®é˜¶æ¢¯ï¼š</p><ul><li><p>ï¼ˆç¬¬ä¸€ç« ï½ç¬¬ä¸‰ç« ï¼‰ï¼Œæˆ‘ä»¬å°†ä»æ™ºèƒ½ä½“çš„å®šä¹‰ã€ç±»å‹ä¸å‘å±•å†å²è®²èµ·ï¼Œä¸ºä½ æ¢³ç†\"æ™ºèƒ½ä½“\"è¿™ä¸€æ¦‚å¿µçš„æ¥é¾™å»è„‰ã€‚éšåï¼Œæˆ‘ä»¬ä¼šå¿«é€Ÿå·©å›ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œä¸ºä½ çš„å®è·µä¹‹æ—…æ‰“ä¸‹åšå®çš„ç†è®ºåœ°åŸºã€‚</p></li><li><p>ï¼ˆç¬¬å››ç« ï½ç¬¬ä¸ƒç« ï¼‰ï¼Œè¿™æ˜¯ä½ åŠ¨æ‰‹å®è·µçš„èµ·ç‚¹ã€‚ä½ å°†äº²æ‰‹å®ç° ReAct ç­‰ç»å…¸èŒƒå¼ï¼Œä½“éªŒ Coze ç­‰ä½ä»£ç å¹³å°çš„ä¾¿æ·ï¼Œå¹¶æŒæ¡ Langgraph ç­‰ä¸»æµæ¡†æ¶çš„åº”ç”¨ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜ä¼šå¸¦ä½ ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè®©ä½ å…¼å…·â€œç”¨è½®å­â€ä¸â€œé€ è½®å­â€çš„èƒ½åŠ›ã€‚</p></li><li><p>ï¼ˆç¬¬å…«ç« ï½ç¬¬åäºŒç« ï¼‰ï¼Œåœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œä½ çš„æ™ºèƒ½ä½“å°†â€œå­¦ä¼šâ€æ€è€ƒä¸åä½œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬äºŒéƒ¨åˆ†çš„è‡ªç ”æ¡†æ¶ï¼Œæ·±å…¥æ¢ç´¢è®°å¿†ä¸æ£€ç´¢ã€ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Agent è®­ç»ƒç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¶å­¦ä¹ å¤šæ™ºèƒ½ä½“é—´çš„é€šä¿¡åè®®ã€‚æœ€ç»ˆï¼Œä½ å°†æŒæ¡è¯„ä¼°æ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½çš„ä¸“ä¸šæ–¹æ³•ã€‚</p></li><li><p>ï¼ˆç¬¬åä¸‰ç« ï½ç¬¬åäº”ç« ï¼‰ï¼Œè¿™é‡Œæ˜¯ç†è®ºä¸å®è·µçš„äº¤æ±‡ç‚¹ã€‚ä½ å°†æŠŠæ‰€å­¦èä¼šè´¯é€šï¼Œäº²æ‰‹æ‰“é€ æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œä¹ƒè‡³ä¸€ä¸ªæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€çš„èµ›åšå°é•‡ï¼Œåœ¨çœŸå®æœ‰è¶£çš„é¡¹ç›®ä¸­æ·¬ç‚¼ä½ çš„æ„å»ºèƒ½åŠ›ã€‚</p></li><li><p>ï¼ˆç¬¬åå…­ç« ï¼‰ï¼Œåœ¨æ—…ç¨‹çš„ç»ˆç‚¹ï¼Œä½ å°†è¿æ¥ä¸€ä¸ªæ¯•ä¸šè®¾è®¡ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€å±äºä½ è‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ï¼Œå…¨é¢æ£€éªŒä½ çš„å­¦ä¹ æˆæœã€‚æˆ‘ä»¬è¿˜å°†ä¸ä½ ä¸€åŒå±•æœ›æ™ºèƒ½ä½“çš„æœªæ¥ï¼Œæ¢ç´¢æ¿€åŠ¨äººå¿ƒçš„å‰æ²¿æ–¹å‘ã€‚</p></li></ul><p>â€ƒâ€ƒæ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªé£é€Ÿå‘å±•ä¸”æåº¦ä¾èµ–å®è·µçš„é¢†åŸŸã€‚ä¸ºäº†è·å¾—æœ€ä½³çš„å­¦ä¹ æ•ˆæœï¼Œæˆ‘ä»¬åœ¨é¡¹ç›®çš„æ–‡ä»¶å¤¹å†…æä¾›äº†é…å¥—çš„å…¨éƒ¨ä»£ç ï¼Œå¼ºçƒˆå»ºè®®ä½ ã€‚è¯·åŠ¡å¿…äº²æ‰‹è¿è¡Œã€è°ƒè¯•ç”šè‡³ä¿®æ”¹é¡¹ç›®é‡Œæä¾›çš„æ¯ä¸€ä»½ä»£ç ã€‚æ¬¢è¿ä½ éšæ—¶å…³æ³¨ Datawhale ä»¥åŠå…¶ä»– Agent ç›¸å…³ç¤¾åŒºï¼Œå½“é‡åˆ°é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥éšæ—¶åœ¨æœ¬é¡¹ç›®çš„ issue åŒºæé—®ã€‚</p><p>â€ƒâ€ƒç°åœ¨ï¼Œå‡†å¤‡å¥½è¿›å…¥æ™ºèƒ½ä½“çš„å¥‡å¦™ä¸–ç•Œäº†å—ï¼Ÿè®©æˆ‘ä»¬å³åˆ»å¯ç¨‹ï¼</p><ul><li>è§†é¢‘è¯¾ç¨‹é™†ç»­æ”¾å‡ºï¼ˆå°†ä¼šæ›´åŠ ç»†è‡´ï¼Œå®è·µè¯¾å¸¦é¢†å¤§å®¶ä»è®¾è®¡æ€è·¯åˆ°å®æ–½ï¼Œæˆäººä»¥é±¼ä¹Ÿæˆäººä»¥æ¸”ï¼‰</li><li>å®Œå–„HelloAgentsæ¡†æ¶ï¼Œå¼€å±•Devåˆ†æ”¯ç»§ç»­ç»´æŠ¤ï¼Œå…¼å®¹å­¦ä¹ ç‰ˆæœ¬ã€‚</li><li>æ„Ÿè°¢å¤§å®¶åŠ©åŠ›2W Star! è¾¾åˆ°3W Starå°†ä¼šæ›´æ–°ç»­ä½œï¼Œã€Šä»é›¶å¼€å§‹è®­ç»ƒæ™ºèƒ½ä½“ã€‹ï¼Œå¸®åŠ©æ¯ä¸€ä¸ªå­¦ä¹ è€…æŒæ¡ä»é›¶åˆ°ä¸€è®­ç»ƒè‡ªå®šä¹‰åœºæ™¯æ™ºèƒ½ä½“æ¨¡å‹çš„èƒ½åŠ›ã€‚</li></ul><ul><li>ğŸ›  - å‘ç°å†…å®¹æˆ–ä»£ç é—®é¢˜ï¼Œè¯·æäº¤ Issue</li><li>ğŸ“  - å¸®åŠ©æ”¹è¿›æ•™ç¨‹ï¼Œæäº¤ä½ çš„ Pull Request</li><li>âœï¸  - åœ¨\"ç¤¾åŒºè´¡çŒ®ç²¾é€‰\"ä¸­åˆ†äº«ä½ çš„å­¦ä¹ ç¬”è®°å’Œé¡¹ç›®</li></ul><ul></ul><div align=\"center\"><p>â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼</p></div><div align=\"center\"><img src=\"https://github.com/datawhalechina/%E8%AF%BB%E8%80%85%E7%BE%A4%E4%BA%8C%E7%BB%B4%E7%A0%81.png\" alt=\"è¯»è€…ç¾¤äºŒç»´ç \" width=\"30%\"></div><div align=\"center\"><img src=\"https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/datawhale.png\" alt=\"Datawhale\" width=\"30%\"><p>æ‰«æäºŒç»´ç å…³æ³¨ Datawhale å…¬ä¼—å·ï¼Œè·å–æ›´å¤šä¼˜è´¨å¼€æºå†…å®¹</p></div>",
      "contentLength": 5613,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/304993fad23244acec62f3a713267cafb86815ac8cab6942820b5f5d11a324f7/datawhalechina/hello-agents",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "liyupi/ai-guide",
      "url": "https://github.com/liyupi/ai-guide",
      "date": 1772074985,
      "author": "",
      "guid": 48394,
      "unread": true,
      "content": "<p>ç¨‹åºå‘˜é±¼çš®çš„ AI èµ„æºå¤§å…¨ + Vibe Coding é›¶åŸºç¡€æ•™ç¨‹ï¼Œåˆ†äº«å¤§æ¨¡å‹é€‰æ‹©æŒ‡å—ï¼ˆDeepSeek / GPT / Gemini / Claudeï¼‰ã€æœ€æ–° AI èµ„è®¯ã€Prompt æç¤ºè¯å¤§å…¨ã€AI çŸ¥è¯†ç™¾ç§‘ï¼ˆRAG / MCP / A2Aï¼‰ã€AI ç¼–ç¨‹æ•™ç¨‹ã€AI å·¥å…·ç”¨æ³•ï¼ˆCursor / Claude Code / OpenClaw / TRAE / Lovable / Agent Skillsï¼‰ã€AI å¼€å‘æ¡†æ¶æ•™ç¨‹ï¼ˆSpring AI / LangChainï¼‰ã€AI äº§å“å˜ç°æŒ‡å—ï¼Œå¸®ä½ å¿«é€ŸæŒæ¡ AI æŠ€æœ¯ï¼Œèµ°åœ¨æ—¶ä»£å‰æ²¿ã€‚æœ¬é¡¹ç›®ä¸ºå¼€æºæ–‡æ¡£ç‰ˆæœ¬ï¼Œå·²å‡çº§ä¸ºé±¼çš® AI å¯¼èˆªç½‘ç«™</p><p align=\"center\"><b>å®Œå…¨å…è´¹å¼€æ”¾çš„ AI çŸ¥è¯†å…±äº«å¹³å° | å‡å°‘ä¿¡æ¯å·®ï¼Œè®©æ¯ä¸ªäººéƒ½èƒ½äº«å—æŠ€æœ¯çº¢åˆ©</b></p><p>è¿™æ˜¯ä¸€ä¸ª  çš„ AI çŸ¥è¯†å…±äº«å¹³å°ï¼Œæ±‡æ€»æ•´åˆç›®å‰çƒ­é—¨çš„ AI å·¥å…·ç›¸å…³ä¿¡æ¯ï¼ŒåŒ…æ‹¬äº§å“ä»‹ç»ã€ä½¿ç”¨æŒ‡å—ã€å·¥å…·æµ‹è¯„ã€æŠ€å·§åˆ†äº«ã€åº”ç”¨åœºæ™¯ã€AI å˜ç°ã€è¡Œä¸šèµ„è®¯ã€æ•™ç¨‹èµ„æºç­‰ä¸€ç³»åˆ—å†…å®¹ã€‚</p><p>é±¼çš®å¸Œæœ›å¸¦é¢†å¤§å®¶æ‰“ç ´ AI æŠ€æœ¯çš„ä¿¡æ¯å£å’ï¼Œè®©æ¯ä¸ªäººéƒ½èƒ½å¹³ç­‰è·å– AI æ—¶ä»£çš„å·¥å…·ä¸è®¤çŸ¥ï¼Œåˆ©ç”¨ç§‘æŠ€è®©ç”Ÿæ´»æ›´ç¾å¥½ã€‚</p><h2>ğŸ”¥ é±¼çš®çš„ Vibe Coding é›¶åŸºç¡€å…¥é—¨æ•™ç¨‹</h2><p>å¦‚ä»Š  å·²ç»ç«éå…¨ç½‘ï¼ä¸ä»…æ˜¯ç¨‹åºå‘˜ï¼Œè¿è®¾è®¡å¸ˆã€äº§å“è¿è¥ã€ç”šè‡³å®Œå…¨ä¸æ‡‚æŠ€æœ¯çš„äººéƒ½å¼€å§‹ç”¨ Vibe Coding å®ç°è‡ªå·±çš„æƒ³æ³•ï¼Œç”¨ AI åšå‡ºäº†è‡ªå·±çš„äº§å“å¹¶ç›ˆåˆ©å˜ç°ã€‚</p><p><strong>å¸®åŠ©ä»»ä½•äººå¿«é€ŸæŒæ¡ Vibe Codingï¼Œå“ªæ€•é›¶åŸºç¡€ï¼Œä¹Ÿèƒ½å¿«é€Ÿå¼€å‘ä¸Šçº¿è‡ªå·±çš„äº§å“å¹¶ç›ˆåˆ©ã€‚</strong></p><p>è‡­ä¸è¦è„¸ä¸€ä¸‹ï¼Œæˆ‘æ•¢è¯´è¿™å¥—å…è´¹æ•™ç¨‹åŠæ‰“ 90% çš„ä»˜è´¹ Vibe Coding å†…å®¹ã€‚</p><p>æˆ‘ç²¾å¿ƒæ¢³ç†äº†å†…å®¹ç»“æ„ï¼Œè®©ä½ èƒ½å¤Ÿä¸€æ¡é¾™å­¦ä¹ ï¼Œæˆ–è€…å¿«é€Ÿæ‰¾åˆ°é€‚åˆè‡ªå·±é˜…è¯»çš„å†…å®¹ã€‚</p><ul><li>åŸºç¡€å¿…è¯»ï¼šå¸®ä½ å¿«é€Ÿç†è§£ Vibe Coding å¹¶ä¸Šæ‰‹å®è·µï¼Œ10 åˆ†é’Ÿåšå‡ºç¬¬ä¸€ä¸ªä½œå“</li><li>ç¼–ç¨‹å·¥å…·ï¼šå¸®ä½ é€‰æ‹©é€‚åˆè‡ªå·±çš„ AI ç¼–ç¨‹å·¥å…·ï¼ŒåŒ…æ‹¬ AI æ¨¡å‹é€‰æ‹©ã€AI é›¶ä»£ç å¹³å°ã€AI æ™ºèƒ½ä½“å¹³å°ã€AI ä»£ç ç¼–è¾‘å™¨ã€AI å‘½ä»¤è¡Œå·¥å…·ã€IDE æ’ä»¶ç­‰</li><li>é¡¹ç›®å®æˆ˜ï¼šæ‰‹æŠŠæ‰‹å¸¦ä½ ä» 0 åˆ° 1 åšå‡ºçœŸå®å¯ç”¨çš„äº§å“ï¼Œè¦†ç›–ä¸ªäººå·¥å…·ã€AI åº”ç”¨ã€å…¨æ ˆåº”ç”¨ã€å°ç¨‹åºç­‰å¤šç§ç±»å‹</li><li>ç»éªŒæŠ€å·§ï¼šå¸®ä½ æå‡ Vibe Coding æ•ˆç‡å’Œè´¨é‡ï¼ŒåŒ…æ‹¬æ ¸å¿ƒå¿ƒæ³•ã€å¯¹è¯å·¥ç¨‹ã€ä¸Šä¸‹æ–‡ç®¡ç†ã€å¹»è§‰å¤„ç†ã€ä»£ç è´¨é‡ä¿éšœç­‰</li><li>äº§å“å˜ç°ï¼šæ•™ä½ å¦‚ä½•è®©äº§å“äº§ç”Ÿä»·å€¼ï¼Œæ¶µç›–éœ€æ±‚åˆ†æã€æŠ€æœ¯é€‰å‹ã€æ¶æ„è®¾è®¡ã€ç›ˆåˆ©æ¨¡å¼ã€SEO ä¼˜åŒ–ã€è‡ªåª’ä½“è¿è¥ç­‰</li><li>ç¼–ç¨‹å­¦ä¹ ï¼šä¸ºæƒ³æ·±å…¥å­¦ä¹ ç¼–ç¨‹çš„åŒå­¦å‡†å¤‡çš„è¿›é˜¶å†…å®¹ï¼ŒåŒ…æ‹¬å­¦ä¹ è·¯çº¿ã€çŸ¥è¯†ç™¾ç§‘ã€èµ„æºå¤§å…¨ã€MCP å¼€å‘ã€é¢è¯•åˆ·é¢˜ç­‰</li><li>èµ„æºå®åº“ï¼šæ±‡é›†å„ç§å®ç”¨èµ„æºï¼ŒåŒ…æ‹¬å·¥å…·å¤§å…¨ã€æç¤ºè¯æ¨¡æ¿ã€AI æ¦‚å¿µå¤§å…¨ã€Vibe Coding å¸¸è§é—®é¢˜ç­‰</li></ul><ul><li>ç¬¬ 1 å¤©ï¼šè¯»å®ŒåŸºç¡€å¿…è¯»ï¼Œç†è§£ Vibe Coding å¹¶åšå‡ºç¬¬ä¸€ä¸ªä½œå“</li><li>ç¬¬ 1-2 å‘¨ï¼šå­¦ä¹  AI ç¼–ç¨‹å·¥å…· + åšå‡ ä¸ªç®€å•é¡¹ç›®</li></ul><ul><li>ç¬¬ 1 å‘¨ï¼šå­¦ä¹ ä¸»æµ AI ç¼–ç¨‹å·¥å…·ï¼Œå°è¯•é‡æ„ä¹‹å‰çš„é¡¹ç›®</li></ul><pre><code>ai-guide/\nâ”œâ”€â”€ ğŸ”¥ Vibe Coding é›¶åŸºç¡€æ•™ç¨‹/     # é‡ç£…æ•™ç¨‹ï¼Œå¼ºçƒˆæ¨è\nâ”‚   â”œâ”€â”€ 00 Vibe Coding ç®€ä»‹\nâ”‚   â”œâ”€â”€ 01 å¿«é€Ÿä¸Šæ‰‹ Vibe Coding\nâ”‚   â”œâ”€â”€ 10 ç¼–ç¨‹å·¥å…·/\nâ”‚   â”œâ”€â”€ 20 é¡¹ç›®å®æˆ˜/\nâ”‚   â”œâ”€â”€ 30 ç»éªŒæŠ€å·§/\nâ”‚   â”œâ”€â”€ 40 ç¼–ç¨‹å­¦ä¹ /\nâ”‚   â”œâ”€â”€ 50 äº§å“å˜ç°/\nâ”‚   â””â”€â”€ ...\nâ”œâ”€â”€ AI/\nâ”‚   â”œâ”€â”€ é±¼çš®çš„ AI æŒ‡å—/            # AI æ ¸å¿ƒæ¦‚å¿µã€å·¥å…·ã€æŠ€å·§\nâ”‚   â”œâ”€â”€ å…³äº DeepSeek/             # DeepSeek åŸºç¡€çŸ¥è¯†\nâ”‚   â”œâ”€â”€ DeepSeek ä½¿ç”¨æŒ‡å—/         # å®‰è£…ã€ä½¿ç”¨ã€æŠ€å·§å¤§å…¨\nâ”‚   â”œâ”€â”€ DeepSeek æŠ€æœ¯è§£æ/         # æ·±åº¦æŠ€æœ¯è§£è¯»\nâ”‚   â”œâ”€â”€ DeepSeek èµ„æºæ±‡æ€»/         # èµ„æºã€æ•™ç¨‹ã€å¼€æºé¡¹ç›®\nâ”‚   â”œâ”€â”€ AI åº”ç”¨åœºæ™¯/               # åˆ›æ„è®¾è®¡ã€æ•ˆç‡æå‡ã€ç¼–ç¨‹å¼€å‘\nâ”‚   â”œâ”€â”€ AI é¡¹ç›®æ•™ç¨‹/               # å®æˆ˜é¡¹ç›®æ•™ç¨‹\nâ”‚   â””â”€â”€ AI è¡Œä¸šèµ„è®¯/               # æœ€æ–°è¡Œä¸šåŠ¨æ€\nâ””â”€â”€ äº§å“æœåŠ¡/                      # é±¼çš®çš„äº§å“å’ŒæœåŠ¡\n</code></pre><p>æ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„ AI äº¤æµç¾¤ï¼Œå…³æ³¨å…¬ä¼—å·ï¼šï¼Œè·å–æ›´å¤šæœ€ä¸€æ‰‹ AI èµ„è®¯ï¼Œä¸€èµ·æ¢è®¨ AI åº”ç”¨å®è·µã€‚</p><p>å¦‚æœä½ ä¹Ÿæ˜¯ AI æ¢ç´¢è€…ã€çˆ±å¥½è€…ï¼Œå¹¶ä¸”ä¹äºåˆ†äº«å’Œæ²‰æ·€ä½ çš„çŸ¥è¯†å’Œå¥‡æ€å¦™æƒ³ï¼Œæ¬¢è¿åŠ å…¥è¿›æ¥å‚ä¸çŸ¥è¯†åº“å…±å»ºï¼Œä¸€èµ·æ„å»ºå±äºæ‰€æœ‰äººçš„ AI çŸ¥è¯†å®è—ï¼</p><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸€ä¸ª  â­ï¸ æ”¯æŒä¸€ä¸‹ï¼</p><p>è¿™å¥—æ•™ç¨‹å®Œå…¨å…è´¹å¼€æºï¼Œå¸Œæœ›èƒ½å¸®æ›´å¤šäººæ‰“å¼€ Vibe Coding çš„å¤§é—¨ã€‚</p><p>ä½†æ¯•ç«Ÿæ˜¯ä¸€ä¸ªäººç¼–å†™çš„ï¼Œä¼šæœ‰ä¸è¶³çš„åœ°æ–¹ï¼Œæˆ‘ä¼šæŒç»­æ›´æ–°å’Œå®Œå–„å†…å®¹ã€‚</p><p><strong>å¦‚æœè¿™å¥—æ•™ç¨‹å¯¹ä½ æœ‰å¸®åŠ©çš„è¯ï¼Œå¸Œæœ›èƒ½ç‚¹èµæˆ–è€… Star â­ï¸ æ”¯æŒä¸€ä¸‹ï¼</strong></p><p>åˆ«çŠ¹è±«ï¼Œç°åœ¨å°±æ‰“å¼€æ•™ç¨‹ï¼Œ10 åˆ†é’Ÿåä½ å°±èƒ½åšå‡ºç¬¬ä¸€ä¸ªä½œå“ï¼Œè·Ÿç€é±¼çš®ä¸€èµ·å¼€å¯ Vibe Coding ä¹‹æ—…å§ï¼ğŸ›«</p>",
      "contentLength": 4611,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/384c7cb40b76df20002586d01fa7d3e12faf3209db2613273ff7f59f1f2966b2/liyupi/ai-guide",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ruvnet/ruvector",
      "url": "https://github.com/ruvnet/ruvector",
      "date": 1771988741,
      "author": "",
      "guid": 48065,
      "unread": true,
      "content": "<p>RuVector is a High Performance, Real-Time, Self-Learning, Vector Graph Neural Network, and Database built in Rust.</p><p><strong>The vector database that gets smarter the more you use it â€” and now ships as a cognitive container.</strong></p><p>Most vector databases are static â€” they store embeddings and search them. That's it. RuVector is different: it learns from every query, runs LLMs locally, scales horizontally, boots as a Linux microservice from a single file, and costs nothing to operate.</p><table><thead><tr></tr></thead><tbody><tr><td>ğŸ§  <strong>Search improves over time</strong></td><td>âœ… The more you search, the better results get</td></tr><tr><td>âœ… Run AI models on your own machine for free</td></tr><tr><td>âœ… Ask questions about relationships between data</td></tr><tr><td>âœ… System watches what works and gets smarter</td></tr><tr><td>âœ… Update connections instantly, no downtime</td></tr><tr><td>âœ… One file â€” copy it anywhere and it just works</td></tr><tr><td>ğŸ” </td><td>âœ… Every operation is cryptographically recorded</td></tr><tr><td>âœ… Runs in browsers, phones, IoT, and bare metal</td></tr><tr><td>âœ… Free forever â€” open source (MIT)</td></tr><tr><td>âœ… Add nodes freely, no per-vector fees</td></tr><tr><td>âœ… Branch your data like code â€” only changes are copied</td></tr><tr><td>âœ… O(log n) sparse linear systems, PageRank, spectral methods</td></tr><tr><td>ğŸ”¬ <strong>Proof-Gated Graph Transformers</strong></td><td>âœ… 8 verified modules: physics, bio, manifold, temporal, economic</td></tr></tbody></table><p><strong>One package. Everything included:</strong> vector search, graph queries, GNN learning, <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer\">proof-gated graph transformers</a> (8 verified modules â€” physics, biological, manifold, temporal, economic), distributed clustering, local LLMs, 46 attention mechanisms, cognitive containers (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">RVF</a> â€” self-booting  files with eBPF, witness chains, and COW branching), and WASM support.</p><p><em>Think of it as: <strong>Pinecone + Neo4j + PyTorch + llama.cpp + postgres + etcd + Docker</strong> â€” in one Rust package.</em></p><p><em>The <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">RVF cognitive container</a> is the Docker part: a single  file that stores vectors, ships models, boots as a Linux microservice in 125 ms, accelerates queries via eBPF, branches like Git at cluster granularity, and proves every operation through a cryptographic witness chain â€” all without external dependencies.</em></p><h3>Ecosystem: AI Agent Orchestration</h3><p>RuVector powers two major AI orchestration platforms:</p><table><tbody><tr><td>Enterprise multi-agent orchestration for Claude Code</td><td><code>npx @claude-flow/cli@latest</code></td></tr><tr><td>Standalone AI agent framework (any LLM provider)</td></tr></tbody></table><p>Traditional vector search:</p><pre><code>Query â†’ HNSW Index â†’ Top K Results\n</code></pre><pre><code>Query â†’ HNSW Index â†’ GNN Layer â†’ Enhanced Results\n                â†‘                      â”‚\n                â””â”€â”€â”€â”€ learns from â”€â”€â”€â”€â”€â”˜\n</code></pre><ol><li>Takes your query and its nearest neighbors</li><li>Applies multi-head attention to weigh which neighbors matter</li><li>Updates representations based on graph structure</li><li>Returns better-ranked results</li></ol><p>Over time, frequently-accessed paths get reinforced, making common queries faster and more accurate.</p><pre><code># Interactive installer - lists all packages\nnpx ruvector install\n\n# Or install directly\nnpm install ruvector\nnpx ruvector\n\n# Self-learning hooks for Claude Code\nnpx @ruvector/cli hooks init\nnpx @ruvector/cli hooks install\n\n# LLM runtime (SONA learning, HNSW memory)\nnpm install @ruvector/ruvllm\n</code></pre><pre><code># Install\nnpm install ruvector\n\n# Or try instantly\nnpx ruvector\n</code></pre><table><tbody><tr><td><code>npm install @ruvector/sona</code></td></tr><tr><td><code>npm install @ruvector/rvdna</code></td></tr><tr><td><code>npm install @ruvector/rvf</code></td></tr><tr><td><code>npm install ruvector-wasm</code></td></tr><tr><td><code>cargo add ruvector-core ruvector-graph ruvector-gnn</code></td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td><code>npx @ruvector/rvf-mcp-server --transport stdio</code></td></tr></tbody></table><pre><code># Run tests\ncargo test --workspace\n\n# Run benchmarks\ncargo bench --workspace\n\n# Build WASM\ncargo build -p ruvector-gnn-wasm --target wasm32-unknown-unknown\n</code></pre><p>MIT License â€” free for commercial and personal use.</p>",
      "contentLength": 3459,
      "flags": null,
      "enclosureUrl": "https://repository-images.githubusercontent.com/1099547803/948d2495-1db9-47f6-9ea1-f7f977343e5f",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "GVCLab/PersonaLive",
      "url": "https://github.com/GVCLab/PersonaLive",
      "date": 1771988741,
      "author": "",
      "guid": 48066,
      "unread": true,
      "content": "<p>[CVPR 2026] PersonaLive! : Expressive Portrait Image Animation for Live Streaming</p><img src=\"https://raw.githubusercontent.com/GVCLab/PersonaLive/main/assets/overview.png\" alt=\"Image 1\" width=\"100%\"><p>We present PersonaLive, a  and  diffusion framework capable of generating  portrait animations.</p><pre><code># clone this repo\ngit clone https://github.com/GVCLab/PersonaLive\ncd PersonaLive\n\n# Create conda environment\nconda create -n personalive python=3.10\nconda activate personalive\n\n# Install packages with pip\npip install -r requirements_base.txt\n</code></pre><pre><code>python tools/download_weights.py\n</code></pre><p>Option 2: Download pre-trained weights into the  folder from one of the below URLs:</p><p>Finally, these weights should be organized as follows:</p><pre><code>pretrained_weights\nâ”œâ”€â”€ onnx\nâ”‚   â”œâ”€â”€ unet_opt\nâ”‚   â”‚   â”œâ”€â”€ unet_opt.onnx\nâ”‚   â”‚   â””â”€â”€ unet_opt.onnx.data\nâ”‚   â””â”€â”€ unet\nâ”œâ”€â”€ personalive\nâ”‚   â”œâ”€â”€ denoising_unet.pth\nâ”‚   â”œâ”€â”€ motion_encoder.pth\nâ”‚   â”œâ”€â”€ motion_extractor.pth\nâ”‚   â”œâ”€â”€ pose_guider.pth\nâ”‚   â”œâ”€â”€ reference_unet.pth\nâ”‚   â””â”€â”€ temporal_module.pth\nâ”œâ”€â”€ sd-vae-ft-mse\nâ”‚   â”œâ”€â”€ diffusion_pytorch_model.bin\nâ”‚   â””â”€â”€ config.json\nâ”œâ”€â”€ sd-image-variations-diffusers\nâ”‚   â”œâ”€â”€ image_encoder\nâ”‚   â”‚   â”œâ”€â”€ pytorch_model.bin\nâ”‚   â”‚   â””â”€â”€ config.json\nâ”‚   â”œâ”€â”€ unet\nâ”‚   â”‚   â”œâ”€â”€ diffusion_pytorch_model.bin\nâ”‚   â”‚   â””â”€â”€ config.json\nâ”‚   â””â”€â”€ model_index.json\nâ””â”€â”€ tensorrt\n    â””â”€â”€ unet_work.engine\n</code></pre><p>Run offline inference with the default configuration:</p><pre><code>python inference_offline.py\n</code></pre><ul><li>: Max number of frames to generate. (Default: 100)</li><li>: Enable xFormers memory efficient attention. (Default: True)</li><li>: Enable streaming generation strategy. (Default: True)</li><li>: Path to a specific reference image. Overrides settings in config.</li><li>: Path to a specific driving video. Overrides settings in config.</li></ul><p>âš ï¸ Note for RTX 50-Series (Blackwell) Users: xformers is not yet fully compatible with the new architecture. To avoid crashes, please disable it by running:</p><pre><code>python inference_offline.py --use_xformers False\n</code></pre><pre><code># install Node.js 18+\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash\nnvm install 18\n\nsource web_start.sh\n</code></pre><h4>ğŸï¸ Acceleration (Optional)</h4><p>Converting the model to TensorRT can significantly speed up inference (~ 2x âš¡ï¸). Building the engine may take about  depending on your device. Note that TensorRT optimizations may lead to slight variations or a small drop in output quality.</p><pre><code># Install packages with pip\npip install -r requirements_trt.txt\n\n# Converting the model to TensorRT\npython torch2trt.py\n</code></pre><p>ğŸ’¡ <strong>PyCUDA Installation Issues</strong>: If you encounter a \"Failed to build wheel for pycuda\" error during the installation above, please follow these steps:</p><pre><code># Install PyCUDA manually using Conda (avoids compilation issues):\nconda install -c conda-forge pycuda \"numpy&lt;2.0\"\n\n# Open requirements_trt.txt and comment out or remove the line \"pycuda==2024.1.2\"\n\n# Install other packages with pip\npip install -r requirements_trt.txt\n\n# Converting the model to TensorRT\npython torch2trt.py\n</code></pre><p>âš ï¸ The provided TensorRT model is from an . We recommend  (including H100 users) re-run  locally to ensure best compatibility.</p><pre><code>python inference_online.py --acceleration none (for RTX 50-Series) or xformers or tensorrt\n</code></pre><p>Then open  in your browser. (*If  does not work well, try )</p><p>: Upload Image â¡ï¸ Fuse Reference â¡ï¸ Start Animation â¡ï¸ Enjoy! ğŸ‰</p><p>: Latency varies depending on your device's computing power. You can try the following methods to optimize it:</p><p>Special thanks to the community for providing helpful setups! ğŸ¥‚</p><ul><li><p><strong>Windows + RTX 50-Series Guide</strong>: Thanks to <a href=\"https://github.com/dknos\">@dknos</a> for providing a <a href=\"https://github.com/GVCLab/PersonaLive/issues/10#issuecomment-3662785532\">detailed guide</a> on running this project on Windows with Blackwell GPUs.</p></li><li><p>: Thanks to <a href=\"https://github.com/suruoxi\">@suruoxi</a> for implementing , and to <a href=\"https://github.com/andchir\">@andchir</a> for adding audio merging functionality.</p></li></ul><p>If you find PersonaLive useful for your research, welcome to cite our work using the following BibTeX:</p><pre><code>@article{li2025personalive,\n  title={PersonaLive! Expressive Portrait Image Animation for Live Streaming},\n  author={Li, Zhiyuan and Pun, Chi-Man and Fang, Chen and Wang, Jue and Cun, Xiaodong},\n  journal={arXiv preprint arXiv:2512.11253},\n  year={2025}\n}\n</code></pre>",
      "contentLength": 4122,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/d4680994dab18c7857754af72ccab22cbbcd3bf0883ebeb978c4f888af54e215/GVCLab/PersonaLive",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "openemr/openemr",
      "url": "https://github.com/openemr/openemr",
      "date": 1771988741,
      "author": "",
      "guid": 48067,
      "unread": true,
      "content": "<p>The most popular open source electronic health records and medical practice management solution.</p><p><a href=\"https://open-emr.org\">OpenEMR</a> is a Free and Open Source electronic health records and medical practice management application. It features fully integrated electronic health records, practice management, scheduling, electronic billing, internationalization, free support, a vibrant community, and a whole lot more. It runs on Windows, Linux, Mac OS X, and many other platforms.</p><p>OpenEMR is a leader in healthcare open source software and comprises a large and diverse community of software developers, medical providers and educators with a very healthy mix of both volunteers and professionals. <a href=\"https://open-emr.org/wiki/index.php/FAQ#How_do_I_begin_to_volunteer_for_the_OpenEMR_project.3F\">Join us and learn how to start contributing today!</a></p><blockquote><p>Already comfortable with git? Check out <a href=\"https://raw.githubusercontent.com/openemr/openemr/master/CONTRIBUTING.md\">CONTRIBUTING.md</a> for quick setup instructions and requirements for contributing to OpenEMR by resolving a bug or adding an awesome feature ğŸ˜Š.</p></blockquote><p>Community and Professional support can be found <a href=\"https://open-emr.org/wiki/index.php/OpenEMR_Support_Guide\">here</a>.</p><p>Extensive documentation and forums can be found on the <a href=\"https://open-emr.org\">OpenEMR website</a> that can help you to become more familiar about the project ğŸ“–.</p><h3>Reporting Issues and Bugs</h3><p>Report these on the <a href=\"https://github.com/openemr/openemr/issues\">Issue Tracker</a>. If you are unsure if it is an issue/bug, then always feel free to use the <a href=\"https://community.open-emr.org/\">Forum</a> and <a href=\"https://www.open-emr.org/chat/\">Chat</a> to discuss about the issue ğŸª².</p><h3>Reporting Security Vulnerabilities</h3><p>If using OpenEMR directly from the code repository, then the following commands will build OpenEMR (Node.js version 22.* is required) :</p><pre><code>composer install --no-dev\nnpm install\nnpm run build\ncomposer dump-autoload -o\n</code></pre><p>This project exists thanks to all the people who have contributed. <a href=\"https://raw.githubusercontent.com/openemr/openemr/master/CONTRIBUTING.md\">[Contribute]</a>. <a href=\"https://github.com/openemr/openemr/graphs/contributors\"><img src=\"https://opencollective.com/openemr/contributors.svg?width=890\"></a></p>",
      "contentLength": 1599,
      "flags": null,
      "enclosureUrl": "https://repository-images.githubusercontent.com/679584/58435691-1a0f-4345-8f7c-d6c15875a814",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "HunxByts/GhostTrack",
      "url": "https://github.com/HunxByts/GhostTrack",
      "date": 1771988741,
      "author": "",
      "guid": 48068,
      "unread": true,
      "content": "<p>Useful tool to track location or mobile number</p><p>Useful tool to track location or mobile number, so this tool can be called osint or also information gathering</p><img src=\"https://github.com/HunxByts/GhostTrack/raw/main/asset/bn.png\"><h3>Instalation on Linux (deb)</h3><pre><code>sudo apt-get install git\nsudo apt-get install python3\n</code></pre><pre><code>pkg install git\npkg install python3\n</code></pre><pre><code>git clone https://github.com/HunxByts/GhostTrack.git\ncd GhostTrack\npip3 install -r requirements.txt\npython3 GhostTR.py\n</code></pre><p>Display on the menu </p><img src=\"https://github.com/HunxByts/GhostTrack/blob/main/asset/ip.png \"><p>on the IP Track menu, you can combo with the seeker tool to get the target IP</p><p>Display on the menu </p><img src=\"https://github.com/HunxByts/GhostTrack/raw/main/asset/phone.png\"><p>on this menu you can search for information from the target phone number</p><p>Display on the menu </p><img src=\"https://github.com/HunxByts/GhostTrack/raw/main/asset/User.png\"> on this menu you can search for information from the target username on social media \n",
      "contentLength": 687,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/d771e74d0edc4fa1a3b8241107169b3a8e331edd43bf9e55c164830d98833350/HunxByts/GhostTrack",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "D4Vinci/Scrapling",
      "url": "https://github.com/D4Vinci/Scrapling",
      "date": 1771988741,
      "author": "",
      "guid": 48069,
      "unread": true,
      "content": "<p>ğŸ•·ï¸ An adaptive Web Scraping framework that handles everything from a single request to a full-scale crawl!</p><p>Scrapling is an adaptive Web Scraping framework that handles everything from a single request to a full-scale crawl.</p><p>Its parser learns from website changes and automatically relocates your elements when pages update. Its fetchers bypass anti-bot systems like Cloudflare Turnstile out of the box. And its spider framework lets you scale up to concurrent, multi-session crawls with pause/resume and automatic proxy rotation â€” all in a few lines of Python. One library, zero compromises.</p><p>Blazing fast crawls with real-time stats and streaming. Built by Web Scrapers for Web Scrapers and regular users, there's something for everyone.</p><pre><code>from scrapling.fetchers import Fetcher, AsyncFetcher, StealthyFetcher, DynamicFetcher\nStealthyFetcher.adaptive = True\np = StealthyFetcher.fetch('https://example.com', headless=True, network_idle=True)  # Fetch website under the radar!\nproducts = p.css('.product', auto_save=True)                                        # Scrape data that survives website design changes!\nproducts = p.css('.product', adaptive=True)                                         # Later, if the website structure changes, pass `adaptive=True` to find them!\n</code></pre><p>Or scale up to full crawls</p><pre><code>from scrapling.spiders import Spider, Response\n\nclass MySpider(Spider):\n  name = \"demo\"\n  start_urls = [\"https://example.com/\"]\n\n  async def parse(self, response: Response):\n      for item in response.css('.product'):\n          yield {\"title\": item.css('h2::text').get()}\n\nMySpider().start()\n</code></pre><p><i><sub>Do you want to show your ad here? Click <a href=\"https://github.com/sponsors/D4Vinci\">here</a> and choose the tier that suites you!</sub></i></p><h3>Spiders â€” A Full Crawling Framework</h3><ul><li>ğŸ•·ï¸ : Define spiders with , async  callbacks, and / objects.</li><li>âš¡ : Configurable concurrency limits, per-domain throttling, and download delays.</li><li>ğŸ”„ : Unified interface for HTTP requests, and stealthy headless browsers in a single spider â€” route requests to different sessions by ID.</li><li>ğŸ’¾ : Checkpoint-based crawl persistence. Press Ctrl+C for a graceful shutdown; restart to resume from where you left off.</li><li>ğŸ“¡ : Stream scraped items as they arrive via <code>async for item in spider.stream()</code> with real-time stats â€” ideal for UI, pipelines, and long-running crawls.</li><li>ğŸ›¡ï¸ <strong>Blocked Request Detection</strong>: Automatic detection and retry of blocked requests with customizable logic.</li><li>ğŸ“¦ : Export results through hooks and your own pipeline or the built-in JSON/JSONL with  /  respectively.</li></ul><h3>Advanced Websites Fetching with Session Support</h3><ul><li>: Fast and stealthy HTTP requests with the  class. Can impersonate browsers' TLS fingerprint, headers, and use HTTP/3.</li><li>: Fetch dynamic websites with full browser automation through the  class supporting Playwright's Chromium and Google's Chrome.</li><li>: Advanced stealth capabilities with  and fingerprint spoofing. Can easily bypass all types of Cloudflare's Turnstile/Interstitial with automation.</li><li>: Persistent session support with , , and  classes for cookie and state management across requests.</li><li>: Built-in  with cyclic or custom rotation strategies across all session types, plus per-request proxy overrides.</li><li>: Block requests to specific domains (and their subdomains) in browser-based fetchers.</li><li>: Complete async support across all fetchers and dedicated async session classes.</li></ul><h3>Adaptive Scraping &amp; AI Integration</h3><ul><li>ğŸ”„ : Relocate elements after website changes using intelligent similarity algorithms.</li><li>ğŸ¯ : CSS selectors, XPath selectors, filter-based search, text search, regex search, and more.</li><li>ğŸ” : Automatically locate elements similar to found elements.</li><li>ğŸ¤– <strong>MCP Server to be used with AI</strong>: Built-in MCP server for AI-assisted Web Scraping and data extraction. The MCP server features powerful, custom capabilities that leverage Scrapling to extract targeted content before passing it to the AI (Claude/Cursor/etc), thereby speeding up operations and reducing costs by minimizing token usage. (<a href=\"https://www.youtube.com/watch?v=qyFk3ZNwOxE\">demo video</a>)</li></ul><h3>High-Performance &amp; battle-tested Architecture</h3><ul><li>ğŸš€ : Optimized performance outperforming most Python scraping libraries.</li><li>ğŸ”‹ : Optimized data structures and lazy loading for a minimal memory footprint.</li><li>âš¡ : 10x faster than the standard library.</li><li>ğŸ—ï¸ : Not only does Scrapling have 92% test coverage and full type hints coverage, but it has been used daily by hundreds of Web Scrapers over the past year.</li></ul><h3>Developer/Web Scraper Friendly Experience</h3><ul><li>ğŸ¯ <strong>Interactive Web Scraping Shell</strong>: Optional built-in IPython shell with Scrapling integration, shortcuts, and new tools to speed up Web Scraping scripts development, like converting curl requests to Scrapling requests and viewing requests results in your browser.</li><li>ğŸš€ <strong>Use it directly from the Terminal</strong>: Optionally, you can use Scrapling to scrape a URL without writing a single line of code!</li><li>ğŸ› ï¸ : Advanced DOM traversal with parent, sibling, and child navigation methods.</li><li>ğŸ§¬ : Built-in regex, cleaning methods, and optimized string operations.</li><li>ğŸ“ : Generate robust CSS/XPath selectors for any element.</li><li>ğŸ”Œ : Similar to Scrapy/BeautifulSoup with the same pseudo-elements used in Scrapy/Parsel.</li><li>ğŸ“˜ : Full type hints for excellent IDE support and code completion. The entire codebase is automatically scanned with  and  with each change.</li><li>ğŸ”‹ : With each release, a Docker image containing all browsers is automatically built and pushed.</li></ul><p>Let's give you a quick glimpse of what Scrapling can do without deep diving.</p><p>HTTP requests with session support</p><pre><code>from scrapling.fetchers import Fetcher, FetcherSession\n\nwith FetcherSession(impersonate='chrome') as session:  # Use latest version of Chrome's TLS fingerprint\n    page = session.get('https://quotes.toscrape.com/', stealthy_headers=True)\n    quotes = page.css('.quote .text::text').getall()\n\n# Or use one-off requests\npage = Fetcher.get('https://quotes.toscrape.com/')\nquotes = page.css('.quote .text::text').getall()\n</code></pre><pre><code>from scrapling.fetchers import StealthyFetcher, StealthySession\n\nwith StealthySession(headless=True, solve_cloudflare=True) as session:  # Keep the browser open until you finish\n    page = session.fetch('https://nopecha.com/demo/cloudflare', google_search=False)\n    data = page.css('#padded_content a').getall()\n\n# Or use one-off request style, it opens the browser for this request, then closes it after finishing\npage = StealthyFetcher.fetch('https://nopecha.com/demo/cloudflare')\ndata = page.css('#padded_content a').getall()\n</code></pre><pre><code>from scrapling.fetchers import DynamicFetcher, DynamicSession\n\nwith DynamicSession(headless=True, disable_resources=False, network_idle=True) as session:  # Keep the browser open until you finish\n    page = session.fetch('https://quotes.toscrape.com/', load_dom=False)\n    data = page.xpath('//span[@class=\"text\"]/text()').getall()  # XPath selector if you prefer it\n\n# Or use one-off request style, it opens the browser for this request, then closes it after finishing\npage = DynamicFetcher.fetch('https://quotes.toscrape.com/')\ndata = page.css('.quote .text::text').getall()\n</code></pre><p>Build full crawlers with concurrent requests, multiple session types, and pause/resume:</p><pre><code>from scrapling.spiders import Spider, Request, Response\n\nclass QuotesSpider(Spider):\n    name = \"quotes\"\n    start_urls = [\"https://quotes.toscrape.com/\"]\n    concurrent_requests = 10\n    \n    async def parse(self, response: Response):\n        for quote in response.css('.quote'):\n            yield {\n                \"text\": quote.css('.text::text').get(),\n                \"author\": quote.css('.author::text').get(),\n            }\n            \n        next_page = response.css('.next a')\n        if next_page:\n            yield response.follow(next_page[0].attrib['href'])\n\nresult = QuotesSpider().start()\nprint(f\"Scraped {len(result.items)} quotes\")\nresult.items.to_json(\"quotes.json\")\n</code></pre><p>Use multiple session types in a single spider:</p><pre><code>from scrapling.spiders import Spider, Request, Response\nfrom scrapling.fetchers import FetcherSession, AsyncStealthySession\n\nclass MultiSessionSpider(Spider):\n    name = \"multi\"\n    start_urls = [\"https://example.com/\"]\n    \n    def configure_sessions(self, manager):\n        manager.add(\"fast\", FetcherSession(impersonate=\"chrome\"))\n        manager.add(\"stealth\", AsyncStealthySession(headless=True), lazy=True)\n    \n    async def parse(self, response: Response):\n        for link in response.css('a::attr(href)').getall():\n            # Route protected pages through the stealth session\n            if \"protected\" in link:\n                yield Request(link, sid=\"stealth\")\n            else:\n                yield Request(link, sid=\"fast\", callback=self.parse)  # explicit callback\n</code></pre><p>Pause and resume long crawls with checkpoints by running the spider like this:</p><pre><code>QuotesSpider(crawldir=\"./crawl_data\").start()\n</code></pre><p>Press Ctrl+C to pause gracefully â€” progress is saved automatically. Later, when you start the spider again, pass the same , and it will resume from where it stopped.</p><h3>Advanced Parsing &amp; Navigation</h3><pre><code>from scrapling.fetchers import Fetcher\n\n# Rich element selection and navigation\npage = Fetcher.get('https://quotes.toscrape.com/')\n\n# Get quotes with multiple selection methods\nquotes = page.css('.quote')  # CSS selector\nquotes = page.xpath('//div[@class=\"quote\"]')  # XPath\nquotes = page.find_all('div', {'class': 'quote'})  # BeautifulSoup-style\n# Same as\nquotes = page.find_all('div', class_='quote')\nquotes = page.find_all(['div'], class_='quote')\nquotes = page.find_all(class_='quote')  # and so on...\n# Find element by text content\nquotes = page.find_by_text('quote', tag='div')\n\n# Advanced navigation\nquote_text = page.css('.quote')[0].css('.text::text').get()\nquote_text = page.css('.quote').css('.text::text').getall()  # Chained selectors\nfirst_quote = page.css('.quote')[0]\nauthor = first_quote.next_sibling.css('.author::text')\nparent_container = first_quote.parent\n\n# Element relationships and similarity\nsimilar_elements = first_quote.find_similar()\nbelow_elements = first_quote.below_elements()\n</code></pre><p>You can use the parser right away if you don't want to fetch websites like below:</p><pre><code>from scrapling.parser import Selector\n\npage = Selector(\"&lt;html&gt;...&lt;/html&gt;\")\n</code></pre><p>And it works precisely the same way!</p><h3>Async Session Management Examples</h3><pre><code>import asyncio\nfrom scrapling.fetchers import FetcherSession, AsyncStealthySession, AsyncDynamicSession\n\nasync with FetcherSession(http3=True) as session:  # `FetcherSession` is context-aware and can work in both sync/async patterns\n    page1 = session.get('https://quotes.toscrape.com/')\n    page2 = session.get('https://quotes.toscrape.com/', impersonate='firefox135')\n\n# Async session usage\nasync with AsyncStealthySession(max_pages=2) as session:\n    tasks = []\n    urls = ['https://example.com/page1', 'https://example.com/page2']\n    \n    for url in urls:\n        task = session.fetch(url)\n        tasks.append(task)\n    \n    print(session.get_pool_stats())  # Optional - The status of the browser tabs pool (busy/free/error)\n    results = await asyncio.gather(*tasks)\n    print(session.get_pool_stats())\n</code></pre><p>Scrapling includes a powerful command-line interface:</p><p>Launch the interactive Web Scraping shell</p><p>Extract pages to a file directly without programming (Extracts the content inside the  tag by default). If the output file ends with , then the text content of the target will be extracted. If it ends in , it will be a Markdown representation of the HTML content; if it ends in , it will be the HTML content itself.</p><pre><code>scrapling extract get 'https://example.com' content.md\nscrapling extract get 'https://example.com' content.txt --css-selector '#fromSkipToProducts' --impersonate 'chrome'  # All elements matching the CSS selector '#fromSkipToProducts'\nscrapling extract fetch 'https://example.com' content.md --css-selector '#fromSkipToProducts' --no-headless\nscrapling extract stealthy-fetch 'https://nopecha.com/demo/cloudflare' captchas.html --css-selector '#padded_content a' --solve-cloudflare\n</code></pre><blockquote><p>[!NOTE] There are many additional features, but we want to keep this page concise, including the MCP server and the interactive Web Scraping Shell. Check out the full documentation <a href=\"https://scrapling.readthedocs.io/en/latest/\">here</a></p></blockquote><p>Scrapling isn't just powerfulâ€”it's also blazing fast. The following benchmarks compare Scrapling's parser with the latest versions of other popular libraries.</p><h3>Text Extraction Speed Test (5000 nested elements)</h3><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><h3>Element Similarity &amp; Text Search Performance</h3><p>Scrapling's adaptive element finding capabilities significantly outperform alternatives:</p><table><thead><tr></tr></thead><tbody></tbody></table><blockquote><p>All benchmarks represent averages of 100+ runs. See <a href=\"https://github.com/D4Vinci/Scrapling/raw/main/benchmarks.py\">benchmarks.py</a> for methodology.</p></blockquote><p>Scrapling requires Python 3.10 or higher:</p><p>This installation only includes the parser engine and its dependencies, without any fetchers or commandline dependencies.</p><ol><li><p>If you are going to use any of the extra features below, the fetchers, or their classes, you will need to install fetchers' dependencies and their browser dependencies as follows:</p><pre><code>pip install \"scrapling[fetchers]\"\n\nscrapling install\n</code></pre><p>This downloads all browsers, along with their system dependencies and fingerprint manipulation dependencies.</p></li><li><ul><li>Install the MCP server feature: <pre><code>pip install \"scrapling[ai]\"\n</code></pre></li><li>Install shell features (Web Scraping shell and the  command): <pre><code>pip install \"scrapling[shell]\"\n</code></pre></li><li>Install everything: <pre><code>pip install \"scrapling[all]\"\n</code></pre></li></ul><p>Remember that you need to install the browser dependencies with  after any of these extras (if you didn't already)</p></li></ol><p>You can also install a Docker image with all extras and browsers with the following command from DockerHub:</p><pre><code>docker pull pyd4vinci/scrapling\n</code></pre><p>Or download it from the GitHub registry:</p><pre><code>docker pull ghcr.io/d4vinci/scrapling:latest\n</code></pre><p>This image is automatically built and pushed using GitHub Actions and the repository's main branch.</p><blockquote><p>[!CAUTION] This library is provided for educational and research purposes only. By using this library, you agree to comply with local and international data scraping and privacy laws. The authors and contributors are not responsible for any misuse of this software. Always respect the terms of service of websites and robots.txt files.</p></blockquote><p>This work is licensed under the BSD-3-Clause License.</p><p>This project includes code adapted from:</p><div align=\"center\"><small>Designed &amp; crafted with â¤ï¸ by Karim Shoair.</small></div>",
      "contentLength": 14045,
      "flags": null,
      "enclosureUrl": "https://repository-images.githubusercontent.com/872119017/7b056203-a221-4d0c-a666-40944ec669a9",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "LadybirdBrowser/ladybird",
      "url": "https://github.com/LadybirdBrowser/ladybird",
      "date": 1771988741,
      "author": "",
      "guid": 48070,
      "unread": true,
      "content": "<p>Truly independent web browser</p><p><a href=\"https://ladybird.org\">Ladybird</a> is a truly independent web browser, using a novel engine based on web standards.</p><blockquote><p>[!IMPORTANT] Ladybird is in a pre-alpha state, and only suitable for use by developers</p></blockquote><p>We aim to build a complete, usable browser for the modern web.</p><p>Ladybird uses a multi-process architecture with a main UI process, several WebContent renderer processes, an ImageDecoder process, and a RequestServer process.</p><p>Image decoding and network connections are done out of process to be more robust against malicious content. Each tab has its own renderer process, which is sandboxed from the rest of the system.</p><p>At the moment, many core library support components are inherited from SerenityOS:</p><ul><li>LibWeb: Web rendering engine</li><li>LibWasm: WebAssembly implementation</li><li>LibCrypto/LibTLS: Cryptography primitives and Transport Layer Security</li><li>LibGfx: 2D Graphics Library, Image Decoding and Rendering</li><li>LibUnicode: Unicode and locale support</li><li>LibMedia: Audio and video playback</li><li>LibCore: Event loop, OS abstraction layer</li><li>LibIPC: Inter-process communication</li></ul><h2>How do I build and run this?</h2><p>Ladybird runs on Linux, macOS, Windows (with WSL2), and many other *Nixes.</p><h2>How do I read the documentation?</h2><p>Code-related documentation can be found in the <a href=\"https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/\">documentation</a> folder.</p><h2>Get in touch and participate!</h2><p>Ladybird is licensed under a 2-clause BSD license.</p>",
      "contentLength": 1320,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/5ca22384a10d538a1b3d9fce5e47a73a5c1611f15a792f37235b865eca8568aa/LadybirdBrowser/ladybird",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "obra/superpowers",
      "url": "https://github.com/obra/superpowers",
      "date": 1771988741,
      "author": "",
      "guid": 48071,
      "unread": true,
      "content": "<p>An agentic skills framework &amp; software development methodology that works.</p><p>Superpowers is a complete software development workflow for your coding agents, built on top of a set of composable \"skills\" and some initial instructions that make sure your agent uses them.</p><p>It starts from the moment you fire up your coding agent. As soon as it sees that you're building something, it  just jump into trying to write code. Instead, it steps back and asks you what you're really trying to do.</p><p>Once it's teased a spec out of the conversation, it shows it to you in chunks short enough to actually read and digest.</p><p>After you've signed off on the design, your agent puts together an implementation plan that's clear enough for an enthusiastic junior engineer with poor taste, no judgement, no project context, and an aversion to testing to follow. It emphasizes true red/green TDD, YAGNI (You Aren't Gonna Need It), and DRY.</p><p>Next up, once you say \"go\", it launches a <em>subagent-driven-development</em> process, having agents work through each engineering task, inspecting and reviewing their work, and continuing forward. It's not uncommon for Claude to be able to work autonomously for a couple hours at a time without deviating from the plan you put together.</p><p>There's a bunch more to it, but that's the core of the system. And because the skills trigger automatically, you don't need to do anything special. Your coding agent just has Superpowers.</p><p>If Superpowers has helped you do stuff that makes money and you are so inclined, I'd greatly appreciate it if you'd consider <a href=\"https://github.com/sponsors/obra\">sponsoring my opensource work</a>.</p><p> Installation differs by platform. Claude Code or Cursor have built-in plugin marketplaces. Codex and OpenCode require manual setup.</p><h3>Claude Code (via Plugin Marketplace)</h3><p>In Claude Code, register the marketplace first:</p><pre><code>/plugin marketplace add obra/superpowers-marketplace\n</code></pre><p>Then install the plugin from this marketplace:</p><pre><code>/plugin install superpowers@superpowers-marketplace\n</code></pre><h3>Cursor (via Plugin Marketplace)</h3><p>In Cursor Agent chat, install from marketplace:</p><pre><code>Fetch and follow instructions from https://raw.githubusercontent.com/obra/superpowers/refs/heads/main/.codex/INSTALL.md\n</code></pre><pre><code>Fetch and follow instructions from https://raw.githubusercontent.com/obra/superpowers/refs/heads/main/.opencode/INSTALL.md\n</code></pre><p>Start a new session in your chosen platform and ask for something that should trigger a skill (for example, \"help me plan this feature\" or \"let's debug this issue\"). The agent should automatically invoke the relevant superpowers skill.</p><ol><li><p> - Activates before writing code. Refines rough ideas through questions, explores alternatives, presents design in sections for validation. Saves design document.</p></li><li><p> - Activates after design approval. Creates isolated workspace on new branch, runs project setup, verifies clean test baseline.</p></li><li><p> - Activates with approved design. Breaks work into bite-sized tasks (2-5 minutes each). Every task has exact file paths, complete code, verification steps.</p></li><li><p><strong>subagent-driven-development</strong> or  - Activates with plan. Dispatches fresh subagent per task with two-stage review (spec compliance, then code quality), or executes in batches with human checkpoints.</p></li><li><p> - Activates during implementation. Enforces RED-GREEN-REFACTOR: write failing test, watch it fail, write minimal code, watch it pass, commit. Deletes code written before tests.</p></li><li><p> - Activates between tasks. Reviews against plan, reports issues by severity. Critical issues block progress.</p></li><li><p><strong>finishing-a-development-branch</strong> - Activates when tasks complete. Verifies tests, presents options (merge/PR/keep/discard), cleans up worktree.</p></li></ol><p><strong>The agent checks for relevant skills before any task.</strong> Mandatory workflows, not suggestions.</p><ul><li> - RED-GREEN-REFACTOR cycle (includes testing anti-patterns reference)</li></ul><ul><li> - 4-phase root cause process (includes root-cause-tracing, defense-in-depth, condition-based-waiting techniques)</li><li><strong>verification-before-completion</strong> - Ensure it's actually fixed</li></ul><ul><li> - Socratic design refinement</li><li> - Detailed implementation plans</li><li> - Batch execution with checkpoints</li><li><strong>dispatching-parallel-agents</strong> - Concurrent subagent workflows</li><li> - Pre-review checklist</li><li> - Responding to feedback</li><li> - Parallel development branches</li><li><strong>finishing-a-development-branch</strong> - Merge/PR decision workflow</li><li><strong>subagent-driven-development</strong> - Fast iteration with two-stage review (spec compliance, then code quality)</li></ul><ul><li> - Create new skills following best practices (includes testing methodology)</li><li> - Introduction to the skills system</li></ul><ul><li> - Write tests first, always</li><li> - Process over guessing</li><li> - Simplicity as primary goal</li><li> - Verify before declaring success</li></ul><p>Skills live directly in this repository. To contribute:</p><ol><li>Create a branch for your skill</li><li>Follow the  skill for creating and testing new skills</li></ol><p>See <code>skills/writing-skills/SKILL.md</code> for the complete guide.</p><p>Skills update automatically when you update the plugin:</p><pre><code>/plugin update superpowers\n</code></pre><p>MIT License - see LICENSE file for details</p>",
      "contentLength": 4847,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/30dffc9dcfc52ceba08f2aa806c3f45bc19205eb438f669bea48a2c56d35d00c/obra/superpowers",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "siteboon/claudecodeui",
      "url": "https://github.com/siteboon/claudecodeui",
      "date": 1771902315,
      "author": "",
      "guid": 47743,
      "unread": true,
      "content": "<p>Use Claude Code, Cursor CLI or Codex on mobile and web with CloudCLI (aka Claude Code UI). CloudCLI is a free open source webui/GUI that helps you manage your Claude Code session and projects remotely</p><p>A desktop and mobile UI for <a href=\"https://docs.anthropic.com/en/docs/claude-code\">Claude Code</a>, <a href=\"https://docs.cursor.com/en/cli/overview\">Cursor CLI</a> and <a href=\"https://developers.openai.com/codex\">Codex</a>. You can use it locally or remotely to view your active projects and sessions in Claude Code, Cursor, or Codex and make changes to them from everywhere (mobile or desktop). This gives you a proper interface that works everywhere.</p><ul><li> - Works seamlessly across desktop, tablet, and mobile so you can also use Claude Code, Cursor, or Codex from mobile</li><li><strong>Interactive Chat Interface</strong> - Built-in chat interface for seamless communication with Claude Code, Cursor, or Codex</li><li><strong>Integrated Shell Terminal</strong> - Direct access to Claude Code, Cursor CLI, or Codex through built-in shell functionality</li><li> - Interactive file tree with syntax highlighting and live editing</li><li> - View, stage and commit your changes. You can also switch branches</li><li> - Resume conversations, manage multiple sessions, and track history</li><li><strong>TaskMaster AI Integration</strong> - Advanced project management with AI-powered task planning, PRD parsing, and workflow automation</li><li> - Works with Claude Sonnet 4.5, Opus 4.5, and GPT-5.2</li></ul><h3>One-click Operation (Recommended)</h3><p>No installation required, direct operation:</p><pre><code>npx @siteboon/claude-code-ui\n</code></pre><p>The server will start and be accessible at  (or your configured PORT).</p><p>: Simply run the same  command again after stopping the server</p><h3>Global Installation (For Regular Use)</h3><p>For frequent use, install globally once:</p><pre><code>npm install -g @siteboon/claude-code-ui\n</code></pre><p>Then start with a simple command:</p><p>: Stop with Ctrl+C and run  again.</p><p>After global installation, you have access to both  and  commands:</p><table><thead><tr></tr></thead><tbody><tr><td>Start the server (default)</td></tr><tr><td>Start the server explicitly</td></tr><tr><td>Show configuration and data locations</td></tr><tr><td>Update to the latest version</td></tr><tr></tr><tr></tr><tr><td>Set server port (default: 3001)</td></tr><tr><td>Set custom database location</td></tr></tbody></table><pre><code>cloudcli                          # Start with defaults\ncloudcli -p 8080              # Start on custom port\ncloudcli status                   # Show current configuration\n</code></pre><h3>Run as Background Service (Recommended for Production)</h3><p>For production use, run Claude Code UI as a background service using PM2 (Process Manager 2):</p><h4>Start as Background Service</h4><pre><code># Start the server in background\npm2 start claude-code-ui --name \"claude-code-ui\"\n\n# Or using the shorter alias\npm2 start cloudcli --name \"claude-code-ui\"\n\n# Start on a custom port\npm2 start cloudcli --name \"claude-code-ui\" -- --port 8080\n</code></pre><h4>Auto-Start on System Boot</h4><p>To make Claude Code UI start automatically when your system boots:</p><pre><code># Generate startup script for your platform\npm2 startup\n\n# Save current process list\npm2 save\n</code></pre><h3>Local Development Installation</h3><pre><code>git clone https://github.com/siteboon/claudecodeui.git\ncd claudecodeui\n</code></pre><pre><code>cp .env.example .env\n# Edit .env with your preferred settings\n</code></pre><pre><code># Development mode (with hot reload)\nnpm run dev\n\n</code></pre><p>The application will start at the port you specified in your .env</p><ol start=\"5\"><li><ul><li>Development: </li></ul></li></ol><h2>Security &amp; Tools Configuration</h2><p>: All Claude Code tools are . This prevents potentially harmful operations from running automatically.</p><p>To use Claude Code's full functionality, you'll need to manually enable tools:</p><ol><li> - Click the gear icon in the sidebar</li><li> - Turn on only the tools you need</li><li> - Your preferences are saved locally</li></ol><p>: Start with basic tools enabled and add more as needed. You can always adjust these settings later.</p><h2>TaskMaster AI Integration </h2><p>Claude Code UI supports  (aka claude-task-master) integration for advanced project management and AI-powered task planning.</p><ul><li>AI-powered task generation from PRDs (Product Requirements Documents)</li><li>Smart task breakdown and dependency management</li><li>Visual task boards and progress tracking</li></ul><p>: Visit the <a href=\"https://github.com/eyaltoledano/claude-task-master\">TaskMaster AI GitHub repository</a> for installation instructions, configuration guides, and usage examples. After installing it you should be able to enable it from the Settings</p><p>It automatically discovers Claude Code, Cursor or Codex sessions when available and groups them together into projects session counts</p><ul><li> - Rename, delete, and organize projects</li><li> - Quick access to recent projects and sessions</li><li> - Add your own MCP servers through the UI</li></ul><ul><li><strong>Use responsive chat or Claude Code/Cursor CLI/Codex CLI</strong> - You can either use the adapted chat interface or use the shell button to connect to your selected CLI.</li><li> - Stream responses from your selected CLI (Claude Code/Cursor/Codex) with WebSocket connection</li><li> - Resume previous conversations or start fresh sessions</li><li> - Complete conversation history with timestamps and metadata</li><li> - Text, code blocks, and file references</li></ul><ul><li> - Browse project structure with expand/collapse navigation</li><li> - Read, modify, and save files directly in the interface</li><li> - Support for multiple programming languages</li><li> - Create, rename, delete files and directories</li></ul><h4>TaskMaster AI Integration </h4><ul><li> - Kanban-style interface for managing development tasks</li><li> - Create Product Requirements Documents and parse them into structured tasks</li><li> - Real-time status updates and completion tracking</li></ul><ul><li> - All conversations automatically saved</li><li> - Group sessions by project and timestamp</li><li> - Rename, delete, and export conversation history</li><li> - Access sessions from any device</li></ul><ul><li> - Optimized for all screen sizes</li><li> - Swipe gestures and touch navigation</li><li> - Bottom tab bar for easy thumb navigation</li><li> - Collapsible sidebar and smart content prioritization</li><li><strong>Add shortcut to Home Screen</strong> - Add a shortcut to your home screen and the app will behave like a PWA</li></ul><pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Frontend      â”‚    â”‚   Backend       â”‚    â”‚  Agent     â”‚\nâ”‚   (React/Vite)  â”‚â—„â”€â”€â–ºâ”‚ (Express/WS)    â”‚â—„â”€â”€â–ºâ”‚  Integration    â”‚\nâ”‚                 â”‚    â”‚                 â”‚    â”‚                â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</code></pre><h3>Backend (Node.js + Express)</h3><ul><li> - RESTful API with static file serving</li><li> - Communication for chats and project refresh</li><li><strong>Agent Integration (Claude Code / Cursor CLI / Codex)</strong> - Process spawning and management</li><li> - Exposing file browser for projects</li></ul><ul><li> - Modern component architecture with hooks</li><li> - Advanced code editor with syntax highlighting</li></ul><p>We welcome contributions! Please see our <a href=\"https://raw.githubusercontent.com/siteboon/claudecodeui/main/CONTRIBUTING.md\">Contributing Guide</a> for details on commit conventions, development workflow, and release process.</p><h3>Common Issues &amp; Solutions</h3><h4>\"No Claude projects found\"</h4><p>: The UI shows no projects or empty project list :</p><ul><li>Run  command in at least one project directory to initialize</li><li>Verify  directory exists and has proper permissions</li></ul><p>: Files not loading, permission errors, empty directories :</p><ul><li>Check project directory permissions ( in terminal)</li><li>Verify the project path exists and is accessible</li><li>Review server console logs for detailed error messages</li><li>Ensure you're not trying to access system directories outside project scope</li></ul><p>GNU General Public License v3.0 - see <a href=\"https://raw.githubusercontent.com/siteboon/claudecodeui/main/LICENSE\">LICENSE</a> file for details.</p><p>This project is open source and free to use, modify, and distribute under the GPL v3 license.</p><ul><li> this repository to show support</li><li> for updates and new releases</li><li> the project for announcements</li></ul><div align=\"center\"><strong>Made with care for the Claude Code, Cursor and Codex community.</strong></div>",
      "contentLength": 7250,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/5fceb106b2663a05cdffa26485450de38c1e0fe2de5807b8881e9a8d1798c5dd/siteboon/claudecodeui",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "VectifyAI/PageIndex",
      "url": "https://github.com/VectifyAI/PageIndex",
      "date": 1771902315,
      "author": "",
      "guid": 47744,
      "unread": true,
      "content": "<p>ğŸ“‘ PageIndex: Document Index for Vectorless, Reasoning-based RAG</p><p>Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic  rather than true . But  â€” what we truly need in retrieval is , and that requires . When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.</p><p>Inspired by AlphaGo, we propose  â€” a ,  system that builds a  from long documents and uses LLMs to  for <strong>agentic, context-aware retrieval</strong>. It simulates how  navigate and extract knowledge from complex documents through , enabling LLMs to  and  their way to the most relevant document sections. PageIndex performs retrieval in two steps:</p><ol><li>Generate a â€œTable-of-Contentsâ€  of documents</li><li>Perform reasoning-based retrieval through </li></ol><p>Compared to traditional vector-based RAG,  features:</p><ul><li>: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.</li><li>: Documents are organized into natural sections, not artificial chunks.</li><li>: Simulates how human experts navigate and extract knowledge from complex documents.</li><li><strong>Better Explainability and Traceability</strong>: Retrieval is based on reasoning â€” traceable and interpretable, with page and section references. No more opaque, approximate vector search (â€œvibe retrievalâ€).</li></ul><p>PageIndex powers a reasoning-based RAG system that achieved <a href=\"https://github.com/VectifyAI/Mafin2.5-FinanceBench\">98.7% accuracy</a> on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our <a href=\"https://vectify.ai/blog/Mafin2.5\">blog post</a> for details).</p><p>The PageIndex service is available as a ChatGPT-style <a href=\"https://chat.pageindex.ai\">chat platform</a>, or can be integrated via <a href=\"https://pageindex.ai/mcp\">MCP</a> or <a href=\"https://docs.pageindex.ai/quickstart\">API</a>.</p><ul><li>Self-host â€” run locally with this open-source repo.</li></ul><ul><li>Try the <a href=\"https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb\"></a> notebook â€” a , hands-on example of reasoning-based RAG using PageIndex.</li></ul><p>PageIndex can transform lengthy PDF documents into a semantic , similar to a  but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.</p><pre><code>...\n{\n  \"title\": \"Financial Stability\",\n  \"node_id\": \"0006\",\n  \"start_index\": 21,\n  \"end_index\": 22,\n  \"summary\": \"The Federal Reserve ...\",\n  \"nodes\": [\n    {\n      \"title\": \"Monitoring Financial Vulnerabilities\",\n      \"node_id\": \"0007\",\n      \"start_index\": 22,\n      \"end_index\": 28,\n      \"summary\": \"The Federal Reserve's monitoring ...\"\n    },\n    {\n      \"title\": \"Domestic and International Cooperation and Coordination\",\n      \"node_id\": \"0008\",\n      \"start_index\": 28,\n      \"end_index\": 31,\n      \"summary\": \"In 2023, the Federal Reserve collaborated ...\"\n    }\n  ]\n}\n...\n</code></pre><p>You can generate the PageIndex tree structure with this open-source repo, or use our <a href=\"https://docs.pageindex.ai/quickstart\">API</a></p><p>You can follow these steps to generate a PageIndex tree from a PDF document.</p><pre><code>pip3 install --upgrade -r requirements.txt\n</code></pre><h3>2. Set your OpenAI API key</h3><p>Create a  file in the root directory and add your API key:</p><pre><code>CHATGPT_API_KEY=your_openai_key_here\n</code></pre><h3>3. Run PageIndex on your PDF</h3><pre><code>python3 run_pageindex.py --pdf_path /path/to/your/document.pdf\n</code></pre><p><a href=\"https://vectify.ai/mafin\">Mafin 2.5</a> is a reasoning-based RAG system for financial document analysis, powered by . It achieved a state-of-the-art <a href=\"https://vectify.ai/blog/Mafin2.5\"></a> on the <a href=\"https://arxiv.org/abs/2311.11944\">FinanceBench</a> benchmark, significantly outperforming traditional vector-based RAG systems.</p><p>PageIndex's hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.</p><ul><li>ğŸ§ª <a href=\"https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex\">Cookbooks</a>: hands-on, runnable examples and advanced use cases.</li><li>ğŸ“– <a href=\"https://docs.pageindex.ai/doc-search\">Tutorials</a>: practical guides and strategies, including  and .</li><li>ğŸ“ <a href=\"https://pageindex.ai/blog\">Blog</a>: technical articles, research insights, and product updates.</li></ul><p>Please cite this work as:</p><pre><code>Mingtian Zhang, Yu Tang and PageIndex Team,\n\"PageIndex: Next-Generation Vectorless, Reasoning-based RAG\",\nPageIndex Blog, Sep 2025.\n</code></pre><p>Or use the BibTeX citation:</p><pre><code>@article{zhang2025pageindex,\n  author = {Mingtian Zhang and Yu Tang and PageIndex Team},\n  title = {PageIndex: Next-Generation Vectorless, Reasoning-based RAG},\n  journal = {PageIndex Blog},\n  year = {2025},\n  month = {September},\n  note = {https://pageindex.ai/blog/pageindex-intro},\n}\n</code></pre><p>Leave us a star ğŸŒŸ if you like our project. Thank you!</p>",
      "contentLength": 4261,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/1f94f8866183ff4b5c6789eaf5237e4ba222fc34aafc3653c28a7726f63a8866/VectifyAI/PageIndex",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "CompVis/stable-diffusion",
      "url": "https://github.com/CompVis/stable-diffusion",
      "date": 1771902315,
      "author": "",
      "guid": 47745,
      "unread": true,
      "content": "<p>A latent text-to-image diffusion model</p><p><em>Stable Diffusion was made possible thanks to a collaboration with <a href=\"https://stability.ai/\">Stability AI</a> and <a href=\"https://runwayml.com/\">Runway</a> and builds upon our previous work:</em></p><p><img src=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/merged-0006.png\" alt=\"txt2img-stable2\"><a href=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#stable-diffusion-v1\">Stable Diffusion</a> is a latent text-to-image diffusion model. Thanks to a generous compute donation from <a href=\"https://stability.ai/\">Stability AI</a> and support from <a href=\"https://laion.ai/\">LAION</a>, we were able to train a Latent Diffusion Model on 512x512 images from a subset of the <a href=\"https://laion.ai/blog/laion-5b/\">LAION-5B</a> database. Similar to Google's <a href=\"https://arxiv.org/abs/2205.11487\">Imagen</a>, this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See <a href=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#stable-diffusion-v1\">this section</a> below and the <a href=\"https://huggingface.co/CompVis/stable-diffusion\">model card</a>.</p><p>A suitable <a href=\"https://conda.io/\">conda</a> environment named  can be created and activated with:</p><pre><code>conda env create -f environment.yaml\nconda activate ldm\n</code></pre><pre><code>conda install pytorch torchvision -c pytorch\npip install transformers==4.19.2 diffusers invisible-watermark\npip install -e .\n</code></pre><p>Stable Diffusion v1 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 860M UNet and CLIP ViT-L/14 text encoder for the diffusion model. The model was pretrained on 256x256 images and then finetuned on 512x512 images.</p><p><em>Note: Stable Diffusion v1 is a general text-to-image diffusion model and therefore mirrors biases and (mis-)conceptions that are present in its training data. Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding <a href=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/Stable_Diffusion_v1_Model_Card.md\">model card</a>.</em></p><p>We currently provide the following checkpoints:</p><ul><li>: 237k steps at resolution  on <a href=\"https://huggingface.co/datasets/laion/laion2B-en\">laion2B-en</a>. 194k steps at resolution  on <a href=\"https://huggingface.co/datasets/laion/laion-high-resolution\">laion-high-resolution</a> (170M examples from LAION-5B with resolution ).</li><li>: Resumed from . 515k steps at resolution  on <a href=\"https://laion.ai/blog/laion-aesthetics/\">laion-aesthetics v2 5+</a> (a subset of laion2B-en with estimated aesthetics score , and additionally filtered to images with an original size , and an estimated watermark probability . The watermark estimate is from the <a href=\"https://laion.ai/blog/laion-5b/\">LAION-5B</a> metadata, the aesthetics score is estimated using the <a href=\"https://github.com/christophschuhmann/improved-aesthetic-predictor\">LAION-Aesthetics Predictor V2</a>).</li><li>: Resumed from . 195k steps at resolution  on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve <a href=\"https://arxiv.org/abs/2207.12598\">classifier-free guidance sampling</a>.</li><li>: Resumed from . 225k steps at resolution  on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve <a href=\"https://arxiv.org/abs/2207.12598\">classifier-free guidance sampling</a>.</li></ul><p>Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling steps show the relative improvements of the checkpoints: <img src=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/v1-variants-scores.jpg\" alt=\"sd evaluation results\"></p><h3>Text-to-Image with Stable Diffusion</h3><p>Stable Diffusion is a latent diffusion model conditioned on the (non-pooled) text embeddings of a CLIP ViT-L/14 text encoder. We provide a <a href=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#reference-sampling-script\">reference script for sampling</a>, but there also exists a <a href=\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#diffusers-integration\">diffusers integration</a>, which we expect to see more active community development.</p><h4>Reference Sampling Script</h4><p>We provide a reference sampling script, which incorporates</p><pre><code>mkdir -p models/ldm/stable-diffusion-v1/\nln -s &lt;path/to/model.ckpt&gt; models/ldm/stable-diffusion-v1/model.ckpt \n</code></pre><pre><code>python scripts/txt2img.py --prompt \"a photograph of an astronaut riding a horse\" --plms \n</code></pre><p>By default, this uses a guidance scale of , <a href=\"https://github.com/CompVis/latent-diffusion/pull/51\">Katherine Crowson's implementation</a> of the <a href=\"https://arxiv.org/abs/2202.09778\">PLMS</a> sampler, and renders images of size 512x512 (which it was trained on) in 50 steps. All supported arguments are listed below (type <code>python scripts/txt2img.py --help</code>).</p><pre></pre><p>Note: The inference config for all v1 versions is designed to be used with EMA-only checkpoints. For this reason  is set in the configuration, otherwise the code will try to switch from non-EMA to EMA weights. If you want to examine the effect of EMA vs no EMA, we provide \"full\" checkpoints which contain both types of weights. For these,  will load and use the non-EMA weights.</p><p>A simple way to download and sample Stable Diffusion is by using the <a href=\"https://github.com/huggingface/diffusers/tree/main#new--stable-diffusion-is-now-fully-compatible-with-diffusers\">diffusers library</a>:</p><pre><code># make sure you're logged in with `huggingface-cli login`\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n\t\"CompVis/stable-diffusion-v1-4\", \n\tuse_auth_token=True\n).to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt)[\"sample\"][0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n</code></pre><h3>Image Modification with Stable Diffusion</h3><p>By using a diffusion-denoising mechanism as first proposed by <a href=\"https://arxiv.org/abs/2108.01073\">SDEdit</a>, the model can be used for different tasks such as text-guided image-to-image translation and upscaling. Similar to the txt2img sampling script, we provide a script to perform image modification with Stable Diffusion.</p><p>The following describes an example where a rough sketch made in <a href=\"https://www.pinta-project.com/\">Pinta</a> is converted into a detailed artwork.</p><pre><code>python scripts/img2img.py --prompt \"A fantasy landscape, trending on artstation\" --init-img &lt;path-to-img.jpg&gt; --strength 0.8\n</code></pre><p>Here, strength is a value between 0.0 and 1.0, that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input. See the following example.</p><p>This procedure can, for example, also be used to upscale samples from the base model.</p><pre><code>@misc{rombach2021highresolution,\n      title={High-Resolution Image Synthesis with Latent Diffusion Models}, \n      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and BjÃ¶rn Ommer},\n      year={2021},\n      eprint={2112.10752},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>",
      "contentLength": 5519,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "muratcankoylan/Agent-Skills-for-Context-Engineering",
      "url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering",
      "date": 1771902315,
      "author": "",
      "guid": 47746,
      "unread": true,
      "content": "<p>A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.</p><p>A comprehensive, open collection of Agent Skills focused on context engineering principles for building production-grade AI agent systems. These skills teach the art and science of curating context to maximize agent effectiveness across any agent platform.</p><h2>What is Context Engineering?</h2><p>Context engineering is the discipline of managing the language model's context window. Unlike prompt engineering, which focuses on crafting effective instructions, context engineering addresses the holistic curation of all information that enters the model's limited attention budget: system prompts, tool definitions, retrieved documents, message history, and tool outputs.</p><p>The fundamental challenge is that context windows are constrained not by raw token capacity but by attention mechanics. As context length increases, models exhibit predictable degradation patterns: the \"lost-in-the-middle\" phenomenon, U-shaped attention curves, and attention scarcity. Effective context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.</p><p>This repository is cited in academic research as foundational work on static skill architecture:</p><blockquote><p>\"While static skills are well-recognized [Anthropic, 2025b; Muratcan Koylan, 2025], MCE is among the first to dynamically evolve them, bridging manual skill engineering and autonomous self-improvement.\"</p></blockquote><p>These skills establish the foundational understanding required for all subsequent context engineering work.</p><table><tbody><tr><td>Understand what context is, why it matters, and the anatomy of context in agent systems</td></tr><tr><td>Recognize patterns of context failure: lost-in-middle, poisoning, distraction, and clash</td></tr></tbody></table><p>These skills cover the patterns and structures for building effective agent systems.</p><table><tbody><tr><td>Design short-term, long-term, and graph-based memory architectures</td></tr><tr><td>Use filesystems for dynamic context discovery, tool output offloading, and plan persistence</td></tr><tr><td> Build background coding agents with sandboxed VMs, pre-built images, multiplayer support, and multi-client interfaces</td></tr></tbody></table><p>These skills address the ongoing operation and optimization of agent systems.</p><table><tbody><tr><td>Build evaluation frameworks for agent systems</td></tr><tr><td>Master LLM-as-a-Judge techniques: direct scoring, pairwise comparison, rubric generation, and bias mitigation</td></tr></tbody></table><p>These skills cover the meta-level practices for building LLM-powered projects.</p><table><tbody><tr><td>Design and build LLM projects from ideation through deployment, including task-model fit analysis, pipeline architecture, and structured output design</td></tr></tbody></table><h3>Cognitive Architecture Skills</h3><p>These skills cover formal cognitive modeling for rational agent systems.</p><table><tbody><tr><td> Transform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns for deliberative reasoning and explainability</td></tr></tbody></table><p>Each skill is structured for efficient context use. At startup, agents load only skill names and descriptions. Full content loads only when a skill is activated for relevant tasks.</p><p>These skills focus on transferable principles rather than vendor-specific implementations. The patterns work across Claude Code, Cursor, and any agent platform that supports skills or allows custom instructions.</p><h3>Conceptual Foundation with Practical Examples</h3><p>Scripts and examples demonstrate concepts using Python pseudocode that works across environments without requiring specific dependency installations.</p><p>This repository is a <strong>Claude Code Plugin Marketplace</strong> containing context engineering skills that Claude automatically discovers and activates based on your task context.</p><p><strong>Step 1: Add the Marketplace</strong></p><p>Run this command in Claude Code to register this repository as a plugin source:</p><pre><code>/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering\n</code></pre><p><strong>Step 2: Browse and Install</strong></p><p>Option A - Browse available plugins:</p><ol><li>Select <code>Browse and install plugins</code></li><li>Select <code>context-engineering-marketplace</code></li><li>Choose a plugin (e.g., <code>context-engineering-fundamentals</code>, )</li></ol><p>Option B - Direct install via command:</p><pre><code>/plugin install context-engineering-fundamentals@context-engineering-marketplace\n/plugin install agent-architecture@context-engineering-marketplace\n/plugin install agent-evaluation@context-engineering-marketplace\n/plugin install agent-development@context-engineering-marketplace\n/plugin install cognitive-architecture@context-engineering-marketplace\n</code></pre><table><tbody><tr><td><code>context-engineering-fundamentals</code></td><td>context-fundamentals, context-degradation, context-compression, context-optimization</td></tr><tr><td>multi-agent-patterns, memory-systems, tool-design, filesystem-context, hosted-agents</td></tr><tr><td>evaluation, advanced-evaluation</td></tr><tr></tr><tr></tr></tbody></table><table><tbody><tr><td>\"understand context\", \"explain context windows\", \"design agent architecture\"</td></tr><tr><td>\"diagnose context problems\", \"fix lost-in-middle\", \"debug agent failures\"</td></tr><tr><td>\"compress context\", \"summarize conversation\", \"reduce token usage\"</td></tr><tr><td>\"optimize context\", \"reduce token costs\", \"implement KV-cache\"</td></tr><tr><td>\"design multi-agent system\", \"implement supervisor pattern\"</td></tr><tr><td>\"implement agent memory\", \"build knowledge graph\", \"track entities\"</td></tr><tr><td>\"design agent tools\", \"reduce tool complexity\", \"implement MCP tools\"</td></tr><tr><td>\"offload context to files\", \"dynamic context discovery\", \"agent scratch pad\", \"file-based context\"</td></tr><tr><td>\"build background agent\", \"create hosted coding agent\", \"sandboxed execution\", \"multiplayer agent\", \"Modal sandboxes\"</td></tr><tr><td>\"evaluate agent performance\", \"build test framework\", \"measure quality\"</td></tr><tr><td>\"implement LLM-as-judge\", \"compare model outputs\", \"mitigate bias\"</td></tr><tr><td>\"start LLM project\", \"design batch pipeline\", \"evaluate task-model fit\"</td></tr><tr><td>\"model agent mental states\", \"implement BDI architecture\", \"transform RDF to beliefs\", \"build cognitive agent\"</td></tr></tbody></table><img width=\"1014\" height=\"894\" alt=\"Screenshot 2025-12-26 at 12 34 47â€¯PM\" src=\"https://github.com/user-attachments/assets/f79aaf03-fd2d-4c71-a630-7027adeb9bfe\"><p>Copy skill content into  or create project-specific Skills folders. The skills provide the context and guidelines that agent needs for effective context engineering and agent design.</p><h3>For Custom Implementations</h3><p>Extract the principles and patterns from any skill and implement them in your agent framework. The skills are deliberately platform-agnostic.</p><p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/\">examples</a> folder contains complete system designs that demonstrate how multiple skills work together in practice.</p><table><thead><tr></tr></thead><tbody><tr><td> Personal operating system for founders and creators. Complete Claude Code skill with 6 modules, 4 automation scripts</td><td>context-fundamentals, context-optimization, memory-systems, tool-design, multi-agent-patterns, evaluation, project-development</td></tr><tr><td>Multi-agent system that monitors X accounts and generates daily synthesized books</td><td>multi-agent-patterns, memory-systems, context-optimization, tool-design, evaluation</td></tr><tr><td>Production-ready LLM evaluation tools with TypeScript implementation, 19 passing tests</td><td>advanced-evaluation, tool-design, context-fundamentals, evaluation</td></tr><tr><td>Train models to write in any author's style. Includes Gertrude Stein case study with 70% human score on Pangram, $2 total cost</td><td>project-development, context-compression, multi-agent-patterns, evaluation</td></tr></tbody></table><ul><li>Complete PRD with architecture decisions</li><li>Skills mapping showing which concepts informed each decision</li></ul><h3>Digital Brain Skill Example</h3><p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/digital-brain-skill/\">digital-brain-skill</a> example is a complete personal operating system demonstrating comprehensive skills application:</p><ul><li>: 3-level loading (SKILL.md â†’ MODULE.md â†’ data files)</li><li>: 6 independent modules (identity, content, knowledge, network, operations, agents)</li><li>: JSONL files with schema-first lines for agent-friendly parsing</li><li>: 4 consolidated tools (weekly_review, content_ideas, stale_contacts, idea_to_draft)</li></ul><p>Includes detailed traceability in <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/digital-brain-skill/HOW-SKILLS-BUILT-THIS.md\">HOW-SKILLS-BUILT-THIS.md</a> mapping every architectural decision to specific skill principles.</p><h3>LLM-as-Judge Skills Example</h3><ul><li>: Evaluate responses against weighted criteria with rubric support</li><li>: Compare responses with position bias mitigation</li><li>: Create domain-specific evaluation standards</li><li>: High-level agent combining all evaluation capabilities</li></ul><h3>Book SFT Pipeline Example</h3><p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/book-sft-pipeline/\">book-sft-pipeline</a> example demonstrates training small models (8B) to write in any author's style:</p><ul><li>: Two-tier chunking with overlap for maximum training examples</li><li>: 15+ templates to prevent memorization and force style learning</li><li>: Complete LoRA training workflow with $2 total cost</li><li>: Modern scenario testing proves style transfer vs content memorization</li></ul><p>Integrates with context engineering skills: project-development, context-compression, multi-agent-patterns, evaluation.</p><img width=\"3664\" height=\"2648\" alt=\"star-history-2026224\" src=\"https://github.com/user-attachments/assets/b3bdbf23-4b6a-4774-ae85-42ef4d9b2d79\"><p>Each skill follows the Agent Skills specification:</p><pre><code>skill-name/\nâ”œâ”€â”€ SKILL.md              # Required: instructions + metadata\nâ”œâ”€â”€ scripts/              # Optional: executable code demonstrating concepts\nâ””â”€â”€ references/           # Optional: additional documentation and resources\n</code></pre><p>See the <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/template/\">template</a> folder for the canonical skill structure.</p><p>This repository follows the Agent Skills open development model. Contributions are welcome from the broader ecosystem. When contributing:</p><ol><li>Follow the skill template structure</li><li>Provide clear, actionable instructions</li><li>Include working examples where appropriate</li><li>Document trade-offs and potential issues</li><li>Keep SKILL.md under 500 lines for optimal performance</li></ol><p>Feel free to contact <a href=\"https://x.com/koylanai\">Muratcan Koylan</a> for collaboration opportunities or any inquiries.</p><p>MIT License - see LICENSE file for details.</p><p>The principles in these skills are derived from research and production experience at leading AI labs and framework developers. Each skill includes references to the underlying research and case studies that inform its recommendations.</p>",
      "contentLength": 9409,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/cb6192c622a9911ed5f756fa4f319cac4569267adda829ee1bd2e1fbaa08e33a/muratcankoylan/Agent-Skills-for-Context-Engineering",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "clash-verge-rev/clash-verge-rev",
      "url": "https://github.com/clash-verge-rev/clash-verge-rev",
      "date": 1771902315,
      "author": "",
      "guid": 47747,
      "unread": true,
      "content": "<p>A modern GUI client based on Tauri, designed to run in Windows, macOS and Linux for tailored proxy experience</p><h3 align=\"center\"> A Clash Meta GUI based on <a href=\"https://github.com/tauri-apps/tauri\">Tauri</a>. </h3><p>è¯·åˆ°å‘å¸ƒé¡µé¢ä¸‹è½½å¯¹åº”çš„å®‰è£…åŒ…ï¼š<a href=\"https://github.com/clash-verge-rev/clash-verge-rev/releases\">Release page</a> Go to the <a href=\"https://github.com/clash-verge-rev/clash-verge-rev/releases\">Release page</a> to download the corresponding installation package Supports Windows (x64/x86), Linux (x64/arm64) and macOS 10.15+ (intel/apple).</p><p>ğŸš€ é«˜æ€§èƒ½æµ·å¤–æŠ€æœ¯æµæœºåœºï¼Œæ”¯æŒå…è´¹è¯•ç”¨ä¸ä¼˜æƒ å¥—é¤ï¼Œå…¨é¢è§£é”æµåª’ä½“åŠ AI æœåŠ¡ï¼Œå…¨çƒé¦–å®¶é‡‡ç”¨ ã€‚</p><p>ğŸ ä½¿ç”¨  æ³¨å†Œå³é€ ï¼Œæ¯æ—¥ ï¼šğŸ‘‰ <a href=\"https://verge.dginv.click/#/register?code=oaxsAGo6\">ç‚¹æ­¤æ³¨å†Œ</a></p><ul><li>ğŸ“± è‡ªç ” iOS å®¢æˆ·ç«¯ï¼ˆä¸šå†…\"å”¯ä¸€\"ï¼‰æŠ€æœ¯ç»å¾—èµ·è€ƒéªŒï¼Œæå¤§æŠ•å…¥</li><li>ğŸ§‘â€ğŸ’» (é¡ºå¸¦è§£å†³ Clash Verge ä½¿ç”¨é—®é¢˜)</li><li>ğŸ’° ä¼˜æƒ å¥—é¤æ¯æœˆ</li><li>âš™ï¸ è®¾è®¡ï¼Œï¼Œé«˜é€Ÿä¸“çº¿(å…¼å®¹è€å®¢æˆ·ç«¯)ï¼Œæä½å»¶è¿Ÿï¼Œæ— è§†æ™šé«˜å³°ï¼Œ4K ç§’å¼€</li><li>âš¡ å…¨çƒé¦–å®¶ï¼Œç°å·²ä¸Šçº¿æ›´å¿«çš„ Tuic åè®®(Clash Verge å®¢æˆ·ç«¯æœ€ä½³æ­é…)</li></ul><ul><li>åŸºäºæ€§èƒ½å¼ºåŠ²çš„ Rust å’Œ Tauri 2 æ¡†æ¶</li><li>ç®€æ´ç¾è§‚çš„ç”¨æˆ·ç•Œé¢ï¼Œæ”¯æŒè‡ªå®šä¹‰ä¸»é¢˜é¢œè‰²ã€ä»£ç†ç»„/æ‰˜ç›˜å›¾æ ‡ä»¥åŠ ã€‚</li><li>é…ç½®æ–‡ä»¶ç®¡ç†å’Œå¢å¼ºï¼ˆMerge å’Œ Scriptï¼‰ï¼Œé…ç½®æ–‡ä»¶è¯­æ³•æç¤ºã€‚</li></ul><p>To run the development server, execute the following commands after all prerequisites for  are installed:</p><pre><code>pnpm i\npnpm run prebuild\npnpm dev\n</code></pre><p>Clash Verge rev was based on or inspired by these projects and so on:</p>",
      "contentLength": 1303,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "f/prompts.chat",
      "url": "https://github.com/f/prompts.chat",
      "date": 1771902315,
      "author": "",
      "guid": 47748,
      "unread": true,
      "content": "<p>a.k.a. Awesome ChatGPT Prompts. Share, discover, and collect prompts from the community. Free and open source â€” self-host for your organization with complete privacy.</p><p align=\"center\"><strong>The world's largest open-source prompt library for AI</strong><sub>Works with ChatGPT, Claude, Gemini, Llama, Mistral, and more</sub></p><p align=\"center\"><sub>formerly known as Awesome ChatGPT Prompts</sub></p><p>A curated collection of  for AI chat models. Originally created for ChatGPT, these prompts work great with any modern AI assistant.</p><h2>ğŸ“– The Interactive Book of Prompting</h2><p>Learn prompt engineering with our  â€” 25+ chapters covering everything from basics to advanced techniques like chain-of-thought reasoning, few-shot learning, and AI agents.</p><p>An interactive, game-based adventure to teach children (ages 8-14) how to communicate with AI through fun puzzles and stories.</p><p>Deploy your own private prompt library with custom branding, themes, and authentication.</p><pre><code>npx prompts.chat new my-prompt-library\ncd my-prompt-library\n</code></pre><pre><code>git clone https://github.com/f/prompts.chat.git\ncd prompts.chat\nnpm install &amp;&amp; npm run setup\n</code></pre><p>The setup wizard configures branding, theme, authentication (GitHub/Google/Azure AD), and features.</p><pre><code>/plugin marketplace add f/prompts.chat\n/plugin install prompts.chat@prompts.chat\n</code></pre><p>Use prompts.chat as an MCP server in your AI tools.</p><pre><code>{\n  \"mcpServers\": {\n    \"prompts.chat\": {\n      \"url\": \"https://prompts.chat/api/mcp\"\n    }\n  }\n}\n</code></pre><pre><code>{\n  \"mcpServers\": {\n    \"prompts.chat\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"prompts.chat\", \"mcp\"]\n    }\n  }\n}\n</code></pre><a href=\"https://github.com/f/prompts.chat/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=f/prompts.chat\"></a>",
      "contentLength": 1482,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "NevaMind-AI/memU",
      "url": "https://github.com/NevaMind-AI/memU",
      "date": 1771902315,
      "author": "",
      "guid": 47749,
      "unread": true,
      "content": "<p>Memory for 24/7 proactive agents like openclaw (moltbot, clawdbot).</p><p>memU is a memory framework built for . It is designed for long-running use and greatly <strong>reduces the LLM token cost</strong> of keeping agents always online, making always-on, evolving agents practical in production systems. memU <strong>continuously captures and understands user intent</strong>. Even without a command, the agent can tell what you are about to do and act on it by itself.</p><img width=\"100%\" src=\"https://github.com/NevaMind-AI/memU/raw/main/assets/memUbot.png\"><ul><li><strong>Download-and-use and simple</strong> to get started.</li><li>Builds long-term memory to  and act proactively.</li><li> with smaller context.</li></ul><h2>ğŸ—ƒï¸ Memory as File System, File System as Memory</h2><p>memU treats <strong>memory like a file system</strong>â€”structured, hierarchical, and instantly accessible.</p><table><tbody><tr><td>ğŸ·ï¸ Categories (auto-organized topics)</td></tr><tr><td>ğŸ§  Memory Items (extracted facts, preferences, skills)</td></tr><tr><td>ğŸ”„ Cross-references (related memories linked)</td></tr><tr><td>ğŸ“¥ Resources (conversations, documents, images)</td></tr></tbody></table><ul><li> like browsing directoriesâ€”drill down from broad categories to specific facts</li><li> instantlyâ€”conversations and documents become queryable memory</li><li>â€”memories reference each other, building a connected knowledge graph</li><li>â€”export, backup, and transfer memory like files</li></ul><pre><code>memory/\nâ”œâ”€â”€ preferences/\nâ”‚   â”œâ”€â”€ communication_style.md\nâ”‚   â””â”€â”€ topic_interests.md\nâ”œâ”€â”€ relationships/\nâ”‚   â”œâ”€â”€ contacts/\nâ”‚   â””â”€â”€ interaction_history/\nâ”œâ”€â”€ knowledge/\nâ”‚   â”œâ”€â”€ domain_expertise/\nâ”‚   â””â”€â”€ learned_skills/\nâ””â”€â”€ context/\n    â”œâ”€â”€ recent_conversations/\n    â””â”€â”€ pending_tasks/\n</code></pre><p>Just as a file system turns raw bytes into organized data, memU transforms raw interactions into <strong>structured, searchable, proactive intelligence</strong>.</p><img width=\"100%\" src=\"https://github.com/NevaMind-AI/memU/raw/main/assets/star.gif\"> If you find memU useful or interesting, a GitHub Star â­ï¸ would be greatly appreciated. \n<table><tbody><tr><td>Always-on memory agent that works continuously in the backgroundâ€”never sleeps, never forgets</td></tr><tr><td>Understands and remembers user goals, preferences, and context across sessions automatically</td></tr><tr><td>Reduces long-running token costs by caching insights and avoiding redundant LLM calls</td></tr></tbody></table><h2>ğŸ”„ How Proactive Memory Works</h2><pre><code>\ncd examples/proactive\npython proactive.py\n\n</code></pre><h3>Proactive Memory Lifecycle</h3><pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                         USER QUERY                                               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                 â”‚                                                           â”‚\n                 â–¼                                                           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚         ğŸ¤– MAIN AGENT                  â”‚         â”‚              ğŸ§  MEMU BOT                       â”‚\nâ”‚                                        â”‚         â”‚                                                â”‚\nâ”‚  Handle user queries &amp; execute tasks   â”‚  â—„â”€â”€â”€â–º  â”‚  Monitor, memorize &amp; proactive intelligence   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                        â”‚         â”‚                                                â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚  1. RECEIVE USER INPUT           â”‚  â”‚         â”‚  â”‚  1. MONITOR INPUT/OUTPUT                 â”‚  â”‚\nâ”‚  â”‚     Parse query, understand      â”‚  â”‚   â”€â”€â”€â–º  â”‚  â”‚     Observe agent interactions           â”‚  â”‚\nâ”‚  â”‚     context and intent           â”‚  â”‚         â”‚  â”‚     Track conversation flow              â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                 â”‚                      â”‚         â”‚                    â”‚                           â”‚\nâ”‚                 â–¼                      â”‚         â”‚                    â–¼                           â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚  2. PLAN &amp; EXECUTE               â”‚  â”‚         â”‚  â”‚  2. MEMORIZE &amp; EXTRACT                   â”‚  â”‚\nâ”‚  â”‚     Break down tasks             â”‚  â”‚   â—„â”€â”€â”€  â”‚  â”‚     Store insights, facts, preferences   â”‚  â”‚\nâ”‚  â”‚     Call tools, retrieve data    â”‚  â”‚  inject â”‚  â”‚     Extract skills &amp; knowledge           â”‚  â”‚\nâ”‚  â”‚     Generate responses           â”‚  â”‚  memory â”‚  â”‚     Update user profile                  â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                 â”‚                      â”‚         â”‚                    â”‚                           â”‚\nâ”‚                 â–¼                      â”‚         â”‚                    â–¼                           â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚  3. RESPOND TO USER              â”‚  â”‚         â”‚  â”‚  3. PREDICT USER INTENT                  â”‚  â”‚\nâ”‚  â”‚     Deliver answer/result        â”‚  â”‚   â”€â”€â”€â–º  â”‚  â”‚     Anticipate next steps                â”‚  â”‚\nâ”‚  â”‚     Continue conversation        â”‚  â”‚         â”‚  â”‚     Identify upcoming needs              â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                 â”‚                      â”‚         â”‚                    â”‚                           â”‚\nâ”‚                 â–¼                      â”‚         â”‚                    â–¼                           â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚  4. LOOP                         â”‚  â”‚         â”‚  â”‚  4. RUN PROACTIVE TASKS                  â”‚  â”‚\nâ”‚  â”‚     Wait for next user input     â”‚  â”‚   â—„â”€â”€â”€  â”‚  â”‚     Pre-fetch relevant context           â”‚  â”‚\nâ”‚  â”‚     or proactive suggestions     â”‚  â”‚  suggestâ”‚  â”‚     Prepare recommendations              â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â”‚     Update todolist autonomously         â”‚  â”‚\nâ”‚                                        â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                 â”‚                                                           â”‚\n                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                             â–¼\n                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                              â”‚     CONTINUOUS SYNC LOOP     â”‚\n                              â”‚  Agent â—„â”€â”€â–º MemU Bot â—„â”€â”€â–º DB â”‚\n                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</code></pre><h3>1. <strong>Information Recommendation</strong></h3><p><em>Agent monitors interests and proactively surfaces relevant content</em></p><pre><code># User has been researching AI topics\nMemU tracks: reading history, saved articles, search queries\n\n# When new content arrives:\nAgent: \"I found 3 new papers on RAG optimization that align with\n        your recent research on retrieval systems. One author\n        (Dr. Chen) you've cited before published yesterday.\"\n\n# Proactive behaviors:\n- Learns topic preferences from browsing patterns\n- Tracks author/source credibility preferences\n- Filters noise based on engagement history\n- Times recommendations for optimal attention\n</code></pre><p><em>Agent learns communication patterns and handles routine correspondence</em></p><pre><code># MemU observes email patterns over time:\n- Response templates for common scenarios\n- Priority contacts and urgent keywords\n- Scheduling preferences and availability\n- Writing style and tone variations\n\n# Proactive email assistance:\nAgent: \"You have 12 new emails. I've drafted responses for 3 routine\n        requests and flagged 2 urgent items from your priority contacts.\n        Should I also reschedule tomorrow's meeting based on the\n        conflict John mentioned?\"\n\n# Autonomous actions:\nâœ“ Draft context-aware replies\nâœ“ Categorize and prioritize inbox\nâœ“ Detect scheduling conflicts\nâœ“ Summarize long threads with key decisions\n</code></pre><h3>3. <strong>Trading &amp; Financial Monitoring</strong></h3><p><em>Agent tracks market context and user investment behavior</em></p><pre><code># MemU learns trading preferences:\n- Risk tolerance from historical decisions\n- Preferred sectors and asset classes\n- Response patterns to market events\n- Portfolio rebalancing triggers\n\n# Proactive alerts:\nAgent: \"NVDA dropped 5% in after-hours trading. Based on your past\n        behavior, you typically buy tech dips above 3%. Your current\n        allocation allows for $2,000 additional exposure while\n        maintaining your 70/30 equity-bond target.\"\n\n# Continuous monitoring:\n- Track price alerts tied to user-defined thresholds\n- Correlate news events with portfolio impact\n- Learn from executed vs. ignored recommendations\n- Anticipate tax-loss harvesting opportunities\n</code></pre><h2>ğŸ—‚ï¸ Hierarchical Memory Architecture</h2><p>MemU's three-layer system enables both  and <strong>proactive context loading</strong>:</p><img width=\"100%\" alt=\"structure\" src=\"https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/structure.png\"><table><thead><tr></tr></thead><tbody><tr><td>Direct access to original data</td><td>Background monitoring for new patterns</td></tr><tr><td>Real-time extraction from ongoing interactions</td></tr><tr><td>Automatic context assembly for anticipation</td></tr></tbody></table><ul><li>: New memories self-organize into topics</li><li>: System identifies recurring themes</li><li>: Anticipates what information will be needed next</li></ul><p>Experience proactive memory instantly:</p><p>ğŸ‘‰  - Hosted service with 7Ã—24 continuous learning</p><p>For enterprise deployment with custom proactive workflows, contact </p><table><thead><tr></tr></thead><tbody><tr><td><code>Authorization: Bearer YOUR_API_KEY</code></td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>Register continuous learning task</td></tr><tr><td><code>/api/v3/memory/memorize/status/{task_id}</code></td><td>Check real-time processing status</td></tr><tr><td><code>/api/v3/memory/categories</code></td><td>List auto-generated categories</td></tr><tr><td>Query memory (supports proactive context loading)</td></tr></tbody></table><blockquote><p>: Python 3.13+ and an OpenAI API key</p></blockquote><p> (in-memory):</p><pre><code>export OPENAI_API_KEY=your_api_key\ncd tests\npython test_inmemory.py\n</code></pre><p><strong>Test with Persistent Storage</strong> (PostgreSQL):</p><pre><code># Start PostgreSQL with pgvector\ndocker run -d \\\n  --name memu-postgres \\\n  -e POSTGRES_USER=postgres \\\n  -e POSTGRES_PASSWORD=postgres \\\n  -e POSTGRES_DB=memu \\\n  -p 5432:5432 \\\n  pgvector/pgvector:pg16\n\n# Run continuous learning test\nexport OPENAI_API_KEY=your_api_key\ncd tests\npython test_postgres.py\n</code></pre><p>Both examples demonstrate <strong>proactive memory workflows</strong>:</p><ol><li>: Process multiple files sequentially</li><li>: Immediate memory creation</li><li>: Context-aware memory surfacing</li></ol><h3>Custom LLM and Embedding Providers</h3><p>MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via :</p><pre><code>from memu import MemUService\n\nservice = MemUService(\n    llm_profiles={\n        # Default profile for LLM operations\n        \"default\": {\n            \"base_url\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n            \"api_key\": \"your_api_key\",\n            \"chat_model\": \"qwen3-max\",\n            \"client_backend\": \"sdk\"  # \"sdk\" or \"http\"\n        },\n        # Separate profile for embeddings\n        \"embedding\": {\n            \"base_url\": \"https://api.voyageai.com/v1\",\n            \"api_key\": \"your_voyage_api_key\",\n            \"embed_model\": \"voyage-3.5-lite\"\n        }\n    },\n    # ... other configuration\n)\n</code></pre><p>MemU supports <a href=\"https://openrouter.ai\">OpenRouter</a> as a model provider, giving you access to multiple LLM providers through a single API.</p><pre><code>from memu import MemoryService\n\nservice = MemoryService(\n    llm_profiles={\n        \"default\": {\n            \"provider\": \"openrouter\",\n            \"client_backend\": \"httpx\",\n            \"base_url\": \"https://openrouter.ai\",\n            \"api_key\": \"your_openrouter_api_key\",\n            \"chat_model\": \"anthropic/claude-3.5-sonnet\",  # Any OpenRouter model\n            \"embed_model\": \"openai/text-embedding-3-small\",  # Embedding model\n        },\n    },\n    database_config={\n        \"metadata_store\": {\"provider\": \"inmemory\"},\n    },\n)\n</code></pre><table><tbody><tr><td>Works with any OpenRouter chat model</td></tr><tr><td>Use OpenAI embedding models via OpenRouter</td></tr><tr><td>Use vision-capable models (e.g., )</td></tr></tbody></table><pre><code>export OPENROUTER_API_KEY=your_api_key\n\n# Full workflow test (memorize + retrieve)\npython tests/test_openrouter.py\n\n# Embedding-specific tests\npython tests/test_openrouter_embedding.py\n\n# Vision-specific tests\npython tests/test_openrouter_vision.py\n</code></pre><h3> - Continuous Learning Pipeline</h3><p>Processes inputs in real-time and immediately updates memory:</p><img width=\"100%\" alt=\"memorize\" src=\"https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/memorize.png\"><pre><code>result = await service.memorize(\n    resource_url=\"path/to/file.json\",  # File path or URL\n    modality=\"conversation\",            # conversation | document | image | video | audio\n    user={\"user_id\": \"123\"}             # Optional: scope to a user\n)\n\n# Returns immediately with extracted memory:\n{\n    \"resource\": {...},      # Stored resource metadata\n    \"items\": [...],         # Extracted memory items (available instantly)\n    \"categories\": [...]     # Auto-updated category structure\n}\n</code></pre><ul><li>Zero-delay processingâ€”memories available immediately</li><li>Automatic categorization without manual tagging</li><li>Cross-reference with existing memories for pattern detection</li></ul><h3> - Dual-Mode Intelligence</h3><p>MemU supports both <strong>proactive context loading</strong> and :</p><img width=\"100%\" alt=\"retrieve\" src=\"https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/retrieve.png\"><h4>RAG-based Retrieval ()</h4><p>Fast <strong>proactive context assembly</strong> using embeddings:</p><ul><li>âœ… : Sub-second memory surfacing</li><li>âœ… : Can run continuously without LLM costs</li><li>âœ… : Identifies most relevant memories automatically</li></ul><h4>LLM-based Retrieval ()</h4><p>Deep  for complex contexts:</p><ul><li>âœ… : LLM infers what user needs before they ask</li><li>âœ… : Automatically refines search as context develops</li><li>âœ… : Stops when sufficient context is gathered</li></ul><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr><td>Triggered context loading</td></tr><tr></tr></tbody></table><pre><code># Proactive retrieval with context history\nresult = await service.retrieve(\n    queries=[\n        {\"role\": \"user\", \"content\": {\"text\": \"What are their preferences?\"}},\n        {\"role\": \"user\", \"content\": {\"text\": \"Tell me about work habits\"}}\n    ],\n    where={\"user_id\": \"123\"},  # Optional: scope filter\n    method=\"rag\"  # or \"llm\" for deeper reasoning\n)\n\n# Returns context-aware results:\n{\n    \"categories\": [...],     # Relevant topic areas (auto-prioritized)\n    \"items\": [...],          # Specific memory facts\n    \"resources\": [...],      # Original sources for traceability\n    \"next_step_query\": \"...\" # Predicted follow-up context\n}\n</code></pre><p>: Use  to scope continuous monitoring:</p><ul><li> - User-specific context</li><li><code>where={\"agent_id__in\": [\"1\", \"2\"]}</code> - Multi-agent coordination</li><li>Omit  for global context awareness</li></ul><h3>Example 1: Always-Learning Assistant</h3><p>Continuously learns from every interaction without explicit memory commands:</p><pre><code>export OPENAI_API_KEY=your_api_key\npython examples/example_1_conversation_memory.py\n</code></pre><ul><li>Automatically extracts preferences from casual mentions</li><li>Builds relationship models from interaction patterns</li><li>Surfaces relevant context in future conversations</li><li>Adapts communication style based on learned preferences</li></ul><p> Personal AI assistants, customer support that remembers, social chatbots</p><h3>Example 2: Self-Improving Agent</h3><p>Learns from execution logs and proactively suggests optimizations:</p><pre><code>export OPENAI_API_KEY=your_api_key\npython examples/example_2_skill_extraction.py\n</code></pre><ul><li>Monitors agent actions and outcomes continuously</li><li>Identifies patterns in successes and failures</li><li>Auto-generates skill guides from experience</li><li>Proactively suggests strategies for similar future tasks</li></ul><p> DevOps automation, agent self-improvement, knowledge capture</p><h3>Example 3: Multimodal Context Builder</h3><p>Unifies memory across different input types for comprehensive context:</p><pre><code>export OPENAI_API_KEY=your_api_key\npython examples/example_3_multimodal_memory.py\n</code></pre><ul><li>Cross-references text, images, and documents automatically</li><li>Builds unified understanding across modalities</li><li>Surfaces visual context when discussing related topics</li><li>Anticipates information needs by combining multiple sources</li></ul><p> Documentation systems, learning platforms, research assistants</p><p>MemU achieves  on the Locomo benchmark across all reasoning tasks, demonstrating reliable proactive memory operations.</p><img width=\"100%\" alt=\"benchmark\" src=\"https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9\"><table><thead><tr></tr></thead><tbody><tr><td>Core proactive memory engine</td><td>7Ã—24 learning pipeline, auto-categorization</td></tr><tr><td>Backend with continuous sync</td><td>Real-time memory updates, webhook triggers</td></tr><tr><td>Live memory evolution monitoring</td></tr></tbody></table><p>We welcome contributions from the community! Whether you're fixing bugs, adding features, or improving documentation, your help is appreciated.</p><p>To start contributing to MemU, you'll need to set up your development environment:</p><ul><li><a href=\"https://github.com/astral-sh/uv\">uv</a> (Python package manager)</li></ul><h4>Setup Development Environment</h4><pre><code># 1. Fork and clone the repository\ngit clone https://github.com/YOUR_USERNAME/memU.git\ncd memU\n\n# 2. Install development dependencies\nmake install\n</code></pre><p>The  command will:</p><ul><li>Create a virtual environment using </li><li>Install all project dependencies</li><li>Set up pre-commit hooks for code quality checks</li></ul><p>Before submitting your contribution, ensure your code passes all quality checks:</p><p>The  command runs:</p><ul><li>: Ensures  consistency</li><li>: Lints code with Ruff, formats with Black</li><li>: Runs  for static type analysis</li><li>: Uses  to find obsolete dependencies</li></ul><p>For detailed contribution guidelines, code standards, and development practices, please see <a href=\"https://raw.githubusercontent.com/NevaMind-AI/memU/main/CONTRIBUTING.md\">CONTRIBUTING.md</a>.</p><ul><li>Create a new branch for each feature or bug fix</li><li>Write clear commit messages</li><li>Add tests for new functionality</li><li>Update documentation as needed</li><li>Run  before pushing</li></ul><div align=\"center\"><p>â­  to get notified about new releases!</p></div>",
      "contentLength": 19624,
      "flags": null,
      "enclosureUrl": "https://repository-images.githubusercontent.com/1028070615/af79b134-ac3f-4efb-9f95-5de0dc77a63d",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "x1xhlol/system-prompts-and-models-of-ai-tools",
      "url": "https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools",
      "date": 1771816069,
      "author": "",
      "guid": 47435,
      "unread": true,
      "content": "<p>FULL Augment Code, Claude Code, Cluely, CodeBuddy, Comet, Cursor, Devin AI, Junie, Kiro, Leap.new, Lovable, Manus, NotionAI, Orchids.app, Perplexity, Poke, Qoder, Replit, Same.dev, Trae, Traycer AI, VSCode Agent, Warp.dev, Windsurf, Xcode, Z.ai Code, Dia &amp; v0. (And other Open Sourced) System Prompts, Internal Tools &amp; AI Models</p><p align=\"center\">Official CA: DEffWzJyaFRNyA4ogUox631hfHuv3KLeCcpBh2ipBAGS (on Solana)</p><a href=\"https://discord.gg/NwzrWErdMU\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1402660735833604126?label=LeaksLab%20Discord&amp;logo=discord&amp;style=for-the-badge\" alt=\"LeaksLab Discord\"></a><p>ğŸ“œ Over  of insights into their structure and functionality.</p><p>If you find this collection valuable and appreciate the effort involved in obtaining and sharing these insights, please consider supporting the project.</p><p>You can show your support via:</p><p>ğŸ™ Thank you for your support!</p><p>Sponsor the most comprehensive repository of AI system prompts and reach thousands of developers.</p><h2>ğŸ›¡ï¸ Security Notice for AI Startups</h2><blockquote><p>âš ï¸  If you're an AI startup, make sure your data is secure. Exposed prompts or AI models can easily become a target for hackers.</p></blockquote><blockquote><p>ğŸ”  Interested in securing your AI systems? Check out , a service designed to help startups  leaks in system instructions, internal tools, and model configurations. <strong>Get a free AI security audit</strong> to ensure your AI is protected from vulnerabilities.</p></blockquote><a href=\"https://www.star-history.com/#x1xhlol/system-prompts-and-models-of-ai-tools&amp;Date\"></a><p>â­ <strong>Drop a star if you find this useful!</strong></p>",
      "contentLength": 1229,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/19ec439540a875a44f7c4e121887b417d68d3285eabda315bb5b169cdd9e8327/x1xhlol/system-prompts-and-models-of-ai-tools",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "OpenBB-finance/OpenBB",
      "url": "https://github.com/OpenBB-finance/OpenBB",
      "date": 1771816069,
      "author": "",
      "guid": 47436,
      "unread": true,
      "content": "<p>Financial data platform for analysts, quants and AI agents.</p><img src=\"https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-light.svg?raw=true#gh-light-mode-only\" alt=\"Open Data Platform by OpenBB logo\" width=\"600\"><img src=\"https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only\" alt=\"Open Data Platform by OpenBB logo\" width=\"600\"><p>Open Data Platform by OpenBB (ODP) is the open-source toolset that helps data engineers integrate proprietary, licensed, and public data sources into downstream applications like AI copilots and research dashboards.</p><p>ODP operates as the \"connect once, consume everywhere\" infrastructure layer that consolidates and exposes data to multiple surfaces at once: Python environments for quants, OpenBB Workspace and Excel for analysts, MCP servers for AI agents, and REST APIs for other applications.</p><a href=\"https://pro.openbb.co\"></a><p>Get started with: </p><pre><code>from openbb import obb\noutput = obb.equity.price.historical(\"AAPL\")\ndf = output.to_dataframe()\n</code></pre><p>While the Open Data Platform provides the open-source data integration foundation,  offers the enterprise UI for analysts to visualize datasets and leverage AI agents. The platform's \"connect once, consume everywhere\" architecture enables seamless integration between the two.</p><a href=\"https://pro.openbb.co\"></a><h3>Integrating Open Data Platform to the OpenBB Workspace</h3><p>Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.</p><pre><code>pip install \"openbb[all]\"\n</code></pre><ul><li>Start the API server over localhost.</li></ul><p>This will launch a FastAPI server, via Uvicorn, at .</p><h4>Integrate the ODP Backend to OpenBB Workspace</h4><ol><li>Click on \"Connect backend\"</li><li>Click on \"Test\". You should get a \"Test successful\" with the number of apps found.</li></ol><p>The ODP Python Package can be installed from <a href=\"https://pypi.org/project/openbb/\">PyPI package</a> by running </p><p>or by cloning the repository directly with <code>git clone https://github.com/OpenBB-finance/OpenBB.git</code>.</p><p>The ODP CLI is a command-line interface that allows you to access the ODP directly from your command line.</p><p>It can be installed by running </p><p>or by cloning the repository directly with <code>git clone https://github.com/OpenBB-finance/OpenBB.git</code>.</p><p>There are three main ways of contributing to this project. (Hopefully you have starred the project by now â­ï¸)</p><p>Distributed under the AGPLv3 License. See <a href=\"https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE\">LICENSE</a> for more information.</p><p>Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.</p><p>Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.</p><p>The data contained in the Open Data Platform is not necessarily accurate.</p><p>OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.</p><p>All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.</p><p>Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.</p><p>If you have any questions about the platform or anything OpenBB, feel free to email us at </p><p>If you want to say hi, or are interested in partnering with us, feel free to reach us at </p><p>This is a proxy of our growth and that we are just getting started.</p><p>OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.</p><a href=\"https://github.com/OpenBB-finance/OpenBB/graphs/contributors\"><img src=\"https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB\" width=\"800\"></a>",
      "contentLength": 3503,
      "flags": null,
      "enclosureUrl": "https://repository-images.githubusercontent.com/323048702/4659bbdb-ae11-4f51-8a16-860fa9dfc551",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "hiddify/hiddify-app",
      "url": "https://github.com/hiddify/hiddify-app",
      "date": 1771729583,
      "author": "",
      "guid": 47261,
      "unread": true,
      "content": "<p>Multi-platform auto-proxy client, supporting Sing-box, X-ray, TUIC, Hysteria, Reality, Trojan, SSH etc. Itâ€™s an open-source, secure and ad-free.</p><p dir=\"ltr\">A multi-platform proxy client based on <a href=\"https://github.com/SagerNet/sing-box\">Sing-box</a> universal proxy tool-chain. Hiddify offers a wide range of capabilities, like automatic node selection, TUN mode, remote profiles etc. Hiddify is ad-free and open-source. With support for a wide range of protocols, it provides a secure and private way for accessing free internet.</p><p>âœˆï¸ Multi-platform: Android, iOS, Windows, macOS and Linux</p><p>â­ Intuitive and accessible UI</p><p>ğŸ” Delay based node selection</p><p>ğŸŸ¡ Wide range of protocols: Vless, Vmess, Reality, TUIC, Hysteria, Wireguard, SSH etc.</p><p>ğŸŸ¡ Subscription link and configuration formats: Sing-box, V2ray, Clash, Clash meta</p><p>ğŸ”„ Automatic subscription update</p><p>ğŸ” Display profile information including remaining days and traffic usage</p><p>ğŸ›¡ Open source, secure and community driven</p><p>âš™ Compatible with all proxy management panels</p><p>â­ Appropriate configuration for Iran, China, Russia and other countries</p><p>ğŸ“± Available on official stores</p><h2>âš™ï¸ Installation and tutorials</h2><p><strong>Find tutorial information on our wiki page by clicking on image below.</strong></p><p>Improve existing languages or add new ones by manually editing the JSON files located in  or by using the <a href=\"https://fink.inlang.com/github.com/hiddify/hiddify-next\">Inlang online editor</a>.</p><p>We would like to express our sincere appreciation to the contributors of the following projects, whose robust foundation and innovative features have significantly enhanced the success and functionality of this project.</p><p>The easiest way to support us is to click on the star (â­) at the top of this page.</p><a href=\"https://next.ossinsight.io/widgets/official/analyze-repo-stars-history?repo_id=643504282\" target=\"_blank\" align=\"center\"></a><p>We also need financial support for our services. All of our activities are done voluntarily and financial support will be spent on the development of the project. You can view our support addresses <a href=\"https://github.com/hiddify/hiddify-server/wiki/support\">here</a>.</p><h2>ğŸ‘©â€ğŸ« Collaboration and Contact Information</h2><p>Hiddify is a community driven project. If you're interested in contributing, please read the <a href=\"https://raw.githubusercontent.com/hiddify/hiddify-app/main/CONTRIBUTING.md\">contribution guidelines</a>. We would specially appreciate any help we can get in these areas: <strong>Flutter, Go, iOS development (Swift), Android development (Kotlin).</strong></p><p align=\"center\"> We appreciate all people who are participating in this project. Some people here and many many more outside of Github. It means a lot to us. â™¥ </p>",
      "contentLength": 2258,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Stremio/stremio-web",
      "url": "https://github.com/Stremio/stremio-web",
      "date": 1771729583,
      "author": "",
      "guid": 47262,
      "unread": true,
      "content": "<p>Stremio - Freedom to Stream</p><p>Stremio is a modern media center that's a one-stop solution for your video entertainment. You discover, watch and organize video content from easy to install addons.</p><ul></ul><pre><code>docker build -t stremio-web .\ndocker run -p 8080:8080 stremio-web\n</code></pre><p>Stremio is copyright 2017-2023 Smart code and available under GPLv2 license. See the <a href=\"https://raw.githubusercontent.com/Stremio/stremio-web/development/LICENSE.md\">LICENSE</a> file in the project for more information.</p>",
      "contentLength": 392,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ggml-org/ggml",
      "url": "https://github.com/ggml-org/ggml",
      "date": 1771729583,
      "author": "",
      "guid": 47263,
      "unread": true,
      "content": "<p>Tensor library for machine learning</p><p>Tensor library for machine learning</p><p><em><strong>Note that this project is under active development.  Some of the development is currently happening in the <a href=\"https://github.com/ggerganov/llama.cpp\">llama.cpp</a> and <a href=\"https://github.com/ggerganov/whisper.cpp\">whisper.cpp</a> repos</strong></em></p><ul><li>Low-level cross-platform implementation</li><li>Integer quantization support</li><li>Automatic differentiation</li><li>ADAM and L-BFGS optimizers</li><li>No third-party dependencies</li><li>Zero memory allocations during runtime</li></ul><pre><code>git clone https://github.com/ggml-org/ggml\ncd ggml\n\n# install python dependencies in a virtual environment\npython3.10 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n\n# build the examples\nmkdir build &amp;&amp; cd build\ncmake ..\ncmake --build . --config Release -j 8\n</code></pre><pre><code># run the GPT-2 small 117M model\n../examples/gpt-2/download-ggml-model.sh 117M\n./bin/gpt-2-backend -m models/gpt-2-117M/ggml-model.bin -p \"This is an example\"\n</code></pre><p>For more information, checkout the corresponding programs in the <a href=\"https://raw.githubusercontent.com/ggml-org/ggml/master/examples\">examples</a> folder.</p><pre><code># fix the path to point to your CUDA compiler\ncmake -DGGML_CUDA=ON -DCMAKE_CUDA_COMPILER=/usr/local/cuda-12.1/bin/nvcc ..\n</code></pre><pre><code>cmake -DCMAKE_C_COMPILER=\"$(hipconfig -l)/clang\" -DCMAKE_CXX_COMPILER=\"$(hipconfig -l)/clang++\" -DGGML_HIP=ON\n</code></pre><pre><code># linux\nsource /opt/intel/oneapi/setvars.sh\ncmake -G \"Ninja\" -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL=ON ..\n\n# windows\n\"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat\"\ncmake -G \"Ninja\" -DCMAKE_C_COMPILER=cl -DCMAKE_CXX_COMPILER=icx -DGGML_SYCL=ON ..\n</code></pre><p>Download and unzip the NDK from this download <a href=\"https://developer.android.com/ndk/downloads\">page</a>. Set the NDK_ROOT_PATH environment variable or provide the absolute path to the CMAKE_ANDROID_NDK in the command below.</p><pre><code>cmake .. \\\n   -DCMAKE_SYSTEM_NAME=Android \\\n   -DCMAKE_SYSTEM_VERSION=33 \\\n   -DCMAKE_ANDROID_ARCH_ABI=arm64-v8a \\\n   -DCMAKE_ANDROID_NDK=$NDK_ROOT_PATH\n   -DCMAKE_ANDROID_STL_TYPE=c++_shared\n</code></pre><pre><code># create directories\nadb shell 'mkdir /data/local/tmp/bin'\nadb shell 'mkdir /data/local/tmp/models'\n\n# push the compiled binaries to the folder\nadb push bin/* /data/local/tmp/bin/\n\n# push the ggml library\nadb push src/libggml.so /data/local/tmp/\n\n# push model files\nadb push models/gpt-2-117M/ggml-model.bin /data/local/tmp/models/\n\nadb shell\ncd /data/local/tmp\nexport LD_LIBRARY_PATH=/data/local/tmp\n./bin/gpt-2-backend -m models/ggml-model.bin -p \"this is an example\"\n</code></pre>",
      "contentLength": 2249,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "abhigyanpatwari/GitNexus",
      "url": "https://github.com/abhigyanpatwari/GitNexus",
      "date": 1771729583,
      "author": "",
      "guid": 47264,
      "unread": true,
      "content": "<p>GitNexus: The Zero-Server Code Intelligence Engine - GitNexus is a client-side knowledge graph creator that runs entirely in your browser. Drop in a GitHub repo or ZIP file, and get an interactive knowledge graph wit a built in Graph RAG Agent. Perfect for code exploration</p><p><strong>Building git for agent context.</strong></p><p>Indexes any codebase into a knowledge graph â€” every dependency, call chain, cluster, and execution flow â€” then exposes it through smart tools so AI agents never miss code.</p><blockquote><p><em>Like DeepWiki, but deeper.</em> DeepWiki helps you  code. GitNexus lets you  it â€” because a knowledge graph tracks every relationship, not just descriptions.</p></blockquote><p> The  is a quick way to chat with any repo. The  is how you make your AI agent actually reliable â€” it gives Cursor, Claude Code, and friends a deep architectural view of your codebase so they stop missing dependencies, breaking call chains, and shipping blind edits. Even smaller models get full architectural clarity, making it compete with goliath models.</p><table><tbody><tr><td>Index repos locally, connect AI agents via MCP</td><td>Visual graph explorer + AI chat in browser</td></tr><tr><td>Daily development with Cursor, Claude Code, Windsurf, OpenCode</td><td>Quick exploration, demos, one-off analysis</td></tr><tr><td>Limited by browser memory (~5k files), or unlimited via backend mode</td></tr><tr><td>KuzuDB native (fast, persistent)</td><td>KuzuDB WASM (in-memory, per session)</td></tr><tr><td>Tree-sitter native bindings</td></tr><tr><td>Everything local, no network</td><td>Everything in-browser, no server</td></tr></tbody></table><blockquote><p> connects the two â€” the web UI auto-detects the local server and can browse all your CLI-indexed repos without re-uploading or re-indexing.</p></blockquote><p>The CLI indexes your repository and runs an MCP server that gives AI agents deep codebase awareness.</p><pre><code># Index your repo (run from repo root)\nnpx gitnexus analyze\n</code></pre><p>That's it. This indexes the codebase, installs agent skills, registers Claude Code hooks, and creates  /  context files â€” all in one command.</p><p>To configure MCP for your editor, run  once â€” or set it up manually below.</p><p> auto-detects your editors and writes the correct global MCP config. You only need to run it once.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><blockquote><p> gets the deepest integration: MCP tools + agent skills + PreToolUse hooks that automatically enrich grep/glob/bash calls with knowledge graph context.</p></blockquote><p>If you prefer manual configuration:</p><p> (full support â€” MCP + skills + hooks):</p><pre><code>claude mcp add gitnexus -- npx -y gitnexus@latest mcp\n</code></pre><p> ( â€” global, works for all projects):</p><pre><code>{\n  \"mcpServers\": {\n    \"gitnexus\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"gitnexus@latest\", \"mcp\"]\n    }\n  }\n}\n</code></pre><p> (<code>~/.config/opencode/config.json</code>):</p><pre><code>{\n  \"mcp\": {\n    \"gitnexus\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"gitnexus@latest\", \"mcp\"]\n    }\n  }\n}\n</code></pre><pre><code>gitnexus setup                    # Configure MCP for your editors (one-time)\ngitnexus analyze [path]           # Index a repository (or update stale index)\ngitnexus analyze --force          # Force full re-index\ngitnexus analyze --skip-embeddings  # Skip embedding generation (faster)\ngitnexus mcp                     # Start MCP server (stdio) â€” serves all indexed repos\ngitnexus serve                   # Start local HTTP server (multi-repo) for web UI connection\ngitnexus list                    # List all indexed repositories\ngitnexus status                  # Show index status for current repo\ngitnexus clean                   # Delete index for current repo\ngitnexus clean --all --force     # Delete all indexes\ngitnexus wiki [path]             # Generate repository wiki from knowledge graph\ngitnexus wiki --model &lt;model&gt;    # Wiki with custom LLM model (default: gpt-4o-mini)\ngitnexus wiki --base-url &lt;url&gt;   # Wiki with custom LLM API base URL\n</code></pre><table><thead><tr></tr></thead><tbody><tr><td>Discover all indexed repositories</td></tr><tr><td>Process-grouped hybrid search (BM25 + semantic + RRF)</td></tr><tr><td>360-degree symbol view â€” categorized refs, process participation</td></tr><tr><td>Blast radius analysis with depth grouping and confidence</td></tr><tr><td>Git-diff impact â€” maps changed lines to affected processes</td></tr><tr><td>Multi-file coordinated rename with graph + text search</td></tr><tr></tr></tbody></table><blockquote><p>When only one repo is indexed, the  parameter is optional. With multiple repos, specify which one: <code>query({query: \"auth\", repo: \"my-app\"})</code>.</p></blockquote><p> for instant context:</p><table><tbody><tr><td>List all indexed repositories (read this first)</td></tr><tr><td><code>gitnexus://repo/{name}/context</code></td><td>Codebase stats, staleness check, and available tools</td></tr><tr><td><code>gitnexus://repo/{name}/clusters</code></td><td>All functional clusters with cohesion scores</td></tr><tr><td><code>gitnexus://repo/{name}/cluster/{name}</code></td><td>Cluster members and details</td></tr><tr><td><code>gitnexus://repo/{name}/processes</code></td></tr><tr><td><code>gitnexus://repo/{name}/process/{name}</code></td><td>Full process trace with steps</td></tr><tr><td><code>gitnexus://repo/{name}/schema</code></td><td>Graph schema for Cypher queries</td></tr></tbody></table><p> for guided workflows:</p><table><tbody><tr><td>Pre-commit change analysis â€” scope, affected processes, risk level</td></tr><tr><td>Architecture documentation from the knowledge graph with mermaid diagrams</td></tr></tbody></table><p> installed to  automatically:</p><ul><li> â€” Navigate unfamiliar code using the knowledge graph</li><li> â€” Trace bugs through call chains</li><li> â€” Analyze blast radius before changes</li><li> â€” Plan safe refactors using dependency mapping</li></ul><h2>Multi-Repo MCP Architecture</h2><p>GitNexus uses a  so one MCP server can serve multiple indexed repos. No per-project MCP config needed â€” set it up once and it works everywhere.</p><pre><code>flowchart TD\n    subgraph CLI [CLI Commands]\n        Setup[\"gitnexus setup\"]\n        Analyze[\"gitnexus analyze\"]\n        Clean[\"gitnexus clean\"]\n        List[\"gitnexus list\"]\n    end\n\n    subgraph Registry [\"~/.gitnexus/\"]\n        RegFile[\"registry.json\"]\n    end\n\n    subgraph Repos [Project Repos]\n        RepoA[\".gitnexus/ in repo A\"]\n        RepoB[\".gitnexus/ in repo B\"]\n    end\n\n    subgraph MCP [MCP Server]\n        Server[\"server.ts\"]\n        Backend[\"LocalBackend\"]\n        Pool[\"Connection Pool\"]\n        ConnA[\"KuzuDB conn A\"]\n        ConnB[\"KuzuDB conn B\"]\n    end\n\n    Setup --&gt;|\"writes global MCP config\"| CursorConfig[\"~/.cursor/mcp.json\"]\n    Analyze --&gt;|\"registers repo\"| RegFile\n    Analyze --&gt;|\"stores index\"| RepoA\n    Clean --&gt;|\"unregisters repo\"| RegFile\n    List --&gt;|\"reads\"| RegFile\n    Server --&gt;|\"reads registry\"| RegFile\n    Server --&gt; Backend\n    Backend --&gt; Pool\n    Pool --&gt;|\"lazy open\"| ConnA\n    Pool --&gt;|\"lazy open\"| ConnB\n    ConnA --&gt;|\"queries\"| RepoA\n    ConnB --&gt;|\"queries\"| RepoB\n</code></pre><p> Each  stores the index in  inside the repo (portable, gitignored) and registers a pointer in <code>~/.gitnexus/registry.json</code>. When an AI agent starts, the MCP server reads the registry and can serve any indexed repo. KuzuDB connections are opened lazily on first query and evicted after 5 minutes of inactivity (max 5 concurrent). If only one repo is indexed, the  parameter is optional on all tools â€” agents don't need to change anything.</p><p>A fully client-side graph explorer and AI chat. No server, no install â€” your code never leaves the browser.</p><img width=\"2550\" height=\"1343\" alt=\"gitnexus_img\" src=\"https://github.com/user-attachments/assets/cc5d637d-e0e5-48e6-93ff-5bcfdb929285\"><pre><code>git clone https://github.com/abhigyanpatwari/gitnexus.git\ncd gitnexus/gitnexus-web\nnpm install\nnpm run dev\n</code></pre><p>The web UI uses the same indexing pipeline as the CLI but runs entirely in WebAssembly (Tree-sitter WASM, KuzuDB WASM, in-browser embeddings). It's great for quick exploration but limited by browser memory for larger repos.</p><p> Run  and open the web UI locally â€” it auto-detects the server and shows all your indexed repos, with full AI chat support. No need to re-upload or re-index. The agent's tools (Cypher queries, search, code navigation) route through the backend HTTP API automatically.</p><h2>The Problem GitNexus Solves</h2><p>Tools like , , , , and  are powerful â€” but they don't truly know your codebase structure.</p><ol><li>AI edits </li><li>Doesn't know 47 functions depend on its return type</li></ol><h3>Traditional Graph RAG vs GitNexus</h3><p>Traditional approaches give the LLM raw graph edges and hope it explores enough. GitNexus <strong>precomputes structure at index time</strong> â€” clustering, tracing, scoring â€” so tools return complete context in one call:</p><pre><code>flowchart TB\n    subgraph Traditional[\"Traditional Graph RAG\"]\n        direction TB\n        U1[\"User: What depends on UserService?\"]\n        U1 --&gt; LLM1[\"LLM receives raw graph\"]\n        LLM1 --&gt; Q1[\"Query 1: Find callers\"]\n        Q1 --&gt; Q2[\"Query 2: What files?\"]\n        Q2 --&gt; Q3[\"Query 3: Filter tests?\"]\n        Q3 --&gt; Q4[\"Query 4: High-risk?\"]\n        Q4 --&gt; OUT1[\"Answer after 4+ queries\"]\n    end\n\n    subgraph GN[\"GitNexus Smart Tools\"]\n        direction TB\n        U2[\"User: What depends on UserService?\"]\n        U2 --&gt; TOOL[\"impact UserService upstream\"]\n        TOOL --&gt; PRECOMP[\"Pre-structured response:\n        8 callers, 3 clusters, all 90%+ confidence\"]\n        PRECOMP --&gt; OUT2[\"Complete answer, 1 query\"]\n    end\n</code></pre><p><strong>Core innovation: Precomputed Relational Intelligence</strong></p><ul><li> â€” LLM can't miss context, it's already in the tool response</li><li> â€” No 10-query chains to understand one function</li><li> â€” Smaller LLMs work because tools do the heavy lifting</li></ul><p>GitNexus builds a complete knowledge graph of your codebase through a multi-phase indexing pipeline:</p><ol><li> â€” Walks the file tree and maps folder/file relationships</li><li> â€” Extracts functions, classes, methods, and interfaces using Tree-sitter ASTs</li><li> â€” Resolves imports and function calls across files with language-aware logic</li><li> â€” Groups related symbols into functional communities</li><li> â€” Traces execution flows from entry points through call chains</li><li> â€” Builds hybrid search indexes for fast retrieval</li></ol><p>TypeScript, JavaScript, Python, Java, C, C++, C#, Go, Rust</p><pre><code>impact({target: \"UserService\", direction: \"upstream\", minConfidence: 0.8})\n\nTARGET: Class UserService (src/services/user.ts)\n\nUPSTREAM (what depends on this):\n  Depth 1 (WILL BREAK):\n    handleLogin [CALLS 90%] -&gt; src/api/auth.ts:45\n    handleRegister [CALLS 90%] -&gt; src/api/auth.ts:78\n    UserController [CALLS 85%] -&gt; src/controllers/user.ts:12\n  Depth 2 (LIKELY AFFECTED):\n    authRouter [IMPORTS] -&gt; src/routes/auth.ts\n</code></pre><p>Options: , ,  (, , , ), </p><pre><code>query({query: \"authentication middleware\"})\n\nprocesses:\n  - summary: \"LoginFlow\"\n    priority: 0.042\n    symbol_count: 4\n    process_type: cross_community\n    step_count: 7\n\nprocess_symbols:\n  - name: validateUser\n    type: Function\n    filePath: src/auth/validate.ts\n    process_id: proc_login\n    step_index: 2\n\ndefinitions:\n  - name: AuthConfig\n    type: Interface\n    filePath: src/types/auth.ts\n</code></pre><h3>Context (360-degree Symbol View)</h3><pre><code>context({name: \"validateUser\"})\n\nsymbol:\n  uid: \"Function:validateUser\"\n  kind: Function\n  filePath: src/auth/validate.ts\n  startLine: 15\n\nincoming:\n  calls: [handleLogin, handleRegister, UserController]\n  imports: [authRouter]\n\noutgoing:\n  calls: [checkPassword, createSession]\n\nprocesses:\n  - name: LoginFlow (step 2/7)\n  - name: RegistrationFlow (step 3/5)\n</code></pre><h3>Detect Changes (Pre-Commit)</h3><pre><code>detect_changes({scope: \"all\"})\n\nsummary:\n  changed_count: 12\n  affected_count: 3\n  changed_files: 4\n  risk_level: medium\n\nchanged_symbols: [validateUser, AuthService, ...]\naffected_processes: [LoginFlow, RegistrationFlow, ...]\n</code></pre><pre><code>rename({symbol_name: \"validateUser\", new_name: \"verifyUser\", dry_run: true})\n\nstatus: success\nfiles_affected: 5\ntotal_edits: 8\ngraph_edits: 6     (high confidence)\ntext_search_edits: 2  (review carefully)\nchanges: [...]\n</code></pre><pre><code>-- Find what calls auth functions with high confidence\nMATCH (c:Community {heuristicLabel: 'Authentication'})&lt;-[:CodeRelation {type: 'MEMBER_OF'}]-(fn)\nMATCH (caller)-[r:CodeRelation {type: 'CALLS'}]-&gt;(fn)\nWHERE r.confidence &gt; 0.8\nRETURN caller.name, fn.name, r.confidence\nORDER BY r.confidence DESC\n</code></pre><p>Generate LLM-powered documentation from your knowledge graph:</p><pre><code># Requires an LLM API key (OPENAI_API_KEY, etc.)\ngitnexus wiki\n\n# Use a custom model or provider\ngitnexus wiki --model gpt-4o\ngitnexus wiki --base-url https://api.anthropic.com/v1\n\n# Force full regeneration\ngitnexus wiki --force\n</code></pre><p>The wiki generator reads the indexed graph structure, groups files into modules via LLM, generates per-module documentation pages, and creates an overview page â€” all with cross-references to the knowledge graph.</p><table><tbody><tr></tr><tr><td>Tree-sitter native bindings</td></tr><tr></tr><tr><td>HuggingFace transformers.js (GPU/CPU)</td><td>transformers.js (WebGPU/WASM)</td></tr><tr></tr><tr></tr><tr><td>Sigma.js + Graphology (WebGL)</td></tr><tr><td>React 18, TypeScript, Vite, Tailwind v4</td></tr><tr></tr><tr></tr></tbody></table><ul><li>: Everything runs locally on your machine. No network calls. Index stored in  (gitignored). Global registry at  stores only paths and metadata.</li><li>: Everything runs in your browser. No code uploaded to any server. API keys stored in localStorage only.</li><li>Open source â€” audit the code yourself.</li></ul>",
      "contentLength": 12069,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/827f5f910872c7b2b8d9277367fe236770248979bb0f5de58aab9b1e76f44f6e/abhigyanpatwari/GitNexus",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "cloudflare/agents",
      "url": "https://github.com/cloudflare/agents",
      "date": 1771729583,
      "author": "",
      "guid": 47265,
      "unread": true,
      "content": "<p>Build and deploy AI Agents on Cloudflare</p><p>Agents are persistent, stateful execution environments for agentic workloads, powered by Cloudflare <a href=\"https://developers.cloudflare.com/durable-objects/\">Durable Objects</a>. Each agent has its own state, storage, and lifecycle â€” with built-in support for real-time communication, scheduling, AI model calls, MCP, workflows, and more.</p><p>Agents hibernate when idle and wake on demand. You can run millions of them â€” one per user, per session, per game room â€” each costs nothing when inactive.</p><pre><code>npm create cloudflare@latest -- --template cloudflare/agents-starter\n</code></pre><p>Or add to an existing project:</p><p>A counter agent with persistent state, callable methods, and real-time sync to a React frontend:</p><pre><code>// server.ts\nimport { Agent, routeAgentRequest, callable } from \"agents\";\n\nexport type CounterState = { count: number };\n\nexport class CounterAgent extends Agent&lt;Env, CounterState&gt; {\n  initialState = { count: 0 };\n\n  @callable()\n  increment() {\n    this.setState({ count: this.state.count + 1 });\n    return this.state.count;\n  }\n\n  @callable()\n  decrement() {\n    this.setState({ count: this.state.count - 1 });\n    return this.state.count;\n  }\n}\n\nexport default {\n  async fetch(request: Request, env: Env, ctx: ExecutionContext) {\n    return (\n      (await routeAgentRequest(request, env)) ??\n      new Response(\"Not found\", { status: 404 })\n    );\n  }\n};\n</code></pre><pre><code>// client.tsx\nimport { useAgent } from \"agents/react\";\nimport { useState } from \"react\";\nimport type { CounterAgent, CounterState } from \"./server\";\n\nfunction Counter() {\n  const [count, setCount] = useState(0);\n\n  const agent = useAgent&lt;CounterAgent, CounterState&gt;({\n    agent: \"CounterAgent\",\n    onStateUpdate: (state) =&gt; setCount(state.count)\n  });\n\n  return (\n    &lt;div&gt;\n      &lt;span&gt;{count}&lt;/span&gt;\n      &lt;button onClick={() =&gt; agent.stub.increment()}&gt;+&lt;/button&gt;\n      &lt;button onClick={() =&gt; agent.stub.decrement()}&gt;-&lt;/button&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre><p>State changes sync to all connected clients automatically. Call methods like they're local functions.</p><table><tbody><tr><td>Syncs to all connected clients, survives restarts</td></tr><tr><td>Type-safe RPC via the  decorator</td></tr><tr><td>One-time, recurring, and cron-based tasks</td></tr><tr><td>Real-time bidirectional communication with lifecycle hooks</td></tr><tr><td>Message persistence, resumable streaming, server/client tool execution</td></tr><tr><td>Act as MCP servers or connect as MCP clients</td></tr><tr><td>Durable multi-step tasks with human-in-the-loop approval</td></tr><tr><td>Receive and respond via Cloudflare Email Routing</td></tr><tr><td>LLMs generate executable TypeScript instead of individual tool calls</td></tr><tr><td>Direct SQLite queries via Durable Objects</td></tr><tr><td> and  for frontend integration</td></tr><tr><td> for non-React environments</td></tr></tbody></table><p> Realtime voice agents, web browsing (headless browser), sandboxed code execution, and multi-channel communication (SMS, messengers).</p><table><tbody><tr><td>Core SDK â€” Agent class, routing, state, scheduling, MCP, email, workflows</td></tr><tr><td>Higher-level AI chat â€” persistent messages, resumable streaming, tool execution</td></tr><tr><td>Hono middleware for adding agents to Hono apps</td></tr></tbody></table><p>The <a href=\"https://raw.githubusercontent.com/cloudflare/agents/main/examples\"></a> directory has self-contained demos covering most SDK features â€” MCP servers/clients, workflows, email agents, webhooks, tic-tac-toe, resumable streaming, and more. The <a href=\"https://raw.githubusercontent.com/cloudflare/agents/main/examples/playground\"></a> is the kitchen-sink showcase with everything in one UI.</p><pre><code>cd examples/playground\nnpm run dev\n</code></pre><p>Node 24+ required. Uses npm workspaces.</p><pre><code>npm install          # install all workspaces\nnpm run build        # build all packages\nnpm run check        # full CI check (format, lint, typecheck, exports)\nCI=true npm test     # run tests (vitest + vitest-pool-workers)\n</code></pre><p>Changes to  need a changeset:</p><p>We are not accepting external pull requests at this time â€” the SDK is evolving quickly and we want to keep the surface area manageable. That said, we'd love to hear from you:</p>",
      "contentLength": 3611,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "anthropics/claude-code",
      "url": "https://github.com/anthropics/claude-code",
      "date": 1771729583,
      "author": "",
      "guid": 47266,
      "unread": true,
      "content": "<p>Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.</p><p>Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows -- all through natural language commands. Use it in your terminal, IDE, or tag @claude on Github.</p><img src=\"https://raw.githubusercontent.com/anthropics/claude-code/main/demo.gif\"><blockquote><p>[!NOTE] Installation via npm is deprecated. Use one of the recommended methods below.</p></blockquote><p>For more installation options, uninstall steps, and troubleshooting, see the <a href=\"https://code.claude.com/docs/en/setup\">setup documentation</a>.</p><ol><li><p><strong>MacOS/Linux (Recommended):</strong></p><pre><code>curl -fsSL https://claude.ai/install.sh | bash\n</code></pre><pre><code>brew install --cask claude-code\n</code></pre><pre><code>irm https://claude.ai/install.ps1 | iex\n</code></pre><pre><code>winget install Anthropic.ClaudeCode\n</code></pre><pre><code>npm install -g @anthropic-ai/claude-code\n</code></pre></li><li><p>Navigate to your project directory and run .</p></li></ol><p>This repository includes several Claude Code plugins that extend functionality with custom commands and agents. See the <a href=\"https://raw.githubusercontent.com/anthropics/claude-code/main/plugins/README.md\">plugins directory</a> for detailed documentation on available plugins.</p><p>We welcome your feedback. Use the  command to report issues directly within Claude Code, or file a <a href=\"https://github.com/anthropics/claude-code/issues\">GitHub issue</a>.</p><p>Join the <a href=\"https://anthropic.com/discord\">Claude Developers Discord</a> to connect with other developers using Claude Code. Get help, share feedback, and discuss your projects with the community.</p><h2>Data collection, usage, and retention</h2><p>When you use Claude Code, we collect feedback, which includes usage data (such as code acceptance or rejections), associated conversation data, and user feedback submitted via the  command.</p><p>We have implemented several safeguards to protect your data, including limited retention periods for sensitive information, restricted access to user session data, and clear policies against using feedback for model training.</p>",
      "contentLength": 1892,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "stan-smith/FossFLOW",
      "url": "https://github.com/stan-smith/FossFLOW",
      "date": 1771729583,
      "author": "",
      "guid": 47267,
      "unread": true,
      "content": "<p>Make beautiful isometric infrastructure diagrams</p><p> Stan here, if you've used FossFLOW and it's helped you, <b>I'd really appreciate if you could donate something small :)</b> I work full time, and finding the time to work on this project is challenging enough. If you've had a feature that I've implemented for you, or fixed a bug it'd be great if you could :) if not, that's not a problem, this software will always remain free!</p><p> If you haven't yet, please check out the underlying library this is built on by <a href=\"https://github.com/markmanx/isoflow\">@markmanx</a> I truly stand on the shoulders of a giant here ğŸ«¡</p><p align=\"center\"> Go to <b> --&gt; https://stan-smith.github.io/FossFLOW/ &lt;-- </b></p><p>FossFLOW is a powerful, open-source Progressive Web App (PWA) for creating beautiful isometric diagrams. Built with React and the <a href=\"https://github.com/markmanx/isoflow\">Isoflow</a> (Now forked and published to NPM as fossflow) library, it runs entirely in your browser with offline support.</p><h2>ğŸ³ Quick Deploy with Docker</h2><pre><code># Using Docker Compose (recommended - includes persistent storage)\ndocker compose up\n\n# Or run directly from Docker Hub with persistent storage\ndocker run -p 80:80 -v $(pwd)/diagrams:/data/diagrams stnsmith/fossflow:latest\n</code></pre><p>Server storage is enabled by default in Docker. Your diagrams will be saved to  on the host.</p><p>To disable server storage, set <code>ENABLE_SERVER_STORAGE=false</code>:</p><pre><code>docker run -p 80:80 -e ENABLE_SERVER_STORAGE=false stnsmith/fossflow:latest\n</code></pre><h2>Quick Start (Local Development)</h2><pre><code># Clone the repository\ngit clone https://github.com/stan-smith/FossFLOW\ncd FossFLOW\n\n# Install dependencies\nnpm install\n\n# Build the library (required first time)\nnpm run build:lib\n\n# Start development server\nnpm run dev\n</code></pre><p>This is a monorepo containing two packages:</p><ul><li> - React component library for drawing network diagrams (built with Webpack)</li><li> - Progressive Web App which wraps the lib and presents it (built with RSBuild)</li></ul><pre><code># Development\nnpm run dev          # Start app development server\nnpm run dev:lib      # Watch mode for library development\n\n# Building\nnpm run build        # Build both library and app\nnpm run build:lib    # Build library only\nnpm run build:app    # Build app only\n\n# Testing &amp; Linting\nnpm test             # Run unit tests\nnpm run lint         # Check for linting errors\n\n# E2E Tests (Selenium)\ncd e2e-tests\n./run-tests.sh       # Run end-to-end tests (requires Docker &amp; Python)\n\n# Publishing\nnpm run publish:lib  # Publish library to npm\n</code></pre><ol><li><ul><li>Press the \"+\" button on the top right menu, the library of components will appear on the left</li><li>Drag and drop components from the library onto the canvas</li><li>Or right-click on the grid and select \"Add node\"</li></ul></li><li><ul><li>Select the Connector tool (press 'C' or click connector icon)</li><li> (default): Click first node, then click second node</li><li> (optional): Click and drag from first to second node</li><li>Switch modes in Settings â†’ Connectors tab</li></ul></li><li><ul><li> - Saves to browser session</li><li> - Download as JSON file</li><li> - Load from JSON file</li></ul></li></ol><ul><li>: Temporary saves cleared when browser closes</li><li>: Permanent storage as JSON files</li><li>: Automatically saves changes every 5 seconds to session</li></ul>",
      "contentLength": 2948,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "PowerShell/PowerShell",
      "url": "https://github.com/PowerShell/PowerShell",
      "date": 1771729583,
      "author": "",
      "guid": 47268,
      "unread": true,
      "content": "<p>PowerShell for every system!</p><p>Welcome to the PowerShell GitHub Community! <a href=\"https://learn.microsoft.com/powershell/scripting/overview\">PowerShell</a> is a cross-platform (Windows, Linux, and macOS) automation and configuration tool/framework that works well with your existing tools and is optimized for dealing with structured data (e.g. JSON, CSV, XML, etc.), REST APIs, and object models. It includes a command-line shell, an associated scripting language, and a framework for processing cmdlets.</p><h2>Windows PowerShell vs. PowerShell 7+</h2><p>Although this repository started as a fork of the Windows PowerShell codebase, changes made in this repository are not ported back to Windows PowerShell 5.1. This also means that <a href=\"https://github.com/PowerShell/PowerShell/issues\">issues tracked here</a> are only for PowerShell 7.x and higher. Windows PowerShell specific issues should be reported with the <a href=\"https://support.microsoft.com/windows/send-feedback-to-microsoft-with-the-feedback-hub-app-f59187f8-8739-22d6-ba93-f66612949332\">Feedback Hub app</a>, by choosing \"Apps &gt; PowerShell\" in the category.</p><p>If you are new to PowerShell and want to learn more, we recommend reviewing the <a href=\"https://learn.microsoft.com/powershell/scripting/learn/more-powershell-learning\">getting started</a> documentation.</p><p>PowerShell is supported on Windows, macOS, and a variety of Linux platforms. For more information, see <a href=\"https://learn.microsoft.com/powershell/scripting/install/installing-powershell\">Installing PowerShell</a>.</p><p>For best results when upgrading, you should use the same install method you used when you first installed PowerShell. The update method is different for each platform and install method.</p><p><a href=\"https://aka.ms/PSPublicDashboard\">Dashboard</a> with visualizations for community contributions and project status using PowerShell, Azure, and PowerBI.</p><p>For more information on how and why we built this dashboard, check out this <a href=\"https://devblogs.microsoft.com/powershell/powershell-open-source-community-dashboard/\">blog post</a>.</p><p><a href=\"https://docs.github.com/discussions/quickstart\">GitHub Discussions</a> is a feature to enable free and open discussions within the community for topics that are not related to code, unlike issues.</p><p>This is an experiment we are trying in our repositories, to see if it helps move discussions out of issues so that issues remain actionable by the team or members of the community. There should be no expectation that PowerShell team members are regular participants in these discussions. Individual PowerShell team members may choose to participate in discussions, but the expectation is that community members help drive discussions so that team members can focus on issues.</p><p>Want to chat with other members of the PowerShell community?</p><p>There are dozens of topic-specific channels on our community-driven PowerShell Virtual User Group, which you can join on:</p><h2>Developing and Contributing</h2><p>Want to contribute to PowerShell? Please start with the <a href=\"https://raw.githubusercontent.com/PowerShell/PowerShell/master/.github/CONTRIBUTING.md\">Contribution Guide</a> to learn how to develop and contribute.</p><p>If you are developing .NET Core C# applications targeting PowerShell Core, <a href=\"https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/FAQ.md#where-do-i-get-the-powershell-core-sdk-package\">check out our FAQ</a> to learn more about the PowerShell SDK NuGet package.</p><p>Also, make sure to check out our <a href=\"https://github.com/powershell/powershell-rfc\">PowerShell-RFC repository</a> for request-for-comments (RFC) documents to submit and give comments on proposed and future designs.</p><p>If you have any problems building PowerShell, please start by consulting the developer <a href=\"https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/FAQ.md\">FAQ</a>.</p><h2>Downloading the Source Code</h2><p>You can clone the repository:</p><pre><code>git clone https://github.com/PowerShell/PowerShell.git\n</code></pre><blockquote><p>[!Important] The PowerShell container images are now <a href=\"https://github.com/PowerShell/Announcements/issues/75\">maintained by the .NET team</a>. The containers at <code>mcr.microsoft.com/powershell</code> are currently not maintained.</p></blockquote><p>License: By requesting and using the Container OS Image for Windows containers, you acknowledge, understand, and consent to the Supplemental License Terms available on <a href=\"https://mcr.microsoft.com/en-us/product/powershell/tags\">Microsoft Artifact Registry</a>.</p><p>Please visit our <a href=\"https://learn.microsoft.com/powershell/module/microsoft.powershell.core/about/about_telemetry\">about_Telemetry</a> topic to read details about telemetry gathered by PowerShell.</p>",
      "contentLength": 3353,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "databricks-solutions/ai-dev-kit",
      "url": "https://github.com/databricks-solutions/ai-dev-kit",
      "date": 1771642723,
      "author": "",
      "guid": 47065,
      "unread": true,
      "content": "<p>Databricks Toolkit for Coding Agents provided by Field Engineering</p>",
      "contentLength": 66,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "eslint/eslint",
      "url": "https://github.com/eslint/eslint",
      "date": 1771642723,
      "author": "",
      "guid": 47066,
      "unread": true,
      "content": "<p>Find and fix problems in your JavaScript code.</p><p>ESLint is a tool for identifying and reporting on patterns found in ECMAScript/JavaScript code. In many ways, it is similar to JSLint and JSHint with a few exceptions:</p><ul><li>ESLint uses <a href=\"https://github.com/eslint/js/tree/main/packages/espree\">Espree</a> for JavaScript parsing.</li><li>ESLint uses an AST to evaluate patterns in code.</li><li>ESLint is completely pluggable, every single rule is a plugin and you can add more at runtime.</li></ul><p>Prerequisites: <a href=\"https://nodejs.org/\">Node.js</a> (, , or ) built with SSL support. (If you are using an official Node.js distribution, SSL is always built in.)</p><p>You can install and configure ESLint using this command:</p><pre><code>npm init @eslint/config@latest\n</code></pre><p>After that, you can run ESLint on any file or directory like this:</p><p>To use ESLint with pnpm, we recommend setting up a  file with at least the following settings:</p><pre><code>auto-install-peers=true\nnode-linker=hoisted\n</code></pre><p>This ensures that pnpm installs dependencies in a way that is more compatible with npm and is less likely to produce errors.</p><p>You can configure rules in your  files as in this example:</p><pre><code>import { defineConfig } from \"eslint/config\";\n\nexport default defineConfig([\n\t{\n\t\tfiles: [\"**/*.js\", \"**/*.cjs\", \"**/*.mjs\"],\n\t\trules: {\n\t\t\t\"prefer-const\": \"warn\",\n\t\t\t\"no-constant-binary-expression\": \"error\",\n\t\t},\n\t},\n]);\n</code></pre><p>The names  and <code>\"no-constant-binary-expression\"</code> are the names of <a href=\"https://eslint.org/docs/rules\">rules</a> in ESLint. The first value is the error level of the rule and can be one of these values:</p><ul><li> or  - turn the rule off</li><li> or  - turn the rule on as a warning (doesn't affect exit code)</li><li> or  - turn the rule on as an error (exit code will be 1)</li></ul><p>The three error levels allow you fine-grained control over how ESLint applies rules (for more configuration options and details, see the <a href=\"https://eslint.org/docs/latest/use/configure\">configuration docs</a>).</p><p>The ESLint team provides ongoing support for the current version and six months of limited support for the previous version. Limited support includes critical bug fixes, security issues, and compatibility issues only.</p><p>ESLint offers commercial support for both current and previous versions through our partners, <a href=\"https://tidelift.com/funding/github/npm/eslint\">Tidelift</a> and <a href=\"https://www.herodevs.com/support/eslint-nes?utm_source=ESLintWebsite&amp;utm_medium=ESLintWebsite&amp;utm_campaign=ESLintNES&amp;utm_id=ESLintNES\">HeroDevs</a>.</p><p>Before filing an issue, please be sure to read the guidelines for what you're reporting:</p><h2>Frequently Asked Questions</h2><p>Yes, ESLint natively supports parsing JSX syntax (this must be enabled in <a href=\"https://eslint.org/docs/latest/use/configure\">configuration</a>). Please note that supporting JSX syntax  the same as supporting React. React applies specific semantics to JSX syntax that ESLint doesn't recognize. We recommend using <a href=\"https://www.npmjs.com/package/eslint-plugin-react\">eslint-plugin-react</a> if you are using React and want React semantics.</p><h3>Does Prettier replace ESLint?</h3><p>No, ESLint and Prettier have different jobs: ESLint is a linter (looking for problematic patterns) and Prettier is a code formatter. Using both tools is common, refer to <a href=\"https://prettier.io/docs/en/install#eslint-and-other-linters\">Prettier's documentation</a> to learn how to configure them to work well with each other.</p><h3>What ECMAScript versions does ESLint support?</h3><p>ESLint has full support for ECMAScript 3, 5, and every year from 2015 up until the most recent stage 4 specification (the default). You can set your desired ECMAScript syntax and other settings (like global variables) through <a href=\"https://eslint.org/docs/latest/use/configure\">configuration</a>.</p><h3>What about experimental features?</h3><p>ESLint's parser only officially supports the latest final ECMAScript standard. We will make changes to core rules in order to avoid crashes on stage 3 ECMAScript syntax proposals (as long as they are implemented using the correct experimental ESTree syntax). We may make changes to core rules to better work with language extensions (such as JSX, Flow, and TypeScript) on a case-by-case basis.</p><p>In other cases (including if rules need to warn on more or fewer cases due to new syntax, rather than just not crashing), we recommend you use other parsers and/or rule plugins. If you are using Babel, you can use <a href=\"https://www.npmjs.com/package/@babel/eslint-parser\">@babel/eslint-parser</a> and <a href=\"https://www.npmjs.com/package/@babel/eslint-plugin\">@babel/eslint-plugin</a> to use any option available in Babel.</p><p>Once a language feature has been adopted into the ECMAScript standard (stage 4 according to the <a href=\"https://tc39.github.io/process-document/\">TC39 process</a>), we will accept issues and pull requests related to the new feature, subject to our <a href=\"https://eslint.org/docs/latest/contribute\">contributing guidelines</a>. Until then, please use the appropriate parser and plugin(s) for your experimental feature.</p><h3>Which Node.js versions does ESLint support?</h3><p>ESLint updates the supported Node.js versions with each major release of ESLint. At that time, ESLint's supported Node.js versions are updated to be:</p><ol><li>The most recent maintenance release of Node.js</li><li>The lowest minor version of the Node.js LTS release that includes the features the ESLint team wants to use.</li><li>The Node.js Current release</li></ol><p>ESLint is also expected to work with Node.js versions released after the Node.js Current release.</p><p>Refer to the <a href=\"https://eslint.org/docs/latest/use/getting-started#prerequisites\">Quick Start Guide</a> for the officially supported Node.js versions for a given ESLint release.</p><h3>Why doesn't ESLint lock dependency versions?</h3><p>Lock files like  are helpful for deployed applications. They ensure that dependencies are consistent between environments and across deployments.</p><p>Packages like  that get published to the npm registry do not include lock files.  as a user will respect version constraints in ESLint's . ESLint and its dependencies will be included in the user's lock file if one exists, but ESLint's own lock file would not be used.</p><p>We intentionally don't lock dependency versions so that we have the latest compatible dependency versions in development and CI that our users get when installing ESLint in a project.</p><p>We have scheduled releases every two weeks on Friday or Saturday. You can follow a <a href=\"https://github.com/eslint/eslint/issues?q=is%3Aopen+is%3Aissue+label%3Arelease\">release issue</a> for updates about the scheduling of any particular release.</p><p>ESLint takes security seriously. We work hard to ensure that ESLint is safe for everyone and that security issues are addressed quickly and responsibly. Read the full <a href=\"https://github.com/eslint/.github/raw/master/SECURITY.md\">security policy</a>.</p><h2>Semantic Versioning Policy</h2><p>ESLint follows <a href=\"https://semver.org\">semantic versioning</a>. However, due to the nature of ESLint as a code quality tool, it's not always clear when a minor or major version bump occurs. To help clarify this for everyone, we've defined the following semantic versioning policy for ESLint:</p><ul><li>Patch release (intended to not break your lint build) \n  <ul><li>A bug fix in a rule that results in ESLint reporting fewer linting errors.</li><li>A bug fix to the CLI or core (including formatters).</li><li>Improvements to documentation.</li><li>Non-user-facing changes such as refactoring code, adding, deleting, or modifying tests, and increasing test coverage.</li><li>Re-releasing after a failed release (i.e., publishing a release that doesn't work for anyone).</li></ul></li><li>Minor release (might break your lint build) \n  <ul><li>A bug fix in a rule that results in ESLint reporting more linting errors.</li><li>A new option to an existing rule that does not result in ESLint reporting more linting errors by default.</li><li>A new addition to an existing rule to support a newly-added language feature (within the last 12 months) that will result in ESLint reporting more linting errors by default.</li><li>An existing rule is deprecated.</li><li>A new CLI capability is created.</li><li>New capabilities to the public API are added (new classes, new methods, new arguments to existing methods, etc.).</li><li>A new formatter is created.</li><li> is updated and will result in strictly fewer linting errors (e.g., rule removals).</li></ul></li><li>Major release (likely to break your lint build) \n  <ul><li> is updated and may result in new linting errors (e.g., rule additions, most rule option updates).</li><li>A new option to an existing rule that results in ESLint reporting more linting errors by default.</li><li>An existing formatter is removed.</li><li>Part of the public API is removed or changed in an incompatible way. The public API includes: \n    <ul><li>Rule, formatter, parser, plugin APIs</li></ul></li></ul></li></ul><p>According to our policy, any minor update may report more linting errors than the previous release (ex: from a bug fix). As such, we recommend using the tilde () in  e.g.  to guarantee the results of your builds.</p><p>Since ESLint is a CommonJS package, there are restrictions on which ESM-only packages can be used as dependencies.</p><p>Packages that are controlled by the ESLint team and have no external dependencies can be safely loaded synchronously using <a href=\"https://nodejs.org/api/modules.html#loading-ecmascript-modules-using-require\"></a> and therefore used in any contexts.</p><p>For external packages, we don't use  because a package could add a top-level  and thus break ESLint. We can use an external ESM-only package only in case it is needed only in asynchronous code, in which case it can be loaded using dynamic .</p><p>Copyright OpenJS Foundation and other contributors, &lt;www.openjsf.org&gt;</p><p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p><p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p><p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p><p>These folks keep the project moving and are resources for help.</p><h3>Technical Steering Committee (TSC)</h3><p>The people who manage releases, review feature requests, and meet regularly to ensure ESLint is properly maintained.</p><p>The people who review and implement new features.</p><p>The people who review and fix bugs and help triage issues.</p><p>Team members who focus specifically on eslint.org</p><p>The following companies, organizations, and individuals support ESLint's ongoing maintenance and development. <a href=\"https://eslint.org/donate\">Become a Sponsor</a> to get your logo on our READMEs and <a href=\"https://eslint.org/sponsors\">website</a>.</p> Technology sponsors allow us to use their products and services for free as part of a contribution to the open source ecosystem and our work. \n",
      "contentLength": 9918,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "PostHog/posthog",
      "url": "https://github.com/PostHog/posthog",
      "date": 1771642723,
      "author": "",
      "guid": 47067,
      "unread": true,
      "content": "<p>ğŸ¦” PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack.</p><h2>PostHog is an all-in-one, open source platform for building successful products</h2><p><a href=\"https://posthog.com/\">PostHog</a> provides every tool you need to build a successful product including:</p><ul><li><a href=\"https://posthog.com/product-analytics\">Product Analytics</a>: Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL.</li><li><a href=\"https://posthog.com/web-analytics\">Web Analytics</a>: Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue.</li><li><a href=\"https://posthog.com/session-replay\">Session Replays</a>: Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior.</li><li><a href=\"https://posthog.com/feature-flags\">Feature Flags</a>: Safely roll out features to select users or cohorts with feature flags.</li><li><a href=\"https://posthog.com/experiments\">Experiments</a>: Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too.</li><li><a href=\"https://posthog.com/error-tracking\">Error Tracking</a>: Track errors, get alerts, and resolve issues to improve your product.</li><li><a href=\"https://posthog.com/surveys\">Surveys</a>: Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder.</li><li><a href=\"https://posthog.com/data-warehouse\">Data warehouse</a>: Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data.</li><li><a href=\"https://posthog.com/cdp\">Data pipelines</a>: Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse.</li><li><a href=\"https://posthog.com/docs/llm-analytics\">LLM analytics</a>: Capture traces, generations, latency, and cost for your LLM-powered app.</li><li><a href=\"https://posthog.com/docs/workflows\">Workflows</a>: Create workflows that automate actions or send messages to your users.</li></ul><h2>Getting started with PostHog</h2><h3>PostHog Cloud (Recommended)</h3><p>The fastest and most reliable way to get started with PostHog is signing up for free to&nbsp;<a href=\"https://us.posthog.com/signup\">PostHog Cloud</a> or <a href=\"https://eu.posthog.com/signup\">PostHog Cloud EU</a>. Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage.</p><h3>Self-hosting the open-source hobby deploy (Advanced)</h3><p>If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker (recommended 4GB memory):</p><pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)\"\n</code></pre><p>We have SDKs and libraries for popular languages and frameworks like:</p><h2>Learning more about PostHog</h2><p>We &lt;3 contributions big and small:</p><p>Need ? Check out our <a href=\"https://github.com/PostHog/posthog-foss\">posthog-foss</a> repository, which is purged of all proprietary code and features.</p><p>The pricing for our paid plan is completely transparent and available on <a href=\"https://posthog.com/pricing\">our pricing page</a>.</p><img src=\"https://res.cloudinary.com/dmukukwp6/image/upload/v1/posthog.com/src/components/Home/images/mission-control-hog\" alt=\"Hedgehog working on a Mission Control Center\" width=\"350px\"><p>Hey! If you're reading this, you've proven yourself as a dedicated README reader.</p>",
      "contentLength": 2826,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "roboflow/trackers",
      "url": "https://github.com/roboflow/trackers",
      "date": 1771642723,
      "author": "",
      "guid": 47068,
      "unread": true,
      "content": "<p>Trackers gives you clean, modular re-implementations of leading multi-object tracking algorithms released under the permissive Apache 2.0 license. You combine them with any detection model you already use.</p><p>Point at a video, webcam, RTSP stream, or image directory. Get tracked output.</p><pre><code>trackers track \\\n    --source video.mp4 \\\n    --output output.mp4 \\\n    --model rfdetr-medium \\\n    --tracker bytetrack \\\n    --show-labels \\\n    --show-trajectories\n</code></pre><p>Plug trackers into your existing detection pipeline. Works with any detector.</p><pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\nfrom trackers import ByteTrackTracker\n\nmodel = get_model(model_id=\"rfdetr-medium\")\ntracker = ByteTrackTracker()\n\nlabel_annotator = sv.LabelAnnotator()\ntrajectory_annotator = sv.TrajectoryAnnotator()\n\ncap = cv2.VideoCapture(\"video.mp4\")\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    tracked = tracker.update(detections)\n\n    frame = label_annotator.annotate(frame, tracked)\n    frame = trajectory_annotator.annotate(frame, tracked)\n</code></pre><p>Benchmark your tracker against ground truth with standard MOT metrics.</p><pre><code>trackers eval \\\n    --gt-dir data/gt \\\n    --tracker-dir data/trackers \\\n    --metrics CLEAR HOTA Identity\n</code></pre><pre><code>Sequence                        MOTA    HOTA    IDF1  IDSW\n----------------------------------------------------------\nMOT17-02-FRCNN                75.600  62.300  72.100    42\nMOT17-04-FRCNN                78.200  65.100  74.800    31\n----------------------------------------------------------\nCOMBINED                      75.033  62.400  72.033    73\n</code></pre><p>Clean, modular implementations of leading trackers. See the <a href=\"https://trackers.roboflow.com/develop/trackers/comparison/\">tracker comparison</a> for detailed benchmarks.</p>",
      "contentLength": 1787,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "blackboardsh/electrobun",
      "url": "https://github.com/blackboardsh/electrobun",
      "date": 1771642723,
      "author": "",
      "guid": 47069,
      "unread": true,
      "content": "<p>Build ultra fast, tiny, and cross-platform desktop apps with Typescript.</p><div align=\"center\">\n  Get started with a template \n </div><p>Electrobun aims to be a complete  for building, updating, and shipping ultra fast, tiny, and cross-platform desktop applications written in Typescript. Under the hood it uses <a href=\"https://bun.sh\">bun</a> to execute the main process and to bundle webview typescript, and has native bindings written in <a href=\"https://ziglang.org/\">zig</a>.</p><ul><li>Write typescript for the main process and webviews without having to think about it.</li><li>Isolation between main and webview processes with fast, typed, easy to implement RPC between them.</li><li>Small self-extracting app bundles ~12MB (when using system webview, most of this is the bun runtime)</li><li>Even smaller app updates as small as 14KB (using bsdiff it only downloads tiny patches between versions)</li><li>Provide everything you need in one tightly integrated workflow to start writing code in 5 minutes and distribute in 10.</li></ul><h2>Apps Built with Electrobun</h2><ul><li><a href=\"https://github.com/blackboardsh/audio-tts\">Audio TTS</a> - desktop text-to-speech app using Qwen3-TTS for voice design, cloning, and generation</li><li><a href=\"https://blackboard.sh/colab/\">Co(lab)</a> - a hybrid web browser + code editor for deep work</li></ul><ul><li>Create and participate in Github issues and discussions</li><li>Let me know what you're building with Electrobun</li></ul><p>Building apps with Electrobun is as easy as updating your package.json dependencies with  or try one of our templates via .</p><p><strong>This section is for building Electrobun from source locally in order to contribute fixes to it.</strong></p><ul><li>cmake (install via homebrew: )</li></ul><ul><li>Visual Studio Build Tools or Visual Studio with C++ development tools</li></ul><ul><li>webkit2gtk and GTK development packages</li></ul><p>On Ubuntu/Debian based distros: <code>sudo apt install build-essential cmake pkg-config libgtk-3-dev libwebkit2gtk-4.1-dev libayatana-appindicator3-dev librsvg2-dev</code></p><pre><code>git clone --recurse-submodules https://github.com/blackboardsh/electrobun.git\ncd electrobun/package\nbun install\nbun dev:clean\n</code></pre><pre><code># All commands are run from the /package directory\ncd electrobun/package\n\n# After making changes to source code\nbun dev\n\n# If you only changed kitchen sink code (not electrobun source)\nbun dev:rerun\n\n# If you need a completely fresh start\nbun dev:clean\n</code></pre><p>All commands are run from the  directory:</p><ul><li> - Build and run kitchen sink in canary mode</li><li> - Build electrobun in development mode</li><li> - Build electrobun in release mode</li></ul><p> Use <code>lldb &lt;path-to-bundle&gt;/Contents/MacOS/launcher</code> and then  to debug release builds</p><table><tbody><tr><td>Other Linux distros (gtk3, webkit2gtk-4.1)</td></tr></tbody></table>",
      "contentLength": 2351,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "Effect-TS/effect-smol",
      "url": "https://github.com/Effect-TS/effect-smol",
      "date": 1771642723,
      "author": "",
      "guid": 47070,
      "unread": true,
      "content": "<p>Core libraries and experimental work for Effect v4</p>",
      "contentLength": 50,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "google-research/timesfm",
      "url": "https://github.com/google-research/timesfm",
      "date": 1771642723,
      "author": "",
      "guid": 47071,
      "unread": true,
      "content": "<p>TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.</p><p>TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.</p><p>This open version is not an officially supported Google product.</p><p> TimesFM 2.5</p><ul><li>1.0 and 2.0: relevant code archived in the sub directory . You can <code>pip install timesfm==1.3.0</code> to install an older version of this package to load them.</li></ul><p>Added back the covariate support through XReg for TimesFM 2.5.</p><p>Comparing to TimesFM 2.0, this new 2.5 model:</p><ul><li>uses 200M parameters, down from 500M.</li><li>supports up to 16k context length, up from 2048.</li><li>supports continuous quantile forecast up to 1k horizon via an optional 30M quantile head.</li><li>gets rid of the  indicator.</li><li>has a couple of new forecasting flags.</li></ul><p>Along with the model upgrade we have also upgraded the inference API. This repo will be under construction over the next few weeks to</p><ol><li>add support for an upcoming Flax version of the model (faster inference).</li><li>add back covariate support.</li><li>populate more docstrings, docs and notebook.</li></ol><ol><li><pre><code>git clone https://github.com/google-research/timesfm.git\ncd timesfm\n</code></pre></li><li><p>Create a virtual environment and install dependencies using :</p><pre><code># Create a virtual environment\nuv venv\n\n# Activate the environment\nsource .venv/bin/activate\n\n# Install the package in editable mode with torch\nuv pip install -e .[torch]\n# Or with flax\nuv pip install -e .[flax]\n# Or XReg is needed\nuv pip install -e .[xreg]\n</code></pre></li><li><p>[Optional] Install your preferred  /  backend based on your OS and accelerators (CPU, GPU, TPU or Apple Silicon).:</p></li></ol><pre><code>import torch\nimport numpy as np\nimport timesfm\n\ntorch.set_float32_matmul_precision(\"high\")\n\nmodel = timesfm.TimesFM_2p5_200M_torch.from_pretrained(\"google/timesfm-2.5-200m-pytorch\")\n\nmodel.compile(\n    timesfm.ForecastConfig(\n        max_context=1024,\n        max_horizon=256,\n        normalize_inputs=True,\n        use_continuous_quantile_head=True,\n        force_flip_invariance=True,\n        infer_is_positive=True,\n        fix_quantile_crossing=True,\n    )\n)\npoint_forecast, quantile_forecast = model.forecast(\n    horizon=12,\n    inputs=[\n        np.linspace(0, 1, 100),\n        np.sin(np.linspace(0, 20, 67)),\n    ],  # Two dummy inputs\n)\npoint_forecast.shape  # (2, 12)\nquantile_forecast.shape  # (2, 12, 10): mean, then 10th to 90th quantiles.\n</code></pre>",
      "contentLength": 2385,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "huggingface/skills",
      "url": "https://github.com/huggingface/skills",
      "date": 1771642723,
      "author": "",
      "guid": 47072,
      "unread": true,
      "content": "<p>Hugging Face Skills are definitions for AI/ML tasks like dataset creation, model training, and evaluation. They are interoperable with all major coding agent tools like OpenAI Codex, Anthropic's Claude Code, Google DeepMind's Gemini CLI, and Cursor.</p><p>The Skills in this repository follow the standardized format <a href=\"https://agentskills.io/home\">Agent Skill</a> format.</p><p>In practice, skills are self-contained folders that package instructions, scripts, and resources together for an AI agent to use on a specific use case. Each folder includes a  file with YAML frontmatter (name and description) followed by the guidance your coding agent follows while the skill is active.</p><blockquote><p>[!NOTE] 'Skills' is actually an Anthropic term used within Claude AI and Claude Code and not adopted by other agent tools, but we love it! OpenAI Codex uses the open <a href=\"https://agentskills.io/specification\">Agent Skills</a> format, where each skill is a directory with a  file that Codex discovers from standard  locations documented in the <a href=\"https://developers.openai.com/codex/skills/\">Codex Skills guide</a>. Codex can also work with an  file. Google Gemini uses 'extensions' to define the instructions for your coding agent in a  file. <strong>This repo is compatible with all of them, and more!</strong></p></blockquote><blockquote><p>[!TIP] If your agent doesn't support skills, you can use <a href=\"https://raw.githubusercontent.com/huggingface/skills/main/agents/AGENTS.md\"></a> directly as a fallback.</p></blockquote><p>Hugging Face skills are compatible with Claude Code, Codex, Gemini CLI, and Cursor.</p><ol><li>Register the repository as a plugin marketplace:</li></ol><pre><code>/plugin marketplace add huggingface/skills\n</code></pre><pre><code>/plugin install &lt;skill-name&gt;@huggingface/skills\n</code></pre><pre><code>/plugin install hugging-face-cli@huggingface/skills\n</code></pre><ol><li><p>Copy or symlink any skills you want to use from this repository's  directory into one of Codex's standard  locations (for example, <code>$REPO_ROOT/.agents/skills</code> or ) as described in the <a href=\"https://developers.openai.com/codex/skills/\">Codex Skills guide</a>.</p></li><li><p>Once a skill is available in one of those locations, Codex will discover it using the Agent Skills standard and load the  instructions when it decides to use that skill or when you explicitly invoke it.</p></li><li><p>If your Codex setup still relies on , you can use the generated <a href=\"https://raw.githubusercontent.com/huggingface/skills/main/agents/AGENTS.md\"></a> file in this repo as a fallback bundle of instructions.</p></li></ol><ol><li><p>This repo includes  to integrate with the Gemini CLI.</p></li></ol><pre><code>gemini extensions install . --consent\n</code></pre><pre><code>gemini extensions install https://github.com/huggingface/skills.git --consent\n</code></pre><p>This repository includes Cursor plugin manifests:</p><ul><li><code>.cursor-plugin/plugin.json</code></li><li> (configured with the Hugging Face MCP server URL)</li></ul><p>Install from repository URL (or local checkout) via the Cursor plugin flow.</p><p>For contributors, regenerate manifests with:</p><p>This repository contains a few skills to get you started. You can also contribute your own skills to the repository.</p><table><thead><tr></tr></thead><tbody><tr><td>Build Gradio web UIs and demos in Python. Use when creating or editing Gradio apps, components, event listeners, layouts, or chatbots.</td></tr><tr><td>Execute Hugging Face Hub operations using the hf CLI. Download models/datasets, upload files, manage repos, and run cloud compute jobs.</td></tr><tr><td>Create and manage datasets on Hugging Face Hub. Supports initializing repos, defining configs/system prompts, streaming row updates, and SQL-based dataset querying/transformation.</td></tr><tr><td>Add and manage evaluation results in Hugging Face model cards. Supports extracting eval tables from README content, importing scores from Artificial Analysis API, and running custom evaluations with vLLM/lighteval.</td></tr><tr><td>Run compute jobs on Hugging Face infrastructure. Execute Python scripts, manage scheduled jobs, and monitor job status.</td></tr><tr><td><code>hugging-face-model-trainer</code></td><td>Train or fine-tune language models using TRL on Hugging Face Jobs infrastructure. Covers SFT, DPO, GRPO and reward modeling training methods, plus GGUF conversion for local deployment. Includes hardware selection, cost estimation, Trackio monitoring, and Hub persistence.</td></tr><tr><td><code>hugging-face-paper-publisher</code></td><td>Publish and manage research papers on Hugging Face Hub. Supports creating paper pages, linking papers to models/datasets, claiming authorship, and generating professional markdown-based research articles.</td></tr><tr><td><code>hugging-face-tool-builder</code></td><td>Build reusable scripts for Hugging Face API operations. Useful for chaining API calls or automating repeated tasks.</td></tr><tr><td>Track and visualize ML training experiments with Trackio. Log metrics via Python API and retrieve them via CLI. Supports real-time dashboards synced to HF Spaces.</td></tr></tbody></table><h3>Using skills in your coding agent</h3><p>Once a skill is installed, mention it directly while giving your coding agent instructions:</p><ul><li>\"Use the HF LLM trainer skill to estimate the GPU memory needed for a 70B model run.\"</li><li>\"Use the HF model evaluation skill to launch  on the latest checkpoint.\"</li><li>\"Use the HF dataset creator skill to draft new few-shot classification templates.\"</li><li>\"Use the HF paper publisher skill to index my arXiv paper and link it to my model.\"</li></ul><p>Your coding agent automatically loads the corresponding  instructions and helper scripts while it completes the task.</p><h3>Contribute or customize a skill</h3><ol><li>Copy one of the existing skill folders (for example, ) and rename it.</li><li>Update the new folder's  frontmatter: <pre><code>---\nname: my-skill-name\ndescription: Describe what the skill does and when to use it\n---\n\n# Skill Title\nGuidance + examples + guardrails\n</code></pre></li><li>Add or edit supporting scripts, templates, and documents referenced by your instructions.</li><li>Add an entry to <code>.claude-plugin/marketplace.json</code> with a concise, human-readable description.</li><li>Run:  to regenerate and validate all generated metadata.</li><li>Reinstall or reload the skill bundle in your coding agent so the updated folder is available.</li></ol><p>The <code>.claude-plugin/marketplace.json</code> file lists skills with human-readable descriptions for the plugin marketplace. The CI validates that skill names and paths match between  files and , but descriptions are maintained separately:  descriptions guide when Claude activates the skill, while marketplace descriptions are written for humans browsing available skills.</p><ul><li>Review Hugging Face documentation for the specific libraries or workflows you reference inside each skill.</li></ul>",
      "contentLength": 5783,
      "flags": null,
      "enclosureUrl": "https://opengraph.githubassets.com/5f319cf4761fba4bbe09e964eece6338f75515eb4dbd9ad81b946917a29d3728/huggingface/skills",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "vxcontrol/pentagi",
      "url": "https://github.com/vxcontrol/pentagi",
      "date": 1771642723,
      "author": "",
      "guid": 47073,
      "unread": true,
      "content": "<p>âœ¨ Fully autonomous AI Agents system capable of performing complex penetration testing tasks</p><div align=\"center\">enetration testing \n rtificial \n eneral \n ntelligence \n</div><div align=\"center\"><blockquote><p>ğŸš€  Connect with security researchers, AI enthusiasts, and fellow ethical hackers. Get support, share insights, and stay updated with the latest PentAGI developments.</p></blockquote></div><p>PentAGI is an innovative tool for automated security testing that leverages cutting-edge artificial intelligence technologies. The project is designed for information security professionals, researchers, and enthusiasts who need a powerful and flexible solution for conducting penetration tests.</p><p>You can watch the video : <a href=\"https://youtu.be/R70x5Ddzs1o\"><img src=\"https://github.com/user-attachments/assets/0828dc3e-15f1-4a1d-858e-9696a146e478\" alt=\"PentAGI Overview Video\"></a></p><ul><li>ğŸ›¡ï¸ Secure &amp; Isolated. All operations are performed in a sandboxed Docker environment with complete isolation.</li><li>ğŸ¤– Fully Autonomous. AI-powered agent that automatically determines and executes penetration testing steps.</li><li>ğŸ”¬ Professional Pentesting Tools. Built-in suite of 20+ professional security tools including nmap, metasploit, sqlmap, and more.</li><li>ğŸ§  Smart Memory System. Long-term storage of research results and successful approaches for future use.</li><li>ğŸ“š Knowledge Graph Integration. Graphiti-powered knowledge graph using Neo4j for semantic relationship tracking and advanced context understanding.</li><li>ğŸ” Web Intelligence. Built-in browser via <a href=\"https://hub.docker.com/r/vxcontrol/scraper\">scraper</a> for gathering latest information from web sources.</li><li>ğŸ‘¥ Team of Specialists. Delegation system with specialized AI agents for research, development, and infrastructure tasks.</li><li>ğŸ“Š Comprehensive Monitoring. Detailed logging and integration with Grafana/Prometheus for real-time system observation.</li><li>ğŸ“ Detailed Reporting. Generation of thorough vulnerability reports with exploitation guides.</li><li>ğŸ“¦ Smart Container Management. Automatic Docker image selection based on specific task requirements.</li><li>ğŸ“± Modern Interface. Clean and intuitive web UI for system management and monitoring.</li><li>ğŸ”Œ Comprehensive APIs. Full-featured REST and GraphQL APIs with Bearer token authentication for automation and integration.</li><li>ğŸ’¾ Persistent Storage. All commands and outputs are stored in PostgreSQL with <a href=\"https://hub.docker.com/r/vxcontrol/pgvector\">pgvector</a> extension.</li><li>ğŸ¯ Scalable Architecture. Microservices-based design supporting horizontal scaling.</li><li>ğŸ  Self-Hosted Solution. Complete control over your deployment and data.</li><li>ğŸ” API Token Authentication. Secure Bearer token system for programmatic access to REST and GraphQL APIs.</li><li>âš¡ Quick Deployment. Easy setup through <a href=\"https://docs.docker.com/compose/\">Docker Compose</a> with comprehensive environment configuration.</li></ul><pre><code>flowchart TB\n    classDef person fill:#08427B,stroke:#073B6F,color:#fff\n    classDef system fill:#1168BD,stroke:#0B4884,color:#fff\n    classDef external fill:#666666,stroke:#0B4884,color:#fff\n\n    pentester[\"ğŸ‘¤ Security Engineer\n    (User of the system)\"]\n\n    pentagi[\"âœ¨ PentAGI\n    (Autonomous penetration testing system)\"]\n\n    target[\"ğŸ¯ target-system\n    (System under test)\"]\n    llm[\"ğŸ§  llm-provider\n    (OpenAI/Anthropic/Ollama/Bedrock/Gemini/Custom)\"]\n    search[\"ğŸ” search-systems\n    (Google/DuckDuckGo/Tavily/Traversaal/Perplexity/Searxng)\"]\n    langfuse[\"ğŸ“Š langfuse-ui\n    (LLM Observability Dashboard)\"]\n    grafana[\"ğŸ“ˆ grafana\n    (System Monitoring Dashboard)\"]\n\n    pentester --&gt; |Uses HTTPS| pentagi\n    pentester --&gt; |Monitors AI HTTPS| langfuse\n    pentester --&gt; |Monitors System HTTPS| grafana\n    pentagi --&gt; |Tests Various protocols| target\n    pentagi --&gt; |Queries HTTPS| llm\n    pentagi --&gt; |Searches HTTPS| search\n    pentagi --&gt; |Reports HTTPS| langfuse\n    pentagi --&gt; |Reports HTTPS| grafana\n\n    class pentester person\n    class pentagi system\n    class target,llm,search,langfuse,grafana external\n\n    linkStyle default stroke:#ffffff,color:#ffffff\n</code></pre><p>The architecture of PentAGI is designed to be modular, scalable, and secure. Here are the key components:</p><ol><li><ul><li>Frontend UI: React-based web interface with TypeScript for type safety</li><li>Backend API: Go-based REST and GraphQL APIs with Bearer token authentication for programmatic access</li><li>Vector Store: PostgreSQL with pgvector for semantic search and memory storage</li><li>Task Queue: Async task processing system for reliable operation</li><li>AI Agent: Multi-agent system with specialized roles for efficient testing</li></ul></li><li><ul><li>Graphiti: Knowledge graph API for semantic relationship tracking and contextual understanding</li><li>Neo4j: Graph database for storing and querying relationships between entities, actions, and outcomes</li><li>Automatic capturing of agent responses and tool executions for building comprehensive knowledge base</li></ul></li><li><ul><li>OpenTelemetry: Unified observability data collection and correlation</li><li>Grafana: Real-time visualization and alerting dashboards</li><li>VictoriaMetrics: High-performance time-series metrics storage</li><li>Jaeger: End-to-end distributed tracing for debugging</li><li>Loki: Scalable log aggregation and analysis</li></ul></li><li><ul><li>Langfuse: Advanced LLM observability and performance analytics</li><li>ClickHouse: Column-oriented analytics data warehouse</li><li>Redis: High-speed caching and rate limiting</li><li>MinIO: S3-compatible object storage for artifacts</li></ul></li><li><ul><li>Web Scraper: Isolated browser environment for safe web interaction</li><li>Pentesting Tools: Comprehensive suite of 20+ professional security tools</li><li>Sandboxed Execution: All operations run in isolated containers</li></ul></li><li><ul><li>Long-term Memory: Persistent storage of knowledge and experiences</li><li>Working Memory: Active context and goals for current operations</li><li>Episodic Memory: Historical actions and success patterns</li><li>Knowledge Base: Structured domain expertise and tool capabilities</li><li>Context Management: Intelligently manages growing LLM context windows using chain summarization</li></ul></li></ol><p>The system uses Docker containers for isolation and easy deployment, with separate networks for core services, monitoring, and analytics to ensure proper security boundaries. Each component is designed to scale horizontally and can be configured for high availability in production environments.</p><ul><li>Docker and Docker Compose</li><li>Internet access for downloading images and updates</li></ul><h3>Using Installer (Recommended)</h3><p>PentAGI provides an interactive installer with a terminal-based UI for streamlined configuration and deployment. The installer guides you through system checks, LLM provider setup, search engine configuration, and security hardening.</p><p><strong>Quick Installation (Linux amd64):</strong></p><pre><code># Create installation directory\nmkdir -p pentagi &amp;&amp; cd pentagi\n\n# Download installer\nwget -O installer.zip https://pentagi.com/downloads/linux/amd64/installer-latest.zip\n\n# Extract\nunzip installer.zip\n\n# Run interactive installer\n./installer\n</code></pre><p><strong>Prerequisites &amp; Permissions:</strong></p><p>The installer requires appropriate privileges to interact with the Docker API for proper operation. By default, it uses the Docker socket () which requires either:</p><ul><li><p><strong>Option 1 (Recommended for production):</strong> Run the installer as root:</p></li><li><p><strong>Option 2 (Development environments):</strong> Grant your user access to the Docker socket by adding them to the  group:</p><pre><code># Add your user to the docker group\nsudo usermod -aG docker $USER\n\n# Log out and log back in, or activate the group immediately\nnewgrp docker\n\n# Verify Docker access (should run without sudo)\ndocker ps\n</code></pre><p>âš ï¸  Adding a user to the  group grants root-equivalent privileges. Only do this for trusted users in controlled environments. For production deployments, consider using rootless Docker mode or running the installer with sudo.</p></li></ul><ol><li>: Verify Docker, network connectivity, and system requirements</li><li>: Create and configure  file with optimal defaults</li><li>: Set up LLM providers (OpenAI, Anthropic, Gemini, Bedrock, Ollama, Custom)</li><li>: Configure DuckDuckGo, Google, Tavily, Traversaal, Perplexity, Searxng</li><li>: Generate secure credentials and configure SSL certificates</li><li>: Start PentAGI with docker-compose</li></ol><p><strong>For Production &amp; Enhanced Security:</strong></p><p>For production deployments or security-sensitive environments, we  using a distributed two-node architecture where worker operations are isolated on a separate server. This prevents untrusted code execution and network access issues on your main system.</p><p>The two-node setup provides:</p><ul><li>: Worker containers run on dedicated hardware</li><li>: Separate network boundaries for penetration testing</li><li>: Docker-in-Docker with TLS authentication</li><li>: Dedicated port ranges for out-of-band techniques</li></ul><ol><li>Create a working directory or clone the repository:</li></ol><pre><code>mkdir pentagi &amp;&amp; cd pentagi\n</code></pre><ol start=\"2\"><li>Copy  to  or download it:</li></ol><pre><code>curl -o .env https://raw.githubusercontent.com/vxcontrol/pentagi/master/.env.example\n</code></pre><ol start=\"3\"><li>Touch examples files (<code>example.custom.provider.yml</code>, <code>example.ollama.provider.yml</code>) or download it:</li></ol><pre><code>curl -o example.custom.provider.yml https://raw.githubusercontent.com/vxcontrol/pentagi/master/examples/configs/custom-openai.provider.yml\ncurl -o example.ollama.provider.yml https://raw.githubusercontent.com/vxcontrol/pentagi/master/examples/configs/ollama-llama318b.provider.yml\n</code></pre><ol start=\"4\"><li>Fill in the required API keys in  file.</li></ol><pre><code># Required: At least one of these LLM providers\nOPEN_AI_KEY=your_openai_key\nANTHROPIC_API_KEY=your_anthropic_key\nGEMINI_API_KEY=your_gemini_key\n\n# Optional: AWS Bedrock provider (enterprise-grade models)\nBEDROCK_REGION=us-east-1\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key\n\n# Optional: Local LLM provider (zero-cost inference)\nOLLAMA_SERVER_URL=http://localhost:11434\nOLLAMA_SERVER_MODEL=your_model_name\n\n# Optional: Additional search capabilities\nDUCKDUCKGO_ENABLED=true\nGOOGLE_API_KEY=your_google_key\nGOOGLE_CX_KEY=your_google_cx\nTAVILY_API_KEY=your_tavily_key\nTRAVERSAAL_API_KEY=your_traversaal_key\nPERPLEXITY_API_KEY=your_perplexity_key\nPERPLEXITY_MODEL=sonar-pro\nPERPLEXITY_CONTEXT_SIZE=medium\n\n# Searxng meta search engine (aggregates results from multiple sources)\nSEARXNG_URL=http://your-searxng-instance:8080\nSEARXNG_CATEGORIES=general\nSEARXNG_LANGUAGE=\nSEARXNG_SAFESEARCH=0\nSEARXNG_TIME_RANGE=\n\n## Graphiti knowledge graph settings\nGRAPHITI_ENABLED=true\nGRAPHITI_TIMEOUT=30\nGRAPHITI_URL=http://graphiti:8000\nGRAPHITI_MODEL_NAME=gpt-5-mini\n\n# Neo4j settings (used by Graphiti stack)\nNEO4J_USER=neo4j\nNEO4J_DATABASE=neo4j\nNEO4J_PASSWORD=devpassword\nNEO4J_URI=bolt://neo4j:7687\n\n# Assistant configuration\nASSISTANT_USE_AGENTS=false         # Default value for agent usage when creating new assistants\n</code></pre><ol start=\"5\"><li>Change all security related environment variables in  file to improve security.</li></ol><ol start=\"6\"><li>Remove all inline comments from  file if you want to use it in VSCode or other IDEs as a envFile option:</li></ol><pre><code>perl -i -pe 's/\\s+#.*$//' .env\n</code></pre><pre><code>curl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose.yml\ndocker compose up -d\n</code></pre><p>Visit <a href=\"https://localhost:8443\">localhost:8443</a> to access PentAGI Web UI (default is  / )</p><blockquote><p>[!NOTE] If you caught an error about  or  or  you need to run  firstly to create these networks and after that run <code>docker-compose-langfuse.yml</code>, <code>docker-compose-graphiti.yml</code>, and <code>docker-compose-observability.yml</code> to use Langfuse, Graphiti, and Observability services.</p><p>You have to set at least one Language Model provider (OpenAI, Anthropic, Gemini, AWS Bedrock, or Ollama) to use PentAGI. AWS Bedrock provides enterprise-grade access to multiple foundation models from leading AI companies, while Ollama provides zero-cost local inference if you have sufficient computational resources. Additional API keys for search engines are optional but recommended for better results.</p><p> environment variables are experimental feature and will be changed in the future. Right now you can use them to specify custom LLM server URL and one model for all agent types.</p><p> is a global proxy URL for all LLM providers and external search systems. You can use it for isolation from external networks.</p><p>The  file runs the PentAGI service as root user because it needs access to docker.sock for container management. If you're using TCP/IP network connection to Docker instead of socket file, you can remove root privileges and use the default  user for better security.</p></blockquote><p>PentAGI allows you to configure default behavior for assistants:</p><table><thead><tr></tr></thead><tbody><tr><td>Controls the default value for agent usage when creating new assistants</td></tr></tbody></table><p>The  setting affects the initial state of the \"Use Agents\" toggle when creating a new assistant in the UI:</p><ul><li> (default): New assistants are created with agent delegation disabled by default</li><li>: New assistants are created with agent delegation enabled by default</li></ul><p>Note that users can always override this setting by toggling the \"Use Agents\" button in the UI when creating or editing an assistant. This environment variable only controls the initial default state.</p><p>PentAGI provides comprehensive programmatic access through both REST and GraphQL APIs, allowing you to integrate penetration testing workflows into your automation pipelines, CI/CD processes, and custom applications.</p><p>API tokens are managed through the PentAGI web interface:</p><ol><li>Navigate to  â†’  in the web UI</li><li>Click  to generate a new API token</li><li>Configure token properties: \n  <ul><li> (optional): A descriptive name for the token</li><li>: When the token will expire (minimum 1 minute, maximum 3 years)</li></ul></li><li>Click  and <strong>copy the token immediately</strong> - it will only be shown once for security reasons</li><li>Use the token as a Bearer token in your API requests</li></ol><p>Each token is associated with your user account and inherits your role's permissions.</p><p>Include the API token in the  header of your HTTP requests:</p><pre><code># GraphQL API example\ncurl -X POST https://your-pentagi-instance:8443/api/v1/graphql \\\n  -H \"Authorization: Bearer YOUR_API_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"{ flows { id title status } }\"}'\n\n# REST API example\ncurl https://your-pentagi-instance:8443/api/v1/flows \\\n  -H \"Authorization: Bearer YOUR_API_TOKEN\"\n</code></pre><h3>API Exploration and Testing</h3><p>PentAGI provides interactive documentation for exploring and testing API endpoints:</p><p>Access the GraphQL Playground at <code>https://your-pentagi-instance:8443/api/v1/graphql/playground</code></p><ol><li>Click the  tab at the bottom</li><li>Add your authorization header: <pre><code>{\n  \"Authorization\": \"Bearer YOUR_API_TOKEN\"\n}\n</code></pre></li><li>Explore the schema, run queries, and test mutations interactively</li></ol><p>Access the REST API documentation at <code>https://your-pentagi-instance:8443/api/v1/swagger/index.html</code></p><ol><li>Enter your token in the format: </li><li>Test endpoints directly from the Swagger UI</li></ol><p>You can generate type-safe API clients for your preferred programming language using the schema files included with PentAGI:</p><p>The GraphQL schema is available at:</p><ul><li>: Navigate to Settings to download </li><li>: <code>backend/pkg/graph/schema.graphqls</code> in the repository</li></ul><p>Generate clients using tools like:</p><p>The OpenAPI specification is available at:</p><ul><li>: <code>https://your-pentagi-instance:8443/api/v1/swagger/doc.json</code></li><li>: Available in <code>backend/pkg/server/docs/swagger.yaml</code></li></ul><p>When working with API tokens:</p><ul><li><strong>Never commit tokens to version control</strong> - use environment variables or secrets management</li><li> - set appropriate expiration dates and create new tokens periodically</li><li><strong>Use separate tokens for different applications</strong> - makes it easier to revoke access if needed</li><li> - review API token activity in the Settings page</li><li> - disable or delete tokens that are no longer needed</li><li> - never send API tokens over unencrypted connections</li></ul><ul><li>: See all your active tokens in Settings â†’ API Tokens</li><li>: Update token names or revoke tokens</li><li>: Permanently remove tokens (this action cannot be undone)</li><li>: Each token has a unique ID that can be copied for reference</li></ul><ul><li>Token ID (unique identifier)</li><li>Status (active/revoked/expired)</li></ul><h3>Custom LLM Provider Configuration</h3><p>When using custom LLM providers with the  variables, you can fine-tune the reasoning format used in requests:</p><table><thead><tr></tr></thead><tbody><tr><td>Base URL for the custom LLM API endpoint</td></tr><tr><td>API key for the custom LLM provider</td></tr><tr><td>Default model to use (can be overridden in provider config)</td></tr><tr><td>Path to the YAML configuration file for agent-specific models</td></tr><tr><td>Provider name prefix for model names (e.g., ,  for LiteLLM proxy)</td></tr><tr><td><code>LLM_SERVER_LEGACY_REASONING</code></td><td>Controls reasoning format in API requests</td></tr><tr><td><code>LLM_SERVER_PRESERVE_REASONING</code></td><td>Preserve reasoning content in multi-turn conversations (required by some providers)</td></tr></tbody></table><p>The  setting is particularly useful when using , which adds a provider prefix to model names. For example, when connecting to Moonshot API through LiteLLM, models like  become . By setting <code>LLM_SERVER_PROVIDER=moonshot</code>, you can use the same provider configuration file for both direct API access and LiteLLM proxy access without modifications.</p><p>The <code>LLM_SERVER_LEGACY_REASONING</code> setting affects how reasoning parameters are sent to the LLM:</p><ul><li> (default): Uses modern format where reasoning is sent as a structured object with  parameter</li><li>: Uses legacy format with string-based  parameter</li></ul><p>This setting is important when working with different LLM providers as they may expect different reasoning formats in their API requests. If you encounter reasoning-related errors with custom providers, try changing this setting.</p><p>The <code>LLM_SERVER_PRESERVE_REASONING</code> setting controls whether reasoning content is preserved in multi-turn conversations:</p><ul><li> (default): Reasoning content is not preserved in conversation history</li><li>: Reasoning content is preserved and sent in subsequent API calls</li></ul><p>This setting is required by some LLM providers (e.g., Moonshot) that return errors like \"thinking is enabled but reasoning_content is missing in assistant tool call message\" when reasoning content is not included in multi-turn conversations. Enable this setting if your provider requires reasoning content to be preserved.</p><h3>Local LLM Provider Configuration</h3><p>PentAGI supports Ollama for local LLM inference, providing zero-cost operation and enhanced privacy:</p><table><thead><tr></tr></thead><tbody><tr><td>URL of your Ollama server</td></tr><tr><td><code>llama3.1:8b-instruct-q8_0</code></td><td>Default model for inference</td></tr><tr><td><code>OLLAMA_SERVER_CONFIG_PATH</code></td><td>Path to custom agent configuration file</td></tr><tr><td><code>OLLAMA_SERVER_PULL_MODELS_TIMEOUT</code></td><td>Timeout for model downloads (seconds)</td></tr><tr><td><code>OLLAMA_SERVER_PULL_MODELS_ENABLED</code></td><td>Auto-download models on startup</td></tr><tr><td><code>OLLAMA_SERVER_LOAD_MODELS_ENABLED</code></td><td>Query server for available models</td></tr></tbody></table><pre><code># Basic Ollama setup with default model\nOLLAMA_SERVER_URL=http://localhost:11434\nOLLAMA_SERVER_MODEL=llama3.1:8b-instruct-q8_0\n\n# Production setup with auto-pull and model discovery\nOLLAMA_SERVER_URL=http://ollama-server:11434\nOLLAMA_SERVER_PULL_MODELS_ENABLED=true\nOLLAMA_SERVER_PULL_MODELS_TIMEOUT=900\nOLLAMA_SERVER_LOAD_MODELS_ENABLED=true\n\n# Custom configuration with agent-specific models\nOLLAMA_SERVER_CONFIG_PATH=/path/to/ollama-config.yml\n\n# Default configuration file inside docker container\nOLLAMA_SERVER_CONFIG_PATH=/opt/pentagi/conf/ollama-llama318b.provider.yml\n</code></pre><p><strong>Performance Considerations:</strong></p><ul><li> (<code>OLLAMA_SERVER_LOAD_MODELS_ENABLED=true</code>): Adds 1-2s startup latency querying Ollama API</li><li> (<code>OLLAMA_SERVER_PULL_MODELS_ENABLED=true</code>): First startup may take several minutes downloading models</li><li> (<code>OLLAMA_SERVER_PULL_MODELS_TIMEOUT=900</code>): 15 minutes in seconds</li><li>: Disable both flags and specify models in config file for fastest startup</li></ul><h4>Creating Custom Ollama Models with Extended Context</h4><p>PentAGI requires models with larger context windows than the default Ollama configurations. You need to create custom models with increased  parameter through Modelfiles. While typical agent workflows consume around 64K tokens, PentAGI uses 110K context size for safety margin and handling complex penetration testing scenarios.</p><p>: The  parameter can only be set during model creation via Modelfile - it cannot be changed after model creation or overridden at runtime.</p><h5>Example: Qwen3 32B FP16 with Extended Context</h5><p>Create a Modelfile named <code>Modelfile_qwen3_32b_fp16_tc</code>:</p><pre><code>FROM qwen3:32b-fp16\nPARAMETER num_ctx 110000\nPARAMETER temperature 0.3\nPARAMETER top_p 0.8\nPARAMETER min_p 0.0\nPARAMETER top_k 20\nPARAMETER repeat_penalty 1.1\n</code></pre><pre><code>ollama create qwen3:32b-fp16-tc -f Modelfile_qwen3_32b_fp16_tc\n</code></pre><h5>Example: QwQ 32B FP16 with Extended Context</h5><p>Create a Modelfile named <code>Modelfile_qwq_32b_fp16_tc</code>:</p><pre><code>FROM qwq:32b-fp16\nPARAMETER num_ctx 110000\nPARAMETER temperature 0.2\nPARAMETER top_p 0.7\nPARAMETER min_p 0.0\nPARAMETER top_k 40\nPARAMETER repeat_penalty 1.2\n</code></pre><pre><code>ollama create qwq:32b-fp16-tc -f Modelfile_qwq_32b_fp16_tc\n</code></pre><blockquote><p>: The QwQ 32B FP16 model requires approximately  for inference. Ensure your system has sufficient GPU memory before attempting to use this model.</p></blockquote><p>These custom models are referenced in the pre-built provider configuration files (<code>ollama-qwen332b-fp16-tc.provider.yml</code> and <code>ollama-qwq32b-fp16-tc.provider.yml</code>) that are included in the Docker image at .</p><h3>OpenAI Provider Configuration</h3><p>PentAGI supports OpenAI's advanced language models, including the latest reasoning-capable o-series models designed for complex analytical tasks:</p><table><thead><tr></tr></thead><tbody><tr><td>API key for OpenAI services</td></tr><tr><td><code>https://api.openai.com/v1</code></td></tr></tbody></table><pre><code># Basic OpenAI setup\nOPEN_AI_KEY=your_openai_api_key\nOPEN_AI_SERVER_URL=https://api.openai.com/v1\n\n# Using with proxy for enhanced security\nOPEN_AI_KEY=your_openai_api_key\nPROXY_URL=http://your-proxy:8080\n</code></pre><p>The OpenAI provider offers cutting-edge capabilities including:</p><ul><li>: Advanced o-series models (o1, o3, o4-mini) with step-by-step analytical thinking</li><li>: Flagship models optimized for complex security research and exploit development</li><li>: From nano models for high-volume scanning to powerful reasoning models for deep analysis</li><li>: Fast, intelligent models perfect for multi-step security analysis and penetration testing</li><li>: Industry-leading models with consistent performance across diverse security scenarios</li></ul><p>The system automatically selects appropriate OpenAI models based on task complexity, optimizing for both performance and cost-effectiveness.</p><h3>Anthropic Provider Configuration</h3><p>PentAGI integrates with Anthropic's Claude models, known for their exceptional safety, reasoning capabilities, and sophisticated understanding of complex security contexts:</p><table><thead><tr></tr></thead><tbody><tr><td>API key for Anthropic services</td></tr><tr><td><code>https://api.anthropic.com/v1</code></td></tr></tbody></table><pre><code># Basic Anthropic setup\nANTHROPIC_API_KEY=your_anthropic_api_key\nANTHROPIC_SERVER_URL=https://api.anthropic.com/v1\n\n# Using with proxy for secure environments\nANTHROPIC_API_KEY=your_anthropic_api_key\nPROXY_URL=http://your-proxy:8080\n</code></pre><p>The Anthropic provider delivers superior capabilities including:</p><ul><li>: Claude 4 series with exceptional reasoning for sophisticated penetration testing</li><li>: Claude 3.7 with step-by-step thinking capabilities for methodical security research</li><li>: Claude 3.5 Haiku for blazing-fast vulnerability scans and real-time monitoring</li><li>: Claude Sonnet models for complex security analysis and threat hunting</li><li>: Built-in safety mechanisms ensuring responsible security testing practices</li></ul><p>The system leverages Claude's advanced understanding of security contexts to provide thorough and responsible penetration testing guidance.</p><h3>Google AI (Gemini) Provider Configuration</h3><p>PentAGI supports Google's Gemini models through the Google AI API, offering state-of-the-art reasoning capabilities and multimodal features:</p><table><thead><tr></tr></thead><tbody><tr><td>API key for Google AI services</td></tr><tr><td><code>https://generativelanguage.googleapis.com</code></td></tr></tbody></table><pre><code># Basic Gemini setup\nGEMINI_API_KEY=your_gemini_api_key\nGEMINI_SERVER_URL=https://generativelanguage.googleapis.com\n\n# Using with proxy\nGEMINI_API_KEY=your_gemini_api_key\nPROXY_URL=http://your-proxy:8080\n</code></pre><p>The Gemini provider offers advanced features including:</p><ul><li>: Advanced reasoning models (Gemini 2.5 series) with step-by-step analysis</li><li>: Text and image processing for comprehensive security assessments</li><li>: Up to 2M tokens for analyzing extensive codebases and documentation</li><li>: From high-performance pro models to economical flash variants</li><li>: Specialized configurations optimized for penetration testing workflows</li></ul><p>The system automatically selects appropriate Gemini models based on agent requirements, balancing performance, capabilities, and cost-effectiveness.</p><h3>AWS Bedrock Provider Configuration</h3><p>PentAGI integrates with Amazon Bedrock, offering access to a wide range of foundation models from leading AI companies including Anthropic, AI21, Cohere, Meta, and Amazon's own models:</p><table><thead><tr></tr></thead><tbody><tr><td>AWS region for Bedrock service</td></tr><tr><td>AWS access key ID for authentication</td></tr><tr><td><code>BEDROCK_SECRET_ACCESS_KEY</code></td><td>AWS secret access key for authentication</td></tr><tr><td>AWS session token as alternative way for authentication</td></tr><tr><td>Optional custom Bedrock endpoint URL</td></tr></tbody></table><pre><code># Basic AWS Bedrock setup with credentials\nBEDROCK_REGION=us-east-1\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key\n\n# Using with proxy for enhanced security\nBEDROCK_REGION=us-east-1\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key\nPROXY_URL=http://your-proxy:8080\n\n# Using custom endpoint (for VPC endpoints or testing)\nBEDROCK_REGION=us-east-1\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key\nBEDROCK_SERVER_URL=https://bedrock-runtime.us-east-1.amazonaws.com\n</code></pre><blockquote><p>[!IMPORTANT] <strong>AWS Bedrock Rate Limits Warning</strong></p><p>The default PentAGI configuration for AWS Bedrock uses two primary models:</p><ul><li><code>us.anthropic.claude-sonnet-4-20250514-v1:0</code> (for most agents) -  for new AWS accounts</li><li><code>us.anthropic.claude-3-5-haiku-20241022-v1:0</code> (for simple tasks) -  for new AWS accounts</li></ul><p>These default rate limits are  for comfortable penetration testing scenarios and will significantly impact your workflow. We :</p><ol><li> for your AWS Bedrock models through the AWS Service Quotas console</li><li><strong>Use provisioned throughput models</strong> with hourly billing for higher throughput requirements</li><li><strong>Switch to alternative models</strong> with higher default quotas (e.g., Amazon Nova series, Meta Llama models)</li><li><strong>Consider using a different LLM provider</strong> (OpenAI, Anthropic, Gemini) if you need immediate high-throughput access</li></ol><p>Without adequate rate limits, you may experience frequent delays, timeouts, and degraded testing performance.</p></blockquote><p>The AWS Bedrock provider delivers comprehensive capabilities including:</p><ul><li>: Access to models from Anthropic (Claude), AI21 (Jamba), Cohere (Command), Meta (Llama), Amazon (Nova, Titan), and DeepSeek (R1) through a single interface</li><li>: Support for Claude 4 and other reasoning-capable models with step-by-step thinking</li><li>: Amazon Nova series supporting text, image, and video processing for comprehensive security analysis</li><li>: AWS-native security controls, VPC integration, and compliance certifications</li><li>: Wide range of model sizes and capabilities for cost-effective penetration testing</li><li>: Deploy models in your preferred AWS region for data residency and performance</li><li>: Low-latency inference through AWS's global infrastructure</li></ul><p>The system automatically selects appropriate Bedrock models based on task complexity and requirements, leveraging the full spectrum of available foundation models for optimal security testing results.</p><blockquote><p>[!WARNING] <strong>Converse API Requirements</strong></p><p>PentAGI uses the <strong>Amazon Bedrock Converse API</strong> for model interactions, which requires models to support the following features:</p><ul><li>âœ…  - Basic conversation API support</li><li>âœ…  - Streaming response support</li><li>âœ…  - Function calling capabilities for penetration testing tools</li><li>âœ…  - Real-time tool execution feedback</li></ul><p>âš ï¸ : Some models like AI21 Jurassic-2 and Cohere Command (Text) have  and may not work properly with PentAGI's multi-turn conversation workflows.</p></blockquote><blockquote><p>: AWS credentials can also be provided through IAM roles, environment variables, or AWS credential files following standard AWS SDK authentication patterns. Ensure your AWS account has appropriate permissions for Amazon Bedrock service access.</p></blockquote><p>For advanced configuration options and detailed setup instructions, please visit our <a href=\"https://docs.pentagi.com\">documentation</a>.</p><p>Langfuse provides advanced capabilities for monitoring and analyzing AI agent operations.</p><ol><li>Configure Langfuse environment variables in existing  file.</li></ol><ol start=\"2\"><li>Enable integration with Langfuse for PentAGI service in  file.</li></ol><pre><code>LANGFUSE_BASE_URL=http://langfuse-web:3000\nLANGFUSE_PROJECT_ID= # default: value from ${LANGFUSE_INIT_PROJECT_ID}\nLANGFUSE_PUBLIC_KEY= # default: value from ${LANGFUSE_INIT_PROJECT_PUBLIC_KEY}\nLANGFUSE_SECRET_KEY= # default: value from ${LANGFUSE_INIT_PROJECT_SECRET_KEY}\n</code></pre><pre><code>curl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose-langfuse.yml\ndocker compose -f docker-compose.yml -f docker-compose-langfuse.yml up -d\n</code></pre><p>Visit <a href=\"http://localhost:4000\">localhost:4000</a> to access Langfuse Web UI with credentials from  file:</p><ul><li> - Admin email</li><li><code>LANGFUSE_INIT_USER_PASSWORD</code> - Admin password</li></ul><h3>Monitoring and Observability</h3><p>For detailed system operation tracking, integration with monitoring tools is available.</p><ol><li>Enable integration with OpenTelemetry and all observability services for PentAGI in  file.</li></ol><ol start=\"2\"><li>Run the observability stack:</li></ol><pre><code>curl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose-observability.yml\ndocker compose -f docker-compose.yml -f docker-compose-observability.yml up -d\n</code></pre><blockquote><p>[!NOTE] If you want to use Observability stack with Langfuse, you need to enable integration in  file to set <code>LANGFUSE_OTEL_EXPORTER_OTLP_ENDPOINT</code> to .</p><p>To run all available stacks together (Langfuse, Graphiti, and Observability):</p><pre><code>docker compose -f docker-compose.yml -f docker-compose-langfuse.yml -f docker-compose-graphiti.yml -f docker-compose-observability.yml up -d\n</code></pre><p>You can also register aliases for these commands in your shell to run it faster:</p><pre><code>alias pentagi=\"docker compose -f docker-compose.yml -f docker-compose-langfuse.yml -f docker-compose-graphiti.yml -f docker-compose-observability.yml\"\nalias pentagi-up=\"docker compose -f docker-compose.yml -f docker-compose-langfuse.yml -f docker-compose-graphiti.yml -f docker-compose-observability.yml up -d\"\nalias pentagi-down=\"docker compose -f docker-compose.yml -f docker-compose-langfuse.yml -f docker-compose-graphiti.yml -f docker-compose-observability.yml down\"\n</code></pre></blockquote><h3>Knowledge Graph Integration (Graphiti)</h3><p>PentAGI integrates with <a href=\"https://github.com/vxcontrol/pentagi-graphiti\">Graphiti</a>, a temporal knowledge graph system powered by Neo4j, to provide advanced semantic understanding and relationship tracking for AI agent operations. The vxcontrol fork provides custom entity and edge types that are specific to pentesting purposes.</p><p>Graphiti automatically extracts and stores structured knowledge from agent interactions, building a graph of entities, relationships, and temporal context. This enables:</p><ul><li>: Store and recall relationships between tools, targets, vulnerabilities, and techniques</li><li>: Track how different pentesting actions relate to each other over time</li><li>: Learn from past penetration tests and apply insights to new assessments</li><li>: Search for complex patterns like \"What tools were effective against similar targets?\"</li></ul><p>The Graphiti knowledge graph is  and disabled by default. To enable it:</p><ol><li>Configure Graphiti environment variables in  file:</li></ol><pre><code>## Graphiti knowledge graph settings\nGRAPHITI_ENABLED=true\nGRAPHITI_TIMEOUT=30\nGRAPHITI_URL=http://graphiti:8000\nGRAPHITI_MODEL_NAME=gpt-5-mini\n\n# Neo4j settings (used by Graphiti stack)\nNEO4J_USER=neo4j\nNEO4J_DATABASE=neo4j\nNEO4J_PASSWORD=devpassword\nNEO4J_URI=bolt://neo4j:7687\n\n# OpenAI API key (required by Graphiti for entity extraction)\nOPEN_AI_KEY=your_openai_api_key\n</code></pre><ol start=\"2\"><li>Run the Graphiti stack along with the main PentAGI services:</li></ol><pre><code># Download the Graphiti compose file if needed\ncurl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose-graphiti.yml\n\n# Start PentAGI with Graphiti\ndocker compose -f docker-compose.yml -f docker-compose-graphiti.yml up -d\n</code></pre><ol start=\"3\"><li>Verify Graphiti is running:</li></ol><pre><code># Check service health\ndocker compose -f docker-compose.yml -f docker-compose-graphiti.yml ps graphiti neo4j\n\n# View Graphiti logs\ndocker compose -f docker-compose.yml -f docker-compose-graphiti.yml logs -f graphiti\n\n# Access Neo4j Browser (optional)\n# Visit http://localhost:7474 and login with NEO4J_USER/NEO4J_PASSWORD\n\n# Access Graphiti API (optional, for debugging)\n# Visit http://localhost:8000/docs for Swagger API documentation\n</code></pre><blockquote><p>[!NOTE] The Graphiti service is defined in <code>docker-compose-graphiti.yml</code> as a separate stack. You must run both compose files together to enable the knowledge graph functionality. The pre-built Docker image <code>vxcontrol/graphiti:latest</code> is used by default.</p></blockquote><p>When enabled, PentAGI automatically captures:</p><ul><li>: All agent reasoning, analysis, and decisions</li><li>: Commands executed, tools used, and their results</li><li>: Flow, task, and subtask hierarchy</li></ul><h3>GitHub and Google OAuth Integration</h3><p>OAuth integration with GitHub and Google allows users to authenticate using their existing accounts on these platforms. This provides several benefits:</p><ul><li>Simplified login process without need to create separate credentials</li><li>Enhanced security through trusted identity providers</li><li>Access to user profile information from GitHub/Google accounts</li><li>Seamless integration with existing development workflows</li></ul><p>For using GitHub OAuth you need to create a new OAuth application in your GitHub account and set the  and  in  file.</p><p>For using Google OAuth you need to create a new OAuth application in your Google account and set the  and  in  file.</p><h3>Docker Image Configuration</h3><p>PentAGI allows you to configure Docker image selection for executing various tasks. The system automatically chooses the most appropriate image based on the task type, but you can constrain this selection by specifying your preferred images:</p><table><thead><tr></tr></thead><tbody><tr><td>Default Docker image for general tasks and ambiguous cases</td></tr><tr><td><code>DOCKER_DEFAULT_IMAGE_FOR_PENTEST</code></td><td>Default Docker image for security/penetration testing tasks</td></tr></tbody></table><p>When these environment variables are set, AI agents will be limited to the image choices you specify. This is particularly useful for:</p><ul><li>: Restricting usage to only verified and trusted images</li><li><strong>Environment Standardization</strong>: Using corporate or customized images across all operations</li><li>: Utilizing pre-built images with necessary tools already installed</li></ul><pre><code># Using a custom image for general tasks\nDOCKER_DEFAULT_IMAGE=mycompany/custom-debian:latest\n\n# Using a specialized image for penetration testing\nDOCKER_DEFAULT_IMAGE_FOR_PENTEST=mycompany/pentest-tools:v2.0\n</code></pre><blockquote><p>[!NOTE] If a user explicitly specifies a particular Docker image in their task, the system will try to use that exact image, ignoring these settings. These variables only affect the system's automatic image selection process.</p></blockquote><ul></ul><p>Run once <code>cd backend &amp;&amp; go mod download</code> to install needed packages.</p><p>For generating swagger files have to run</p><pre><code>swag init -g ../../pkg/server/router.go -o pkg/server/docs/ --parseDependency --parseInternal --parseDepth 2 -d cmd/pentagi\n</code></pre><p>before installing  package via</p><pre><code>go install github.com/swaggo/swag/cmd/swag@v1.8.7\n</code></pre><p>For generating graphql resolver files have to run</p><pre><code>go run github.com/99designs/gqlgen --config ./gqlgen/gqlgen.yml\n</code></pre><p>after that you can see the generated files in  folder.</p><p>For generating ORM methods (database package) from sqlc configuration</p><pre><code>docker run --rm -v $(pwd):/src -w /src --network pentagi-network -e DATABASE_URL=\"{URL}\" sqlc/sqlc generate -f sqlc/sqlc.yml\n</code></pre><p>For generating Langfuse SDK from OpenAPI specification</p><p>For running tests <code>cd backend &amp;&amp; go test -v ./...</code></p><p>Run once <code>cd frontend &amp;&amp; npm install</code> to install needed packages.</p><p>For generating graphql files have to run  which using  file.</p><p>Be sure that you have  installed globally:</p><pre><code>npm install -g graphql-codegen\n</code></pre><ul><li> to check if your code is formatted correctly</li><li> to fix it</li><li> to check if your code is linted correctly</li><li> to fix it</li></ul><p>For generating SSL certificates you need to run  which using  file or it will be generated automatically when you run .</p><p>Edit the configuration for  in  file:</p><ul><li> - PostgreSQL database URL (eg. <code>postgres://postgres:postgres@localhost:5432/pentagidb?sslmode=disable</code>)</li><li> - Docker SDK API (eg. for macOS <code>DOCKER_HOST=unix:///Users/&lt;my-user&gt;/Library/Containers/com.docker.docker/Data/docker.raw.sock</code>) <a href=\"https://stackoverflow.com/a/62757128/5922857\">more info</a></li></ul><ul><li> - Port to run the server (default: )</li><li> - Enable SSL for the server (default: )</li></ul><p>Edit the configuration for  in  file:</p><ul><li> - Backend API URL.  the URL scheme (e.g., )</li><li> - Enable SSL for the server (default: )</li><li> - Port to run the server (default: )</li><li> - Host to run the server (default: )</li></ul><p>Run the command(s) in  folder:</p><ul><li>Use  file to set environment variables like a </li><li>Run <code>go run cmd/pentagi/main.go</code> to start the server</li></ul><blockquote><p>[!NOTE] The first run can take a while as dependencies and docker images need to be downloaded to setup the backend environment.</p></blockquote><p>Run the command(s) in  folder:</p><ul><li>Run  to install the dependencies</li><li>Run  to run the web app</li><li>Run  to build the web app</li></ul><p>Open your browser and visit the web app URL.</p><p>PentAGI includes a powerful utility called  for testing and validating LLM agent capabilities. This tool helps ensure your LLM provider configurations work correctly with different agent types, allowing you to optimize model selection for each specific agent role.</p><p>The utility features parallel testing of multiple agents, detailed reporting, and flexible configuration options.</p><ul><li>: Tests multiple agents simultaneously for faster results</li><li>: Evaluates basic completion, JSON responses, function calling, and penetration testing knowledge</li><li>: Generates markdown reports with success rates and performance metrics</li><li>: Test specific agents or test groups as needed</li><li>: Includes domain-specific tests for cybersecurity and penetration testing scenarios</li></ul><h4>For Developers (with local Go environment)</h4><p>If you've cloned the repository and have Go installed:</p><pre><code># Default configuration with .env file\ncd backend\ngo run cmd/ctester/*.go -verbose\n\n# Custom provider configuration\ngo run cmd/ctester/*.go -config ../examples/configs/openrouter.provider.yml -verbose\n\n# Generate a report file\ngo run cmd/ctester/*.go -config ../examples/configs/deepinfra.provider.yml -report ../test-report.md\n\n# Test specific agent types only\ngo run cmd/ctester/*.go -agents simple,simple_json,primary_agent -verbose\n\n# Test specific test groups only\ngo run cmd/ctester/*.go -groups basic,advanced -verbose\n</code></pre><h4>For Users (using Docker image)</h4><p>If you prefer to use the pre-built Docker image without setting up a development environment:</p><pre><code># Using Docker to test with default environment\ndocker run --rm -v $(pwd)/.env:/opt/pentagi/.env vxcontrol/pentagi /opt/pentagi/bin/ctester -verbose\n\n# Test with your custom provider configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  -v $(pwd)/my-config.yml:/opt/pentagi/config.yml \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/config.yml -agents simple,primary_agent,coder -verbose\n\n# Generate a detailed report\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  -v $(pwd):/opt/pentagi/output \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -report /opt/pentagi/output/report.md\n</code></pre><h4>Using Pre-configured Providers</h4><p>The Docker image comes with built-in support for major providers (OpenAI, Anthropic, Gemini, Ollama) and pre-configured provider files for additional services (OpenRouter, DeepInfra, DeepSeek, Moonshot):</p><pre><code># Test with OpenRouter configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/openrouter.provider.yml\n\n# Test with DeepInfra configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/deepinfra.provider.yml\n\n# Test with DeepSeek configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/deepseek.provider.yml\n\n# Test with Moonshot configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/moonshot.provider.yml\n\n# Test with OpenAI configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -type openai\n\n# Test with Anthropic configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -type anthropic\n\n# Test with Gemini configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -type gemini\n\n# Test with AWS Bedrock configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -type bedrock\n\n# Test with Custom OpenAI configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/custom-openai.provider.yml\n\n# Test with Ollama configuration (local inference)\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/ollama-llama318b.provider.yml\n\n# Test with Ollama Qwen3 32B configuration (requires custom model creation)\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/ollama-qwen332b-fp16-tc.provider.yml\n\n# Test with Ollama QwQ 32B configuration (requires custom model creation and 71.3GB VRAM)\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/ollama-qwq32b-fp16-tc.provider.yml\n</code></pre><p>To use these configurations, your  file only needs to contain:</p><pre><code>LLM_SERVER_URL=https://openrouter.ai/api/v1      # or https://api.deepinfra.com/v1/openai or https://api.deepseek.com or https://api.openai.com/v1 or https://api.moonshot.ai/v1\nLLM_SERVER_KEY=your_api_key\nLLM_SERVER_MODEL=                                # Leave empty, as models are specified in the config\nLLM_SERVER_CONFIG_PATH=/opt/pentagi/conf/openrouter.provider.yml  # or deepinfra.provider.yml or deepseek.provider.yml or custom-openai.provider.yml or moonshot.provider.yml\nLLM_SERVER_PROVIDER=                             # Provider name for LiteLLM proxy (e.g., openrouter, deepseek, moonshot)\nLLM_SERVER_LEGACY_REASONING=false                # Controls reasoning format, for OpenAI must be true (default: false)\nLLM_SERVER_PRESERVE_REASONING=false              # Preserve reasoning content in multi-turn conversations (required by Moonshot, default: false)\n\n# For OpenAI (official API)\nOPEN_AI_KEY=your_openai_api_key                  # Your OpenAI API key\nOPEN_AI_SERVER_URL=https://api.openai.com/v1     # OpenAI API endpoint\n\n# For Anthropic (Claude models)\nANTHROPIC_API_KEY=your_anthropic_api_key         # Your Anthropic API key\nANTHROPIC_SERVER_URL=https://api.anthropic.com/v1  # Anthropic API endpoint\n\n# For Gemini (Google AI)\nGEMINI_API_KEY=your_gemini_api_key               # Your Google AI API key\nGEMINI_SERVER_URL=https://generativelanguage.googleapis.com  # Google AI API endpoint\n\n# For AWS Bedrock (enterprise foundation models)\nBEDROCK_REGION=us-east-1                         # AWS region for Bedrock service\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key        # AWS access key ID\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key    # AWS secret access key\nBEDROCK_SESSION_TOKEN=your_aws_session_token     # AWS session token (alternative auth method)\nBEDROCK_SERVER_URL=                              # Optional custom Bedrock endpoint\n\n# For Ollama (local inference)\nOLLAMA_SERVER_URL=http://localhost:11434\nOLLAMA_SERVER_MODEL=llama3.1:8b-instruct-q8_0\nOLLAMA_SERVER_CONFIG_PATH=/opt/pentagi/conf/ollama-llama318b.provider.yml\nOLLAMA_SERVER_PULL_MODELS_ENABLED=false\nOLLAMA_SERVER_LOAD_MODELS_ENABLED=false\n</code></pre><h4>Using OpenAI with Unverified Organizations</h4><p>For OpenAI accounts with unverified organizations that don't have access to the latest reasoning models (o1, o3, o4-mini), you need to use a custom configuration.</p><p>To use OpenAI with unverified organization accounts, configure your  file as follows:</p><pre><code>LLM_SERVER_URL=https://api.openai.com/v1\nLLM_SERVER_KEY=your_openai_api_key\nLLM_SERVER_MODEL=                                # Leave empty, models are specified in config\nLLM_SERVER_CONFIG_PATH=/opt/pentagi/conf/custom-openai.provider.yml\nLLM_SERVER_LEGACY_REASONING=true                 # Required for OpenAI reasoning format\n</code></pre><p>This configuration uses the pre-built <code>custom-openai.provider.yml</code> file that maps all agent types to models available for unverified organizations, using  instead of models like , , and .</p><p>You can test this configuration using:</p><pre><code># Test with custom OpenAI configuration for unverified accounts\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/custom-openai.provider.yml\n</code></pre><blockquote><p>[!NOTE] The <code>LLM_SERVER_LEGACY_REASONING=true</code> setting is crucial for OpenAI compatibility as it ensures reasoning parameters are sent in the format expected by OpenAI's API.</p></blockquote><p>When using LiteLLM proxy to access various LLM providers, model names are prefixed with the provider name (e.g.,  instead of ). To use the same provider configuration files with both direct API access and LiteLLM proxy, set the  variable:</p><pre><code># Direct access to Moonshot API\nLLM_SERVER_URL=https://api.moonshot.ai/v1\nLLM_SERVER_KEY=your_moonshot_api_key\nLLM_SERVER_CONFIG_PATH=/opt/pentagi/conf/moonshot.provider.yml\nLLM_SERVER_PROVIDER=                             # Empty for direct access\n\n# Access via LiteLLM proxy\nLLM_SERVER_URL=http://litellm-proxy:4000\nLLM_SERVER_KEY=your_litellm_api_key\nLLM_SERVER_CONFIG_PATH=/opt/pentagi/conf/moonshot.provider.yml\nLLM_SERVER_PROVIDER=moonshot                     # Provider prefix for LiteLLM\n</code></pre><p>With <code>LLM_SERVER_PROVIDER=moonshot</code>, the system automatically prefixes all model names from the configuration file with , making them compatible with LiteLLM's model naming convention.</p><p><strong>Supported provider names for LiteLLM:</strong></p><ul><li> - for OpenAI models via LiteLLM</li><li> - for Anthropic/Claude models via LiteLLM</li><li> - for Google Gemini models via LiteLLM</li><li> - for OpenRouter aggregator</li><li> - for DeepSeek models</li><li> - for DeepInfra hosting</li><li> - for Moonshot AI (Kimi)</li><li>Any other provider name configured in your LiteLLM instance</li></ul><p>This approach allows you to:</p><ul><li>Use the same configuration files for both direct and proxied access</li><li>Switch between providers without modifying configuration files</li><li>Easily test different routing strategies with LiteLLM</li></ul><h4>Running Tests in a Production Environment</h4><p>If you already have a running PentAGI container and want to test the current configuration:</p><pre><code># Run ctester in an existing container using current environment variables\ndocker exec -it pentagi /opt/pentagi/bin/ctester -verbose\n\n# Test specific agent types with deterministic ordering\ndocker exec -it pentagi /opt/pentagi/bin/ctester -agents simple,primary_agent,pentester -groups basic,knowledge -verbose\n\n# Generate a report file inside the container\ndocker exec -it pentagi /opt/pentagi/bin/ctester -report /opt/pentagi/data/agent-test-report.md\n\n# Access the report from the host\ndocker cp pentagi:/opt/pentagi/data/agent-test-report.md ./\n</code></pre><p>The utility accepts several options:</p><ul><li> - Path to environment file (default: )</li><li> - Provider type: , , , , ,  (default: )</li><li> - Path to custom provider config (default: from  env variable)</li><li> - Path to custom tests YAML file (optional)</li><li> - Path to write the report file (optional)</li><li> - Comma-separated list of agent types to test (default: )</li><li> - Comma-separated list of test groups to run (default: )</li><li> - Enable verbose output with detailed test results for each agent</li></ul><p>Agents are tested in the following deterministic order:</p><ol><li> - Basic completion tasks</li><li> - JSON-structured responses</li><li> - Main reasoning agent</li><li> - Interactive assistant mode</li><li> - Content generation</li><li> - Content refinement and improvement</li><li> - Expert advice and consultation</li><li> - Self-reflection and analysis</li><li> - Information gathering and search</li><li> - Data enrichment and expansion</li><li> - Code generation and analysis</li><li> - Installation and setup tasks</li><li> - Penetration testing and security assessment</li></ol><ul><li> - Fundamental completion and prompt response tests</li><li> - Complex reasoning and function calling tests</li><li> - JSON format validation and structure tests (specifically designed for  agent)</li><li> - Domain-specific cybersecurity and penetration testing knowledge tests</li></ul><blockquote><p>: The  test group is specifically designed for the  agent type, while all other agents are tested with , , and  groups. This specialization ensures optimal testing coverage for each agent's intended purpose.</p></blockquote><h3>Example Provider Configuration</h3><p>Provider configuration defines which models to use for different agent types:</p><pre><code>simple:\n  model: \"provider/model-name\"\n  temperature: 0.7\n  top_p: 0.95\n  n: 1\n  max_tokens: 4000\n\nsimple_json:\n  model: \"provider/model-name\"\n  temperature: 0.7\n  top_p: 1.0\n  n: 1\n  max_tokens: 4000\n  json: true\n\n# ... other agent types ...\n</code></pre><ol><li>: Run tests with default configuration to establish benchmark performance</li><li><strong>Analyze agent-specific performance</strong>: Review the deterministic agent ordering to identify underperforming agents</li><li><strong>Test specialized configurations</strong>: Experiment with different models for each agent type using provider-specific configs</li><li><strong>Focus on domain knowledge</strong>: Pay special attention to knowledge group tests for cybersecurity expertise</li><li><strong>Validate function calling</strong>: Ensure tool-based tests pass consistently for critical agent types</li><li>: Look for the best success rate and performance across all test groups</li><li><strong>Deploy optimal configuration</strong>: Use in production with your optimized setup</li></ol><p>This tool helps ensure your AI agents are using the most effective models for their specific tasks, improving reliability while optimizing costs.</p><h2>ğŸ§® Embedding Configuration and Testing</h2><p>PentAGI uses vector embeddings for semantic search, knowledge storage, and memory management. The system supports multiple embedding providers that can be configured according to your needs and preferences.</p><h3>Supported Embedding Providers</h3><p>PentAGI supports the following embedding providers:</p><ul><li> (default): Uses OpenAI's text embedding models</li><li>: Local embedding model through Ollama</li><li>: Mistral AI's embedding models</li><li>: Jina AI's embedding service</li><li>: Models from HuggingFace</li><li>: Google's embedding models</li><li>: VoyageAI's embedding models</li></ul><h3>Embedding Tester Utility (etester)</h3><p>PentAGI includes a specialized  utility for testing, managing, and debugging embedding functionality. This tool is essential for diagnosing and resolving issues related to vector embeddings and knowledge storage.</p><h2>ğŸ” Function Testing with ftester</h2><p>PentAGI includes a versatile utility called  for debugging, testing, and developing specific functions and AI agent behaviors. While  focuses on testing LLM model capabilities,  allows you to directly invoke individual system functions and AI agent components with precise control over execution context.</p><ul><li>: Test individual functions without running the entire system</li><li>: Test functions without a live PentAGI deployment using built-in mocks</li><li>: Fill function arguments interactively for exploratory testing</li><li>: Color-coded terminal output with formatted responses and errors</li><li>: Debug AI agents within the context of specific flows, tasks, and subtasks</li><li><strong>Observability Integration</strong>: All function calls are logged to Langfuse and Observability stack</li></ul><p>Run ftester with specific function and arguments directly from the command line:</p><pre><code># Basic usage with mock mode\ncd backend\ngo run cmd/ftester/main.go [function_name] -[arg1] [value1] -[arg2] [value2]\n\n# Example: Test terminal command in mock mode\ngo run cmd/ftester/main.go terminal -command \"ls -la\" -message \"List files\"\n\n# Using a real flow context\ngo run cmd/ftester/main.go -flow 123 terminal -command \"whoami\" -message \"Check user\"\n\n# Testing AI agent in specific task/subtask context\ngo run cmd/ftester/main.go -flow 123 -task 456 -subtask 789 pentester -message \"Find vulnerabilities\"\n</code></pre><p>Run ftester without arguments for a guided interactive experience:</p><pre><code># Start interactive mode\ngo run cmd/ftester/main.go [function_name]\n\n# For example, to interactively fill browser tool arguments\ngo run cmd/ftester/main.go browser\n</code></pre><p>The main utility accepts several options:</p><ul><li> - Path to environment file (optional, default: )</li><li> - Provider type to use (default: , options: , , , , , )</li><li> - Flow ID for testing (0 means using mocks, default: )</li><li> - Task ID for agent context (optional)</li><li> - Subtask ID for agent context (optional)</li></ul><p>Function-specific arguments are passed after the function name using  format.</p><pre><code>docker build -t local/pentagi:latest .\n</code></pre><blockquote><p>[!NOTE] You can use  to build the image for different platforms like a <code>docker buildx build --platform linux/amd64 -t local/pentagi:latest .</code></p><p>You need to change image name in docker-compose.yml file to  and run  to start the server or use  key option in <a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose.yml\">docker-compose.yml</a> file.</p></blockquote><p>This project is made possible thanks to the following research and developments:</p><p>: Licensed under <a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/LICENSE\">MIT License</a> Copyright (c) 2025 PentAGI Development Team</p><h3>VXControl Cloud SDK Integration</h3><p><strong>VXControl Cloud SDK Integration</strong>: This repository integrates <a href=\"https://github.com/vxcontrol/cloud\">VXControl Cloud SDK</a> under a <strong>special licensing exception</strong> that applies  to the official PentAGI project.</p><h4>âœ… Official PentAGI Project</h4><ul><li>This official repository: <code>https://github.com/vxcontrol/pentagi</code></li><li>Official releases distributed by VXControl LLC</li><li>Code used under direct authorization from VXControl LLC</li></ul><h4>âš ï¸ Important for Forks and Third-Party Use</h4><p>If you fork this project or create derivative works, the VXControl SDK components are subject to  license terms. You must either:</p><ol><li><strong>Remove VXControl SDK integration</strong></li><li><strong>Open source your entire application</strong> (comply with AGPL-3.0 copyleft terms)</li><li><strong>Obtain a commercial license</strong> from VXControl LLC</li></ol><p>For commercial use of VXControl Cloud SDK in proprietary applications, contact:</p>",
      "contentLength": 51867,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "openclaw/openclaw",
      "url": "https://github.com/openclaw/openclaw",
      "date": 1771556616,
      "author": "",
      "guid": 46699,
      "unread": true,
      "content": "<p>Your own personal AI assistant. Any OS. Any Platform. The lobster way. ğŸ¦</p><p> is a  you run on your own devices. It answers you on the channels you already use (WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, Microsoft Teams, WebChat), plus extension channels like BlueBubbles, Matrix, Zalo, and Zalo Personal. It can speak and listen on macOS/iOS/Android, and can render a live Canvas you control. The Gateway is just the control plane â€” the product is the assistant.</p><p>If you want a personal, single-user assistant that feels local, fast, and always-on, this is it.</p><p>Preferred setup: run the onboarding wizard () in your terminal. The wizard guides you step by step through setting up the gateway, workspace, channels, and skills. The CLI wizard is the recommended path and works on <strong>macOS, Linux, and Windows (via WSL2; strongly recommended)</strong>. Works with npm, pnpm, or bun. New install? Start here: <a href=\"https://docs.openclaw.ai/start/getting-started\">Getting started</a></p><p>Model note: while any model is supported, I strongly recommend <strong>Anthropic Pro/Max (100/200) + Opus 4.6</strong> for longâ€‘context strength and better promptâ€‘injection resistance. See <a href=\"https://docs.openclaw.ai/start/onboarding\">Onboarding</a>.</p><h2>Models (selection + auth)</h2><pre><code>npm install -g openclaw@latest\n# or: pnpm add -g openclaw@latest\n\nopenclaw onboard --install-daemon\n</code></pre><p>The wizard installs the Gateway daemon (launchd/systemd user service) so it stays running.</p><pre><code>openclaw onboard --install-daemon\n\nopenclaw gateway --port 18789 --verbose\n\n# Send a message\nopenclaw message send --to +1234567890 --message \"Hello from OpenClaw\"\n\n# Talk to the assistant (optionally deliver back to any connected channel: WhatsApp/Telegram/Slack/Discord/Google Chat/Signal/iMessage/BlueBubbles/Microsoft Teams/Matrix/Zalo/Zalo Personal/WebChat)\nopenclaw agent --message \"Ship checklist\" --thinking high\n</code></pre><ul><li>: tagged releases ( or ), npm dist-tag .</li><li>: prerelease tags (), npm dist-tag  (macOS app may be missing).</li><li>: moving head of , npm dist-tag  (when published).</li></ul><p>Switch channels (git + npm): <code>openclaw update --channel stable|beta|dev</code>. Details: <a href=\"https://docs.openclaw.ai/install/development-channels\">Development channels</a>.</p><h2>From source (development)</h2><p>Prefer  for builds from source. Bun is optional for running TypeScript directly.</p><pre><code>git clone https://github.com/openclaw/openclaw.git\ncd openclaw\n\npnpm install\npnpm ui:build # auto-installs UI deps on first run\npnpm build\n\npnpm openclaw onboard --install-daemon\n\n# Dev loop (auto-reload on TS changes)\npnpm gateway:watch\n</code></pre><p>Note:  runs TypeScript directly (via ).  produces  for running via Node / the packaged  binary.</p><h2>Security defaults (DM access)</h2><p>OpenClaw connects to real messaging surfaces. Treat inbound DMs as .</p><p>Default behavior on Telegram/WhatsApp/Signal/iMessage/Microsoft Teams/Discord/Google Chat/Slack:</p><ul><li> ( / <code>channels.discord.dmPolicy=\"pairing\"</code> / <code>channels.slack.dmPolicy=\"pairing\"</code>; legacy: <code>channels.discord.dm.policy</code>, ): unknown senders receive a short pairing code and the bot does not process their message.</li><li>Approve with: <code>openclaw pairing approve &lt;channel&gt; &lt;code&gt;</code> (then the sender is added to a local allowlist store).</li><li>Public inbound DMs require an explicit opt-in: set  and include  in the channel allowlist ( / <code>channels.discord.allowFrom</code> / ; legacy: <code>channels.discord.dm.allowFrom</code>, <code>channels.slack.dm.allowFrom</code>).</li></ul><p>Run  to surface risky/misconfigured DM policies.</p><ul><li> â€” WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, BlueBubbles (iMessage), iMessage (legacy), Microsoft Teams, Matrix, Zalo, Zalo Personal, WebChat, macOS, iOS/Android.</li><li> â€” route inbound channels/accounts/peers to isolated agents (workspaces + per-agent sessions).</li><li> â€” browser, canvas, nodes, cron, sessions, and Discord/Slack actions.</li></ul><h2>Everything we built so far</h2><pre><code>WhatsApp / Telegram / Slack / Discord / Google Chat / Signal / iMessage / BlueBubbles / Microsoft Teams / Matrix / Zalo / Zalo Personal / WebChat\n               â”‚\n               â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚            Gateway            â”‚\nâ”‚       (control plane)         â”‚\nâ”‚     ws://127.0.0.1:18789      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚\n               â”œâ”€ Pi agent (RPC)\n               â”œâ”€ CLI (openclaw â€¦)\n               â”œâ”€ WebChat UI\n               â”œâ”€ macOS app\n               â””â”€ iOS / Android nodes\n</code></pre><h2>Tailscale access (Gateway dashboard)</h2><p>OpenClaw can auto-configure Tailscale  (tailnet-only) or  (public) while the Gateway stays bound to loopback. Configure :</p><ul><li>: no Tailscale automation (default).</li><li>: tailnet-only HTTPS via  (uses Tailscale identity headers by default).</li><li>: public HTTPS via  (requires shared password auth).</li></ul><ul><li> must stay  when Serve/Funnel is enabled (OpenClaw enforces this).</li><li>Serve can be forced to require a password by setting <code>gateway.auth.mode: \"password\"</code> or <code>gateway.auth.allowTailscale: false</code>.</li><li>Funnel refuses to start unless <code>gateway.auth.mode: \"password\"</code> is set.</li><li>Optional: <code>gateway.tailscale.resetOnExit</code> to undo Serve/Funnel on shutdown.</li></ul><h2>Remote Gateway (Linux is great)</h2><p>Itâ€™s perfectly fine to run the Gateway on a small Linux instance. Clients (macOS app, CLI, WebChat) can connect over  or , and you can still pair device nodes (macOS/iOS/Android) to execute deviceâ€‘local actions when needed.</p><ul><li> runs the exec tool and channel connections by default.</li><li> run deviceâ€‘local actions (, camera, screen recording, notifications) via . In short: exec runs where the Gateway lives; device actions run where the device lives.</li></ul><h2>macOS permissions via the Gateway protocol</h2><p>The macOS app can run in  and advertises its capabilities + permission map over the Gateway WebSocket ( / ). Clients can then execute local actions via :</p><ul><li> runs a local command and returns stdout/stderr/exit code; set <code>needsScreenRecording: true</code> to require screen-recording permission (otherwise youâ€™ll get ).</li><li> posts a user notification and fails if notifications are denied.</li><li>, , , and  are also routed via  and follow TCC permission status.</li></ul><p>Elevated bash (host permissions) is separate from macOS TCC:</p><ul><li>Use  to toggle perâ€‘session elevated access when enabled + allowlisted.</li><li>Gateway persists the perâ€‘session toggle via  (WS method) alongside , , , , and .</li></ul><h2>Agent to Agent (sessions_* tools)</h2><ul><li>Use these to coordinate work across sessions without jumping between chat surfaces.</li><li> â€” discover active sessions (agents) and their metadata.</li><li> â€” fetch transcript logs for a session.</li><li> â€” message another session; optional replyâ€‘back pingâ€‘pong + announce step (, ).</li></ul><h2>Skills registry (ClawHub)</h2><p>ClawHub is a minimal skill registry. With ClawHub enabled, the agent can search for skills automatically and pull in new ones as needed.</p><p>Send these in WhatsApp/Telegram/Slack/Google Chat/Microsoft Teams/WebChat (group commands are owner-only):</p><ul><li> â€” compact session status (model + tokens, cost when available)</li><li> or  â€” reset the session</li><li> â€” compact session context (summary)</li><li> â€” off|minimal|low|medium|high|xhigh (GPT-5.2 + Codex models only)</li><li> â€” per-response usage footer</li><li> â€” restart the gateway (owner-only in groups)</li><li><code>/activation mention|always</code> â€” group activation toggle (groups only)</li></ul><p>The Gateway alone delivers a great experience. All apps are optional and add extra features.</p><p>If you plan to build/run companion apps, follow the platform runbooks below.</p><h3>macOS (OpenClaw.app) (optional)</h3><ul><li>Menu bar control for the Gateway and health.</li><li>Voice Wake + push-to-talk overlay.</li><li>Remote gateway control over SSH.</li></ul><p>Note: signed builds required for macOS permissions to stick across rebuilds (see ).</p><ul><li>Pairs as a node via the Bridge.</li><li>Voice trigger forwarding + Canvas surface.</li><li>Controlled via .</li></ul><ul><li>Pairs via the same Bridge + pairing flow as iOS.</li><li>Exposes Canvas, Camera, and Screen capture commands.</li></ul><ul><li>Workspace root:  (configurable via <code>agents.defaults.workspace</code>).</li><li>Injected prompt files: , , .</li><li>Skills: <code>~/.openclaw/workspace/skills/&lt;skill&gt;/SKILL.md</code>.</li></ul><p>Minimal <code>~/.openclaw/openclaw.json</code> (model + defaults):</p><pre><code>{\n  agent: {\n    model: \"anthropic/claude-opus-4-6\",\n  },\n}\n</code></pre><h2>Security model (important)</h2><ul><li> tools run on the host for the  session, so the agent has full access when itâ€™s just you.</li><li> set <code>agents.defaults.sandbox.mode: \"non-main\"</code> to run  (groups/channels) inside perâ€‘session Docker sandboxes; bash then runs in Docker for those sessions.</li><li> allowlist , , , , , , , , ; denylist , , , , , .</li></ul><ul><li>Link the device: <code>pnpm openclaw channels login</code> (stores creds in ).</li><li>Allowlist who can talk to the assistant via <code>channels.whatsapp.allowFrom</code>.</li><li>If  is set, it becomes a group allowlist; include  to allow all.</li></ul><ul><li>Set  or <code>channels.telegram.botToken</code> (env wins).</li><li>Optional: set  (with <code>channels.telegram.groups.\"*\".requireMention</code>); when set, it is a group allowlist (include  to allow all). Also <code>channels.telegram.allowFrom</code> or <code>channels.telegram.webhookUrl</code> + <code>channels.telegram.webhookSecret</code> as needed.</li></ul><pre><code>{\n  channels: {\n    telegram: {\n      botToken: \"123456:ABCDEF\",\n    },\n  },\n}\n</code></pre><ul><li>Set  +  (or  + ).</li></ul><ul><li>Set  or  (env wins).</li><li>Optional: set , , or , plus <code>channels.discord.allowFrom</code>, , or <code>channels.discord.mediaMaxMb</code> as needed.</li></ul><pre><code>{\n  channels: {\n    discord: {\n      token: \"1234abcd\",\n    },\n  },\n}\n</code></pre><ul><li>Requires  and a  config section.</li></ul><ul><li> iMessage integration.</li><li>Configure <code>channels.bluebubbles.serverUrl</code> + <code>channels.bluebubbles.password</code> and a webhook (<code>channels.bluebubbles.webhookPath</code>).</li><li>The BlueBubbles server runs on macOS; the Gateway can run on macOS or elsewhere.</li></ul><ul><li>Legacy macOS-only integration via  (Messages must be signed in).</li><li>If  is set, it becomes a group allowlist; include  to allow all.</li></ul><ul><li>Configure a Teams app + Bot Framework, then add a  config section.</li><li>Allowlist who can talk via ; group access via  or <code>msteams.groupPolicy: \"open\"</code>.</li></ul><ul><li>Uses the Gateway WebSocket; no separate WebChat port/config.</li></ul><p>Browser control (optional):</p><pre><code>{\n  browser: {\n    enabled: true,\n    color: \"#FF4500\",\n  },\n}\n</code></pre><p>Use these when youâ€™re past the onboarding flow and want the deeper reference.</p><h2>Advanced docs (discovery + control)</h2><h2>Operations &amp; troubleshooting</h2><p>OpenClaw was built for , a space lobster AI assistant. ğŸ¦ by Peter Steinberger and the community.</p><p>See <a href=\"https://raw.githubusercontent.com/openclaw/openclaw/main/CONTRIBUTING.md\">CONTRIBUTING.md</a> for guidelines, maintainers, and how to submit PRs. AI/vibe-coded PRs welcome! ğŸ¤–</p><p>Special thanks to <a href=\"https://mariozechner.at/\">Mario Zechner</a> for his support and for <a href=\"https://github.com/badlogic/pi-mono\">pi-mono</a>. Special thanks to Adam Doppelt for lobster.bot.</p><p>Thanks to all clawtributors:</p>",
      "contentLength": 10081,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "open-mercato/open-mercato",
      "url": "https://github.com/open-mercato/open-mercato",
      "date": 1771556616,
      "author": "",
      "guid": 46700,
      "unread": true,
      "content": "<p>AIâ€‘supportive CRM / ERP foundation framework â€” built to power R&amp;D, new processes, operations, and growth. Itâ€™s modular, extensible, and designed for teams that want strong defaults with room to customize everything. Better than Django, Retool and other alternatives - and Enterprise Grade!</p><p>Open Mercato is a newâ€‘era, AIâ€‘supportive platform for shipping enterpriseâ€‘grade CRMs, ERPs, and commerce backends. Itâ€™s modular, extensible, and designed so teams can mix their own modules, entities, and workflows while keeping the guardrails of a production-ready stack.</p><p> Now, you can have best of both. Use  enterprise ready business features like CRM, Sales, OMS, Encryption and build the remaining  that really makes the difference for your business.</p><ul><li>ğŸ’¼  â€“ model customers, opportunities, and bespoke workflows with infinitely flexible data definitions.</li><li>ğŸ­  â€“ manage orders, production, and service delivery while tailoring modules to match your operational reality.</li><li>ğŸ›’  â€“ launch CPQ flows, B2B ordering portals, or full commerce backends with reusable modules.</li><li>ğŸ¤  â€“ spin up customer or partner portals with configurable forms, guided flows, and granular permissions.</li><li>ğŸ”„  â€“ orchestrate custom data lifecycles and document workflows per tenant or team.</li><li>ğŸ§µ  â€“ coordinate production management with modular entities, automation hooks, and reporting.</li><li>ğŸŒ  â€“ expose rich, well-typed APIs for mobile and web apps using the same extensible data model.</li></ul><ul><li>ğŸ§©  â€“ drop in your own modules, pages, APIs, and entities with auto-discovery and overlay overrides.</li><li>ğŸ§¬ <strong>Custom entities &amp; dynamic forms</strong> â€“ declare fields, validators, and UI widgets per module and manage them live from the admin.</li><li>ğŸ¢  â€“ SaaS-ready tenancy with strict organization/tenant scoping for every entity and API.</li><li>ğŸ›ï¸ <strong>Multi-hierarchical organizations</strong> â€“ built-in organization trees with role- and user-level visibility controls.</li><li>ğŸ›¡ï¸  â€“ combine per-role and per-user feature flags with organization scoping to gate any page or API.</li><li>âš¡  â€“ hybrid JSONB indexing and smart caching for blazing-fast queries across base and custom fields.</li><li>ğŸ”” <strong>Event subscribers &amp; workflows</strong> â€“ publish domain events and process them via persistent subscribers (local or Redis).</li><li>âœ…  â€“ expanding unit and integration tests ensure modules stay reliable as you extend them.</li><li>ğŸ§   â€“ structured for assistive workflows, automation, and conversational interfaces.</li><li>âš™ï¸  â€“ Next.js App Router, TypeScript, zod, Awilix DI, MikroORM, and bcryptjs out of the box.</li></ul><ul><li>ğŸ§© Modules: Each feature lives under  with autoâ€‘discovered frontend/backend pages, APIs, CLI, i18n, and DB entities.</li><li>ğŸ—ƒï¸ Database: MikroORM with perâ€‘module entities and migrations; no global schema. Migrations are generated and applied per module.</li><li>ğŸ§° Dependency Injection: Awilix container constructed per request. Modules can register and override services/components via .</li><li>ğŸ¢ Multiâ€‘tenant: Core  module defines  and . Most entities carry  + .</li><li>ğŸ” Security: RBAC roles, zod validation, bcryptjs hashing, JWT sessions, roleâ€‘based access in routes and APIs.</li></ul><p>Open Mercato includes a built-in AI Assistant that can discover and interact with your data model and APIs. The assistant uses MCP (Model Context Protocol) to expose tools for schema discovery and API execution.</p><ul><li>ğŸ”  â€“ Query database entity schemas including fields, types, and relationships</li><li>ğŸ”—  â€“ Search for API endpoints using natural language queries</li><li>âš¡  â€“ Execute API calls with automatic tenant context and authentication</li><li>ğŸ§   â€“ Uses Meilisearch for fast fulltext + vector search across schemas and endpoints</li></ul><table><tbody><tr><td>Search entity schemas by name or keyword</td></tr><tr><td>Find API endpoints by natural language query</td></tr><tr><td>Execute API calls with tenant context</td></tr><tr><td>Get current authentication context</td></tr></tbody></table><ul><li> () â€“ For Claude Code and local development with API key auth</li><li> () â€“ For web AI chat with session tokens</li></ul><p>Open Mercato ships with tenant-scoped, field-level data encryption so PII and sensitive business data stay protected while you keep the flexibility of custom entities and fields. Encryption maps live in the admin UI/database, letting you pick which system and custom columns are encrypted; MikroORM hooks automatically encrypt on write and decrypt on read while keeping deterministic hashes (e.g., ) for lookups.</p><p>Architecture in two lines: Vault/KMS (or a derived-key fallback) issues per-tenant DEKs and caches them so performance stays snappy; AES-GCM wrappers sit in the ORM lifecycle, storing ciphertext at rest while CRUD and APIs keep working with plaintext. Read the docs to dive deeper: <a href=\"https://docs.openmercato.com/user-guide/encryption\">docs.openmercato.com/user-guide/encryption</a>.</p><p>We have migrated Open Mercato to a monorepo structure. If you're upgrading from a previous version, please note the following changes:</p><p>The codebase is now organized into:</p><ul><li> - Shared libraries and modules (, , , , , , , , , )</li><li> - Applications (main app in , docs in )</li></ul><p><strong>Important note on storage:</strong> The storage folder has been moved to the  folder as well. If you instance has got any attachments uploaded, please make sure you run:</p><pre><code>mv storage apps/mercato/storage\n</code></pre><p>... from the root Open Mercato folder.</p><p>Import aliases have changed from path-based to package-based imports:</p><ul><li>, , </li><li><code>@open-mercato/shared/lib/...</code>, <code>@open-mercato/ui/components/...</code>, <code>@open-mercato/core/modules/...</code>, etc.</li></ul><p>The  file now must live in  instead of the project root. The fastest way to start is to copy the example file:</p><pre><code>cp apps/mercato/.env.example apps/mercato/.env\n</code></pre><p>At minimum, set , , and  (or ) before bootstrapping.</p><p>Yarn 4 is now required. Ensure you have Yarn 4+ installed before proceeding.</p><p>This is a quickest way to get Open Mercato up and running on your localhost / server - ready for testing / demoing or for !</p><pre><code># macOS (Homebrew)\nbrew install node@24\n\n# Windows (Chocolatey)\nchoco install nodejs --version=24.x\n\n# Or use nvm (any platform)\nnvm install 24\nnvm use 24\n</code></pre><pre><code>git clone https://github.com/open-mercato/open-mercato.git\ncd open-mercato\ngit checkout develop\nyarn install\n\ncp apps/mercato/.env.example apps/mercato/.env # EDIT this file to set up your specific files\n#At minimum, set `DATABASE_URL`, `JWT_SECRET`, and `REDIS_URL` (or `EVENTS_REDIS_URL`) before bootstrapping.\n\nyarn generate\nyarn initialize # or yarn reinstall\nyarn dev\n</code></pre><p>For a fresh greenfield boot (build packages, generate registries, reinstall modules, then start dev), run:</p><p>Navigate to <code>http://localhost:3000/backend</code> and sign in with the default credentials printed by .</p><p>Open Mercato offers two Docker Compose configurations â€” one for  (with hot reload) and one for . Both run the full stack (app + PostgreSQL + Redis + Meilisearch) in containers. The dev mode is the <strong>recommended setup for Windows</strong> users.</p><p>Run the entire stack with source code mounted from the host. File changes trigger automatic rebuilds â€” no local Node.js or Yarn required.</p><pre><code>git clone https://github.com/open-mercato/open-mercato.git\ncd open-mercato\ngit checkout develop\ndocker compose -f docker-compose.fullapp.dev.yml up --build\n</code></pre><p> Ensure WSL 2 backend is enabled in Docker Desktop and clone with <code>git config --global core.autocrlf input</code> to avoid line-ending issues.</p><pre><code>docker compose -f docker-compose.fullapp.yml up --build\n</code></pre><ul><li>Start: <code>docker compose -f docker-compose.fullapp.yml up -d</code></li><li>Logs: <code>docker compose -f docker-compose.fullapp.yml logs -f app</code></li><li>Stop: <code>docker compose -f docker-compose.fullapp.yml down</code></li><li>Rebuild: <code>docker compose -f docker-compose.fullapp.yml up --build</code></li></ul><p>Navigate to <code>http://localhost:3000/backend</code> and sign in with the default credentials (<a href=\"mailto:admin@example.com\">admin@example.com</a>).</p><h3>Docker Environment Variables</h3><p>Before starting, you may want to configure the following environment variables. Create a  file in the project root or export them in your shell:</p><table><thead><tr></tr></thead><tbody><tr><td>Secret key for JWT token signing. <strong>Use a strong, unique value in production.</strong></td></tr><tr><td>PostgreSQL database password. <strong>Use a strong password in production.</strong></td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>Meilisearch API key. <strong>Use a strong key in production.</strong></td></tr><tr></tr><tr><td>OpenAI API key (enables AI features)</td></tr><tr><td>Anthropic API key (for opencode service)</td></tr><tr><td>Opencode service exposed port</td></tr></tbody></table><p>Example  file for production:</p><pre><code>JWT_SECRET=your-strong-secret-key-here\nPOSTGRES_PASSWORD=your-strong-db-password\nMEILISEARCH_MASTER_KEY=your-strong-meilisearch-key\nOPENAI_API_KEY=sk-...  # Optional, for AI features\n</code></pre><p>For production deployments, ensure strong , secure database credentials, and consider managed database services. See the <a href=\"https://docs.openmercato.com/installation/setup#docker-deployment-full-stack\">full Docker deployment guide</a> for detailed configuration and production tips.</p><h2>Standalone App &amp; Customization</h2><p>The <strong>recommended way to build on Open Mercato</strong> without modifying the core is to create a standalone app. This gives you a self-contained project that pulls Open Mercato packages from npm â€” your own modules, overrides, and customizations live in your repo while core stays untouched and upgradeable.</p><pre><code>npx create-mercato-app my-store\ncd my-store\ncp .env.example .env   # configure DATABASE_URL, JWT_SECRET, REDIS_URL\ndocker compose up -d   # start PostgreSQL, Redis, Meilisearch\nyarn install\nyarn initialize\nyarn dev\n</code></pre><p>Navigate to <code>http://localhost:3000/backend</code> and sign in with the credentials printed by .</p><p>Drop your own modules into  and register them in  with :</p><pre><code>export const enabledModules: ModuleEntry[] = [\n  // ... core modules\n  { id: 'inventory', from: '@app' },\n]\n</code></pre><p>Run  and  â€” your module's pages, APIs, and entities are auto-discovered.</p><h3>Eject core modules for deep customization</h3><p>When you need to change the internals of a core module (entities, business logic, UI),  it. The  command copies the module source into your  directory and switches it to local, so you can modify it freely while all other modules keep receiving package updates.</p><pre><code># See which modules support ejection\nyarn mercato eject --list\n\n# Eject a module (e.g., currencies)\nyarn mercato eject currencies\nyarn mercato generate all\nyarn dev\n</code></pre><p>Currently ejectable: , , , , , , , , .</p><p>Open Mercato follows a <strong>spec-first development approach</strong>. Before implementing new features or making significant changes, we document the design in the  folder.</p><ul><li>: Specs ensure everyone understands the feature before coding starts</li><li>: Design decisions are documented and can be referenced by humans and AI agents</li><li>: Each spec maintains a changelog tracking the evolution of the feature</li></ul><ol><li>: Check if a spec exists in  (named <code>SPEC-###-YYYY-MM-DD-title.md</code>)</li><li>: Create or update the spec with your design before implementation</li><li>: Update the spec's changelog with a dated summary</li></ol><p>: Specs use the format <code>SPEC-{number}-{date}-{title}.md</code> (e.g., <code>SPEC-007-2026-01-26-sidebar-reorganization.md</code>)</p><p>We welcome contributions of all sizesâ€”from fixes and docs updates to new modules. Start by reading <a href=\"https://raw.githubusercontent.com/open-mercato/open-mercato/main/CONTRIBUTING.md\">CONTRIBUTING.md</a> for branching conventions (, , ), release flow, and the full PR checklist. Then check the open issues or propose an idea in a discussion, and:</p><ol><li>Fork the repository and create a branch that reflects your change.</li><li>Install dependencies with  and bootstrap via  (add  to skip demo CRM content;  for thousands of synthetic contacts, companies, deals, and timeline interactions; or  for high-volume contacts without the heavier extras).</li><li>Develop and validate your changes (, , or the relevant module scripts).</li><li>Open a pull request referencing any related issues and outlining the testing you performed.</li></ol><p>Refer to <a href=\"https://raw.githubusercontent.com/open-mercato/open-mercato/main/AGENTS.md\">AGENTS.md</a> for deeper guidance on architecture and conventions when extending modules.</p><p>Open Mercato let the module developers to expose the custom CLI commands for variouse maintenance tasks. Read more on the <a href=\"https://docs.openmercato.com/cli/overview\">CLI documentation</a></p><ul><li>MIT â€” see  for details.</li></ul>",
      "contentLength": 11363,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "freemocap/freemocap",
      "url": "https://github.com/freemocap/freemocap",
      "date": 1771556616,
      "author": "",
      "guid": 46701,
      "unread": true,
      "content": "<p>Free Motion Capture for Everyone ğŸ’€âœ¨</p><h4 align=\"center\"> A free-and-open-source, hardware-and-software-agnostic, minimal-cost, research-grade, motion capture system and platform for decentralized scientific research, education, and training</h4><h4>0. Create a a Python 3.10 through 3.12 environment (python3.12 recommended)</h4><h4>1. Install software via <a href=\"https://pypi.org/project/freemocap/#description\">pip</a>:</h4><h4>2. Launch the GUI by entering the command:</h4><h4>3. A GUI should pop up that looks like this:</h4><img width=\"1457\" alt=\"image\" src=\"https://github.com/freemocap/freemocap/assets/15314521/90ef7e7b-48f3-4f46-8d4a-5b5bcc3254b3\"><h2>Install/run from source code (i.e. the code in this repo)</h2><ol><li>Create a  environment (Recommended version is )</li></ol><pre><code>conda create -n freemocap-env python=3.11\n</code></pre><ol start=\"2\"><li>Activate that newly created environment</li></ol><pre><code>conda activate freemocap-env\n</code></pre><pre><code>git clone https://github.com/freemocap/freemocap\n</code></pre><ol start=\"4\"><li>Navigate into the newly cloned/downloaded  folder</li></ol><ol start=\"5\"><li>Install the package via the  file</li></ol><ol start=\"6\"><li>Launch the GUI (via the  entry point)</li></ol><p>This project is licensed under the APGL License - see the <a href=\"https://raw.githubusercontent.com/freemocap/freemocap/main/LICENSE\">LICENSE</a> file for details.</p><p>If the AGPL does not work for your needs, we are happy to discuss terms to license this software to you with a different agreement at a price point that increases exponentially as you move <a href=\"https://www.gnu.org/philosophy/open-source-misses-the-point.en.html\">spiritually</a> away from the </p>",
      "contentLength": 1093,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "RichardAtCT/claude-code-telegram",
      "url": "https://github.com/RichardAtCT/claude-code-telegram",
      "date": 1771556616,
      "author": "",
      "guid": 46702,
      "unread": true,
      "content": "<p>A powerful Telegram bot that provides remote access to Claude Code, enabling developers to interact with their projects from anywhere with full AI assistance and session persistence.</p><p>A Telegram bot that gives you remote access to <a href=\"https://claude.ai/code\">Claude Code</a>. Chat naturally with Claude about your projects from anywhere -- no terminal commands needed.</p><p>This bot connects Telegram to Claude Code, providing a conversational AI interface for your codebase:</p><ul><li> -- ask Claude to analyze, edit, or explain your code in plain language</li><li> across conversations with automatic session persistence per project</li><li> from any device with Telegram</li><li><strong>Receive proactive notifications</strong> from webhooks, scheduled jobs, and CI/CD events</li><li> with built-in authentication, directory sandboxing, and audit logging</li></ul><pre><code>You: Can you help me add error handling to src/api.py?\n\nBot: I'll analyze src/api.py and add error handling...\n     [Claude reads your code, suggests improvements, and can apply changes directly]\n\nYou: Looks good. Now run the tests to make sure nothing broke.\n\nBot: Running pytest...\n     All 47 tests passed. The error handling changes are working correctly.\n</code></pre><p>Choose your preferred method:</p><h4>Option A: Install from a release tag (Recommended)</h4><pre><code># Using uv (recommended â€” installs in an isolated environment)\nuv tool install git+https://github.com/RichardAtCT/claude-code-telegram@v1.3.0\n\n# Or using pip\npip install git+https://github.com/RichardAtCT/claude-code-telegram@v1.3.0\n\n# Track the latest stable release\npip install git+https://github.com/RichardAtCT/claude-code-telegram@latest\n</code></pre><h4>Option B: From source (for development)</h4><pre><code>git clone https://github.com/RichardAtCT/claude-code-telegram.git\ncd claude-code-telegram\nmake dev  # requires Poetry\n</code></pre><blockquote><p> Always install from a tagged release (not ) for stability. See <a href=\"https://github.com/RichardAtCT/claude-code-telegram/releases\">Releases</a> for available versions.</p></blockquote><pre><code>cp .env.example .env\n# Edit .env with your settings:\n</code></pre><pre><code>TELEGRAM_BOT_TOKEN=1234567890:ABC-DEF1234ghIkl-zyx57W2v1u123ew11\nTELEGRAM_BOT_USERNAME=my_claude_bot\nAPPROVED_DIRECTORY=/Users/yourname/projects\nALLOWED_USERS=123456789  # Your Telegram user ID\n</code></pre><pre><code>make run          # Production\nmake run-debug    # With debug logging\n</code></pre><p>Message your bot on Telegram to get started.</p><blockquote><p> See <a href=\"https://raw.githubusercontent.com/RichardAtCT/claude-code-telegram/main/docs/setup.md\">docs/setup.md</a> for Claude authentication options and troubleshooting.</p></blockquote><p>The bot supports two interaction modes:</p><p>The default conversational mode. Just talk to Claude naturally -- no special commands required.</p><p>, , , ,  If <code>ENABLE_PROJECT_THREADS=true</code>: </p><pre><code>You: What files are in this project?\nBot: Working... (3s)\n     ğŸ“– Read\n     ğŸ“‚ LS\n     ğŸ’¬ Let me describe the project structure\nBot: [Claude describes the project structure]\n\nYou: Add a retry decorator to the HTTP client\nBot: Working... (8s)\n     ğŸ“– Read: http_client.py\n     ğŸ’¬ I'll add a retry decorator with exponential backoff\n     âœï¸ Edit: http_client.py\n     ğŸ’» Bash: poetry run pytest tests/ -v\nBot: [Claude shows the changes and test results]\n\nYou: /verbose 0\nBot: Verbosity set to 0 (quiet)\n</code></pre><p>Use  to control how much background activity is shown:</p><table><tbody><tr><td>Final response only (typing indicator stays active)</td></tr><tr><td>Tool names + reasoning snippets in real-time</td></tr><tr><td>Tool names with inputs + longer reasoning text</td></tr></tbody></table><p>Claude Code already knows how to use  CLI and . Authenticate on your server with , then work with repos conversationally:</p><pre><code>You: List my repos related to monitoring\nBot: [Claude runs gh repo list, shows results]\n\nYou: Clone the uptime one\nBot: [Claude runs gh repo clone, clones into workspace]\n\nYou: /repo\nBot: ğŸ“¦ uptime-monitor/  â—€\n     ğŸ“ other-project/\n\nYou: Show me the open issues\nBot: [Claude runs gh issue list]\n\nYou: Create a fix branch and push it\nBot: [Claude creates branch, commits, pushes]\n</code></pre><p>Use  to list cloned repos in your workspace, or  to switch directories (sessions auto-resume).</p><p>Set  to enable the full 13-command terminal-like interface with directory navigation, inline keyboards, quick actions, git integration, and session export.</p><p>, , , , , , , , , , , ,  If <code>ENABLE_PROJECT_THREADS=true</code>: </p><pre><code>You: /cd my-web-app\nBot: Directory changed to my-web-app/\n\nYou: /ls\nBot: src/  tests/  package.json  README.md\n\nYou: /actions\nBot: [Run Tests] [Install Deps] [Format Code] [Run Linter]\n</code></pre><p>Beyond direct chat, the bot can respond to external triggers:</p><ul><li> -- Receive GitHub events (push, PR, issues) and route them through Claude for automated summaries or code review</li><li> -- Run recurring Claude tasks on a cron schedule (e.g., daily code health checks)</li><li> -- Deliver agent responses to configured Telegram chats</li></ul><p>Enable with  and . See <a href=\"https://raw.githubusercontent.com/RichardAtCT/claude-code-telegram/main/docs/setup.md\">docs/setup.md</a> for configuration.</p><ul><li><p>Conversational agentic mode (default) with natural language interaction</p></li><li><p>Classic terminal-like mode with 13 commands and inline keyboards</p></li><li><p>Full Claude Code integration with SDK (primary) and CLI (fallback)</p></li><li><p>Automatic session persistence per user/project directory</p></li><li><p>Multi-layer authentication (whitelist + optional token-based)</p></li><li><p>Rate limiting with token bucket algorithm</p></li><li><p>Directory sandboxing with path traversal prevention</p></li><li><p>File upload handling with archive extraction</p></li><li><p>Image/screenshot upload with analysis</p></li><li><p>Git integration with safe repository operations</p></li><li><p>Quick actions system with context-aware buttons</p></li><li><p>Session export in Markdown, HTML, and JSON formats</p></li><li><p>SQLite persistence with migrations</p></li><li><p>Audit logging and security event tracking</p></li><li><p>Event bus for decoupled message routing</p></li><li><p>Webhook API server (GitHub HMAC-SHA256, generic Bearer token auth)</p></li><li><p>Job scheduler with cron expressions and persistent storage</p></li><li><p>Notification service with per-chat rate limiting</p></li><li><p>Tunable verbose output showing Claude's tool usage and reasoning in real-time</p></li><li><p>Persistent typing indicator so users always know the bot is working</p></li><li><p>16 configurable tools with allowlist/disallowlist control (see <a href=\"https://raw.githubusercontent.com/RichardAtCT/claude-code-telegram/main/docs/tools.md\">docs/tools.md</a>)</p></li></ul><ul><li>Plugin system for third-party extensions</li></ul><pre><code>TELEGRAM_BOT_TOKEN=...           # From @BotFather\nTELEGRAM_BOT_USERNAME=...        # Your bot's username\nAPPROVED_DIRECTORY=...           # Base directory for project access\nALLOWED_USERS=123456789          # Comma-separated Telegram user IDs\n</code></pre><pre><code># Claude\nANTHROPIC_API_KEY=sk-ant-...     # API key (optional if using CLI auth)\nCLAUDE_MAX_COST_PER_USER=10.0    # Spending limit per user (USD)\nCLAUDE_TIMEOUT_SECONDS=300       # Operation timeout\n\n# Mode\nAGENTIC_MODE=true                # Agentic (default) or classic mode\nVERBOSE_LEVEL=1                  # 0=quiet, 1=normal (default), 2=detailed\n\n# Rate Limiting\nRATE_LIMIT_REQUESTS=10           # Requests per window\nRATE_LIMIT_WINDOW=60             # Window in seconds\n\n# Features (classic mode)\nENABLE_GIT_INTEGRATION=true\nENABLE_FILE_UPLOADS=true\nENABLE_QUICK_ACTIONS=true\n</code></pre><pre><code># Webhook API Server\nENABLE_API_SERVER=false          # Enable FastAPI webhook server\nAPI_SERVER_PORT=8080             # Server port\n\n# Webhook Authentication\nGITHUB_WEBHOOK_SECRET=...        # GitHub HMAC-SHA256 secret\nWEBHOOK_API_SECRET=...           # Bearer token for generic providers\n\n# Scheduler\nENABLE_SCHEDULER=false           # Enable cron job scheduler\n\n# Notifications\nNOTIFICATION_CHAT_IDS=123,456    # Default chat IDs for proactive notifications\n</code></pre><pre><code># Enable strict topic routing by project\nENABLE_PROJECT_THREADS=true\n\n# Mode: private (default) or group\nPROJECT_THREADS_MODE=private\n\n# YAML registry file (see config/projects.example.yaml)\nPROJECTS_CONFIG_PATH=config/projects.yaml\n\n# Required only when PROJECT_THREADS_MODE=group\nPROJECT_THREADS_CHAT_ID=-1001234567890\n\n# Minimum delay (seconds) between Telegram API calls during topic sync\n# Set 0 to disable pacing\nPROJECT_THREADS_SYNC_ACTION_INTERVAL_SECONDS=1.1\n</code></pre><p>In strict mode, only  and  work outside mapped project topics. In private mode,  auto-syncs project topics for your private bot chat. To use topics with your bot, enable them in BotFather: <code>Bot Settings -&gt; Threaded mode</code>.</p><h3>Finding Your Telegram User ID</h3><p>Message <a href=\"https://t.me/userinfobot\">@userinfobot</a> on Telegram -- it will reply with your user ID number.</p><ul><li>Check your  is correct</li><li>Verify your user ID is in </li><li>Ensure Claude Code CLI is installed and accessible</li><li>Check bot logs with </li></ul><p><strong>Claude integration not working:</strong></p><ul><li>SDK mode (default): Check  or verify </li><li>CLI mode: Verify  and </li><li>Check  includes necessary tools (see <a href=\"https://raw.githubusercontent.com/RichardAtCT/claude-code-telegram/main/docs/tools.md\">docs/tools.md</a> for the full reference)</li></ul><ul><li>Adjust  to set spending limits</li><li>Monitor usage with </li><li>Use shorter, more focused requests</li></ul><p>This bot implements defense-in-depth security:</p><ul><li> -- Whitelist-based user authentication</li><li> -- Sandboxing to approved directories</li><li> -- Request and cost-based limits</li><li> -- Injection and path traversal protection</li><li> -- GitHub HMAC-SHA256 and Bearer token verification</li><li> -- Complete tracking of all user actions</li></ul><pre><code>make dev           # Install all dependencies\nmake test          # Run tests with coverage\nmake lint          # Black + isort + flake8 + mypy\nmake format        # Auto-format code\nmake run-debug     # Run with debug logging\n</code></pre><p>The version is defined once in  and read at runtime via . To cut a release:</p><pre><code>make bump-patch    # 1.2.0 -&gt; 1.2.1 (bug fixes)\nmake bump-minor    # 1.2.0 -&gt; 1.3.0 (new features)\nmake bump-major    # 1.2.0 -&gt; 2.0.0 (breaking changes)\n</code></pre><p>Each command commits, tags, and pushes automatically, triggering CI tests and a GitHub Release with auto-generated notes.</p><ol><li>Create a feature branch: <code>git checkout -b feature/amazing-feature</code></li><li>Make changes with tests: </li></ol><p> Python 3.11+, Black formatting (88 chars), type hints required, pytest with &gt;85% coverage.</p>",
      "contentLength": 9146,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "p2r3/convert",
      "url": "https://github.com/p2r3/convert",
      "date": 1771470321,
      "author": "",
      "guid": 46303,
      "unread": true,
      "content": "<p>Truly universal online file converter</p><p><strong>Truly universal online file converter.</strong></p><p>Many online file conversion tools are  and . They only allow conversion between two formats in the same medium (images to images, videos to videos, etc.), and they require that you <em>upload your files to some server</em>.</p><p>This is not just terrible for privacy, it's also incredibly lame. What if you  need to convert an AVI video to a PDF document? Try to find an online tool for that, I dare you.</p><p><a href=\"https://convert.to.it/\">Convert.to.it</a> aims to be a tool that \"just works\". You're almost  to get an output - perhaps not always the one you expected, but it'll try its best to not leave you hanging.</p><ol><li>Click the big blue box to add your file (or just drag it on to the window).</li><li>An input format should have been automatically selected. If it wasn't, yikes! Try searching for it, or if it's really not there, see the \"Issues\" section below.</li><li>Select an output format from the second list. If you're on desktop, that's the one on the right side. If you're on mobile, it'll be somewhere lower down.</li><li>Hopefully, after a bit (or a lot) of thinking, the program will spit out the file you wanted. If not, see the \"Issues\" section below.</li></ol><p>Ever since the YouTube video released, we've been getting spammed with issues suggesting the addition of all kinds of niche file formats. To keep things organized, I've decided to specify what counts as a valid issue and what doesn't.</p><blockquote><p>[!IMPORTANT] <strong>SIMPLY ASKING FOR A FILE FORMAT TO BE ADDED IS NOT A MEANINGFUL ISSUE!</strong></p></blockquote><p>There are thousands of file formats out there. It can take hours to add support for just one. The math is simple - we can't possibly support every single file. As such, simply listing your favorite file formats is not helpful. We already know that there are formats we don't support, we don't need tickets to tell us that.</p><p>When suggesting a file format, you must :</p><ul><li>Make sure that there isn't already an issue about the same thing, and that we don't already support the format.</li><li>Explain what you expect the conversion to be like (what medium is it converting to/from). It's important to note here that simply parsing the underlying data is . Imagine if we only treated SVG images as raw XML data and didn't support converting them to raster images - that would defeat the point.</li><li>Provide links to existing browser-based solutions if possible, or at the very least a reference for implementing the format, and make sure the license is compatible with GPL-2.0.</li></ul><p>If this seems like a lot, please remember - a developer will have to do 100x more work to actually implement the format. Doing a bit of research not only saves them precious time, it also weeds out \"unserious\" proposals that would only bloat our to-do list.</p><p><strong>If you're submitting a bug report,</strong> you only need to do step 1 - check if the problem isn't already reported by someone else. Bug reports are generally quite important otherwise.</p><p>Though please note, \"converting X to Y doesn't work\" is  a bug report. However, \"converting X to Y works but not how I expected\" likely  a bug report.</p><h3>Local development (Bun + Vite)</h3><ol><li>Clone this repository . You can use <code>git clone --recursive https://github.com/p2r3/convert</code> for that. Omitting submodules will leave you missing a few dependencies.</li><li>Run  to install dependencies.</li><li>Run  to start the development server.</li></ol><p><em>The following steps are optional, but recommended for performance:</em></p><p>When you first open the page, it'll take a while to generate the list of supported formats for each tool. If you open the console, you'll see it complaining a bunch about missing caches.</p><p>After this is done (indicated by a <code>Built initial format list</code> message in the console), use <code>printSupportedFormatCache()</code> to get a JSON string with the cache data. You can then save this string to  to skip that loading screen on startup.</p><p>Docker compose files live in the  directory, so run compose with  from the repository root:</p><pre><code>docker compose -f docker/docker-compose.yml up -d\n</code></pre><p>Alternatively download the  separately and start it by executing  in the same directory.</p><p>This runs the container on <code>http://localhost:8080/convert/</code>.</p><h3>Docker (local build for development)</h3><p>Use the override file to build the image locally:</p><pre><code>docker compose -f docker/docker-compose.yml -f docker/docker-compose.override.yml up --build -d\n</code></pre><p>The first Docker build is expected to be slow because Chromium and related system packages are installed in the build stage (needed for puppeteer in ). Later builds are usually much faster due to Docker layer caching.</p><p>The best way to contribute is by adding support for new file formats (duh). Here's how that works:</p><p>Each \"tool\" used for conversion has to be normalized to a standard form - effectively a \"wrapper\" that abstracts away the internal processes. These wrappers are available in <a href=\"https://raw.githubusercontent.com/p2r3/convert/master/src/handlers/\">src/handlers</a>.</p><p>Below is a super barebones handler that does absolutely nothing. You can use this as a starting point for adding a new format:</p><pre><code>// file: dummy.ts\n\nimport type { FileData, FileFormat, FormatHandler } from \"../FormatHandler.ts\";\nimport CommonFormats from \"src/CommonFormats.ts\";\n\nclass dummyHandler implements FormatHandler {\n\n  public name: string = \"dummy\";\n  public supportedFormats?: FileFormat[];\n  public ready: boolean = false;\n\n  async init () {\n    this.supportedFormats = [\n      // Example PNG format, with both input and output disabled\n      CommonFormats.PNG.builder(\"png\")\n        .markLossless()\n        // .allowFrom()\n        // .allowTo()\n    ];\n    this.ready = true;\n  }\n\n  async doConvert (\n    inputFiles: FileData[],\n    inputFormat: FileFormat,\n    outputFormat: FileFormat\n  ): Promise&lt;FileData[]&gt; {\n    const outputFiles: FileData[] = [];\n    return outputFiles;\n  }\n\n}\n\nexport default dummyHandler;\n</code></pre><p>For more details on how all of these components work, refer to the doc comments in <a href=\"https://raw.githubusercontent.com/p2r3/convert/master/src/FormatHandler.ts\">src/FormatHandler.ts</a>. You can also take a look at existing handlers to get a more practical example.</p><p>There are a few additional things that I want to point out in particular:</p><ul><li>Pay attention to the naming system. If your tool is called , then the class should be called , and the file should be called .</li><li>The handler is responsible for setting the output file's name. This is done to allow for flexibility in rare cases where the  file name matters. Of course, in most cases, you'll only have to swap the file extension.</li><li>The handler is also responsible for ensuring that any byte buffers that enter or exit the handler . If necessary, clone the buffer by wrapping it in .</li><li>When handling MIME types, run them through <a href=\"https://raw.githubusercontent.com/p2r3/convert/master/src/normalizeMimeType.ts\">normalizeMimeType</a> first. One file can have multiple valid MIME types, which isn't great when you're trying to match them algorithmically.</li><li>When implementing a new file format, please treat the file as the media that it represents, not the data that it contains. For example, if you were making an SVG handler, you should treat the file as an , not as XML.</li></ul><p>If your tool requires an external dependency (which it likely does), there are currently two well-established ways of going about this:</p><ul><li>If it's an  package, just install it to the project like you normally would.</li><li>If it's a Git repository, add it as a submodule to <a href=\"https://raw.githubusercontent.com/p2r3/convert/master/src/handlers\">src/handlers</a>.</li></ul><p><strong>Please try to avoid CDNs (Content Delivery Networks).</strong> They're really cool on paper, but they don't work well with TypeScript, and each one introduces a tiny bit of instability. For a project that leans heavily on external dependencies, those bits of instability can add up fast.</p><ul><li>If you need to load a WebAssembly binary (or similar), add its path to <a href=\"https://raw.githubusercontent.com/p2r3/convert/master/vite.config.js\">vite.config.js</a> and target it under . <strong>Do not link to node_modules</strong>.</li></ul>",
      "contentLength": 7479,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "harvard-edge/cs249r_book",
      "url": "https://github.com/harvard-edge/cs249r_book",
      "date": 1771470321,
      "author": "",
      "guid": 46304,
      "unread": true,
      "content": "<p>Introduction to Machine Learning Systems</p><p><em>Principles and Practices of Engineering Artificially Intelligent Systems</em></p><p><strong>The world is rushing to build AI systems. It is not engineering them.</strong></p><p>That gap is what we mean by AI engineering.</p><p><strong>AI engineering is the discipline of building efficient, reliable, safe, and robust intelligent systems that operate in the real world, not just models in isolation.</strong></p><p> Establish AI engineering as a foundational discipline, alongside software engineering and computer engineering, by teaching how to design, build, and evaluate end to end intelligent systems. The long term impact of AI will be shaped by engineers who can turn ideas into working, dependable systems.</p><p>This repository is the open learning stack for AI systems engineering.</p><p>It includes the textbook source, TinyTorch, hardware kits, and upcoming co-labs that connect principles to runnable code and real devices.</p><p>Choose a path based on your goal.</p><p> Start TinyTorch with the <a href=\"https://mlsysbook.ai/tinytorch/getting-started.html\">getting started guide</a>. Begin with Module 01 and work up from CNNs to transformers and the MLPerf benchmarks.</p><p> Pick a <a href=\"https://mlsysbook.ai/kits\">hardware kit</a> and run the labs on Arduino, Raspberry Pi, and other edge devices.</p><p> Say hello in <a href=\"https://github.com/harvard-edge/cs249r_book/discussions\">Discussions</a>. We will do our best to reply.</p><p>The learning stack below shows how the textbook connects to hands on work and deployment. Read the textbook, then pick your path:</p><pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                                               â”‚\nâ”‚                           MACHINE LEARNING SYSTEMS                            â”‚\nâ”‚                              Read the Textbook                                â”‚\nâ”‚                                                                               â”‚\nâ”‚                    Theory â€¢ Concepts â€¢ Best Practices                         â”‚\nâ”‚                                                                               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                        â”‚\n                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                          â”‚             â”‚             â”‚\n                          â–¼             â–¼             â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                            HANDS-ON ACTIVITIES                                â”‚\nâ”‚                           (pick one or all)                                   â”‚\nâ”‚                                                                               â”‚\nâ”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\nâ”‚     â”‚                 â”‚      â”‚                 â”‚      â”‚                 â”‚     â”‚\nâ”‚     â”‚    SOFTWARE     â”‚      â”‚    TINYTORCH    â”‚      â”‚    HARDWARE     â”‚     â”‚\nâ”‚     â”‚    CO-LABS      â”‚      â”‚    FRAMEWORK    â”‚      â”‚      LABS       â”‚     â”‚\nâ”‚     â”‚                 â”‚      â”‚                 â”‚      â”‚                 â”‚     â”‚\nâ”‚     â”‚ EXPLORE         â”‚      â”‚ BUILD           â”‚      â”‚ DEPLOY          â”‚     â”‚\nâ”‚     â”‚                 â”‚      â”‚                 â”‚      â”‚                 â”‚     â”‚\nâ”‚     â”‚ Run controlled  â”‚      â”‚ Understand      â”‚      â”‚ Engineer under  â”‚     â”‚\nâ”‚     â”‚ experiments on  â”‚      â”‚ frameworks by   â”‚      â”‚ real constraintsâ”‚     â”‚\nâ”‚     â”‚ latency, memory,â”‚      â”‚ implementing    â”‚      â”‚ memory, power,  â”‚     â”‚\nâ”‚     â”‚ energy, cost    â”‚      â”‚ them            â”‚      â”‚ timing, safety  â”‚     â”‚\nâ”‚     â”‚                 â”‚      â”‚                 â”‚      â”‚                 â”‚     â”‚\nâ”‚     â”‚ (coming 2026)   â”‚      â”‚                 â”‚      â”‚ Arduino, Pi     â”‚     â”‚\nâ”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\nâ”‚                                                                               â”‚\nâ”‚           EXPLORE                  BUILD                   DEPLOY             â”‚\nâ”‚                                                                               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                        â”‚\n                                        â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                                               â”‚\nâ”‚                                  AI OLYMPICS                                  â”‚\nâ”‚                                 Prove Mastery                                 â”‚\nâ”‚                                                                               â”‚\nâ”‚       Compete across all tracks â€¢ University teams â€¢ Public leaderboards      â”‚\nâ”‚                                                                               â”‚\nâ”‚                                (coming 2026)                                  â”‚\nâ”‚                                                                               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</code></pre><table><tbody><tr><td>Run controlled experiments on latency, memory, energy, cost</td></tr><tr><td>Compete and benchmark across all tracks</td></tr></tbody></table><ul><li> teaches  â€” Understand tradeoffs. Change batch sizes, precision, model architectures and see how latency, memory, and accuracy shift.</li><li> teaches  â€” Understand internals. Implement autograd, optimizers, and attention from scratch to see how TensorFlow and PyTorch actually work.</li><li> teaches  â€” Understand constraints. Face real memory limits, power budgets, and latency requirements on actual hardware.</li></ul><p>This textbook teaches you to think at the intersection of machine learning and systems engineering. Each chapter bridges algorithmic concepts with the infrastructure that makes them work in practice.</p><table><thead><tr></tr></thead><tbody><tr><td>How to fit large models on resource-limited devices</td></tr><tr><td>How GPUs, TPUs, and accelerators execute neural networks</td></tr><tr><td>How mixed-precision and optimization techniques reduce cost</td></tr><tr><td>How to compress models while preserving performance</td></tr><tr><td>How to build efficient data loading and preprocessing</td></tr><tr><td>How to monitor, version, and update models in production</td></tr><tr><td>How to train and adapt models without sending data to the cloud</td></tr></tbody></table><table><tbody><tr><td>Introduction, ML Systems, DL Primer, Architectures</td></tr><tr><td>Workflow, Data Engineering, Frameworks, Training</td></tr><tr><td>Efficient AI, Optimizations, HW Acceleration, Benchmarking</td></tr><tr><td>MLOps, On-device Learning, Privacy, Robustness</td></tr><tr><td>Responsible AI, Sustainable AI, AI for Good</td></tr><tr><td>Emerging trends and future directions</td></tr></tbody></table><h2>What Makes This Different</h2><p>This is a living textbook. We keep it updated as the field grows, with community input along the way.</p><p>AI may feel like it is moving at lightning speed, but the engineering building blocks that make it work do not change as quickly as the headlines. This project is built around those stable foundations.</p><p>Think of it like LEGO. New sets arrive all the time, but the bricks themselves stay the same. Once you learn how the bricks fit together, you can build anything. Here, those \"AI bricks\" are the solid systems principles that make AI work.</p><p>Whether you are reading a chapter, running a lab, or sharing feedback, you are helping make these ideas more accessible to the next learner.</p><h3>Research to Teaching Loop</h3><p>We use the same loop for research and teaching: define the system problem, build a reference implementation, benchmark it, then turn it into curriculum and tooling so others can reproduce and extend it.</p><table><thead><tr></tr></thead><tbody><tr><td>Benchmarks, suites, metrics</td><td>Benchmarking chapter, assignments</td></tr><tr><td>Reference systems, compilers, runtimes</td><td>TinyTorch modules, co-labs</td></tr><tr><td>Hardware targets, constraints, reliability</td></tr></tbody></table><p>We are working toward <strong>1 million learners by 2030</strong> so that AI engineering becomes a shared, teachable discipline, not a collection of isolated practices. Every star, share, and contribution helps move this effort forward.</p><div align=\"center\"><p><em>What gets measured gets improved.</em></p><p>Each star is a learner, educator, or supporter who believes AI systems should be engineered with rigor and real world constraints in mind.</p><p>1 learner â†’ 10 learners â†’ 100 learners â†’ 1,000 learners â†’  â†’ 100,000 learners â†’ </p></div><p>Stars are not the goal. They are a signal.</p><p>A visible, growing community makes it easier for universities, foundations, and industry partners to adopt this material, donate hardware, and fund workshops. That momentum lowers the barrier for the next institution, the next classroom, and the next cohort of learners.</p><p>Support raised through this signal flows into <a href=\"https://opencollective.com/mlsysbook\">Open Collective</a> and funds concrete outcomes such as TinyML4D workshops, hardware kits for underserved classrooms, and the infrastructure required to keep this resource free and open.</p><p>One click can unlock the next classroom, the next contributor, and the next generation of AI engineers.</p><div align=\"center\"><p>All contributions go to <a href=\"https://opencollective.com/mlsysbook\">Open Collective</a>, a transparent fund that supports educational outreach.</p></div><p>We welcome contributions to the book, TinyTorch, and hardware kits!</p><pre><code>@inproceedings{reddi2024mlsysbook,\n  title        = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering},\n  author       = {Reddi, Vijay Janapa},\n  booktitle    = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)},\n  pages        = {41--42},\n  year         = {2024},\n  organization = {IEEE},\n  url          = {https://mlsysbook.org}\n}\n</code></pre><p>This project uses a dual-license structure:</p><table><thead><tr></tr></thead><tbody><tr><td>Share freely with attribution; no commercial use; no derivatives</td></tr><tr><td>Use, modify, and distribute freely; includes patent protection</td></tr></tbody></table><p>The textbook content (chapters, figures, explanations) is educational material that should circulate with attribution and without commercial exploitation. The software framework is a tool designed to be easy for anyone to use, modify, or integrate into their own projects.</p><p>Thanks goes to these wonderful people who have contributed to making this resource better for everyone!</p><p> ğŸª² Bug Hunter Â· ğŸ§‘â€ğŸ’» Code Contributor Â· âœï¸ Doc Wizard Â· ğŸ¨ Design Artist Â· ğŸ§  Idea Spark Â· ğŸ” Code Reviewer Â· ğŸ§ª Test Tinkerer Â· ğŸ› ï¸ Tool Builder</p><h3>ğŸ› ï¸ Hardware Kits Contributors</h3>",
      "contentLength": 11389,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "NirDiamant/RAG_Techniques",
      "url": "https://github.com/NirDiamant/RAG_Techniques",
      "date": 1771470321,
      "author": "",
      "guid": 46305,
      "unread": true,
      "content": "<p>This repository showcases various advanced techniques for Retrieval-Augmented Generation (RAG) systems. RAG systems combine information retrieval with generative models to provide accurate and contextually rich responses.</p><blockquote><p>ğŸŒŸ  Your sponsorship fuels innovation in RAG technologies.  to help maintain and expand this valuable resource!</p></blockquote><p>We gratefully acknowledge the organizations and individuals who have made significant contributions to this project.</p><p>Welcome to one of the most comprehensive and dynamic collections of Retrieval-Augmented Generation (RAG) tutorials available today. This repository serves as a hub for cutting-edge techniques aimed at enhancing the accuracy, efficiency, and contextual richness of RAG systems.</p><div align=\"center\"><table><tbody><tr></tr></tbody></table><p><em>Join over 50,000 AI enthusiasts getting unique cutting-edge insights and free tutorials!</em><em><strong>Plus, subscribers get exclusive early access and special 33% discounts to my book and the upcoming RAG Techniques course!</strong></em></p></div><p>Retrieval-Augmented Generation (RAG) is revolutionizing the way we combine information retrieval with generative AI. This repository showcases a curated collection of advanced techniques designed to supercharge your RAG systems, enabling them to deliver more accurate, contextually relevant, and comprehensive responses.</p><p>Our goal is to provide a valuable resource for researchers and practitioners looking to push the boundaries of what's possible with RAG. By fostering a collaborative environment, we aim to accelerate innovation in this exciting field.</p><p>ğŸš€ Level up with my  repository. It delivers horizontal, code-first tutorials that cover every tool and step in the lifecycle of building production-grade GenAI agents, guiding you from spark to scale with proven patterns and reusable blueprints for real-world launches, making it the smartest place to start if you're serious about shipping agents to production.</p><p>ğŸ¤– Explore my  to discover a variety of AI agent implementations and tutorials, showcasing how different AI technologies can be combined to create powerful, interactive systems.</p><p>ğŸ–‹ï¸ Check out my  for a comprehensive collection of prompting strategies, from basic concepts to advanced techniques, enhancing your ability to interact effectively with AI language models.</p><h2>A Community-Driven Knowledge Hub</h2><p><strong>This repository grows stronger with your contributions!</strong> Join our vibrant communities - the central hubs for shaping and advancing this project together ğŸ¤</p><p>Whether you're an expert or just starting out, your insights can shape the future of RAG. Join us to propose ideas, get feedback, and collaborate on innovative techniques. For contribution guidelines, please refer to our  file. Let's advance RAG technology together!</p><p>ğŸ”— For discussions on GenAI, RAG, or custom agents, or to explore knowledge-sharing opportunities, feel free to .</p><ul><li>ğŸ§  State-of-the-art RAG enhancements</li><li>ğŸ“š Comprehensive documentation for each technique</li><li>ğŸ› ï¸ Practical implementation guidelines</li><li>ğŸŒŸ Regular updates with the latest advancements</li></ul><p>Explore our extensive list of cutting-edge RAG techniques:</p><h3>ğŸŒ± Foundational RAG Techniques</h3><ol><li><p>Enhances the Simple RAG by adding validation and refinement to ensure the accuracy and relevance of retrieved information.</p><p>Check for retrieved document relevancy and highlight the segment of docs used for answering.</p></li><li><p>Selecting an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.</p><p>Experiment with different chunk sizes to find the optimal balance between preserving context and maintaining retrieval speed for your specific use case.</p></li><li><p>Breaking down the text into concise, complete, meaningful sentences allowing for better control and handling of specific queries (especially extracting knowledge).</p><ul><li>ğŸ’ª  The LLM is used in conjunction with a custom prompt to generate factual statements from the document chunks.</li><li>âœ…  The generated propositions are passed through a grading system that evaluates accuracy, clarity, completeness, and conciseness.</li></ul></li></ol><ol start=\"6\"><li><p>Modifying and expanding queries to improve retrieval effectiveness.</p><ul><li>âœï¸  Reformulate queries to improve retrieval.</li><li>ğŸ”™  Generate broader queries for better context retrieval.</li><li>ğŸ§©  Break complex queries into simpler sub-queries.</li></ul></li><li><p>Hypothetical Questions (HyDE Approach) â“</p><p>Generating hypothetical questions to improve alignment between queries and data.</p><p>Create hypothetical questions that point to relevant locations in the data, enhancing query-data matching.</p></li></ol><h3>ğŸ“š Context and Content Enrichment</h3><ol start=\"8\"><li><p>Hypothetical Prompt Embeddings (HyPE) â“ğŸš€</p><p>HyPE (Hypothetical Prompt Embeddings) is an enhancement to traditional RAG retrieval that <strong>precomputes hypothetical prompts at the indexing stage</strong>, but inseting the chunk in their place. This transforms retrieval into a <strong>question-question matching task</strong>. This avoids the need for runtime synthetic answer generation, reducing inference-time computational overhead while <strong>improving retrieval alignment</strong>.</p><ul><li>ğŸ“–  Instead of embedding document chunks, HyPE <strong>generates multiple hypothetical queries per chunk</strong> at indexing time.</li><li>ğŸ” <strong>Question-Question Matching:</strong> User queries are matched against stored hypothetical questions, leading to <strong>better retrieval alignment</strong>.</li><li>âš¡  Unlike HyDE, HyPE does <strong>not require LLM calls at query time</strong>, making retrieval .</li><li>ğŸ“ˆ <strong>Higher Precision &amp; Recall:</strong> Improves retrieval <strong>context precision by up to 42 percentage points</strong> and <strong>claim recall by up to 45 percentage points</strong>.</li></ul></li><li><p>Contextual chunk headers (CCH) is a method of creating document-level and section-level context, and prepending those chunk headers to the chunks prior to embedding them.</p><p>Create a chunk header that includes context about the document and/or section of the document, and prepend that to each chunk in order to improve the retrieval accuracy.</p><p>: open-source retrieval engine that implements this technique (and a few other advanced RAG techniques)</p></li><li><p>Relevant segment extraction (RSE) is a method of dynamically constructing multi-chunk segments of text that are relevant to a given query.</p><p>Perform a retrieval post-processing step that analyzes the most relevant chunks and identifies longer multi-chunk segments to provide more complete context to the LLM.</p></li><li><p>Context Enrichment Techniques ğŸ“</p></li></ol><p>Enhancing retrieval accuracy by embedding individual sentences and extending context to neighboring sentences.</p><p>Retrieve the most relevant sentence while also accessing the sentences before and after it in the original text.</p><p>Dividing documents based on semantic coherence rather than fixed sizes.</p><p>Use NLP techniques to identify topic boundaries or coherent sections within documents for more meaningful retrieval units.</p><ol start=\"13\"><li>Contextual Compression ğŸ—œï¸</li></ol><p>Compressing retrieved information while preserving query-relevant content.</p><p>Use an LLM to compress or summarize retrieved chunks, preserving key information relevant to the query.</p><ol start=\"14\"><li>Document Augmentation through Question Generation for Enhanced Retrieval</li></ol><p>This implementation demonstrates a text augmentation technique that leverages additional question generation to improve document retrieval within a vector database. By generating and incorporating various questions related to each text fragment, the system enhances the standard retrieval process, thus increasing the likelihood of finding relevant documents that can be utilized as context for generative question answering.</p><p>Use an LLM to augment text dataset with all possible questions that can be asked to each document.</p><h3>ğŸš€ Advanced Retrieval Methods</h3><ol start=\"15\"><li><p>Multi-faceted Filtering ğŸ”</p><p>Applying various filtering techniques to refine and improve the quality of retrieved results.</p><ul><li>ğŸ·ï¸  Apply filters based on attributes like date, source, author, or document type.</li><li>ğŸ“Š  Set thresholds for relevance scores to keep only the most pertinent results.</li><li>ğŸ“„  Remove results that don't match specific content criteria or essential keywords.</li><li>ğŸŒˆ  Ensure result diversity by filtering out near-duplicate entries.</li></ul></li><li><p>Creating a multi-tiered system for efficient information navigation and retrieval.</p><p>Implement a two-tiered system for document summaries and detailed chunks, both containing metadata pointing to the same location in the data.</p></li><li><p>Combining multiple retrieval models or techniques for more robust and accurate results.</p><p>Apply different embedding models or retrieval algorithms and use voting or weighting mechanisms to determine the final set of retrieved documents.</p></li><li><p>Optimizing over Relevant Information Gain in Retrieval</p><ul><li>Combine both relevance and diversity into a single scoring function and directly optimize for it.</li><li>POC showing plain simple RAG underperforming when the database is dense, and the dartboard retrieval outperforming it.</li></ul></li></ol><h3>ğŸ” Iterative and Adaptive Techniques</h3><ol start=\"22\"><li><p>Retrieval with Feedback Loops ğŸ”</p><p>Implementing mechanisms to learn from user interactions and improve future retrievals.</p><p>Collect and utilize user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.</p></li><li><p>Dynamically adjusting retrieval strategies based on query types and user contexts.</p><p>Classify queries into different categories and use tailored retrieval strategies for each, considering user context and preferences.</p></li><li><p>Performing multiple rounds of retrieval to refine and enhance result quality.</p><p>Use the LLM to analyze initial results and generate follow-up queries to fill in gaps or clarify information.</p></li></ol><ol start=\"25\"><li><p>: <a href=\"https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_deep_eval.ipynb\"><img src=\"https://img.shields.io/badge/GitHub-View-blue\" height=\"20\"></a><a href=\"https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_deep_eval.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg?sanitize=true\" height=\"20\"></a> | Comprehensive RAG system evaluation |</p><p>Performing evaluations Retrieval-Augmented Generation systems, by covering several metrics and creating test cases.</p><p>Use the  library to conduct test cases on correctness, faithfulness and contextual relevancy of RAG systems.</p></li><li><p>: <a href=\"https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_grouse.ipynb\"><img src=\"https://img.shields.io/badge/GitHub-View-blue\" height=\"20\"></a><a href=\"https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_grouse.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg?sanitize=true\" height=\"20\"></a> | Contextually-grounded LLM evaluation |</p><p>Evaluate the final stage of Retrieval-Augmented Generation using metrics of the GroUSE framework and meta-evaluate your custom LLM judge on GroUSE unit tests.</p><p>Use the  package to evaluate contextually-grounded LLM generations with GPT-4 on the 6 metrics of the GroUSE framework and use unit tests to evaluate a custom Llama 3.1 405B evaluator.</p></li></ol><h3>ğŸ”¬ Explainability and Transparency</h3><ol start=\"27\"><li><p>Providing transparency in the retrieval process to enhance user trust and system refinement.</p><p>Explain why certain pieces of information were retrieved and how they relate to the query.</p></li></ol><h3>ğŸ—ï¸ Advanced Architectures</h3><ol start=\"28\"><li><p>Agentic RAG with Contextual AI ğŸ¤–</p><p>Building production-ready agentic RAG pipelines for financial document analysis with Contextual AI's managed platform. This comprehensive tutorial demonstrates how to leverage agentic RAG to solve complex queries through intelligent query reformulation, document parsing, reranking, and grounded language models.</p><ul><li>: Enterprise-grade parsing with vision models for complex tables, charts, and multi-page documents</li><li><strong>Instruction-Following Reranker</strong>: SOTA reranker with instruction-following capabilities for handling conflicting information</li><li><strong>Grounded Language Model (GLM)</strong>: World's most grounded LLM specifically engineered to minimize hallucinations for RAG use cases</li><li>: Natural language unit testing framework for evaluating and optimizing RAG system performance</li></ul></li><li><p>Graph RAG with Milvus Vector Database ğŸ”</p><p>A simple yet powerful approach to implement Graph RAG using Milvus vector databases. This technique significantly improves performance on complex multi-hop questions by combining relationship-based retrieval with vector search and reranking.</p><ul><li>Store both text passages and relationship triplets (subject-predicate-object) in separate Milvus collections</li><li>Perform multi-way retrieval by querying both collections</li><li>Use an LLM to rerank retrieved relationships based on their relevance to the query</li><li>Retrieve the final passages based on the most relevant relationships</li></ul></li><li><p>Knowledge Graph Integration (Graph RAG) ğŸ•¸ï¸</p><p>Incorporating structured data from knowledge graphs to enrich context and improve retrieval.</p><p>Retrieve entities and their relationships from a knowledge graph relevant to the query, combining this structured data with unstructured text for more informative responses.</p></li><li><p>Microsoft GraphRAG (Open Source) is an advanced RAG system that integrates knowledge graphs to improve the performance of LLMs</p><p>â€¢ Analyze an input corpus by extracting entities, relationships from text units. generates summaries of each community and its constituents from the bottom-up.</p></li><li><p>RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval ğŸŒ³</p><p>Implementing a recursive approach to process and organize retrieved information in a tree structure.</p><p>Use abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.</p></li><li><p>A dynamic approach that combines retrieval-based and generation-based methods, adaptively deciding whether to use retrieved information and how to best utilize it in generating responses.</p><p>â€¢ Implement a multi-step process including retrieval decision, document retrieval, relevance evaluation, response generation, support assessment, and utility evaluation to produce accurate, relevant, and useful outputs.</p></li><li><p>A sophisticated RAG approach that dynamically evaluates and corrects the retrieval process, combining vector databases, web search, and language models for highly accurate and context-aware responses.</p><p>â€¢ Integrate Retrieval Evaluator, Knowledge Refinement, Web Search Query Rewriter, and Response Generator components to create a system that adapts its information sourcing strategy based on relevance scores and combines multiple sources when necessary.</p></li></ol><h2>ğŸŒŸ Special Advanced Technique ğŸŒŸ</h2><ol start=\"35\"><li><p>An advanced RAG solution designed to tackle complex questions that simple semantic similarity-based retrieval cannot solve. This approach uses a sophisticated deterministic graph as the \"brain\" ğŸ§  of a highly controllable autonomous agent, capable of answering non-trivial questions from your own data.</p><p>â€¢ Implement a multi-step process involving question anonymization, high-level planning, task breakdown, adaptive information retrieval and question answering, continuous re-planning, and rigorous answer verification to ensure grounded and accurate responses.</p></li></ol><p>To begin implementing these advanced RAG techniques in your projects:</p><ol><li>Clone this repository: <pre><code>git clone https://github.com/NirDiamant/RAG_Techniques.git\n</code></pre></li><li>Navigate to the technique you're interested in: <pre><code>cd all_rag_techniques/technique-name\n</code></pre></li><li>Follow the detailed implementation guide in each technique's directory.</li></ol><p>We welcome contributions from the community! If you have a new technique or improvement to suggest:</p><ol><li>Create your feature branch: <code>git checkout -b feature/AmazingFeature</code></li><li>Commit your changes: <code>git commit -m 'Add some AmazingFeature'</code></li><li>Push to the branch: <code>git push origin feature/AmazingFeature</code></li></ol><p>This project is licensed under a custom non-commercial license - see the <a href=\"https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/LICENSE\">LICENSE</a> file for details.</p><p>â­ï¸ If you find this repository helpful, please consider giving it a star!</p><p>Keywords: RAG, Retrieval-Augmented Generation, NLP, AI, Machine Learning, Information Retrieval, Natural Language Processing, LLM, Embeddings, Semantic Search</p>",
      "contentLength": 14872,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ComposioHQ/composio",
      "url": "https://github.com/ComposioHQ/composio",
      "date": 1771470321,
      "author": "",
      "guid": 46306,
      "unread": true,
      "content": "<p>Composio powers 1000+ toolkits, tool search, context management, authentication, and a sandboxed workbench to help you build AI agents that turn intent into action.</p><p>This repository contains the official Software Development Kits (SDKs) for Composio, providing seamless integration capabilities for Python and Typescript Agentic Frameworks and Libraries.</p><h3>TypeScript SDK Installation</h3><pre><code># Using npm\nnpm install @composio/core\n\n# Using yarn\nyarn add @composio/core\n\n# Using pnpm\npnpm add @composio/core\n</code></pre><pre><code>import { Composio } from '@composio/core';\n// Initialize the SDK\nconst composio = new Composio({\n  // apiKey: 'your-api-key',\n});\n</code></pre><h4>Simple Agent with OpenAI Agents</h4><pre><code>npm install @composio/openai-agents @openai/agents\n</code></pre><pre><code>import { Composio } from '@composio/core';\nimport { OpenAIAgentsProvider } from '@composio/openai-agents';\nimport { Agent, run } from '@openai/agents';\n\nconst composio = new Composio({\n  provider: new OpenAIAgentsProvider(),\n});\n\nconst userId = 'user@acme.org';\n\nconst tools = await composio.tools.get(userId, {\n  toolkits: ['HACKERNEWS'],\n});\n\nconst agent = new Agent({\n  name: 'Hackernews assistant',\n  tools: tools,\n});\n\nconst result = await run(agent, 'What is the latest hackernews post about?');\n\nconsole.log(JSON.stringify(result.finalOutput, null, 2));\n// will return the response from the agent with data from HACKERNEWS API.\n</code></pre><pre><code># Using pip\npip install composio\n\n# Using poetry\npoetry add composio\n</code></pre><pre><code>from composio import Composio\n\ncomposio = Composio(\n  # api_key=\"your-api-key\",\n)\n</code></pre><h4>Simple Agent with OpenAI Agents</h4><pre><code>pip install composio_openai_agents openai-agents\n</code></pre><pre><code>import asyncio\nfrom agents import Agent, Runner\nfrom composio import Composio\nfrom composio_openai_agents import OpenAIAgentsProvider\n\n# Initialize Composio client with OpenAI Agents Provider\ncomposio = Composio(provider=OpenAIAgentsProvider())\n\nuser_id = \"user@acme.org\"\ntools = composio.tools.get(user_id=user_id, toolkits=[\"HACKERNEWS\"])\n\n# Create an agent with the tools\nagent = Agent(\n    name=\"Hackernews Agent\",\n    instructions=\"You are a helpful assistant.\",\n    tools=tools,\n)\n\n# Run the agent\nasync def main():\n    result = await Runner.run(\n        starting_agent=agent,\n        input=\"What's the latest Hackernews post about?\",\n    )\n    print(result.final_output)\n\nasyncio.run(main())\n# will return the response from the agent with data from HACKERNEWS API.\n</code></pre><p>For more detailed usage instructions and examples, please refer to each SDK's specific documentation.</p><p>To update the OpenAPI specifications used for generating SDK documentation:</p><pre><code># Pull the latest API specifications from the backend\npnpm api:pull\n</code></pre><p>This command pulls the OpenAPI specification from <code>https://backend.composio.dev/api/v3/openapi.json</code> and updates the local API documentation files.</p><p>This is pulled automatically with build step.</p><p>The TypeScript SDK provides a modern, type-safe way to interact with Composio's services. It's designed for both Node.js and browser environments, offering full TypeScript support with comprehensive type definitions.</p><p>The Python SDK offers a Pythonic interface to Composio's services, making it easy to integrate Composio into your Python applications. It supports Python 3.10+ and follows modern Python development practices.</p><p>The following table shows which AI frameworks and platforms are supported in each SDK:</p><table><tbody></tbody></table><p>* <em>LangGraph in TypeScript is supported via the  package.</em></p><p><em>if you are looking for the older sdk, you can find them <a href=\"https://github.com/ComposioHQ/composio/tree/master\">here</a></em></p><p><a href=\"https://rube.app\">Rube</a> is a Model Context Protocol (MCP) server built with Composio. It connects your AI tools to 500+ apps like Gmail, Slack, GitHub, and Notion. Simply install it in your AI client, authenticate once with your apps, and start asking your AI to perform real actions like \"Send an email\" or \"Create a task.\"</p><p>It integrates with major AI clients like Cursor, Claude Desktop, VS Code, Claude Code and any custom MCPâ€‘compatible client. You can switch between these clients and your integrations follow you.</p><p>This project is licensed under the MIT License - see the LICENSE file for details.</p><p>If you encounter any issues or have questions about the SDKs:</p>",
      "contentLength": 4048,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "QwenLM/qwen-code",
      "url": "https://github.com/QwenLM/qwen-code",
      "date": 1771470321,
      "author": "",
      "guid": 46307,
      "unread": true,
      "content": "<p>An open-source AI agent that lives in your terminal.</p><blockquote><p>ğŸ‰ : Qwen3.5-Plus is now live! Sign in via Qwen OAuth to use it directly, or get an API key from <a href=\"https://modelstudio.console.alibabacloud.com?tab=doc#/doc/?type=model&amp;url=2840914_2&amp;modelId=group-qwen3.5-plus\">Alibaba Cloud ModelStudio</a> to access it through the OpenAI-compatible API.</p></blockquote><p>Qwen Code is an open-source AI agent for the terminal, optimized for <a href=\"https://github.com/QwenLM/Qwen3-Coder\">Qwen3-Coder</a>. It helps you understand large codebases, automate tedious work, and ship faster.</p><ul><li><strong>Multi-protocol, OAuth free tier</strong>: use OpenAI / Anthropic / Gemini-compatible APIs, or sign in with Qwen OAuth for 1,000 free requests/day.</li><li>: both the framework and the Qwen3-Coder model are open-sourceâ€”and they ship and evolve together.</li><li><strong>Agentic workflow, feature-rich</strong>: rich built-in tools (Skills, SubAgents) for a full agentic workflow and a Claude Code-like experience.</li><li><strong>Terminal-first, IDE-friendly</strong>: built for developers who live in the command line, with optional integration for VS Code, Zed, and JetBrains IDEs.</li></ul><h3>Quick Install (Recommended)</h3><pre><code>curl -fsSL https://qwen-code-assets.oss-cn-hangzhou.aliyuncs.com/installation/install-qwen.sh | bash\n</code></pre><h4>Windows (Run as Administrator CMD)</h4><pre><code>curl -fsSL -o %TEMP%\\install-qwen.bat https://qwen-code-assets.oss-cn-hangzhou.aliyuncs.com/installation/install-qwen.bat &amp;&amp; %TEMP%\\install-qwen.bat\n</code></pre><blockquote><p>: It's recommended to restart your terminal after installation to ensure environment variables take effect.</p></blockquote><p>Make sure you have Node.js 20 or later installed. Download it from <a href=\"https://nodejs.org/en/download\">nodejs.org</a>.</p><pre><code>npm install -g @qwen-code/qwen-code@latest\n</code></pre><pre><code># Start Qwen Code (interactive)\nqwen\n\n# Then, in the session:\n/help\n/auth\n</code></pre><p>On first use, you'll be prompted to sign in. You can run  anytime to switch authentication methods.</p><pre><code>What does this project do?\nExplain the codebase structure.\nHelp me refactor this function.\nGenerate unit tests for this module.\n</code></pre><p>Qwen Code supports two authentication methods:</p><ul><li><strong>Qwen OAuth (recommended &amp; free)</strong>: sign in with your  account in a browser.</li><li>: use an API key to connect to any supported provider (OpenAI, Anthropic, Google GenAI, Alibaba Cloud Bailian, and other compatible endpoints).</li></ul><p>Choose  and complete the browser flow. Your credentials are cached locally so you usually won't need to log in again.</p><blockquote><p> In non-interactive or headless environments (e.g., CI, SSH, containers), you typically  complete the OAuth browser login flow. In these cases, please use the API-KEY authentication method.</p></blockquote><p>Use this if you want more flexibility over which provider and model to use. Supports multiple protocols:</p><ul><li>: Alibaba Cloud Bailian, ModelScope, OpenAI, OpenRouter, and other OpenAI-compatible providers</li><li>: Gemini models</li></ul><p>The  way to configure models and providers is by editing  (create it if it doesn't exist). This file lets you define all available models, API keys, and default settings in one place.</p><p> Create or edit </p><p>Here is a complete example:</p><pre><code>{\n  \"modelProviders\": {\n    \"openai\": [\n      {\n        \"id\": \"qwen3-coder-plus\",\n        \"name\": \"qwen3-coder-plus\",\n        \"baseUrl\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n        \"description\": \"Qwen3-Coder via Dashscope\",\n        \"envKey\": \"DASHSCOPE_API_KEY\"\n      }\n    ]\n  },\n  \"env\": {\n    \"DASHSCOPE_API_KEY\": \"sk-xxxxxxxxxxxxx\"\n  },\n  \"security\": {\n    \"auth\": {\n      \"selectedType\": \"openai\"\n    }\n  },\n  \"model\": {\n    \"name\": \"qwen3-coder-plus\"\n  }\n}\n</code></pre><p> Understand each field</p><table><tbody><tr><td>Declares which models are available and how to connect to them. Keys like , ,  represent the API protocol.</td></tr><tr><td>The model ID sent to the API (e.g. , ).</td></tr><tr><td>The name of the environment variable that holds your API key.</td></tr><tr><td>The API endpoint URL (required for non-default endpoints).</td></tr><tr><td>A fallback place to store API keys (lowest priority; prefer  files or  for sensitive keys).</td></tr><tr><td><code>security.auth.selectedType</code></td><td>The protocol to use on startup (, , , ).</td></tr><tr><td>The default model to use when Qwen Code starts.</td></tr></tbody></table><p> Start Qwen Code â€” your configuration takes effect automatically:</p><p>Use the  command at any time to switch between all configured models.</p><blockquote><p> You can also set API keys via  in your shell or  files, which take higher priority than  â†’ . See the <a href=\"https://qwenlm.github.io/qwen-code-docs/en/users/configuration/auth/\">authentication guide</a> for full details.</p></blockquote><blockquote><p> Never commit API keys to version control. The  file is in your home directory and should stay private.</p></blockquote><p>As an open-source terminal agent, you can use Qwen Code in four primary ways:</p><ol><li>Interactive mode (terminal UI)</li><li>Headless mode (scripts, CI)</li><li>IDE integration (VS Code, Zed)</li></ol><p>Run  in your project folder to launch the interactive terminal UI. Use  to reference local files (for example ).</p><pre><code>cd your-project/\nqwen -p \"your question\"\n</code></pre><p>Use  to run Qwen Code without the interactive UIâ€”ideal for scripts, automation, and CI/CD. Learn more: <a href=\"https://qwenlm.github.io/qwen-code-docs/en/users/features/headless\">Headless mode</a>.</p><p>Use Qwen Code inside your editor (VS Code, Zed, and JetBrains IDEs):</p><p>Build on top of Qwen Code with the TypeScript SDK:</p><ul><li> - Display available commands</li><li> - Clear conversation history</li><li> - Compress history to save tokens</li><li> - Show current session information</li><li> - Submit a bug report</li><li> or  - Exit Qwen Code</li></ul><ul><li> - Cancel current operation</li><li> - Exit (on empty line)</li><li> - Navigate command history</li></ul><blockquote><p>: In YOLO mode (), vision switching happens automatically without prompts when images are detected. Learn more about <a href=\"https://qwenlm.github.io/qwen-code-docs/en/users/features/approval-mode/\">Approval Mode</a></p></blockquote><p>Qwen Code can be configured via , environment variables, and CLI flags.</p><table><tbody><tr><td>Applies to all your Qwen Code sessions. <strong>Recommended for  and .</strong></td></tr><tr><td>Applies only when running Qwen Code in this project. Overrides user settings.</td></tr></tbody></table><p>The most commonly used top-level fields in :</p><table><tbody><tr><td>Define available models per protocol (, , , ).</td></tr><tr><td>Fallback environment variables (e.g. API keys). Lower priority than shell  and  files.</td></tr><tr><td><code>security.auth.selectedType</code></td><td>The protocol to use on startup (e.g. ).</td></tr><tr><td>The default model to use when Qwen Code starts.</td></tr></tbody></table><h3>Terminal-Bench Performance</h3><table><tbody><tr></tr><tr></tr></tbody></table><p>Looking for a graphical interface?</p><ul><li><a href=\"https://github.com/iOfficeAI/AionUi\"></a> A modern GUI for command-line AI tools including Qwen Code</li></ul><p>To report a bug from within the CLI, run  and include a short title and repro steps.</p><p>This project is based on <a href=\"https://github.com/google-gemini/gemini-cli\">Google Gemini CLI</a>. We acknowledge and appreciate the excellent work of the Gemini CLI team. Our main contribution focuses on parser-level adaptations to better support Qwen-Coder models.</p>",
      "contentLength": 5942,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "HailToDodongo/pyrite64",
      "url": "https://github.com/HailToDodongo/pyrite64",
      "date": 1771470321,
      "author": "",
      "guid": 46308,
      "unread": true,
      "content": "<p>N64 Game-Engine and Editor using libdragon &amp; tiny3d</p><blockquote><p>Note: This project does NOT use any proprietary N64 SDKs or libraries.</p></blockquote><p>Pyrite64 is a visual editor + runtime-engine to create 3D games that can run on a real N64 console or accurate emulators. Besides the usual editor, some extra features include:</p><ul><li>Automatic toolchain installation on Windows</li><li>3D-Model import (GLTF) from blender with <a href=\"https://github.com/Fast-64/fast64\">fast64</a> material support.</li><li>Runtime engine handling scene-management, rendering, collision, audio and more.</li><li>Global asset management with automatic memory cleanup</li><li>Node-Graph editor to script basic control flow</li></ul><p>Note that this project focuses on real hardware, so accurate emulation is required to run/test games on PC. Emulators that are accurate enough include <a href=\"https://ares-emu.net/\">Ares (v147 or newer)</a> and <a href=\"https://github.com/gopher64/gopher64\">gopher64</a>.</p><blockquote><p>[!WARNING] This project is still in early development, so features are going to be missing. Documentation is also still a work in progress, and breaking API changes are to be expected.</p></blockquote><p>Before starting, please read the <a href=\"https://raw.githubusercontent.com/HailToDodongo/pyrite64/main/docs/faq.md\">FAQ</a>!</p><p align=\"center\"><a href=\"https://www.youtube.com/watch?v=zz_wByA_k6E\" target=\"_blank\"><img src=\"https://img.youtube.com/vi/zz_wByA_k6E/0.jpg\" width=\"250\"></a><a href=\"https://www.youtube.com/watch?v=4BCmKnN5eGA\" target=\"_blank\"><img src=\"https://img.youtube.com/vi/4BCmKnN5eGA/0.jpg\" width=\"250\"></a> Cathode Quest 64 (YouTube) &nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp; Pyrite64 Release Video &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </p><p>Â© 2025-2026 - Max BebÃ¶k (HailToDodongo)</p><p>Pyrite64 is licensed under the MIT License, see the <a href=\"https://raw.githubusercontent.com/HailToDodongo/pyrite64/main/LICENSE\">LICENSE</a> file for more information. Licenses for external libraries used in the editor can be found in their respective directory under </p><p>Pyrite64 does NOT force any restrictions or licenses on games made with it. Pyrite64 does NOT claim any copyright or force licenses for assets / source-code generated by the editor.</p><p>While not required, please consider crediting Pyrite64 with a logo and/or name in your credits and/or boot logo sequence.</p>",
      "contentLength": 1603,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "steipete/summarize",
      "url": "https://github.com/steipete/summarize",
      "date": 1771383966,
      "author": "",
      "guid": 45958,
      "unread": true,
      "content": "<p>Point at any URL/YouTube/Podcast or file. Get the gist. CLI and Chrome Extension.</p><p>Fast summaries from URLs, files, and media. Works in the terminal, a Chrome Side Panel and Firefox Sidebar.</p><p><strong>0.11.0 preview (unreleased):</strong> this README reflects the upcoming release.</p><h2>0.11.0 preview highlights (most interesting first)</h2><ul><li>Chrome Side Panel  (streaming agent + history) inside the sidebar.</li><li>: screenshots + OCR + transcript cards, timestamped seek, OCR/Transcript toggle.</li><li>Media-aware summaries: autoâ€‘detect video/audio vs page content.</li><li>Streaming Markdown + metrics + cacheâ€‘aware status.</li><li>CLI supports URLs, files, podcasts, YouTube, audio/video, PDFs.</li></ul><ul><li>URLs, files, and media: web pages, PDFs, images, audio/video, YouTube, podcasts, RSS.</li><li>Slide extraction for video sources (YouTube/direct media) with OCR + timestamped cards.</li><li>Transcript-first media flow: published transcripts when available, Whisper fallback when not.</li><li>Streaming output with Markdown rendering, metrics, and cache-aware status.</li><li>Local, paid, and free models: OpenAIâ€‘compatible local endpoints, paid providers, plus an OpenRouter free preset.</li><li>Output modes: Markdown/text, JSON diagnostics, extract-only, metrics, timing, and cost estimates.</li><li>Smart default: if content is shorter than the requested length, we return it as-is (use  to override).</li></ul><h2>Get the extension (recommended)</h2><p>Oneâ€‘click summarizer for the current tab. Chrome Side Panel + Firefox Sidebar + local daemon for streaming Markdown.</p><p>YouTube slide screenshots (from the browser):</p><h3>Beginner quickstart (extension)</h3><ol><li>Install the CLI (choose one): \n  <ul><li> (crossâ€‘platform): <code>npm i -g @steipete/summarize</code></li><li> (macOS arm64): <code>brew install steipete/tap/summarize</code></li></ul></li><li>Install the extension (Chrome Web Store link above) and open the Side Panel.</li><li>The panel shows a token + install command. Run it in Terminal: \n  <ul><li><code>summarize daemon install --token &lt;TOKEN&gt;</code></li></ul></li></ol><ul><li>The extension canâ€™t run heavy extraction inside the browser. It talks to a local background service on  for fast streaming and media tools (ytâ€‘dlp, ffmpeg, OCR, transcription).</li><li>The service autostarts (launchd/systemd/Scheduled Task) so the Side Panel is always ready.</li></ul><p>If you only want the , you can skip the daemon install entirely.</p><ul><li>Summarization only runs when the Side Panel is open.</li><li>Auto mode summarizes on navigation (incl. SPAs); otherwise use the button.</li><li>Daemon is localhost-only and requires a shared token.</li><li>Autostart: macOS (launchd), Linux (systemd user), Windows (Scheduled Task).</li><li>Tip: configure  via  (needs ). Add  to set model=.</li></ul><ul><li>Select  in the Summarize picker.</li><li>Slides render at the top; expand to fullâ€‘width cards with timestamps.</li><li>Click a slide to seek the video; toggle  when OCR is significant.</li><li>Requirements:  +  for extraction;  for OCR. Missing tools show an inâ€‘panel notice.</li></ul><h3>Advanced (unpacked / dev)</h3><ol><li>Build + load the extension (unpacked): \n  <ul><li>Chrome: <code>pnpm -C apps/chrome-extension build</code><ul><li> â†’ Developer mode â†’ Load unpacked</li><li>Pick: <code>apps/chrome-extension/.output/chrome-mv3</code></li></ul></li><li>Firefox: <code>pnpm -C apps/chrome-extension build:firefox</code><ul><li><code>about:debugging#/runtime/this-firefox</code> â†’ Load Temporary Add-on</li><li>Pick: <code>apps/chrome-extension/.output/firefox-mv3/manifest.json</code></li></ul></li></ul></li><li>Open Side Panel/Sidebar â†’ copy token.</li><li>Install daemon in dev mode: \n  <ul><li><code>pnpm summarize daemon install --token &lt;TOKEN&gt; --dev</code></li></ul></li></ol><pre><code>npx -y @steipete/summarize \"https://example.com\"\n</code></pre><pre><code>npm i -g @steipete/summarize\n</code></pre><ul><li>npm (library / minimal deps):</li></ul><pre><code>npm i @steipete/summarize-core\n</code></pre><pre><code>import { createLinkPreviewClient } from \"@steipete/summarize-core/content\";\n</code></pre><pre><code>brew install steipete/tap/summarize\n</code></pre><p>Apple Silicon only (arm64).</p><ul><li> just install via npm/Homebrew and run  (no daemon needed).</li><li><strong>Chrome/Firefox extension:</strong> install the CLI  run <code>summarize daemon install --token &lt;TOKEN&gt;</code> so the Side Panel can stream results and use local tools.</li></ul><pre><code>summarize \"https://example.com\"\n</code></pre><pre><code>summarize \"/path/to/file.pdf\" --model google/gemini-3-flash-preview\nsummarize \"https://example.com/report.pdf\" --model google/gemini-3-flash-preview\nsummarize \"/path/to/audio.mp3\"\nsummarize \"/path/to/video.mp4\"\n</code></pre><p>Stdin (pipe content using ):</p><pre><code>echo \"content\" | summarize -\npbpaste | summarize -\n# binary stdin also works (PDF/image/audio/video bytes)\ncat /path/to/file.pdf | summarize -\n</code></pre><ul><li>Stdin has a 50MB size limit</li><li>The  argument tells summarize to read from standard input</li><li>Text stdin is treated as UTF-8 text (whitespace-only input is rejected as empty)</li><li>Binary stdin is preserved as raw bytes and file type is auto-detected when possible</li><li>Useful for piping clipboard content or command output</li></ul><p>YouTube (supports  and ):</p><pre><code>summarize \"https://youtu.be/dQw4w9WgXcQ\" --youtube auto\n</code></pre><p>Podcast RSS (transcribes latest enclosure):</p><pre><code>summarize \"https://feeds.npr.org/500005/podcast.xml\"\n</code></pre><p>Apple Podcasts episode page:</p><pre><code>summarize \"https://podcasts.apple.com/us/podcast/2424-jelly-roll/id360084272?i=1000740717432\"\n</code></pre><p>Spotify episode page (best-effort; may fail for exclusives):</p><pre><code>summarize \"https://open.spotify.com/episode/5auotqWAXhhKyb9ymCuBJY\"\n</code></pre><p> controls how much output we ask for (guideline), not a hard cap.</p><pre><code>summarize \"https://example.com\" --length long\nsummarize \"https://example.com\" --length 20k\n</code></pre><ul><li>Presets: </li><li>Character targets: , , </li><li>Optional hard cap: <code>--max-output-tokens &lt;count&gt;</code> (e.g. , ) \n  <ul><li>Provider/model APIs still enforce their own maximum output limits.</li><li>If omitted, no max token parameter is sent (provider default).</li><li>Prefer  unless you need a hard cap.</li></ul></li><li>Short content: when extracted content is shorter than the requested length, the CLI returns the content as-is. \n  <ul><li>Override with  to always run the LLM.</li></ul></li><li>Minimums:  numeric values must be &gt;= 50 chars;  must be &gt;= 16.</li><li>Preset targets (source of truth: <code>packages/core/src/prompts/summary-lengths.ts</code>): \n  <ul><li>short: target ~900 chars (range 600-1,200)</li><li>medium: target ~1,800 chars (range 1,200-2,500)</li><li>long: target ~4,200 chars (range 2,500-6,000)</li><li>xl: target ~9,000 chars (range 6,000-14,000)</li><li>xxl: target ~17,000 chars (range 14,000-22,000)</li></ul></li></ul><p>Best effort and provider-dependent. These usually work well:</p><ul><li> and common structured text (, , , , , ...) \n  <ul><li>Text-like files are inlined into the prompt for better provider compatibility.</li></ul></li><li>PDFs:  (provider support varies; Google is the most reliable here)</li><li>Images: , , , </li><li>Audio/Video: ,  (local audio/video files MP3/WAV/M4A/OGG/FLAC/MP4/MOV/WEBM automatically transcribed, when supported by the model)</li></ul><ul><li>If a provider rejects a media type, the CLI fails fast with a friendly message.</li><li>xAI models do not support attaching generic files (like PDFs) via the AI SDK; use Google/OpenAI/Anthropic for those.</li></ul><p>Use gateway-style ids: .</p><ul><li><code>anthropic/claude-sonnet-4-5</code></li><li><code>xai/grok-4-fast-non-reasoning</code></li><li><code>google/gemini-3-flash-preview</code></li><li><code>openrouter/openai/gpt-5-mini</code> (force OpenRouter)</li></ul><p>Note: some models/providers do not support streaming or certain file media types. When that happens, the CLI prints a friendly error (or auto-disables streaming for that model when supported by the provider).</p><ul><li>Text inputs over 10 MB are rejected before tokenization.</li><li>Text prompts are preflighted against the model input limit (LiteLLM catalog), using a GPT tokenizer.</li></ul><pre><code>summarize &lt;input&gt; [flags]\n</code></pre><p>Use  or  for the full help text.</p><ul><li>: which model to use (defaults to )</li><li>: automatic model selection + fallback (default)</li><li>: use a config-defined model (see Configuration)</li><li>: , ,  (default )</li><li>: LLM retry attempts on timeout (default )</li><li><code>--length short|medium|long|xl|xxl|s|m|l|&lt;chars&gt;</code></li><li><code>--language, --lang &lt;language&gt;</code>: output language ( = match source)</li><li><code>--max-output-tokens &lt;count&gt;</code>: hard cap for LLM output tokens</li><li>: use a CLI provider (). Supports , , , . If omitted, uses auto selection with CLI enabled.</li><li>: stream LLM output ( = TTY only; disabled in  mode)</li><li>: keep raw output (no ANSI/OSC Markdown rendering)</li><li>: disable ANSI colors</li><li>: CLI theme (, , , )</li><li>: website/file content format (default )</li><li><code>--markdown-mode off|auto|llm|readability</code>: HTML -&gt; Markdown mode (default )</li><li><code>--preprocess off|auto|always</code>: controls  usage (default ) \n  </li><li>: print extracted content and exit (URLs only; stdin  is not supported) \n  <ul><li>Deprecated alias: </li></ul></li><li>: extract slides for YouTube/direct video URLs and render them inline in the summary narrative (auto-renders inline in supported terminals)</li><li>: run OCR on extracted slides (requires )</li><li>: base output dir for slide images (default )</li><li><code>--slides-scene-threshold &lt;value&gt;</code>: scene detection threshold (0.1-1.0)</li><li>: maximum slides to extract (default )</li><li><code>--slides-min-duration &lt;seconds&gt;</code>: minimum seconds between slides</li><li>: machine-readable output with diagnostics, prompt, , and optional summary</li><li>: debug/diagnostics on stderr</li><li><code>--metrics off|on|detailed</code>: metrics output (default )</li></ul><h3>Coding CLIs (Codex, Claude, Gemini, Agent)</h3><p>Summarize can use common coding CLIs as local model backends:</p><ul><li> -&gt;  / <code>--model cli/codex/&lt;model&gt;</code></li><li> -&gt;  / <code>--model cli/claude/&lt;model&gt;</code></li><li> -&gt;  / <code>--model cli/gemini/&lt;model&gt;</code></li><li> (Cursor Agent CLI) -&gt;  / <code>--model cli/agent/&lt;model&gt;</code></li></ul><ul><li>Binary installed and on  (or set , , , )</li><li>Provider authenticated (, ,  login flow,  or )</li></ul><pre><code>printf \"Summarize CLI smoke input.\\nOne short paragraph. Reply can be brief.\\n\" &gt;/tmp/summarize-cli-smoke.txt\n\nsummarize --cli codex --plain --timeout 2m /tmp/summarize-cli-smoke.txt\nsummarize --cli claude --plain --timeout 2m /tmp/summarize-cli-smoke.txt\nsummarize --cli gemini --plain --timeout 2m /tmp/summarize-cli-smoke.txt\nsummarize --cli agent --plain --timeout 2m /tmp/summarize-cli-smoke.txt\n</code></pre><p>Set explicit CLI allowlist/order:</p><pre><code>{\n  \"cli\": { \"enabled\": [\"codex\", \"claude\", \"gemini\", \"agent\"] }\n}\n</code></pre><p>Configure implicit auto CLI fallback:</p><pre><code>{\n  \"cli\": {\n    \"autoFallback\": {\n      \"enabled\": true,\n      \"onlyWhenNoApiKeys\": true,\n      \"order\": [\"claude\", \"gemini\", \"codex\", \"agent\"]\n    }\n  }\n}\n</code></pre><p> builds candidate attempts from built-in rules (or your  overrides). CLI attempts are prepended when:</p><ul><li> is set (explicit allowlist/order), or</li><li>implicit auto selection is active and  is enabled.</li></ul><p>Default fallback behavior: only when no API keys are configured, order <code>claude, gemini, codex, agent</code>, and remember/prioritize last successful provider (<code>~/.summarize/cli-state.json</code>).</p><p>Set explicit CLI attempts:</p><pre><code>{\n  \"cli\": { \"enabled\": [\"gemini\"] }\n}\n</code></pre><p>Disable implicit auto CLI fallback:</p><pre><code>{\n  \"cli\": { \"autoFallback\": { \"enabled\": false } }\n}\n</code></pre><p>Note: explicit  does not trigger implicit auto CLI fallback unless  is set.</p><h3>Website extraction (Firecrawl + Markdown)</h3><p>Non-YouTube URLs go through a fetch -&gt; extract pipeline. When direct fetch/extraction is blocked or too thin,  can fall back to Firecrawl (if configured).</p><ul><li><code>--firecrawl off|auto|always</code> (default )</li><li><code>--extract --format md|text</code> (default ; if  is omitted,  defaults to  for non-YouTube URLs)</li><li><code>--markdown-mode off|auto|llm|readability</code> (default ) \n  <ul><li>: use an LLM converter when configured; may fall back to </li><li>: force LLM conversion (requires a configured model key)</li><li>: disable LLM conversion (still may return Firecrawl Markdown when configured)</li></ul></li><li>Plain-text mode: use .</li></ul><p> tries best-effort web transcript endpoints first. When captions are not available, it falls back to:</p><ol><li>Apify (if  is set): uses a scraping actor ()</li><li>yt-dlp + Whisper (if  is available): downloads audio, then transcribes with local  when installed (preferred), otherwise falls back to OpenAI () or FAL ()</li></ol><p>Environment variables for yt-dlp mode:</p><ul><li> - optional path to yt-dlp binary (otherwise  is resolved via )</li><li><code>SUMMARIZE_WHISPER_CPP_MODEL_PATH</code> - optional override for the local  model file</li><li><code>SUMMARIZE_WHISPER_CPP_BINARY</code> - optional override for the local binary (default: )</li><li><code>SUMMARIZE_DISABLE_LOCAL_WHISPER_CPP=1</code> - disable local whisper.cpp (force remote)</li><li> - OpenAI Whisper transcription</li><li> - optional OpenAI-compatible Whisper endpoint override</li><li> - FAL AI Whisper fallback</li></ul><p>Apify costs money but tends to be more reliable when captions exist.</p><h3>Slide extraction (YouTube + direct video URLs)</h3><p>Extract slide screenshots (scene detection via ) and optional OCR:</p><pre><code>summarize \"https://www.youtube.com/watch?v=...\" --slides\nsummarize \"https://www.youtube.com/watch?v=...\" --slides --slides-ocr\n</code></pre><p>Outputs are written under  (or ). OCR results are included in JSON output () and stored in  inside the slide directory. When scene detection is too sparse, the extractor also samples at a fixed interval to improve coverage. When using , supported terminals (kitty/iTerm/Konsole) render inline thumbnails automatically inside the summary narrative (the model inserts  markers). Timestamp links are clickable when the terminal supports OSC-8 (YouTube/Vimeo/Loom/Dropbox). If inline images are unsupported, Summarize prints a note with the on-disk slide directory.</p><p>Use  to print the full timed transcript and insert slide images inline at matching timestamps.</p><p>Format the extracted transcript as Markdown (headings + paragraphs) via an LLM:</p><pre><code>summarize \"https://www.youtube.com/watch?v=...\" --extract --format md --markdown-mode llm\n</code></pre><h3>Media transcription (Whisper)</h3><p>Local audio/video files are transcribed first, then summarized.  forces direct media URLs (and embedded media) through Whisper first. Prefers local  when available; otherwise requires  or .</p><h3>Local ONNX transcription (Parakeet/Canary)</h3><p>Summarize can use NVIDIA Parakeet/Canary ONNX models via a local CLI you provide. Auto selection (default) prefers ONNX when configured.</p><ul><li>Setup helper: <code>summarize transcriber setup</code></li><li>Install  from upstream binaries/build (Homebrew may not have a formula)</li><li>Auto selection: set <code>SUMMARIZE_ONNX_PARAKEET_CMD</code> or <code>SUMMARIZE_ONNX_CANARY_CMD</code> (no flag needed)</li><li>Force a model: <code>--transcriber parakeet|canary|whisper|auto</code></li><li>Docs: <code>docs/nvidia-onnx-transcription.md</code></li></ul><h3>Verified podcast services (2025-12-25)</h3><ul><li>Amazon Music / Audible podcast pages</li><li>RSS feeds (Podcasting 2.0 transcripts when available)</li><li>Embedded YouTube podcast pages (e.g. JREPodcast)</li></ul><p>Transcription: prefers local  when installed; otherwise uses OpenAI Whisper or FAL when keys are set.</p><p> controls the output language of the summary (and other LLM-generated text). Default is .</p><p>When the input is audio/video, the CLI needs a transcript first. The transcript comes from one of these paths:</p><ol><li>Existing transcript (preferred) \n  <ul><li>YouTube: uses  /  when available.</li><li>Podcasts: uses Podcasting 2.0 RSS  (JSON/VTT) when the feed publishes it.</li></ul></li><li>Whisper transcription (fallback) \n  <ul><li>YouTube: falls back to yt-dlp (audio download) + Whisper transcription when configured; Apify is a last resort.</li><li>Prefers local  when installed + model available.</li><li>Otherwise uses cloud Whisper (OpenAI ) or FAL ().</li></ul></li></ol><p>For direct media URLs, use  to force transcribe -&gt; summarize:</p><pre><code>summarize https://example.com/file.mp4 --video-mode transcript --lang en\n</code></pre><pre><code>{\n  \"model\": { \"id\": \"openai/gpt-5-mini\" },\n  \"env\": { \"OPENAI_API_KEY\": \"sk-...\" },\n  \"ui\": { \"theme\": \"ember\" }\n}\n</code></pre><pre><code>{\n  \"model\": \"openai/gpt-5-mini\"\n}\n</code></pre><ul><li> (customize candidates / ordering)</li><li> (define presets selectable via )</li><li> (generic env var defaults; process env still wins)</li><li> (legacy shortcut, mapped to env names; prefer  for new configs)</li><li> (media download cache: TTL 7 days, 2048 MB cap by default;  disables)</li><li><code>media.videoMode: \"auto\"|\"transcript\"|\"understand\"</code></li><li> /  /  /  (defaults for )</li><li><code>ui.theme: \"aurora\"|\"ember\"|\"moss\"|\"mono\"</code></li><li><code>openai.useChatCompletions: true</code> (force OpenAI-compatible chat completions)</li></ul><p>Note: the config is parsed leniently (JSON5), but comments are not allowed. Unknown keys are ignored.</p><pre><code>{\n  \"cache\": {\n    \"media\": { \"enabled\": true, \"ttlDays\": 7, \"maxMb\": 2048, \"verify\": \"size\" }\n  }\n}\n</code></pre><p>Note:  bypasses summary caching only (LLM output). Extract/transcript caches still apply. Use  to skip media files.</p><ol></ol><ol><li> ()</li></ol><p>Environment variable precedence:</p><ol><li> ()</li><li> (, legacy)</li></ol><p>Set the key matching your chosen :</p><ul><li><p>Optional fallback defaults can be stored in config:</p><ul><li> -&gt; <code>\"env\": { \"OPENAI_API_KEY\": \"sk-...\" }</code></li><li>process env always takes precedence</li><li>legacy  still works (mapped to env names)</li></ul></li><li><p> (for )</p></li><li><p> (for )</p></li><li><p> (for )</p></li><li><p> (for ; supports  alias)</p></li><li><p> (for )</p><ul><li>also accepts <code>GOOGLE_GENERATIVE_AI_API_KEY</code> and  as aliases</li></ul></li></ul><p>OpenAI-compatible chat completions toggle:</p><ul><li><code>OPENAI_USE_CHAT_COMPLETIONS=1</code> (or set <code>openai.useChatCompletions</code> in config)</li></ul><ul><li><code>SUMMARIZE_THEME=aurora|ember|moss|mono</code></li><li> (force 24-bit ANSI)</li><li> (disable 24-bit ANSI)</li></ul><p>OpenRouter (OpenAI-compatible):</p><ul><li>Set </li><li>Prefer forcing OpenRouter per model id: <code>--model openrouter/&lt;author&gt;/&lt;slug&gt;</code></li><li>Built-in preset:  (uses a default set of OpenRouter  models)</li></ul><p>Quick start: make free the default (keep  available)</p><pre><code>summarize refresh-free --set-default\nsummarize \"https://example.com\"\nsummarize \"https://example.com\" --model auto\n</code></pre><p>Regenerates the  preset ( in ) by:</p><ul><li>Fetching OpenRouter , filtering </li><li>Skipping models that look very small (&lt;27B by default) based on the model id/name</li><li>Testing which ones return non-empty text (concurrency 4, timeout 10s)</li><li>Picking a mix of smart-ish (bigger  / output cap) and fast models</li><li>Refining timings and writing the sorted list back</li></ul><p>If  stops working, run:</p><ul><li> (default): extra timing runs per selected model (total runs = 1 + runs)</li><li> (default): how many smart-first picks (rest filled by fastest)</li><li> (default): ignore models with inferred size smaller than N billion parameters</li><li> (default): ignore models older than N days (set 0 to disable)</li><li>: also sets  in </li></ul><pre><code>OPENROUTER_API_KEY=sk-or-... summarize \"https://example.com\" --model openrouter/meta-llama/llama-3.1-8b-instruct:free\nOPENROUTER_API_KEY=sk-or-... summarize \"https://example.com\" --model openrouter/minimax/minimax-m2.5\n</code></pre><p>If your OpenRouter account enforces an allowed-provider list, make sure at least one provider is allowed for the selected model. When routing fails,  prints the exact providers to allow.</p><p>Legacy: <code>OPENAI_BASE_URL=https://openrouter.ai/api/v1</code> (and either  or ) also works.</p><p>NVIDIA API Catalog (OpenAI-compatible; free credits):</p><ul><li>Optional: <code>NVIDIA_BASE_URL=https://integrate.api.nvidia.com/v1</code></li><li>Credits: API Catalog trial starts with 1000 free API credits on signup (up to 5000 total via â€œRequest Moreâ€ in the API Catalog profile)</li><li>Pick a model id from  (examples: fast <code>stepfun-ai/step-3.5-flash</code>, strong but slower )</li></ul><pre><code>export NVIDIA_API_KEY=\"nvapi-...\"\nsummarize \"https://example.com\" --model nvidia/stepfun-ai/step-3.5-flash\n</code></pre><p>Z.AI (OpenAI-compatible):</p><ul><li> (or )</li><li>Optional base URL override: </li></ul><ul><li> (website extraction fallback)</li><li> (path to yt-dlp binary for audio extraction)</li><li> (FAL AI API key for audio transcription via Whisper)</li><li> (YouTube transcript fallback)</li></ul><p>The CLI uses the LiteLLM model catalog for model limits (like max output tokens):</p><ul><li>Downloaded from: <code>https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json</code></li><li>Cached at: </li></ul><p>Recommended (minimal deps):</p><ul><li><code>@steipete/summarize-core/content</code></li><li><code>@steipete/summarize-core/prompts</code></li></ul><p>Compatibility (pulls in CLI deps):</p><ul><li><code>@steipete/summarize/content</code></li><li><code>@steipete/summarize/prompts</code></li></ul><ul><li>\"Receiving end does not exist\": Chrome did not inject the content script yet. \n  <ul><li>Extension details -&gt; Site access -&gt; On all sites (or allow this domain)</li></ul></li><li>\"Failed to fetch\" / daemon unreachable: \n  <ul><li>Logs: <code>~/.summarize/logs/daemon.err.log</code></li></ul></li></ul>",
      "contentLength": 18337,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    }
  ],
  "tags": []
}
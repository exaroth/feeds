{
  "id": "KWcVEiwEzuiDsTqLn6LojC9X2yBX45N36DRtYHsFby6oiM4t597YhKBSdxr5wqMdj8kvSgDuiUuV6yvogw",
  "title": "GitHub All Languages Daily Trending",
  "displayTitle": "Github Trending",
  "url": "https://mshibanami.github.io/GitHubTrendingRSS/daily/all.xml",
  "feedLink": "http://mshibanami.github.io/GitHubTrendingRSS",
  "isQuery": false,
  "isEmpty": false,
  "isHidden": false,
  "itemCount": 24,
  "items": [
    {
      "title": "Asabeneh/30-Days-Of-Python",
      "url": "https://github.com/Asabeneh/30-Days-Of-Python",
      "date": 1769222533,
      "author": "",
      "guid": 38476,
      "unread": true,
      "content": "<p>Our amazing sponsors for supporting my open-source contribution and the  series!</p><p>Every contribution, big or small, makes a huge difference. Thank you for your support! üåü</p><p> for deciding to participate in a  programming challenge. In this challenge, you will learn everything you need to be a python programmer and the whole concept of programming. In the end of the challenge you will get a  programming challenge certificate.</p><p>Python is a high-level programming language for general-purpose programming. It is an open source, interpreted, object-oriented programming language. Python was created by a Dutch programmer, Guido van Rossum. The name of the Python programming language was derived from a British sketch comedy series, <em>Monty Python's Flying Circus</em>. The first version was released on February 20, 1991. This 30 days of Python challenge will help you learn the latest version of Python, Python 3 step by step. The topics are broken down into 30 days, where each day contains several topics with easy-to-understand explanations, real-world examples, and many hands on exercises and projects.</p><p>This challenge is designed for beginners and professionals who want to learn python programming language. It may take 30 to 100 days to complete the challenge. People who actively participate in the telegram group have a high probability of completing the challenge.</p><p>This challenge is easy to read, written in conversational English, engaging, motivating and at the same time, it is very demanding. You need to allocate much time to finish this challenge. If you are a visual learner, you may get the video lesson on <a href=\"https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw\"> Washera</a> YouTube channel. You may start from <a href=\"https://youtu.be/OCCWZheOesI\">Python for Absolute Beginners video</a>. Subscribe the channel, comment and ask questions on YouTube videos and be proactive, the author will eventually notice you.</p><p>The author likes to hear your opinion about the challenge, share the author by expressing your thoughts about the 30DaysOfPython challenge. You can leave your testimonial on this <a href=\"https://www.asabeneh.com/testimonials\">link</a></p><p>It is a programming language which is very close to human language and because of that, it is easy to learn and use. Python is used by various industries and companies (including Google). It has been used to develop web applications, desktop applications, system administration, and machine learning libraries. Python is a highly embraced language in the data science and machine learning community. I hope this is enough to convince you to start learning Python. Python is eating the world and you are killing it before it eats you.</p><p>To run a python script you need to install python. Let's <a href=\"https://www.python.org/\">download</a> python. If your are a windows user, click the button encircled in red.</p><p>If you are a macOS user, click the button encircled in red.</p><p>To check if python is installed write the following command on your device terminal.</p><p>As you can see from the terminal, I am using  version at the moment. Your version of Python might be different from mine by but it should be 3.6 or above. If you manage to see the python version, well done. Python has been installed on your machine. Continue to the next section.</p><p>Python is an interpreted scripting language, so it does not need to be compiled. It means it executes the code line by line. Python comes with a <em>Python Shell (Python Interactive Shell)</em>. It is used to execute a single python command and get the result.</p><p>Python Shell waits for the Python code from the user. When you enter the code, it interprets the code and shows the result in the next line. Open your terminal or command prompt(cmd) and write:</p><p>The Python interactive shell is opened and it is waiting for you to write Python code(Python script). You will write your Python script next to this symbol &gt;&gt;&gt; and then click Enter. Let us write our very first script on the Python scripting shell.</p><p>Well done, you wrote your first Python script on Python interactive shell. How do we close the Python interactive shell ? To close the shell, next to this symbol &gt;&gt;&gt; write  command and press Enter.</p><p>Now, you know how to open the Python interactive shell and how to exit from it.</p><p>Python will give you results if you write scripts that Python understands, if not it returns errors. Let's make a deliberate mistake and see what Python will return.</p><p>As you can see from the returned error, Python is so clever that it knows the mistake we made and which was <em>Syntax Error: invalid syntax</em>. Using x as multiplication in Python is a syntax error because (x) is not a valid syntax in Python. Instead of () we use asterisk (*) for multiplication. The returned error clearly shows what to fix.</p><p>The process of identifying and removing errors from a program is called . Let us debug it by putting * in place of .</p><p>Our bug was fixed, the code ran and we got a result we were expecting. As a programmer you will see such kind of errors on daily basis. It is good to know how to debug. To be good at debugging you should understand what kind of errors you are facing. Some of the Python errors you may encounter are , , , , , , , , ,  etc. We will see more about different Python  in later sections.</p><p>Let us practice more how to use Python interactive shell. Go to your terminal or command prompt and write the word .</p><p>The Python interactive shell is opened. Let us do some basic mathematical operations (addition, subtraction, multiplication, division, modulus, exponentiation).</p><p>Let us do some maths first before we write any Python code:</p><ul></ul><p>In python, we have the following additional operations:</p><ul><li>3 % 2 = 1 =&gt; which means finding the remainder</li><li>3 // 2 = 1 =&gt; which means removing the remainder</li></ul><p>Let us change the above mathematical expressions to Python code. The Python shell has been opened and let us write a comment at the very beginning of the shell.</p><p>A  is a part of the code which is not executed by python. So we can leave some text in our code to make our code more readable. Python does not run the comment part. A comment in python starts with hash(#) symbol. This is how you write a comment in python</p><pre><code> # comment starts with hash\n # this is a python comment, because it starts with a (#) symbol\n</code></pre><p>Before we move on to the next section, let us practice more on the Python interactive shell. Close the opened shell by writing  on the shell and open it again and let us practice how to write text on the Python shell.</p><h3>Installing Visual Studio Code</h3><p>The Python interactive shell is good to try and test small script codes but it will not be for a big project. In real work environment, developers use different code editors to write codes. In this 30 days of Python programming challenge, we will use Visual Studio Code. Visual Studio Code is a very popular open source text editor. I am a fan of vscode and I would recommend to <a href=\"https://code.visualstudio.com/\">download</a> visual studio code, but if you are in favor of other editors, feel free to follow with what you have.</p><p>If you installed visual studio code, let us see how to use it. If you prefer a video, you can follow this Visual Studio Code for Python <a href=\"https://www.youtube.com/watch?v=bn7Cx4z-vSo\">Video tutorial</a></p><h4>How to use visual studio code</h4><p>Open the visual studio code by double clicking the visual studio icon. When you open it you will get this kind of interface. Try to interact with the labeled icons.</p><p>Create a folder named 30DaysOfPython on your desktop. Then open it using visual studio code.</p><p>After opening it, you will see shortcuts for creating files and folders inside of 30DaysOfPython project's directory. As you can see below, I have created the very first file, . You can do the same.</p><p>After a long day of coding, you want to close your code editor, right? This is how you will close the opened project.</p><p>Congratulations, you have finished setting up the development environment. Let us start coding.</p><p>A Python script can be written in Python interactive shell or in the code editor. A Python file has an extension .py.</p><p>An indentation is a white space in a text. Indentation in many languages is used to increase code readability; however, Python uses indentation to create blocks of code. In other programming languages, curly brackets are used to create code blocks instead of indentation. One of the common bugs when writing Python code is incorrect indentation.</p><p>Comments play a crucial role in enhancing code readability and allowing developers to leave notes within their code. In Python, any text preceded by a hash (#) symbol is considered a comment and is not executed when the code runs.</p><p><strong>Example: Single Line Comment</strong></p><pre><code>    # This is the first comment\n    # This is the second comment\n    # Python is eating the world\n</code></pre><p><strong>Example: Multiline Comment</strong></p><p>Triple quote can be used for multiline comment if it is not assigned to a variable</p><pre><code>\"\"\"This is multiline comment\nmultiline comment takes multiple lines.\npython is eating the world\n\"\"\"\n</code></pre><p>In Python there are several types of data types. Let us get started with the most common ones. Different data types will be covered in detail in other sections. For the time being, let us just go through the different data types and get familiar with them. You do not have to have a clear understanding now.</p><ul><li>Integer: Integer(negative, zero and positive) numbers Example: ... -3, -2, -1, 0, 1, 2, 3 ...</li><li>Float: Decimal number Example ... -3.5, -2.25, -1.0, 0.0, 1.1, 2.2, 3.5 ...</li><li>Complex Example 1 + j, 2 + 4j</li></ul><p>A collection of one or more characters under a single or double quote. If a string is more than one sentence then we use a triple quote.</p><pre><code>'Asabeneh'\n'Finland'\n'Python'\n'I love teaching'\n'I hope you are enjoying the first day of 30DaysOfPython Challenge'\n</code></pre><p>A boolean data type is either a True or False value. T and F should be always uppercase.</p><pre><code>    True  #  Is the light on? If it is on, then the value is True\n    False # Is the light on? If it is off, then the value is False\n</code></pre><p>Python list is an ordered collection which allows to store different data type items. A list is similar to an array in JavaScript.</p><pre><code>[0, 1, 2, 3, 4, 5]  # all are the same data types - a list of numbers\n['Banana', 'Orange', 'Mango', 'Avocado'] # all the same data types - a list of strings (fruits)\n['Finland','Estonia', 'Sweden','Norway'] # all the same data types - a list of strings (countries)\n['Banana', 10, False, 9.81] # different data types in the list - string, integer, boolean and float\n</code></pre><p>A Python dictionary object is an unordered collection of data in a key value pair format.</p><pre><code>{\n'first_name':'Asabeneh',\n'last_name':'Yetayeh',\n'country':'Finland',\n'age':250,\n'is_married':True,\n'skills':['JS', 'React', 'Node', 'Python']\n}\n</code></pre><p>A tuple is an ordered collection of different data types like list but tuples can not be modified once they are created. They are immutable.</p><pre><code>('Asabeneh', 'Pawel', 'Brook', 'Abraham', 'Lidiya') # Names\n</code></pre><pre><code>('Earth', 'Jupiter', 'Neptune', 'Mars', 'Venus', 'Saturn', 'Uranus', 'Mercury') # planets\n</code></pre><p>A set is a collection of data types similar to list and tuple. Unlike list and tuple, set is not an ordered collection of items. Like in Mathematics, set in Python stores only unique items.</p><p>In later sections, we will go in detail about each and every Python data type.</p><pre><code>{2, 4, 3, 5}\n{3.14, 9.81, 2.7} # order is not important in set\n</code></pre><p>To check the data type of certain data/variable we use the  function. In the following terminal you will see different python data types:</p><p>First open your project folder, 30DaysOfPython. If you don't have this folder, create a folder name called 30DaysOfPython. Inside this folder, create a file called helloworld.py. Now, let's do what we did on python interactive shell using visual studio code.</p><p>The Python interactive shell was printing without using  but on visual studio code to see our result we should use a built in function . The  built-in function takes one or more arguments as follows <em>print('arument1', 'argument2', 'argument3')</em>. See the examples below.</p><p>The file name is </p><pre><code># Day 1 - 30DaysOfPython Challenge\n\nprint(2 + 3)             # addition(+)\nprint(3 - 1)             # subtraction(-)\nprint(2 * 3)             # multiplication(*)\nprint(3 / 2)             # division(/)\nprint(3 ** 2)            # exponential(**)\nprint(3 % 2)             # modulus(%)\nprint(3 // 2)            # Floor division operator(//)\n\n# Checking data types\nprint(type(10))          # Int\nprint(type(3.14))        # Float\nprint(type(1 + 3j))      # Complex number\nprint(type('Asabeneh'))  # String\nprint(type([1, 2, 3]))   # List\nprint(type({'name':'Asabeneh'})) # Dictionary\nprint(type({9.8, 3.14, 2.7}))    # Set\nprint(type((9.8, 3.14, 2.7)))    # Tuple\n</code></pre><p>To run the python file check the image below. You can run the python file either by running the green button on Visual Studio Code or by typing  in the terminal .</p><p>üåï You are amazing. You have just completed day 1 challenge and you are on your way to greatness. Now do some exercises for your brain and muscles.</p><ol><li>Check the python version you are using</li><li>Open the python interactive shell and do the following operations. The operands are 3 and 4. \n   <ul><li>floor division operator(//)</li></ul></li><li>Write strings on the python interactive shell. The strings are the following: \n   <ul><li>I am enjoying 30 days of python</li></ul></li><li>Check the data types of the following data: \n   <ul><li>['Asabeneh', 'Python', 'Finland']</li></ul></li></ol><ol><li>Create a folder named day_1 inside 30DaysOfPython folder. Inside day_1 folder, create a python file helloworld.py and repeat questions 1, 2, 3 and 4. Remember to use  when you are working on a python file. Navigate to the directory where you have saved your file, and run it.</li></ol><ol><li>Write an example for different Python data types such as Number(Integer, Float, Complex), String, Boolean, List, Tuple, Set and Dictionary.</li></ol>",
      "contentLength": 13401,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "KellerJordan/modded-nanogpt",
      "url": "https://github.com/KellerJordan/modded-nanogpt",
      "date": 1769222533,
      "author": "",
      "guid": 38477,
      "unread": true,
      "content": "<p>NanoGPT (124M) in 2 minutes</p><p>This repository hosts the , in which we (collaboratively|competitively) search for the fastest algorithm to use 8 NVIDIA H100 GPUs to train a language model that attains 3.28 cross-entropy loss on the <a href=\"https://huggingface.co/datasets/HuggingFaceFW/fineweb\">FineWeb</a> validation set.</p><ul><li>Under 100 seconds on 8xH100 (the llm.c GPT-2 replication needed 45 minutes)</li><li>under 500M tokens (the llm.c GPT-2 replication needed 10B)</li></ul><p>This improvement in training speed has been brought about by the following techniques:</p><ul><li>Modernized architecture: Rotary embeddings, QK-Norm, and ReLU¬≤</li><li>Use FP8 matmul for head, and asymmetric rescale and softcap logits</li><li>Initialization of projections to zero (muP-like)</li><li>Skip connections from embedding to every block as well as from block 3 to 6</li><li>Extra embeddings which are mixed into the values in attention layers (inspired by Zhou et al. 2024)</li><li>Flash Attention 3 with long-short sliding window attention pattern (inspired by Gemma 2) and window size warmup with YaRN</li><li>Align training batch starts with EoS and set a max document length</li><li>Accumulate gradients for 2 steps for embedding and lm_head before updating parameters</li><li>Enable model to back out contributions from first 2/3 layers before prediction</li><li>Polar Express implementation in Muon</li><li>Smear module to enable 1 token look back</li><li>Cautious Weight Decay w/ schedule tied to LR</li><li>Exponential decay of residual stream</li><li>Untie embed and lm_head at 2/3 of training</li><li>Additional gating on value embeddings and skip connection</li></ul><p>As well as many systems optimizations.</p><h2>Running the current record</h2><p>To run the current record, run the following commands.</p><pre><code>git clone https://github.com/KellerJordan/modded-nanogpt.git &amp;&amp; cd modded-nanogpt\npip install -r requirements.txt\npip install torch==2.10.0.dev20251210+cu126 --index-url https://download.pytorch.org/whl/nightly/cu126\n# downloads only the first 900M training tokens to save time\npython data/cached_fineweb10B.py 9\n./run.sh\n</code></pre><p>Add torchrun to path if ./run.sh gives error <code>torchrun: command not found</code>.</p><p><strong>Note: torch.compile will add around 7 minutes of latency the first time you run the code.</strong></p><h2>Alternative: Running with Docker (recommended for precise timing)</h2><p>For cases where CUDA or NCCL versions aren't compatible with your current system setup, Docker can be a helpful alternative. This approach standardizes versions for CUDA, NCCL, CUDNN, and Python, reducing dependency issues and simplifying setup. Note: an NVIDIA driver must already be installed on the system (useful if only the NVIDIA driver and Docker are available).</p><pre><code>git clone https://github.com/KellerJordan/modded-nanogpt.git &amp;&amp; cd modded-nanogpt\nsudo docker build -t modded-nanogpt .\nsudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt python data/cached_fineweb10B.py 8\nsudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt sh run.sh\n</code></pre><p>To get an interactive docker, you can use</p><pre><code>sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt bash\n</code></pre><p>The following is the historical progression of world speed records for the following competitive task:</p><blockquote><p><em>Train a neural network to ‚â§3.28 validation loss on FineWeb using 8x NVIDIA H100s.</em></p></blockquote><ol><li>Not modify the train or validation data pipelines. (You can change the batch size, sequence length, attention structure etc.; just don't change the underlying streams of tokens.)</li><li>Attain ‚â§3.28 mean val loss. (Due to inter-run variance, submissions must provide enough run logs to attain a statistical significance level of p&lt;0.01 that their mean val loss is ‚â§3.28. Example code to compute p-value can be found <a href=\"https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-04_SoftCap#softer-softcap\">here</a>. For submissions which improve speed by optimizing the systems performance, without touching the ML, this requirement is waived.)</li><li>Not use any extra  or  flags. (These can save a few seconds, but they can also make compilation take &gt;30min. This rule was introduced after the 21st record.)</li><li>Run faster than the prior record when baselined on the same hardware.</li></ol><p>Discretionary reasons why a PR may not be accepted:</p><ol><li>Disproportionately degrades the readability of the codebase. A 200 line kernel to drop 300ms is considered worthwhile. 500 lines that convolute the optimizer layout for a 50ms gain will likely be rejected.</li><li>The current record is intentionally kept roughly 0.001-0.002 loss below 3.28 to make validation simpler. If a PR substantially consumes this buffer, it should do so in a way that outperforms a simple step count decrease, when measured at equivalent loss.</li></ol><blockquote><p>Note: <code>torch._inductor.config.coordinate_descent_tuning</code> is allowed for GPT-2 Medium track (a.k.a. 2.92 track).</p></blockquote><p>Other than that, anything and everything is fair game!</p><h3>Comment on the target metric</h3><p>The target metric is <em>cross-entropy loss on the FineWeb val set</em>. To speak mathematically, the goal of the speedrun is *to obtain a probability model of language which assigns a probability of at least <code>math.exp(-3.28 * 10485760)</code> to the first 10,485,760 tokens of the FineWeb valset. Hence, e.g., we allow evaluation at any sequence length, so long as we still have a valid probability model of language.</p><h3>Timing change after record 21</h3><p>After the 21st record, we made two changes to the timing. First, there used to be an initial \"grace period\" of 10 untimed steps to allow kernel warmup. We replaced this with an explicit kernel-warmup section which is untimed and uses dummy data. This results in an extra runtime of 850ms from the 10 extra timed steps. Second, we banned the use of <code>torch._inductor.config.coordinate_descent_tuning</code>. This saves ~25min of untimed pre-run compilation, but results in an extra runtime of ~3s.</p><ul><li><a href=\"https://x.com/alexjc/status/1881410039639863622\">@alexjc's 01/20/2025 2.77-minute TokenMonster-based record</a>. This record is technically outside the rules of the speedrun, since we specified that the train/val tokens must be kept fixed. However, it's very interesting, and worth including. The run is not more data-efficient; rather, the speedup comes from the improved tokenizer allowing the vocabulary size to be reduced (nearly halved!) while preserving the same bytes-per-token, which saves lots of parameters and FLOPs in the head and embeddings.</li></ul><h2>Speedrun track 2: GPT-2 Medium</h2><p>The target loss for this track is lowered from 3.28 to 2.92, as per Andrej Karpathy's 350M-parameter llm.c baseline. This baseline generates a model with performance similar to the original GPT-2 Medium, whereas the first track's baseline generates a model on par with GPT-2 Small. All other rules remain the same.</p><blockquote><p>Note: <code>torch._inductor.config.coordinate_descent_tuning</code> is turned on after the record 6 (*).</p></blockquote><h3>Q: What is the point of NanoGPT speedrunning?</h3><p>A: The officially stated goal of NanoGPT speedrunning is as follows: . But for something a little more verbose involving an argument for good benchmarking, here's some kind of manifesto, adorned with a blessing from the master. <a href=\"https://x.com/karpathy/status/1846790537262571739\">https://x.com/karpathy/status/1846790537262571739</a></p><h3>Q: What makes \"NanoGPT speedrunning\" not just another idiosyncratic benchmark?</h3><p>A: Because it is a  benchmark. In particular, if you attain a new speed record (using whatever method you want), there is an open invitation for you to post that record (on arXiv or X) and thereby vacuum up all the clout for yourself. I will even help you do it by reposting you as much as I can.</p><h3>Q: NanoGPT speedrunning is cool and all, but meh it probably won't scale and is just overfitting to val loss</h3><p>A: This is hard to refute, since \"at scale\" is an infinite category (what if the methods stop working only for &gt;100T models?), making it impossible to fully prove. Also, I would agree that some of the methods used in the speedrun are unlikely to scale, particularly those which <em>impose additional structure</em> on the network, such as logit softcapping. But if the reader cares about 1.5B models, they might be convinced by this result:</p><p><em>Straightforwardly scaling up the speedrun (10/18/24 version) to 1.5B parameters yields a model with GPT-2 (1.5B)-level HellaSwag performance 2.5x more cheaply than <a href=\"https://github.com/karpathy/llm.c/discussions/677\">@karpathy's baseline</a> ($233 instead of $576):</em></p><p>Muon is defined as follows:</p><p>Where NewtonSchulz5 is the following Newton-Schulz iteration [2, 3], which approximately replaces  with  where .</p><pre><code>@torch.compile\ndef zeroth_power_via_newtonschulz5(G, steps=5, eps=1e-7):\n    assert len(G.shape) == 2\n    a, b, c = (3.4445, -4.7750,  2.0315)\n    X = G.bfloat16() / (G.norm() + eps)\n    if G.size(0) &gt; G.size(1):\n        X = X.T \n    for _ in range(steps):\n        A = X @ X.T\n        B = b * A + c * A @ A\n        X = a * X + B @ X\n    if G.size(0) &gt; G.size(1):\n        X = X.T \n    return X.to(G.dtype)\n</code></pre><p>For this training scenario, Muon has the following favorable properties:</p><ul><li>Lower memory usage than Adam</li><li>~1.5x better sample-efficiency</li></ul><p>Many of the choices made to generate this optimizer were obtained experimentally by our pursuit of <a href=\"https://github.com/KellerJordan/cifar10-airbench\">CIFAR-10 speedrunning</a>. In particular, we experimentally obtained the following practices:</p><ul><li>Using Nesterov momentum inside the update, with orthogonalization applied after momentum.</li><li>Using a specifically quintic Newton-Schulz iteration as the method of orthogonalization.</li><li>Using non-convergent coefficients for the quintic polynomial in order to maximize slope at zero, and thereby minimize the number of necessary Newton-Schulz iterations. It turns out that the variance doesn't actually matter that much, so we end up with a quintic that rapidly converges to the range 0.68, 1.13 upon repeated application, rather than converging more slowly to 1.</li><li>Running the Newton-Schulz iteration in bfloat16 (whereas Shampoo implementations often depend on inverse-pth-roots run in fp32 or fp64).</li></ul><p>Our use of a Newton-Schulz iteration for orthogonalization traces to <a href=\"https://arxiv.org/abs/2409.20325\">Bernstein &amp; Newhouse (2024)</a>, who suggested it as a way to compute Shampoo [5, 6] preconditioners, and theoretically explored Shampoo without preconditioner accumulation. In particular, Jeremy Bernstein @jxbz sent us the draft, which caused us to experiment with various Newton-Schulz iterations as the orthogonalization method for this optimizer. If we had used SVD instead of a Newton-Schulz iteration, this optimizer would have been too slow to be useful. Bernstein &amp; Newhouse also pointed out that Shampoo without preconditioner accumulation is equivalent to steepest descent in the spectral norm, and therefore Shampoo can be thought of as a way to smooth out spectral steepest descent. The proposed optimizer can be thought of as a second way of smoothing spectral steepest descent, with a different set of memory and runtime tradeoffs compared to Shampoo.</p><ul><li>To run experiments on fewer GPUs, simply modify  to have a different . This should not change the behavior of the training.</li><li>If you're running out of memory, you may need to reduce the sequence length for FlexAttention (which does change the training. see <a href=\"https://github.com/KellerJordan/modded-nanogpt/pull/38\">here</a> for a guide)</li></ul><pre><code>@misc{modded_nanogpt_2024,\n  author       = {Keller Jordan and Jeremy Bernstein and Brendan Rappazzo and\n                  @fernbear.bsky.social and Boza Vlado and You Jiacheng and\n                  Franz Cesista and Braden Koszarsky and @Grad62304977},\n  title        = {modded-nanogpt: Speedrunning the NanoGPT baseline},\n  year         = {2024},\n  url          = {https://github.com/KellerJordan/modded-nanogpt}\n}\n</code></pre><img src=\"https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/dofa.jpg\" alt=\"itsover_wereback\">",
      "contentLength": 11027,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "lyogavin/airllm",
      "url": "https://github.com/lyogavin/airllm",
      "date": 1769222533,
      "author": "",
      "guid": 38478,
      "unread": true,
      "content": "<p>AirLLM 70B inference with single 4GB GPU</p><p> optimizes inference memory usage, allowing 70B large language models to run inference on a single 4GB GPU card without quantization, distillation and pruning. And you can run  on  now.</p><h2>AI Agents Recommendation:</h2><p>[2024/08/20] v2.11.0: Support Qwen2.5</p><p>[2024/08/18] v2.10.1 Support CPU inference. Support non sharded models. Thanks @NavodPeiris for the great work!</p><p>[2024/07/30] Support Llama3.1  (<a href=\"https://colab.research.google.com/github/lyogavin/airllm/blob/main/air_llm/examples/run_llama3.1_405B.ipynb\">example notebook</a>). Support .</p><p>[2024/04/20] AirLLM supports Llama3 natively already. Run Llama3 70B on 4GB single GPU.</p><p>[2023/12/25] v2.8.2: Support MacOS running 70B large language models.</p><p>[2023/12/20] v2.7: Support AirLLMMixtral.</p><p>[2023/12/20] v2.6: Added AutoModel, automatically detect model type, no need to provide model class to initialize model.</p><p>[2023/12/18] v2.5: added prefetching to overlap the model loading and compute. 10% speed improvement.</p><p>[2023/12/03] added support of , , , , !</p><p>[2023/12/02] added support for safetensors. Now support all top 10 models in open llm leaderboard.</p><p>[2023/12/01] airllm 2.0. Support compressions: </p><p>[2023/11/20] airllm Initial version!</p><p>First, install the airllm pip package.</p><p>Then, initialize AirLLMLlama2, pass in the huggingface repo ID of the model being used, or the local path, and inference can be performed similar to a regular transformer model.</p><p>(<em>You can also specify the path to save the splitted layered model through  when init AirLLMLlama2.</em></p><pre><code>from airllm import AutoModel\n\nMAX_LENGTH = 128\n# could use hugging face model repo id:\nmodel = AutoModel.from_pretrained(\"garage-bAInd/Platypus2-70B-instruct\")\n\n# or use model's local path...\n#model = AutoModel.from_pretrained(\"/home/ubuntu/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/b585e74bcaae02e52665d9ac6d23f4d0dbc81a0f\")\n\ninput_text = [\n        'What is the capital of United States?',\n        #'I like',\n    ]\n\ninput_tokens = model.tokenizer(input_text,\n    return_tensors=\"pt\", \n    return_attention_mask=False, \n    truncation=True, \n    max_length=MAX_LENGTH, \n    padding=False)\n           \ngeneration_output = model.generate(\n    input_tokens['input_ids'].cuda(), \n    max_new_tokens=20,\n    use_cache=True,\n    return_dict_in_generate=True)\n\noutput = model.tokenizer.decode(generation_output.sequences[0])\n\nprint(output)\n\n</code></pre><p>Note: During inference, the original model will first be decomposed and saved layer-wise. Please ensure there is sufficient disk space in the huggingface cache directory.</p><h2>Model Compression - 3x Inference Speed Up!</h2><p>We just added model compression based on block-wise quantization-based model compression. Which can further <strong>speed up the inference speed</strong> for up to  , with <strong>almost ignorable accuracy loss!</strong> (see more performance evaluation and why we use block-wise quantization in <a href=\"https://arxiv.org/abs/2212.09720\">this paper</a>)</p><h4>How to enable model compression speed up:</h4><ul><li>Step 1. make sure you have <a href=\"https://github.com/TimDettmers/bitsandbytes\">bitsandbytes</a> installed by <code>pip install -U bitsandbytes </code></li><li>Step 2. make sure airllm verion later than 2.0.0: </li><li>Step 3. when initialize the model, passing the argument compression ('4bit' or '8bit'):</li></ul><pre><code>model = AutoModel.from_pretrained(\"garage-bAInd/Platypus2-70B-instruct\",\n                     compression='4bit' # specify '8bit' for 8-bit block-wise quantization \n                    )\n</code></pre><h4>What are the differences between model compression and quantization?</h4><p>Quantization normally needs to quantize both weights and activations to really speed things up. Which makes it harder to maintain accuracy and avoid the impact of outliers in all kinds of inputs.</p><p>While in our case the bottleneck is mainly at the disk loading, we only need to make the model loading size smaller. So, we get to only quantize the weights' part, which is easier to ensure the accuracy.</p><p>When initialize the model, we support the following configurations:</p><ul><li>: supported options: 4bit, 8bit for 4-bit or 8-bit block-wise quantization, or by default None for no compression</li><li>: supported options: True to output time consumptions or by default False</li><li>: optionally another path to save the splitted model</li><li>: huggingface token can be provided here if downloading gated models like: </li><li>: prefetching to overlap the model loading and compute. By default, turned on. For now, only AirLLMLlama2 supports this.</li><li>: if you don't have too much disk space, you can set delete_original to true to delete the original downloaded hugging face model, only keep the transformed one to save half of the disk space.</li></ul><p>Just install airllm and run the code the same as on linux. See more in <a href=\"https://raw.githubusercontent.com/lyogavin/airllm/main/#quickstart\">Quick Start</a>.</p><ul><li>make sure you installed <a href=\"https://github.com/ml-explore/mlx?tab=readme-ov-file#installation\">mlx</a> and torch</li><li>you probably need to install python native see more <a href=\"https://stackoverflow.com/a/65432861/21230266\">here</a></li></ul><a target=\"_blank\" href=\"https://colab.research.google.com/github/lyogavin/airllm/blob/main/air_llm/examples/run_all_types_of_models.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg?sanitize=true\" alt=\"Open In Colab\"></a><h4>example of other models (ChatGLM, QWen, Baichuan, Mistral, etc):</h4><h4>To request other model support: <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe0Io9ANMT964Zi-OQOq1TJmnvP-G3_ZgQDhP7SatN0IEdbOg/viewform?usp=sf_link\">here</a></h4><p>A lot of the code are based on SimJeg's great work in the Kaggle exam competition. Big shoutout to SimJeg:</p><h3>1. MetadataIncompleteBuffer</h3><p>safetensors_rust.SafetensorError: Error while deserializing header: MetadataIncompleteBuffer</p><p>If you run into this error, most possible cause is you run out of disk space. The process of splitting model is very disk-consuming. See <a href=\"https://huggingface.co/TheBloke/guanaco-65B-GPTQ/discussions/12\">this</a>. You may need to extend your disk space, clear huggingface <a href=\"https://huggingface.co/docs/datasets/cache\">.cache</a> and rerun.</p><h3>2. ValueError: max() arg is an empty sequence</h3><p>Most likely you are loading QWen or ChatGLM model with Llama2 class. Try the following:</p><pre><code>from airllm import AutoModel #&lt;----- instead of AirLLMLlama2\nAutoModel.from_pretrained(...)\n</code></pre><pre><code>from airllm import AutoModel #&lt;----- instead of AirLLMLlama2\nAutoModel.from_pretrained(...)\n</code></pre><h3>3. 401 Client Error....Repo model ... is gated.</h3><p>Some models are gated models, needs huggingface api token. You can provide hf_token:</p><pre><code>model = AutoModel.from_pretrained(\"meta-llama/Llama-2-7b-hf\", #hf_token='HF_API_TOKEN')\n</code></pre><h3>4. ValueError: Asking to pad but the tokenizer does not have a padding token.</h3><p>Some model's tokenizer doesn't have padding token, so you can set a padding token or simply turn the padding config off:</p><pre><code>input_tokens = model.tokenizer(input_text,\n   return_tensors=\"pt\", \n   return_attention_mask=False, \n   truncation=True, \n   max_length=MAX_LENGTH, \n   padding=False  #&lt;-----------   turn off padding \n)\n</code></pre><p>If you find AirLLM useful in your research and wish to cite it, please use the following BibTex entry:</p><pre><code>@software{airllm2023,\n  author = {Gavin Li},\n  title = {AirLLM: scaling large language models on low-end commodity computers},\n  url = {https://github.com/lyogavin/airllm/},\n  version = {0.0},\n  year = {2023},\n}\n</code></pre><p>Welcomed contributions, ideas and discussions!</p><p>If you find it useful, please ‚≠ê or buy me a coffee! üôè</p>",
      "contentLength": 6440,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "anthropics/claude-code",
      "url": "https://github.com/anthropics/claude-code",
      "date": 1769222533,
      "author": "",
      "guid": 38479,
      "unread": true,
      "content": "<p>Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.</p><p>Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows -- all through natural language commands. Use it in your terminal, IDE, or tag @claude on Github.</p><img src=\"https://raw.githubusercontent.com/anthropics/claude-code/main/demo.gif\"><blockquote><p>[!NOTE] Installation via npm is deprecated. Use one of the recommended methods below.</p></blockquote><p>For more installation options, uninstall steps, and troubleshooting, see the <a href=\"https://code.claude.com/docs/en/setup\">setup documentation</a>.</p><ol><li><p><strong>MacOS/Linux (Recommended):</strong></p><pre><code>curl -fsSL https://claude.ai/install.sh | bash\n</code></pre><pre><code>brew install --cask claude-code\n</code></pre><pre><code>irm https://claude.ai/install.ps1 | iex\n</code></pre><pre><code>winget install Anthropic.ClaudeCode\n</code></pre><pre><code>npm install -g @anthropic-ai/claude-code\n</code></pre></li><li><p>Navigate to your project directory and run .</p></li></ol><p>This repository includes several Claude Code plugins that extend functionality with custom commands and agents. See the <a href=\"https://raw.githubusercontent.com/anthropics/claude-code/main/plugins/README.md\">plugins directory</a> for detailed documentation on available plugins.</p><p>We welcome your feedback. Use the  command to report issues directly within Claude Code, or file a <a href=\"https://github.com/anthropics/claude-code/issues\">GitHub issue</a>.</p><p>Join the <a href=\"https://anthropic.com/discord\">Claude Developers Discord</a> to connect with other developers using Claude Code. Get help, share feedback, and discuss your projects with the community.</p><h2>Data collection, usage, and retention</h2><p>When you use Claude Code, we collect feedback, which includes usage data (such as code acceptance or rejections), associated conversation data, and user feedback submitted via the  command.</p><p>We have implemented several safeguards to protect your data, including limited retention periods for sensitive information, restricted access to user session data, and clear policies against using feedback for model training.</p>",
      "contentLength": 1892,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "github/copilot-cli",
      "url": "https://github.com/github/copilot-cli",
      "date": 1769222533,
      "author": "",
      "guid": 38480,
      "unread": true,
      "content": "<p>GitHub Copilot CLI brings the power of Copilot coding agent directly to your terminal.</p><p>The power of GitHub Copilot, now in your terminal.</p><p>GitHub Copilot CLI brings AI-powered coding assistance directly to your command line, enabling you to build, debug, and understand code through natural language conversations. Powered by the same agentic harness as GitHub's Copilot coding agent, it provides intelligent assistance while staying deeply integrated with your GitHub workflow.</p><h2>üöÄ Introduction and Overview</h2><p>We're bringing the power of GitHub Copilot coding agent directly to your terminal. With GitHub Copilot CLI, you can work locally and synchronously with an AI agent that understands your code and GitHub context.</p><ul><li><strong>Terminal-native development:</strong> Work with Copilot coding agent directly in your command line ‚Äî no context switching required.</li><li><strong>GitHub integration out of the box:</strong> Access your repositories, issues, and pull requests using natural language, all authenticated with your existing GitHub account.</li><li> Build, edit, debug, and refactor code with an AI collaborator that can plan and execute complex tasks.</li><li><strong>MCP-powered extensibility:</strong> Take advantage of the fact that the coding agent ships with GitHub's MCP server by default and supports custom MCP servers to extend capabilities.</li><li> Preview every action before execution ‚Äî nothing happens without your explicit approval.</li></ul><p>We're still early in our journey, but with your feedback, we're rapidly iterating to make the GitHub Copilot CLI the best possible companion in your terminal.</p><ul><li>(On Windows)  v6 or higher</li></ul><pre><code>winget install GitHub.Copilot\n</code></pre><pre><code>winget install GitHub.Copilot.Prerelease\n</code></pre><pre><code>brew install copilot-cli@prerelease\n</code></pre><p>Install with <a href=\"https://www.npmjs.com/package/@github/copilot\">npm</a> (macOS, Linux, and Windows):</p><pre><code>npm install -g @github/copilot\n</code></pre><pre><code>npm install -g @github/copilot@prerelease\n</code></pre><p>Install with the install script (macOS and Linux):</p><pre><code>curl -fsSL https://gh.io/copilot-install | bash\n</code></pre><pre><code>wget -qO- https://gh.io/copilot-install | bash\n</code></pre><p>Use  to run as root and install to .</p><p>Set  to install to  directory. Defaults to  when run as root or  when run as a non-root user.</p><p>Set  to install a specific version. Defaults to the latest version.</p><p>For example, to install version  to a custom directory:</p><pre><code>curl -fsSL https://gh.io/copilot-install | VERSION=\"v0.0.369\" PREFIX=\"$HOME/custom\" bash\n</code></pre><p>On first launch, you'll be greeted with our adorable animated banner! If you'd like to see this banner again, launch  with the  flag.</p><p>If you're not currently logged in to GitHub, you'll be prompted to use the  slash command. Enter this command and follow the on-screen instructions to authenticate.</p><h4>Authenticate with a Personal Access Token (PAT)</h4><p>You can also authenticate using a fine-grained PAT with the \"Copilot Requests\" permission enabled.</p><p>Launch  in a folder that contains code you want to work with.</p><p>By default,  utilizes Claude Sonnet 4.5. Run the  slash command to choose from other available models, including Claude Sonnet 4 and GPT-5.</p><p>Each time you submit a prompt to GitHub Copilot CLI, your monthly quota of premium requests is reduced by one. For information about premium requests, see <a href=\"https://docs.github.com/copilot/managing-copilot/monitoring-usage-and-entitlements/about-premium-requests\">About premium requests</a>.</p><h2>üì¢ Feedback and Participation</h2><p>We're excited to have you join us early in the Copilot CLI journey.</p><p>This is an early-stage preview, and we're building quickly. Expect frequent updates--please keep your client up to date for the latest features and fixes!</p><p>Your insights are invaluable! Open issue in this repo, join Discussions, and run  from the CLI to submit a confidential feedback survey!</p>",
      "contentLength": 3470,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "microsoft/VibeVoice",
      "url": "https://github.com/microsoft/VibeVoice",
      "date": 1769222533,
      "author": "",
      "guid": 38481,
      "unread": true,
      "content": "<p>Open-Source Frontier Voice AI</p><div align=\"left\"><p><strong>2026-01-21: üì£ We open-sourced <a href=\"https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md\"></a>, a unified speech-to-text model designed to handle 60-minute long-form audio in a single pass, generating structured transcriptions containing Who (Speaker), When (Timestamps), and What (Content), with support for User-Customized Context. Try it in <a href=\"https://aka.ms/vibevoice-asr\">Playground</a></strong>.</p><p>2025-12-16: üì£ We added experimental speakers to <a href=\"https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md\"></a> for exploration, including multilingual voices in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) and 11 distinct English style voices. <a href=\"https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#optional-more-experimental-voices\">Try it</a>. More speaker types will be added over time.</p><p>2025-12-03: üì£ We open-sourced <a href=\"https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md\"></a>, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on <a href=\"https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb\">Colab</a>.</p><p>2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have removed the VibeVoice-TTS code from this repository.</p><p>2025-08-25: üì£ We open-sourced <a href=\"https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md\"></a>, a long-form multi-speaker text-to-speech model that can synthesize speech up to 90 minutes long with up to 4 distinct speakers.</p></div><p>VibeVoice is a <strong>family of open-source frontier voice AI models</strong> that includes both Text-to-Speech (TTS) and Automatic Speech Recognition (ASR) models.</p><p>A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of . These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a <a href=\"https://arxiv.org/abs/2412.08635\">next-token diffusion</a> framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.</p><p>For more information, demos, and examples, please visit our <a href=\"https://microsoft.github.io/VibeVoice\">Project Page</a>.</p><p> is a unified speech-to-text model designed to handle <strong>60-minute long-form audio</strong> in a single pass, generating structured transcriptions containing <strong>Who (Speaker), When (Timestamps), and What (Content)</strong>, with support for .</p><ul><li><p><strong>üïí 60-minute Single-Pass Processing</strong>: Unlike conventional ASR models that slice audio into short chunks (often losing global context), VibeVoice ASR accepts up to  of continuous audio input within 64K token length. This ensures consistent speaker tracking and semantic coherence across the entire hour.</p></li><li><p>: Users can provide customized hotwords (e.g., specific names, technical terms, or background info) to guide the recognition process, significantly improving accuracy on domain-specific content.</p></li><li><p><strong>üìù Rich Transcription (Who, When, What)</strong>: The model jointly performs ASR, diarization, and timestamping, producing a structured output that indicates  said  and .</p></li></ul><p>: Long-form conversational audio, podcasts, multi-speaker dialogues</p><ul><li><p><strong>‚è±Ô∏è 90-minute Long-form Generation</strong>: Synthesizes conversational/single-speaker speech up to  in a single pass, maintaining speaker consistency and semantic coherence throughout.</p></li><li><p>: Supports up to  in a single conversation, with natural turn-taking and speaker consistency across long dialogues.</p></li><li><p>: Generates expressive, natural-sounding speech that captures conversational dynamics and emotional nuances.</p></li><li><p>: Supports English, Chinese and other languages.</p></li></ul><p><strong>Long Conversation with 4 people</strong></p><p>VibeVoice-Realtime is a  text-to-speech model supporting  and <strong>robust long-form speech generation</strong>.</p><ul><li>Parameter size: 0.5B (deployment-friendly)</li><li>Real-time TTS (~300 milliseconds first audible latency)</li><li>Robust long-form speech generation (~10 minutes)</li></ul><p>While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.</p><p>We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.</p>",
      "contentLength": 4681,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "browser-use/browser-use",
      "url": "https://github.com/browser-use/browser-use",
      "date": 1769222533,
      "author": "",
      "guid": 38482,
      "unread": true,
      "content": "<p>üåê Make websites accessible for AI agents. Automate tasks online with ease.</p><p>üå§Ô∏è Want to skip the setup? Use our  for faster, scalable, stealth-enabled browser automation!</p><ol><li>Direct your favorite coding agent (Cursor, Claude Code, etc) to <a href=\"https://docs.browser-use.com/llms-full.txt\">Agents.md</a></li></ol><p><strong>1. Create environment with <a href=\"https://docs.astral.sh/uv/\">uv</a> (Python&gt;=3.11):</strong></p><p><strong>2. Install Browser-Use package:</strong></p><pre><code>#  We ship every day - use the latest version!\nuv add browser-use\nuv sync\n</code></pre><p><strong>3. Get your API key from <a href=\"https://cloud.browser-use.com/new-api-key\">Browser Use Cloud</a> and add it to your  file (new signups get $10 free credits):</strong></p><pre><code># .env\nBROWSER_USE_API_KEY=your-key\n</code></pre><p><strong>4. Install Chromium browser:</strong></p><pre><code>from browser_use import Agent, Browser, ChatBrowserUse\nimport asyncio\n\nasync def example():\n    browser = Browser(\n        # use_cloud=True,  # Uncomment to use a stealth browser on Browser Use Cloud\n    )\n\n    llm = ChatBrowserUse()\n\n    agent = Agent(\n        task=\"Find the number of stars of the browser-use repo\",\n        llm=llm,\n        browser=browser,\n    )\n\n    history = await agent.run()\n    return history\n\nif __name__ == \"__main__\":\n    history = asyncio.run(example())\n</code></pre><p>We handle agents, browsers, persistence, auth, cookies, and LLMs. The agent runs right next to the browser for minimal latency.</p><pre><code>from browser_use import Browser, sandbox, ChatBrowserUse\nfrom browser_use.agent.service import Agent\nimport asyncio\n\n@sandbox()\nasync def my_task(browser: Browser):\n    agent = Agent(task=\"Find the top HN post\", browser=browser, llm=ChatBrowserUse())\n    await agent.run()\n\n# Just call it like any async function\nasyncio.run(my_task())\n</code></pre><p><strong>Want to get started even faster?</strong> Generate a ready-to-run template:</p><pre><code>uvx browser-use init --template default\n</code></pre><p>This creates a  file with a working example. Available templates:</p><ul><li> - Minimal setup to get started quickly</li><li> - All configuration options with detailed comments</li><li> - Examples of custom tools and extending the agent</li></ul><p>You can also specify a custom output path:</p><pre><code>uvx browser-use init --template default --output my_agent.py\n</code></pre><p>Fast, persistent browser automation from the command line:</p><pre><code>browser-use open https://example.com    # Navigate to URL\nbrowser-use state                       # See clickable elements\nbrowser-use click 5                     # Click element by index\nbrowser-use type \"Hello\"                # Type text\nbrowser-use screenshot page.png         # Take screenshot\nbrowser-use close                       # Close browser\n</code></pre><p>The CLI keeps the browser running between commands for fast iteration. See <a href=\"https://raw.githubusercontent.com/browser-use/browser-use/main/browser_use/skill_cli/README.md\">CLI docs</a> for all commands.</p><p>For <a href=\"https://claude.ai/code\">Claude Code</a>, install the skill to enable AI-assisted browser automation:</p><pre><code>mkdir -p ~/.claude/skills/browser-use\ncurl -o ~/.claude/skills/browser-use/SKILL.md \\\n  https://raw.githubusercontent.com/browser-use/browser-use/main/skills/browser-use/SKILL.md\n</code></pre><h4>Task = \"Fill in this job application with my resume and information.\"</h4><h4>Task = \"Put this list of items into my instacart.\"</h4><h4>Task = \"Help me find parts for a custom PC.\"</h4><h2>Integrations, hosting, custom tools, MCP, and more on our <a href=\"https://docs.browser-use.com\">Docs ‚Üó</a></h2><div align=\"center\">\n  Made with ‚ù§Ô∏è in Zurich and San Francisco \n</div>",
      "contentLength": 2973,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "ai-dynamo/dynamo",
      "url": "https://github.com/ai-dynamo/dynamo",
      "date": 1769222533,
      "author": "",
      "guid": 38483,
      "unread": true,
      "content": "<p>A Datacenter Scale Distributed Inference Serving Framework</p><p>High-throughput, low-latency inference framework designed for serving generative AI and reasoning models in multi-node distributed environments.</p><p>Large language models exceed single-GPU capacity. Tensor parallelism spreads layers across GPUs but creates coordination challenges. Dynamo closes this orchestration gap.</p><p>Dynamo is inference engine agnostic (supports TRT-LLM, vLLM, SGLang) and provides:</p><ul><li><strong>Disaggregated Prefill &amp; Decode</strong> ‚Äì Maximizes GPU throughput with latency/throughput trade-offs</li><li> ‚Äì Optimizes performance based on fluctuating demand</li><li><strong>LLM-Aware Request Routing</strong> ‚Äì Eliminates unnecessary KV cache re-computation</li><li><strong>Accelerated Data Transfer</strong> ‚Äì Reduces inference response time using NIXL</li><li> ‚Äì Leverages multiple memory hierarchies for higher throughput</li></ul><p>Built in Rust for performance and Python for extensibility, Dynamo is fully open-source with an OSS-first development approach.</p><blockquote><p> ‚Äî Detailed compatibility including LoRA, Request Migration, Speculative Decoding, and feature interactions.</p></blockquote><p>Want to help shape the future of distributed LLM inference? We welcome contributors at all levels‚Äîfrom doc fixes to new features.</p><p>The Dynamo team recommends the  Python package manager, although any way works. Install uv:</p><pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre><h3>Install Python Development Headers</h3><p>Backend engines require Python development headers for JIT compilation. Install them with:</p><pre><code>sudo apt install python3-dev\n</code></pre><p>We publish Python wheels specialized for each of our supported engines: vllm, sglang, and trtllm. The examples that follow use SGLang; continue reading for other engines.</p><pre><code>uv venv venv\nsource venv/bin/activate\nuv pip install pip\n\n# Choose one\nuv pip install \"ai-dynamo[sglang]\"  #replace with [vllm], [trtllm], etc.\n</code></pre><p>Before trying out Dynamo, you can verify your system configuration and dependencies:</p><pre><code>python3 deploy/sanity_check.py\n</code></pre><p>This is a quick check for system resources, development tools, LLM frameworks, and Dynamo components.</p><h3>Running an LLM API Server</h3><p>Dynamo provides a simple way to spin up a local set of inference components including:</p><ul><li><strong>OpenAI Compatible Frontend</strong> ‚Äì High performance OpenAI compatible http api server written in Rust.</li><li><strong>Basic and Kv Aware Router</strong> ‚Äì Route and load balance traffic to a set of workers.</li><li> ‚Äì Set of pre-configured LLM serving engines.</li></ul><pre><code># Start an OpenAI compatible HTTP server with prompt templating, tokenization, and routing.\n# For local dev: --store-kv file avoids etcd (workers and frontend must share a disk)\npython3 -m dynamo.frontend --http-port 8000 --store-kv file\n\n# Start the SGLang engine. You can run several of these for the same or different models.\n# The frontend will discover them automatically.\npython3 -m dynamo.sglang --model-path deepseek-ai/DeepSeek-R1-Distill-Llama-8B --store-kv file\n</code></pre><blockquote><p> vLLM workers publish KV cache events by default, which requires NATS. For dependency-free local development with vLLM, add <code>--kv-events-config '{\"enable_kv_cache_events\": false}'</code>. This keeps local prefix caching enabled while disabling event publishing. See <a href=\"https://raw.githubusercontent.com/ai-dynamo/dynamo/main/#service-discovery-and-messaging\">Service Discovery and Messaging</a> for details.</p></blockquote><pre><code>curl localhost:8000/v1/chat/completions   -H \"Content-Type: application/json\"   -d '{\n    \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n    \"messages\": [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello, how are you?\"\n    }\n    ],\n    \"stream\":false,\n    \"max_tokens\": 300\n  }' | jq\n</code></pre><p>Rerun with  and change  in the request to  to get the responses as soon as the engine issues them.</p><p>For production deployments on Kubernetes clusters with multiple GPUs.</p><p>Pre-built deployment configurations for common models and topologies:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>Dynamo is inference engine agnostic. Install the wheel for your chosen engine and run with <code>python3 -m dynamo.&lt;engine&gt; --help</code>.</p><table><thead><tr></tr></thead><tbody><tr><td><code>uv pip install ai-dynamo[vllm]</code></td><td>Broadest feature coverage</td></tr><tr><td><code>uv pip install ai-dynamo[sglang]</code></td></tr><tr><td><code>pip install --pre --extra-index-url https://pypi.nvidia.com ai-dynamo[trtllm]</code></td></tr></tbody></table><blockquote><p> TensorRT-LLM requires  (not ) due to URL-based dependencies. See the <a href=\"https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/backends/trtllm/\">TRT-LLM guide</a> for container setup and prerequisites.</p></blockquote><p>Use  to specify which GPUs to use. Engine-specific options (context length, multi-GPU, etc.) are documented in each backend guide.</p><h2>Service Discovery and Messaging</h2><p>Dynamo uses TCP for inter-component communication. External services are optional for most deployments:</p><table><tbody><tr><td>K8s-native discovery; TCP request plane</td></tr><tr><td>Pass ; vLLM also needs <code>--kv-events-config '{\"enable_kv_cache_events\": false}'</code></td></tr><tr><td>Prefix caching enabled by default requires NATS</td></tr></tbody></table><p>For local development without external dependencies, pass  (avoids etcd) to both the frontend and workers. vLLM users should also pass <code>--kv-events-config '{\"enable_kv_cache_events\": false}'</code> to disable KV event publishing (avoids NATS) while keeping local prefix caching enabled; SGLang and TRT-LLM don't require this flag.</p><p>For distributed non-Kubernetes deployments or KV-aware routing:</p><ul><li><a href=\"https://etcd.io/\">etcd</a> can be run directly as .</li><li><a href=\"https://nats.io/\">nats</a> needs JetStream enabled: .</li></ul><p>To quickly setup both: <code>docker compose -f deploy/docker-compose.yml up -d</code></p><p>Dynamo provides comprehensive benchmarking tools:</p><h2>Frontend OpenAPI Specification</h2><p>The OpenAI-compatible frontend exposes an OpenAPI 3 spec at . To generate without running the server:</p><pre><code>cargo run -p dynamo-llm --bin generate-frontend-openapi\n</code></pre><p>This writes to <code>docs/frontends/openapi.json</code>.</p><p>For contributors who want to build Dynamo from source rather than installing from PyPI.</p><pre><code>sudo apt install -y build-essential libhwloc-dev libudev-dev pkg-config libclang-dev protobuf-compiler python3-dev cmake\n</code></pre><pre><code># if brew is not installed on your system, install it\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre><pre><code>brew install cmake protobuf\n\n## Check that Metal is accessible\nxcrun -sdk macosx metal\n</code></pre><p>If Metal is accessible, you should see an error like <code>metal: error: no input files</code>, which confirms it is installed correctly.</p><pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource $HOME/.cargo/env\n</code></pre><h2>3. Create a Python Virtual Environment</h2><p>Follow the instructions in <a href=\"https://docs.astral.sh/uv/#installation\">uv installation</a> guide to install uv if you don't have  installed. Once uv is installed, create a virtual environment and activate it.</p><pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre><ul><li>Create a virtual environment</li></ul><pre><code>uv venv dynamo\nsource dynamo/bin/activate\n</code></pre><pre><code>uv pip install pip maturin\n</code></pre><p><a href=\"https://github.com/PyO3/maturin\">Maturin</a> is the Rust&lt;-&gt;Python bindings build tool.</p><h2>5. Build the Rust Bindings</h2><pre><code>cd lib/bindings/python\nmaturin develop --uv\n</code></pre><h2>6. Install GPU Memory Service</h2><p>The GPU Memory Service is a Python package with a C++ extension. It requires only Python development headers and a C++ compiler (g++).</p><pre><code>cd $PROJECT_ROOT\nuv pip install -e lib/gpu_memory_service\n</code></pre><pre><code>cd $PROJECT_ROOT\nuv pip install -e .\n</code></pre><p>You should now be able to run <code>python3 -m dynamo.frontend</code>.</p><p>For local development, pass  to avoid external dependencies (see Service Discovery and Messaging section).</p><p>Set the environment variable  to adjust the logging level; for example, . It has the same syntax as .</p><p>If you use vscode or cursor, we have a .devcontainer folder built on <a href=\"https://code.visualstudio.com/docs/devcontainers/containers\">Microsofts Extension</a>. For instructions see the <a href=\"https://raw.githubusercontent.com/ai-dynamo/dynamo/main/.devcontainer/README.md\">ReadMe</a> for more details.</p>",
      "contentLength": 7063,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "OpenBMB/UltraRAG",
      "url": "https://github.com/OpenBMB/UltraRAG",
      "date": 1769222533,
      "author": "",
      "guid": 38484,
      "unread": true,
      "content": "<p>UltraRAG v3: A Low-Code MCP Framework for Building Complex and Innovative RAG Pipelines</p><h3 align=\"center\"> Less Code, Lower Barrier, Faster Deployment </h3><ul><li>[2026.01.23] üéâ UltraRAG 3.0 Released: Say no to \"black box\" development‚Äîmake every line of reasoning logic clearly visible üëâ|<a href=\"https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/ultrarag3_0.md\">üìñ Blog</a>|</li><li>[2026.01.20] üéâ AgentCPM-Report Model Released! DeepResearch is finally localized: 8B on-device writing agent AgentCPM-Report is open-sourced üëâ |<a href=\"https://huggingface.co/openbmb/AgentCPM-Report\">ü§ó Model</a>|</li></ul><p>Designed for research exploration and industrial prototyping, UltraRAG standardizes core RAG components (Retriever, Generation, etc.) as independent , combined with the powerful workflow orchestration capabilities of the . Developers can achieve precise orchestration of complex control structures such as conditional branches and loops simply through YAML configuration.</p><p>UltraRAG UI transcends the boundaries of traditional chat interfaces, evolving into a visual RAG Integrated Development Environment (IDE) that combines orchestration, debugging, and demonstration.</p><p>The system features a powerful built-in Pipeline Builder that supports bidirectional real-time synchronization between \"Canvas Construction\" and \"Code Editing,\" allowing for granular online adjustments of pipeline parameters and prompts. Furthermore, it introduces an Intelligent AI Assistant to empower the entire development lifecycle, from pipeline structural design to parameter tuning and prompt generation. Once constructed, logic flows can be converted into interactive dialogue systems with a single click. The system seamlessly integrates Knowledge Base Management components, enabling users to build custom knowledge bases for document Q&amp;A. This truly realizes a one-stop closed loop, spanning from underlying logic construction and data governance to final application deployment.</p><ul><li><p>üöÄ <strong>Low-Code Orchestration of Complex Workflows</strong></p><ul><li>: Natively supports control structures such as sequential, loop, and conditional branches. Developers only need to write YAML configuration files to implement complex iterative RAG logic in dozens of lines of code.</li></ul></li><li><p>‚ö° <strong>Modular Extension and Reproduction</strong></p><ul><li>: Based on the MCP architecture, functions are decoupled into independent Servers. New features only need to be registered as function-level Tools to seamlessly integrate into workflows, achieving extremely high reusability.</li></ul></li><li><p>üìä <strong>Unified Evaluation and Benchmark Comparison</strong></p><ul><li>: Built-in standardized evaluation workflows, ready-to-use mainstream research benchmarks. Through unified metric management and baseline integration, significantly improves experiment reproducibility and comparison efficiency.</li></ul></li><li><p>‚ú® <strong>Rapid Interactive Prototype Generation</strong></p><ul><li>: Say goodbye to tedious UI development. With just one command, Pipeline logic can be instantly converted into an interactive conversational Web UI, shortening the distance from algorithm to demonstration.</li></ul></li></ul><p>We provide two installation methods: local source code installation (recommended using  for package management) and Docker container deployment</p><h3>Method 1: Source Code Installation</h3><p>We strongly recommend using <a href=\"https://github.com/astral-sh/uv\">uv</a> to manage Python environments and dependencies, as it can greatly improve installation speed.</p><p>If you haven't installed uv yet, please execute:</p><pre><code>## Direct installation\npip install uv\n## Download\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre><pre><code>git clone https://github.com/OpenBMB/UltraRAG.git --depth 1\ncd UltraRAG\n</code></pre><p>Choose one of the following modes to install dependencies based on your use case:</p><p><strong>A: Create a New Environment</strong> Use  to automatically create a virtual environment and synchronize dependencies:</p><ul><li><p>Core dependencies: If you only need to run basic core functions, such as only using UltraRAG UI:</p></li><li><p>Full installation: If you want to fully experience UltraRAG's retrieval, generation, corpus processing, and evaluation functions, please run:</p></li><li><p>On-demand installation: If you only need to run specific modules, keep the corresponding  as needed, for example:</p><pre><code>uv sync --extra retriever   # Retrieval module only\nuv sync --extra generation  # Generation module only\n</code></pre></li></ul><p>Once installed, activate the virtual environment:</p><pre><code># Windows CMD\n.venv\\Scripts\\activate.bat\n\n# Windows Powershell\n.venv\\Scripts\\Activate.ps1\n\n# macOS / Linux\nsource .venv/bin/activate\n</code></pre><p><strong>B: Install into an Existing Environment</strong> To install UltraRAG into your currently active Python environment, use :</p><pre><code># Core dependencies\nuv pip install -e .\n\n# Full installation\nuv pip install -e \".[all]\"\n\n# On-demand installation\nuv pip install -e \".[retriever]\"\n</code></pre><h3>Method 2: Docker Container Deployment</h3><p>If you prefer not to configure a local Python environment, you can deploy using Docker.</p><pre><code># 1. Clone the repository\ngit clone https://github.com/OpenBMB/UltraRAG.git --depth 1\ncd UltraRAG\n\n# 2. Prepare the image (choose one)\n# Option A: Pull from Docker Hub\ndocker pull hdxin2002/ultrarag:v0.3.0-base-cpu # Base version (CPU)\ndocker pull hdxin2002/ultrarag:v0.3.0-base-gpu # Base version (GPU)\ndocker pull hdxin2002/ultrarag:v0.3.0          # Full version (GPU)\n\n# Option B: Build locally\ndocker build -t ultrarag:v0.3.0 .\n\n# 3. Start container (port 5050 is automatically mapped)\ndocker run -it --gpus all -p 5050:5050 &lt;docker_image_name&gt;\n</code></pre><pre><code># Start the container (Port 5050 is mapped by default)\ndocker run -it --gpus all -p 5050:5050 &lt;docker_image_name&gt;\n</code></pre><p>Note: After the container starts, UltraRAG UI will run automatically. You can directly access  in your browser to use it.</p><p>After installation, run the following example command to check if the environment is normal:</p><pre><code>ultrarag run examples/sayhello.yaml\n</code></pre><p>If you see the following output, the installation is successful:</p><p>We provide complete tutorial examples from beginner to advanced. Whether you are conducting academic research or building industrial applications, you can find guidance here. Welcome to visit the <a href=\"https://ultrarag.openbmb.cn/pages/en/getting_started/introduction\">Documentation</a> for more details.</p><p>Designed for researchers, providing data, experimental workflows, and visualization analysis tools.</p><ul><li><a href=\"https://ultrarag.openbmb.cn/pages/en/getting_started/quick_start\">Getting Started</a>: Learn how to quickly run standard RAG experimental workflows based on UltraRAG.</li><li><a href=\"https://ultrarag.openbmb.cn/pages/en/develop_guide/dataset\">Evaluation Data</a>: Download the most commonly used public evaluation datasets in the RAG field and large-scale retrieval corpora, directly for research benchmark testing.</li><li><a href=\"https://ultrarag.openbmb.cn/pages/en/develop_guide/case_study\">Case Analysis</a>: Provides a visual Case Study interface to deeply track each intermediate output of the workflow, assisting in analysis and error attribution.</li><li><a href=\"https://ultrarag.openbmb.cn/pages/en/develop_guide/code_integration\">Code Integration</a>: Learn how to directly call UltraRAG components in Python code to achieve more flexible customized development.</li></ul><p>Designed for developers and end users, providing complete UI interaction and complex application cases.</p><ul><li><a href=\"https://ultrarag.openbmb.cn/pages/en/ui/start\">Quick Start</a>: Learn how to start UltraRAG UI and familiarize yourself with various advanced configurations in administrator mode.</li><li><a href=\"https://ultrarag.openbmb.cn/pages/en/ui/prepare\">Deployment Guide</a>: Detailed production environment deployment tutorials, covering the setup of Retriever, Generation models (LLM), and Milvus vector database.</li><li><a href=\"https://ultrarag.openbmb.cn/pages/en/demo/deepresearch\">Deep Research</a>: Flagship case, deploy a Deep Research Pipeline. Combined with the AgentCPM-Report model, it can automatically perform multi-step retrieval and integration to generate tens of thousands of words of survey reports.</li></ul><p>Thanks to the following contributors for their code submissions and testing. We also welcome new members to join us in collectively building a comprehensive RAG ecosystem!</p><p>You can contribute by following the standard process: <strong>Fork this repository ‚Üí Submit Issues ‚Üí Create Pull Requests (PRs)</strong>.</p><a href=\"https://github.com/OpenBMB/UltraRAG/contributors\"><img src=\"https://contrib.rocks/image?repo=OpenBMB/UltraRAG&amp;nocache=true\"></a><p>If you find this repository helpful for your research, please consider giving us a ‚≠ê to show your support.</p><a href=\"https://star-history.com/#OpenBMB/UltraRAG&amp;Date\"></a><ul><li>For technical issues and feature requests, please use <a href=\"https://github.com/OpenBMB/UltraRAG/issues\">GitHub Issues</a>.</li><li>For questions about usage, feedback, or any discussions related to RAG technologies, you are welcome to join our <a href=\"https://github.com/OpenBMB/UltraRAG/raw/main/docs/wechat_qr.png\">WeChat group</a>, <a href=\"https://github.com/OpenBMB/UltraRAG/raw/main/docs/feishu_qr.png\">Feishu group</a>, and <a href=\"https://discord.gg/yRFFjjJnnS\">Discord</a> to exchange ideas with us.</li></ul>",
      "contentLength": 7684,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "deepseek-ai/FlashMLA",
      "url": "https://github.com/deepseek-ai/FlashMLA",
      "date": 1769136361,
      "author": "",
      "guid": 38149,
      "unread": true,
      "content": "<p>FlashMLA: Efficient Multi-head Latent Attention Kernels</p><p>FlashMLA is DeepSeek's library of optimized attention kernels, powering the <a href=\"https://github.com/deepseek-ai/DeepSeek-V3\">DeepSeek-V3</a> and <a href=\"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp\">DeepSeek-V3.2-Exp</a> models. This repository contains the following implementations:</p><p><em>These kernels power DeepSeek Sparse Attention (DSA), as introduced in <a href=\"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp\">this paper</a>.</em></p><ul><li>Token-level sparse attention for the prefill stage</li><li>Token-level sparse attention for the decoding stage, with FP8 KV cache</li></ul><ul><li>Dense attention for the prefill stage</li><li>Dense attention for the decoding stage</li></ul><ul><li><strong>2025.09.29 Release of Sparse Attention Kernels</strong>: With the launch of <a href=\"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp\">DeepSeek-V3.2</a>, we are releasing the corresponding token-level sparse attention kernels. These kernels power the model's DeepSeek Sparse Attention (DSA) and achieve up to 640 TFlops during prefilling and 410 TFlops during decoding. We also release a deep-dive blog for our new FP8 sparse decoding kernel. Check it out <a href=\"https://raw.githubusercontent.com/deepseek-ai/FlashMLA/main/docs/20250929-hopper-fp8-sparse-deep-dive.md\">here</a>.</li><li><strong>2025.08.01 Kernels for MHA on SM100</strong>: Thanks to <a href=\"https://github.com/deepseek-ai/FlashMLA/pull/76\">NVIDIA's PR</a> for MHA forward / backward kernels on SM100!</li><li><strong>2025.04.22 Deep-Dive Blog</strong>: We'd love to share the technical details behind the new FlashMLA kernel! Check out our deep-dive write-up <a href=\"https://raw.githubusercontent.com/deepseek-ai/FlashMLA/main/docs/20250422-new-kernel-deep-dive.md\">here</a>.</li><li><strong>2025.04.22 Performance Update</strong>: We're excited to announce the new release of Flash MLA, which delivers 5% ~ 15% performance improvement for compute-bound workloads, achieving up to 660 TFlops on NVIDIA H800 SXM5 GPUs. The interface of the new version is fully compatible with the old one. Simply upgrade to the new version for an immediate performance boost! üöÄüöÄüöÄ</li></ul><h4>Test &amp; benchmark MLA decoding (Sparse &amp; Dense):</h4><pre><code>python tests/test_flash_mla_dense_decoding.py\npython tests/test_flash_mla_sparse_decoding.py\n</code></pre><p>The dense MLA decoding kernel achieves up to 3000 GB/s in memory-bound configuration and 660 TFLOPS in computation-bound configuration on H800 SXM5 with CUDA 12.8. The token-level sparse MLA decoding kernel (which uses an FP8 KV cache while performing the matrix multiplication in bfloat16) achieves 410 TFLOPS in compute-bound configuration on H800 SXM5 with CUDA 12.8, and achieves up to 350 TFlops on B200 (which is not really optimized yet).</p><h4>Test &amp; benchmark MHA prefill (Dense):</h4><pre><code>python tests/test_fmha_sm100.py\n</code></pre><p>It achieves up to 1460 TFlops in forward and 1000 TFlops in backward computation on B200, as reported by NVIDIA.</p><h4>Test &amp; benchmark MLA prefill (Sparse):</h4><pre><code>python tests/test_flash_mla_sparse_prefill.py\n</code></pre><p>It achieves up to 640 TFlops in forward computation on H800 SXM5 with CUDA 12.8, and achieves up to 1450 TFlops on B200, CUDA 12.9.</p><ul><li>SM90 / SM100 (See the support matrix below)</li><li>CUDA 12.8 and above (CUDA 12.9+ is required for SM100 kernels)</li></ul><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>[1]: For more details on using FP8 KV cache, see documents below.</p><p>[2]: Here \"MLA Mode\" refers to the mode used for MLA calculation. MQA stands for Multi-Query Attention mode (i.e.  = 576 with  = 512), while MHA stands for Multi-Head Attention mode (i.e.  = 192 / 128 with  = 128). For a detailed explanation of these modes, please refer to the appendix of <a href=\"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp\">DeepSeek V3.2's Paper</a>.</p><pre><code>git clone https://github.com/deepseek-ai/FlashMLA.git flash-mla\ncd flash-mla\ngit submodule update --init --recursive\npip install -v .\n</code></pre><p>To use the MLA decoding kernels, call get_mla_metadata once before the decoding loop to get the tile scheduler metadata. Then, call flash_mla_with_kvcache in each decoding step. For example:</p><pre><code>from flash_mla import get_mla_metadata, flash_mla_with_kvcache\n\ntile_scheduler_metadata, num_splits = get_mla_metadata(\n    cache_seqlens,\n    s_q * h_q // h_kv,\n    h_kv,\n    h_q,\n    is_fp8,\n    topk,\n)\n\nfor i in range(num_layers):\n    ...\n    o_i, lse_i = flash_mla_with_kvcache(\n        q_i, kvcache_i, block_table, cache_seqlens, dv,\n        tile_scheduler_metadata, num_splits,\n        is_causal, is_fp8_kvcache, indices,\n    )\n    ...\n</code></pre><ul><li> is the number of q tokens per q sequence. If MTP (speculative decoding) is disabled, it should be 1.</li><li> is the number of key-value heads.</li><li> is the number of query heads.</li></ul><p> If  is set to , the kernel reads the KV cache in the \"FP8 with scale\" format (described below). It dequantizes the cache to bfloat16 and performs attention computation in bfloat16. The output is also in bfloat16.</p><p>In the \"FP8 with scale\" format, each token's KV cache is 656 Bytes, structured as:</p><ul><li> The \"quantized NoPE\" part, containing 512  values.</li><li> Scale factors, containing 4  values. The first  is the scale for the first 128  values, the second for the next 128, and so on.</li><li> The \"RoPE\" part, containing 64  values. This part is not quantized for accuracy.</li></ul><p>See  for quantization and dequantization details.</p><p><strong>Sparse Attention ( tensor):</strong> The  tensor (if provided) enables token-level sparse attention by instructing the kernel to compute attention only for specified tokens.</p><ul><li> should be a 3D tensor of shape <code>(batch_size, seq_len_q, topk)</code>.</li><li><code>indices_in_kvcache[i][j][k] = (the index of the page block where token t resides) * page_block_size + (the offset of token t within the page block)</code>, where  is the k-th token for the j-th query sequence in the i-th batch. Since the index of the page block has already been encoded into , the kernel does not require the  parameter.</li><li> Set invalid indices to .</li></ul><p> The kernel returns , where:</p><ul><li> is the attention result.</li><li> is the log-sum-exp value of the attention scores for each query head.</li></ul><p>See <code>tests/test_flash_mla_decoding.py</code> for a complete example.</p><p>For the sparse MLA prefill kernel, call  directly with the following parameters:</p><ul><li>: Query tensor of shape </li><li>: Key-Value tensor of shape </li><li>: Indices tensor of shape </li></ul><p> This kernel does not support a batch dimension. For multi-batch inference, reshape the input tensors and adjust the  parameter to simulate batch processing.</p><p> Set invalid entries in  to  or any number .</p><p><strong>Return Values and Equivalent PyTorch Code:</strong> The kernel returns . This is equivalent to the following PyTorch operations:</p><pre><code>Q: [s_q, h_q, d_qk], bfloat16\nkv: [s_kv, h_kv, d_qk], bfloat16\nindices: [s_q, h_kv, topk], int32\n\nkv = kv.squeeze(1)  # [s_kv, d_qk], h_kv must be 1\nindices = indices.squeeze(1)    # [s_q, topk]\nfocused_kv = kv[indices]    # For the i-th sequence (s_q), the corresponding KV tokens are selected from the KV cache based on indices[i, :]. This operation results in a tensor of shape [s_q, topk, d_qk].\n\nP = (Q @ focused_kv.transpose(-1, -2)) * sm_scale * math.log2(math.e)    # [s_q, h_q, topk]\nmax_logits = P.max(dim=-1) # [s_q, h_q]\nlse = log2sumexp2(P, dim=-1, base=2)   # [s_q, h_q]Ôºå\"log2sumexp2\" means that the exponentiation and logarithm are base-2\nS = exp2(P - lse)      # [s_q, h_q, topk]\nout = S @ focused_kv  # [s_q, h_q, d_qk]\n\nreturn (out, max_logits, lse)\n</code></pre><p>See <code>tests/test_flash_mla_prefill.py</code> for a complete example.</p><p>This kernel implements the standard dense Multi-Head Attention (MHA) forward and backward operations. It can be called using:</p><ul><li><code>flash_attn_varlen_qkvpacked_func</code></li><li><code>flash_attn_varlen_kvpacked_func</code></li></ul><p>The usage is similar to the  package. See  for a complete example.</p><p>For MetaX GPUs, visit the official website: <a href=\"https://www.metax-tech.com\">MetaX</a>.</p><p>For the Moore Threads GPU, visit the official website: <a href=\"https://www.mthreads.com/\">Moore Threads</a>.</p><p>For the Intellifusion NNP, visit the official website: <a href=\"https://www.intellif.com\">Intellifusion</a>.</p><p>For AMD Instinct GPUs, visit the official website: <a href=\"https://www.amd.com/en/products/accelerators/instinct.html\">AMD Instinct</a>.</p><p>The corresponding FlashMLA version can be found at: <a href=\"https://github.com/ROCm/aiter/raw/main/aiter/mla.py\">AITER/MLA</a></p><pre><code>@misc{flashmla2025,\n      title={FlashMLA: Efficient Multi-head Latent Attention Kernels},\n      author={Jiashi Li, Shengyu Liu},\n      year={2025},\n      publisher = {GitHub},\n      howpublished = {\\url{https://github.com/deepseek-ai/FlashMLA}},\n}\n</code></pre>",
      "contentLength": 7389,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "remotion-dev/remotion",
      "url": "https://github.com/remotion-dev/remotion",
      "date": 1769136361,
      "author": "",
      "guid": 38150,
      "unread": true,
      "content": "<p>üé• Make videos programmatically with React</p><p>Remotion is a framework for <strong>creating videos programmatically using React.</strong></p><h2>Why create videos in React?</h2><ul><li><strong>Leverage web technologies</strong>: Use all of CSS, Canvas, SVG, WebGL, etc.</li><li>: Use variables, functions, APIs, math and algorithms to create new effects</li><li>: Reusable components, Powerful composition, Fast Refresh, Package ecosystem</li></ul><p>If you already have Node.JS installed, type</p><p>Be aware of that Remotion has a special license and requires obtaining a company license in some cases. Read the <a href=\"https://raw.githubusercontent.com/remotion-dev/remotion/main/LICENSE.md\">LICENSE</a> page for more information.</p>",
      "contentLength": 552,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "iOfficeAI/AionUi",
      "url": "https://github.com/iOfficeAI/AionUi",
      "date": 1769136361,
      "author": "",
      "guid": 38151,
      "unread": true,
      "content": "<p>Free, local, open-source Cowork for Gemini CLI, Claude Code, Codex, Opencode, Qwen Code, Goose Cli, Auggie, and more | üåü Star if you like it!</p><p align=\"center\"><strong>üöÄ Cowork with Your AI, Gemini CLI, Claude Code, Codex, Qwen Code, Goose CLI, Auggie, and more</strong><em>User-friendly | Visual graphical interface | Multi-model support | Local data security</em></p><h3>ü§ñ <strong>Multi-Agent Mode - Cowork for Your Command-Line AI Tools, Unified Graphical Interface</strong></h3><p><em>AionUi includes Gemini CLI built-in, ready to use out of the box with no extra installation. If you already have command-line tools like Gemini CLI, Claude Code, CodeX, Qwen Code, Goose AI, Augment Code installed, AionUi will auto-detect them and provide a unified graphical interface for a richer experience</em></p><ul><li>‚úÖ <strong>Auto Detection + Unified Interface</strong> - Automatically recognizes local CLI tools, provides a unified graphical interface, say goodbye to command line</li><li>‚úÖ <strong>Local Storage + Multi-Session</strong> - Conversations saved locally, supports multiple parallel sessions, each session with independent context</li></ul><h3>üìÅ <strong>Smart File Management (AI Cowork)</strong></h3><p><em>Batch renaming, automatic organization, smart classification, file merging</em></p><ul><li>: Intelligently identify content and auto-classify, keeping folders tidy.</li><li>: One-click rename, merge files, say goodbye to tedious manual tasks.</li></ul><h3>üìÑ <strong>Preview Panel - Quickly View AI-Generated Results</strong></h3><p><em>Supports 9+ formats of visual preview (PDF, Word, Excel, PPT, code, Markdown, images, HTML, Diff, etc.)</em></p><ul><li>‚úÖ  - After AI generates files, view preview immediately without switching apps</li><li>‚úÖ <strong>Real-time Tracking + Editable</strong> - Automatically tracks file changes, editor and preview sync intelligently; supports real-time editing of Markdown, code, HTML, WYSIWYG</li></ul><h3>üé® <strong>AI Image Generation &amp; Editing</strong></h3><p><em>Intelligent image generation, editing, and recognition, powered by Gemini</em></p><h3>üí¨ <strong>Multi-Task Parallel Processing</strong></h3><p><em>Open multiple conversations, tasks don't get mixed up, independent memory, double efficiency</em></p><h3>üåê <strong>Access Anywhere - WebUI Mode</strong></h3><p><em>Remotely control your AI tools - Access AionUi from any device on the network! Securely control local Gemini CLI, Claude Code, Codex, and other tools, data never leaves your device</em></p><pre><code># Basic startup\nAionUi --webui\n\n# Remote access (accessible from other devices on the local network)\nAionUi --webui --remote\n</code></pre><p><strong>Just like Claude Cowork makes Claude Code easier to use, AionUi is the Cowork platform for all your command-line AI tools</strong></p><p>Gemini CLI, Claude Code, Codex, Qwen Code are powerful, but share common pain points: conversations can't be saved, single-session limitations, cumbersome file operations, and only support a single model.</p><p>AionUi provides unified  for these command-line tools:</p><ul><li>üéØ  - One interface to manage all command-line AI tools, no switching needed; includes Gemini CLI, ready to use out of the box and completely free</li><li>üöÄ  - Not only supports Claude Code, but also Gemini CLI, Codex, Qwen Code, and more</li><li>üåê  - Full platform support for macOS, Windows, Linux (Claude Cowork currently only macOS)</li><li>üîÑ  - Flexibly switch between different models in the same interface, meeting different task requirements</li><li>üìÑ  - Visual preview for 9+ formats, immediately view the effects of AI-generated files</li><li>üíæ  - All conversations and files saved locally, data never leaves your device</li></ul><ul><li><strong>Multi-Session + Independent Context</strong> - Open multiple chats simultaneously, each session has independent context memory, no confusion</li><li> - All conversations are saved locally and will not be lost</li></ul><ul><li> - Supports mainstream models like Gemini, OpenAI, Claude, Qwen, flexible switching</li><li> - Supports local model deployment like Ollama, LM Studio, select Custom platform and set local API address (e.g., <code>http://localhost:11434/v1</code>) to connect</li><li><strong>Gemini 3 Subscription Optimization</strong> - Automatically identifies subscribed users, recommends advanced models</li></ul><ul><li><strong>File Tree Browsing + Drag &amp; Drop Upload</strong> - Browse files like folders, support drag and drop files or folders for one-click import</li><li> - You can let AI help organize folders, automatic classification</li></ul><h3>üìÑ <strong>Preview Panel - Give AI Agent a Display</strong></h3><ul><li> - Supports PDF, Word, Excel, PPT, code, Markdown, images, etc., view results immediately after AI generation</li><li><strong>Real-time Tracking + Editable</strong> - Automatically tracks file changes, supports real-time editing and debugging of Markdown, code, HTML</li></ul><h3>üé® <strong>AI Image Generation &amp; Editing</strong></h3><ul><li><strong>Intelligent Image Generation</strong> - Supports multiple image generation models like Gemini 2.5 Flash Image Preview, Nano, Banana</li><li><strong>Image Recognition &amp; Editing</strong> - AI-driven image analysis and editing features</li></ul><ul><li> - Access from any device on the network via browser, supports mobile devices</li><li> - All data stored locally in SQLite database, suitable for server deployment</li></ul><h3>üé® <strong>Personalized Interface Customization</strong></h3><p><em>Customize with your own CSS code, make your interface match your preferences</em></p><ul><li> - Freely customize interface colors, styles, layout through CSS code, create your exclusive experience</li></ul><ul><li>: Windows 10 or higher</li><li>: Ubuntu 18.04+ / Debian 10+ / Fedora 32+</li><li>: Recommended 4GB or more</li><li>: At least 500MB available space</li></ul><h3>üç∫ Install via Homebrew (macOS)</h3><ol><li> AionUi application</li><li> - Support Google account login or API Key authentication</li><li> - Immediately experience modern AI chat interface</li></ol><p> We highly value every user's suggestions and feedback. Whether it's feature ideas, user experience, or issues you encounter, feel free to contact us anytime!</p><p>Welcome to submit Issues and Pull Requests!</p><ol><li>Create a feature branch (<code>git checkout -b feature/AmazingFeature</code>)</li><li>Commit your changes (<code>git commit -m 'Add some AmazingFeature'</code>)</li><li>Push to the branch (<code>git push origin feature/AmazingFeature</code>)</li></ol><p>Thanks to all developers who have contributed to AionUi!</p>",
      "contentLength": 5559,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "block/goose",
      "url": "https://github.com/block/goose",
      "date": 1769136361,
      "author": "",
      "guid": 38152,
      "unread": true,
      "content": "<p>an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM</p><p>goose is your on-machine AI agent, capable of automating complex development tasks from start to finish. More than just code suggestions, goose can build entire projects from scratch, write and execute code, debug failures, orchestrate workflows, and interact with external APIs - .</p><p>Whether you're prototyping an idea, refining existing code, or managing intricate engineering pipelines, goose adapts to your workflow and executes tasks with precision.</p><p>Designed for maximum flexibility, goose works with any LLM and supports multi-model configuration to optimize performance and cost, seamlessly integrates with MCP servers, and is available as both a desktop app as well as CLI - making it the ultimate AI assistant for developers who want to move faster and focus on innovation.</p><blockquote><p>Why did the developer choose goose as their AI agent?</p><p>Because it always helps them \"migrate\" their code to production! üöÄ</p></blockquote>",
      "contentLength": 1017,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "mastra-ai/mastra",
      "url": "https://github.com/mastra-ai/mastra",
      "date": 1769136361,
      "author": "",
      "guid": 38153,
      "unread": true,
      "content": "<p>From the team behind Gatsby, Mastra is a framework for building AI-powered applications and agents with a modern TypeScript stack.</p><p>Mastra is a framework for building AI-powered applications and agents with a modern TypeScript stack.</p><p>It includes everything you need to go from early prototypes to production-ready applications. Mastra integrates with frontend and backend frameworks like React, Next.js, and Node, or you can deploy it anywhere as a standalone server. It's the easiest way to build, tune, and scale reliable AI products.</p><p>Purpose-built for TypeScript and designed around established AI patterns, Mastra gives you everything you need to build great AI applications out-of-the-box.</p><ul><li><p><a href=\"https://mastra.ai/models\"></a> - Connect to 40+ providers through one standard interface. Use models from OpenAI, Anthropic, Gemini, and more.</p></li><li><p><a href=\"https://mastra.ai/docs/agents/overview\"></a> - Build autonomous agents that use LLMs and tools to solve open-ended tasks. Agents reason about goals, decide which tools to use, and iterate internally until the model emits a final answer or an optional stopping condition is met.</p></li><li><p><a href=\"https://mastra.ai/docs/workflows/overview\"></a> - When you need explicit control over execution, use Mastra's graph-based workflow engine to orchestrate complex multi-step processes. Mastra workflows use an intuitive syntax for control flow (, , ).</p></li><li><p><a href=\"https://mastra.ai/docs/workflows/suspend-and-resume\"></a> - Suspend an agent or workflow and await user input or approval before resuming. Mastra uses <a href=\"https://mastra.ai/docs/server-db/storage\">storage</a> to remember execution state, so you can pause indefinitely and resume where you left off.</p></li><li><p> - Give your agents the right context at the right time. Provide <a href=\"https://mastra.ai/docs/memory/conversation-history\">conversation history</a>, <a href=\"https://mastra.ai/docs/rag/overview\">retrieve</a> data from your sources (APIs, databases, files), and add human-like <a href=\"https://mastra.ai/docs/memory/working-memory\">working</a> and <a href=\"https://mastra.ai/docs/memory/semantic-recall\">semantic</a> memory so your agents behave coherently.</p></li><li><p> - Bundle agents and workflows into existing React, Next.js, or Node.js apps, or ship them as standalone endpoints. When building UIs, integrate with agentic libraries like Vercel's AI SDK UI and CopilotKit to bring your AI assistant to life on the web.</p></li><li><p><a href=\"https://mastra.ai/docs/tools-mcp/mcp-overview\"></a> - Author Model Context Protocol servers, exposing agents, tools, and other structured resources via the MCP interface. These can then be accessed by any system or agent that supports the protocol.</p></li><li><p> - Shipping reliable agents takes ongoing insight, evaluation, and iteration. With built-in <a href=\"https://mastra.ai/docs/evals/overview\">evals</a> and <a href=\"https://mastra.ai/docs/observability/overview\">observability</a>, Mastra gives you the tools to observe, measure, and refine continuously.</p></li></ul><p>The  way to get started with Mastra is by running the command below:</p><p>Looking to contribute? All types of help are appreciated, from coding to testing and feature specification.</p><p>If you are a developer and would like to contribute with code, please open an issue to discuss before opening a Pull Request.</p><p>We have an <a href=\"https://discord.gg/BTYqqHKUrf\">open community Discord</a>. Come and say hello and let us know if you have any questions or need any help getting things running.</p><p>It's also super helpful if you leave the project a star here at the <a href=\"https://github.com/mastra-ai/mastra\">top of the page</a></p><p>We are committed to maintaining the security of this repo and of Mastra as a whole. If you discover a security finding we ask you to please responsibly disclose this to us at <a href=\"mailto:security@mastra.ai\">security@mastra.ai</a> and we will get back to you.</p>",
      "contentLength": 3032,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "nexmoe/VidBee",
      "url": "https://github.com/nexmoe/VidBee",
      "date": 1769136361,
      "author": "",
      "guid": 38154,
      "unread": true,
      "content": "<p>Download videos from almost any website worldwide</p><p>VidBee is a modern, open-source video downloader that lets you download videos and audios from 1000+ websites worldwide. Built with Electron and powered by yt-dlp, VidBee offers a clean, intuitive interface with powerful features for all your downloading needs, including RSS auto-download automation that automatically subscribes to feeds and downloads new videos from your favorite creators in the background.</p><p>VidBee is currently under active development, and feedback is welcome for any <a href=\"https://github.com/nexmoe/VidBee/issues\">issue</a> encountered.</p><blockquote><p>, You will receive all release notifications from GitHub without any delay ~</p></blockquote><a href=\"https://next.ossinsight.io/widgets/official/compose-last-28-days-stats?repo_id=1081230042\" target=\"_blank\" align=\"center\"></a><h3>üåç Global Video Download Support</h3><p>Download videos from almost any website worldwide through the powerful yt-dlp engine. Support for 1000+ sites including YouTube, TikTok, Instagram, Twitter, and many more.</p><h3>üé® Best-in-class UI Experience</h3><p>Modern, clean interface with intuitive operations. One-click pause/resume/retry, real-time progress tracking, and comprehensive download queue management.</p><p>Automatically subscribe to RSS feeds and auto-download new videos in the background from your favorite creators across YouTube, TikTok, and more. Set up RSS subscriptions once, and VidBee will automatically download new uploads without manual intervention, perfect for keeping up with your favorite channels and creators.</p><p>You are welcome to join the open source community to build together. For more details, check out:</p><p>This project is distributed under the MIT License. See <a href=\"https://raw.githubusercontent.com/nexmoe/VidBee/main/LICENSE\"></a> for details.</p><ul><li><a href=\"https://github.com/yt-dlp/yt-dlp\">yt-dlp</a> - The powerful video downloader engine</li><li><a href=\"https://ffmpeg.org/\">FFmpeg</a> - The multimedia framework for video and audio processing</li><li><a href=\"https://www.electronjs.org/\">Electron</a> - Build cross-platform desktop apps</li><li><a href=\"https://vitejs.dev/\">Vite</a> - Next generation frontend tooling</li></ul>",
      "contentLength": 1701,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "virattt/dexter",
      "url": "https://github.com/virattt/dexter",
      "date": 1769136361,
      "author": "",
      "guid": 38155,
      "unread": true,
      "content": "<p>An autonomous agent for deep financial research</p><p>Dexter is an autonomous financial research agent that thinks, plans, and learns as it works. It performs analysis using task planning, self-reflection, and real-time market data. Think Claude Code, but built specifically for financial research.</p><img width=\"1098\" height=\"659\" alt=\"Screenshot 2026-01-21 at 5 25 10‚ÄØPM\" src=\"https://github.com/user-attachments/assets/3bcc3a7f-b68a-4f5e-8735-9d22196ff76e\"><p>Dexter takes complex financial questions and turns them into clear, step-by-step research plans. It runs those tasks using live market data, checks its own work, and refines the results until it has a confident, data-backed answer.</p><ul><li><strong>Intelligent Task Planning</strong>: Automatically decomposes complex queries into structured research steps</li><li>: Selects and executes the right tools to gather financial data</li><li>: Checks its own work and iterates until tasks are complete</li><li>: Access to income statements, balance sheets, and cash flow statements</li><li>: Built-in loop detection and step limits to prevent runaway execution</li></ul><img width=\"875\" height=\"558\" alt=\"Screenshot 2026-01-21 at 5 22 19‚ÄØPM\" src=\"https://github.com/user-attachments/assets/72d28363-69ea-4c74-a297-dfa60aa347f7\"><ul><li><a href=\"https://bun.com\">Bun</a> runtime (v1.0 or higher)</li><li>Financial Datasets API key (get <a href=\"https://financialdatasets.ai\">here</a>)</li><li>Tavily API key (get <a href=\"https://tavily.com\">here</a>) - optional, for web search</li></ul><p>If you don't have Bun installed, you can install it using curl:</p><pre><code>curl -fsSL https://bun.com/install | bash\n</code></pre><pre><code>powershell -c \"irm bun.sh/install.ps1|iex\"\n</code></pre><p>After installation, restart your terminal and verify Bun is installed:</p><pre><code>git clone https://github.com/virattt/dexter.git\ncd dexter\n</code></pre><ol start=\"2\"><li>Install dependencies with Bun:</li></ol><ol start=\"3\"><li>Set up your environment variables:</li></ol><pre><code># Copy the example environment file (from parent directory)\ncp env.example .env\n\n# Edit .env and add your API keys (if using cloud providers)\n# OPENAI_API_KEY=your-openai-api-key\n# ANTHROPIC_API_KEY=your-anthropic-api-key\n# GOOGLE_API_KEY=your-google-api-key\n# XAI_API_KEY=your-xai-api-key\n\n# (Optional) If using Ollama locally\n# OLLAMA_BASE_URL=http://127.0.0.1:11434\n\n# Other required keys\n# FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key\n# TAVILY_API_KEY=your-tavily-api-key\n</code></pre><p>Run Dexter in interactive mode:</p><p>Or with watch mode for development:</p><ol></ol><p>: Please keep your pull requests small and focused. This will make it easier to review and merge.</p><p>This project is licensed under the MIT License.</p>",
      "contentLength": 2040,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "xai-org/grok-1",
      "url": "https://github.com/xai-org/grok-1",
      "date": 1769094695,
      "author": "",
      "guid": 37895,
      "unread": true,
      "content": "",
      "contentLength": 0,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "tobi/try",
      "url": "https://github.com/tobi/try",
      "date": 1769094695,
      "author": "",
      "guid": 37896,
      "unread": true,
      "content": "<p>fresh directories for every vibe</p><p><em>Your experiments deserve a home.</em> üè†</p><blockquote><p>For everyone who constantly creates new projects for little experiments, a one-file Ruby script to quickly manage and navigate to keep them somewhat organized</p></blockquote><p>Ever find yourself with 50 directories named , , , , scattered across your filesystem? Or worse, just coding in  and losing everything?</p><p> is here for your beautifully chaotic mind.</p><p>Instantly navigate through all your experiment directories with:</p><ul><li> that just works</li><li> - recently used stuff bubbles to the top</li><li> - creates directories like <code>2025-08-17-redis-experiment</code></li><li> - just one Ruby file, no dependencies</li></ul><pre><code># Bash/Zsh - add to .zshrc or .bashrc\neval \"$(try init)\"\n\n# Fish - add to config.fish\neval (try init | string collect)\n</code></pre><pre><code>curl -sL https://raw.githubusercontent.com/tobi/try/refs/heads/main/try.rb &gt; ~/.local/try.rb\n\n# Make \"try\" executable so it can be run directly\nchmod +x ~/.local/try.rb\n\n# Add to your shell (bash/zsh)\necho 'eval \"$(ruby ~/.local/try.rb init ~/src/tries)\"' &gt;&gt; ~/.zshrc\n\n# for fish shell users\necho 'eval (~/.local/try.rb init ~/src/tries | string collect)' &gt;&gt; ~/.config/fish/config.fish\n</code></pre><p>You're learning Redis. You create . Then . Then <code>~/projects/testing-redis-again</code>. Three weeks later you can't find that brilliant connection pooling solution you wrote at 2am.</p><p>All your experiments in one place, with instant fuzzy search:</p><pre><code>$ try pool\n‚Üí 2025-08-14-redis-connection-pool    2h, 18.5\n  2025-08-03-thread-pool              3d, 12.1\n  2025-07-22-db-pooling               2w, 8.3\n  + Create new: pool\n</code></pre><p>Type, arrow down, enter. You're there.</p><p>Not just substring matching - it's smart:</p><ul><li> matches </li><li>Recent stuff scores higher</li><li>Shorter names win on equal matches</li></ul><ul><li>Shows how long ago you touched each project</li><li>Recently accessed directories float to the top</li><li>Perfect for \"what was I working on yesterday?\"</li></ul><ul><li>Highlights matches as you type</li><li>Shows scores so you know why things are ranked</li><li>Dark mode by default (because obviously)</li></ul><ul><li>Everything lives in  (configurable via )</li><li>Auto-prefixes with dates: </li><li>Skip the date prompt if you already typed a name</li></ul><ul><li><pre><code># default is ~/src/tries\neval \"$(~/.local/try.rb init)\"\n# or pick a path\neval \"$(~/.local/try.rb init ~/src/tries)\"\n</code></pre></li><li><pre><code>eval (~/.local/try.rb init | string collect)\n# or pick a path\neval (~/.local/try.rb init ~/src/tries | string collect)\n</code></pre></li></ul><ul><li>The runtime commands printed by  are shell-neutral (absolute paths, quoted). Only the small wrapper function differs per shell.</li></ul><pre><code>try                                          # Browse all experiments\ntry redis                                    # Jump to redis experiment or create new\ntry new api                                  # Start with \"2025-08-17-new-api\"\ntry . [name]                                   # Create a dated worktree dir for current repo\ntry ./path/to/repo [name]                      # Use another repo as the worktree source\ntry worktree dir [name]                        # Same as above, explicit CLI form\ntry clone https://github.com/user/repo.git  # Clone repo into date-prefixed directory\ntry https://github.com/user/repo.git        # Shorthand for clone (same as above)\ntry --help                                   # See all options\n</code></pre><p>Notes on worktrees ( / ):</p><ul><li>With a custom [name], uses that; otherwise uses cwd‚Äôs basename. Both are prefixed with today‚Äôs date.</li><li>Inside a Git repo: adds a detached HEAD git worktree to the created directory.</li><li>Outside a repo: simply creates the directory and changes into it.</li></ul><p> can automatically clone git repositories into properly named experiment directories:</p><pre><code># Clone with auto-generated directory name\ntry clone https://github.com/tobi/try.git\n# Creates: 2025-08-27-tobi-try\n\n# Clone with custom name\ntry clone https://github.com/tobi/try.git my-fork\n# Creates: my-fork\n\n# Shorthand syntax (no need to type 'clone')\ntry https://github.com/tobi/try.git\n# Creates: 2025-08-27-tobi-try\n</code></pre><p>Supported git URI formats:</p><ul><li><code>https://github.com/user/repo.git</code> (HTTPS GitHub)</li><li><code>git@github.com:user/repo.git</code> (SSH GitHub)</li><li><code>https://gitlab.com/user/repo.git</code> (GitLab)</li><li><code>git@host.com:user/repo.git</code> (SSH other hosts)</li></ul><p>The  suffix is automatically removed from URLs when generating directory names.</p><ul><li> or  - Navigate</li><li> - Delete character</li><li> - Delete directory (with confirmation)</li></ul><p>Set  to change where experiments are stored:</p><pre><code>export TRY_PATH=~/code/sketches\n</code></pre><pre><code>nix run github:tobi/try\nnix run github:tobi/try -- --help\nnix run github:tobi/try init ~/my-tries\n</code></pre><pre><code>{\n  inputs.try.url = \"github:tobi/try\";\n  \n  imports = [ inputs.try.homeManagerModules.default ];\n  \n  programs.try = {\n    enable = true;\n    path = \"~/experiments\";  # optional, defaults to ~/src/tries\n  };\n}\n</code></pre><pre><code>brew tap tobi/try https://github.com/tobi/try\nbrew install try\n</code></pre><p>After installation, add to your shell:</p><ul><li><pre><code># default is ~/src/tries\neval \"$(try init)\"\n# or pick a path\neval \"$(try init ~/src/tries)\"\n</code></pre></li><li><pre><code>eval \"(try init | string collect)\"\n# or pick a path\neval \"(try init ~/src/tries | string collect)\"\n</code></pre></li></ul><ul><li>One file, no dependencies</li><li>Works on any system with Ruby (macOS has it built-in)</li><li>Fast enough for thousands of directories</li></ul><p>Your brain doesn't work in neat folders. You have ideas, you try things, you context-switch like a caffeinated squirrel. This tool embraces that.</p><p>Every experiment gets a home. Every home is instantly findable. Your 2am coding sessions are no longer lost to the void.</p><p><strong>Q: Why not just use  and ?</strong> A: Because you have 200 directories and can't remember if you called it , , or .</p><p> A: fzf is great for files. This is specifically for project directories, with time-awareness and auto-creation built in.</p><p><strong>Q: Can I use this for real projects?</strong> A: You can, but it's designed for experiments. Real projects deserve real names in real locations.</p><p><strong>Q: What if I have thousands of experiments?</strong> A: First, welcome to the club. Second, it handles it fine - the scoring algorithm ensures relevant stuff stays on top.</p><p>It's one file. If you want to change something, just edit it. Send a PR if you think others would like it too.</p><p>MIT - Do whatever you want with it.</p><p><em>Built for developers with ADHD by developers with ADHD.</em></p><p><em>Your experiments deserve a home.</em> üè†</p>",
      "contentLength": 5992,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "EveryInc/compound-engineering-plugin",
      "url": "https://github.com/EveryInc/compound-engineering-plugin",
      "date": 1769094695,
      "author": "",
      "guid": 37897,
      "unread": true,
      "content": "<p>Official Claude Code compound engineering plugin</p><p>A Claude Code plugin marketplace featuring the <strong>Compound Engineering Plugin</strong> ‚Äî tools that make each unit of engineering work easier than the last.</p><pre><code>/plugin marketplace add https://github.com/kieranklaassen/compound-engineering-plugin\n/plugin install compound-engineering\n</code></pre><h2>OpenCode + Codex support (experimental)</h2><p>This repo includes a Bun/TypeScript CLI that converts Claude Code plugins to OpenCode and Codex.</p><pre><code># convert the compound-engineering plugin into OpenCode format\nbunx @every-env/compound-plugin install compound-engineering --to opencode\n\n# convert to Codex format\nbunx @every-env/compound-plugin install compound-engineering --to codex\n</code></pre><pre><code>bun run src/index.ts install ./plugins/compound-engineering --to opencode\n</code></pre><p>OpenCode output is written to  by default, with  at the root and , , and  alongside it. Both provider targets are experimental and may change as the formats evolve. Codex output is written to  and , with each Claude command converted into both a prompt and a skill (the prompt instructs Codex to load the corresponding skill). Generated Codex skill descriptions are truncated to 1024 characters (Codex limit).</p><pre><code>Plan ‚Üí Work ‚Üí Review ‚Üí Compound ‚Üí Repeat\n</code></pre><table><tbody><tr><td>Turn feature ideas into detailed implementation plans</td></tr><tr><td>Execute plans with worktrees and task tracking</td></tr><tr><td>Multi-agent code review before merging</td></tr><tr><td>Document learnings to make future work easier</td></tr></tbody></table><p>Each cycle compounds: plans inform future plans, reviews catch more issues, patterns get documented.</p><p><strong>Each unit of engineering work should make subsequent units easier‚Äînot harder.</strong></p><p>Traditional development accumulates technical debt. Every feature adds complexity. The codebase becomes harder to work with over time.</p><p>Compound engineering inverts this. 80% is in planning and review, 20% is in execution:</p><ul><li>Plan thoroughly before writing code</li><li>Review to catch issues and capture learnings</li><li>Codify knowledge so it's reusable</li><li>Keep quality high so future changes are easy</li></ul>",
      "contentLength": 1958,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "microsoft/agent-lightning",
      "url": "https://github.com/microsoft/agent-lightning",
      "date": 1769094695,
      "author": "",
      "guid": 37898,
      "unread": true,
      "content": "<p>The absolute trainer to light up AI agents.</p><p><strong>The absolute trainer to light up AI agents.</strong></p><ul><li>Turn your agent into an optimizable beast with  (almost)! üí§</li><li>Build with  agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! ü§ñ</li><li> optimize one or more agents in a multi-agent system. üéØ</li><li>Embraces  like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. ü§ó</li></ul><pre><code>pip install agentlightning\n</code></pre><p>For the latest nightly build (cutting-edge features), you can install from Test PyPI:</p><pre><code>pip install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ --pre agentlightning\n</code></pre><p>Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight  helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.</p><p>On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.</p><p>No rewrites, no lock-in, just a clear path from first rollout to steady improvement.</p><p>If you find Agent Lightning useful in your research or projects, please cite our paper:</p><pre><code>@misc{luo2025agentlightningtrainai,\n      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},\n      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},\n      year={2025},\n      eprint={2508.03680},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2508.03680},\n}\n</code></pre><p>This project welcomes contributions and suggestions. Start by reading the <a href=\"https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/community/contributing.md\">Contributing Guide</a> for recommended contribution points, environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit <a href=\"https://cla.opensource.microsoft.com\">https://cla.opensource.microsoft.com</a>.</p><p>When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.</p><p>This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow <a href=\"https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general\">Microsoft's Trademark &amp; Brand Guidelines</a>. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.</p><p>This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.</p><p>This project is licensed under the MIT License. See the <a href=\"https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE\">LICENSE</a> file for details.</p>",
      "contentLength": 3641,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "VectifyAI/PageIndex",
      "url": "https://github.com/VectifyAI/PageIndex",
      "date": 1769094695,
      "author": "",
      "guid": 37899,
      "unread": true,
      "content": "<p>üìë PageIndex: Document Index for Vectorless, Reasoning-based RAG</p><p>Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic  rather than true . But  ‚Äî what we truly need in retrieval is , and that requires . When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.</p><p>Inspired by AlphaGo, we propose  ‚Äî a ,  system that builds a  from long documents and uses LLMs to  for <strong>agentic, context-aware retrieval</strong>. It simulates how  navigate and extract knowledge from complex documents through , enabling LLMs to  and  their way to the most relevant document sections. PageIndex performs retrieval in two steps:</p><ol><li>Generate a ‚ÄúTable-of-Contents‚Äù  of documents</li><li>Perform reasoning-based retrieval through </li></ol><p>Compared to traditional vector-based RAG,  features:</p><ul><li>: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.</li><li>: Documents are organized into natural sections, not artificial chunks.</li><li>: Simulates how human experts navigate and extract knowledge from complex documents.</li><li><strong>Better Explainability and Traceability</strong>: Retrieval is based on reasoning ‚Äî traceable and interpretable, with page and section references. No more opaque, approximate vector search (‚Äúvibe retrieval‚Äù).</li></ul><p>PageIndex powers a reasoning-based RAG system that achieved <a href=\"https://github.com/VectifyAI/Mafin2.5-FinanceBench\">98.7% accuracy</a> on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our <a href=\"https://vectify.ai/blog/Mafin2.5\">blog post</a> for details).</p><p>The PageIndex service is available as a ChatGPT-style <a href=\"https://chat.pageindex.ai\">chat platform</a>, or can be integrated via <a href=\"https://pageindex.ai/mcp\">MCP</a> or <a href=\"https://docs.pageindex.ai/quickstart\">API</a>.</p><ul><li>Self-host ‚Äî run locally with this open-source repo.</li></ul><ul><li>Try the <a href=\"https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb\"></a> notebook ‚Äî a , hands-on example of reasoning-based RAG using PageIndex.</li></ul><p>PageIndex can transform lengthy PDF documents into a semantic , similar to a  but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.</p><pre><code>...\n{\n  \"title\": \"Financial Stability\",\n  \"node_id\": \"0006\",\n  \"start_index\": 21,\n  \"end_index\": 22,\n  \"summary\": \"The Federal Reserve ...\",\n  \"nodes\": [\n    {\n      \"title\": \"Monitoring Financial Vulnerabilities\",\n      \"node_id\": \"0007\",\n      \"start_index\": 22,\n      \"end_index\": 28,\n      \"summary\": \"The Federal Reserve's monitoring ...\"\n    },\n    {\n      \"title\": \"Domestic and International Cooperation and Coordination\",\n      \"node_id\": \"0008\",\n      \"start_index\": 28,\n      \"end_index\": 31,\n      \"summary\": \"In 2023, the Federal Reserve collaborated ...\"\n    }\n  ]\n}\n...\n</code></pre><p>You can generate the PageIndex tree structure with this open-source repo, or use our <a href=\"https://docs.pageindex.ai/quickstart\">API</a></p><p>You can follow these steps to generate a PageIndex tree from a PDF document.</p><pre><code>pip3 install --upgrade -r requirements.txt\n</code></pre><h3>2. Set your OpenAI API key</h3><p>Create a  file in the root directory and add your API key:</p><pre><code>CHATGPT_API_KEY=your_openai_key_here\n</code></pre><h3>3. Run PageIndex on your PDF</h3><pre><code>python3 run_pageindex.py --pdf_path /path/to/your/document.pdf\n</code></pre><p><a href=\"https://vectify.ai/mafin\">Mafin 2.5</a> is a reasoning-based RAG system for financial document analysis, powered by . It achieved a state-of-the-art <a href=\"https://vectify.ai/blog/Mafin2.5\"></a> on the <a href=\"https://arxiv.org/abs/2311.11944\">FinanceBench</a> benchmark, significantly outperforming traditional vector-based RAG systems.</p><p>PageIndex's hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.</p><ul><li>üß™ <a href=\"https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex\">Cookbooks</a>: hands-on, runnable examples and advanced use cases.</li><li>üìñ <a href=\"https://docs.pageindex.ai/doc-search\">Tutorials</a>: practical guides and strategies, including  and .</li><li>üìù <a href=\"https://pageindex.ai/blog\">Blog</a>: technical articles, research insights, and product updates.</li></ul><p>Leave us a star üåü if you like our project. Thank you!</p>",
      "contentLength": 3790,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "twitter/the-algorithm",
      "url": "https://github.com/twitter/the-algorithm",
      "date": 1769094695,
      "author": "",
      "guid": 37900,
      "unread": true,
      "content": "<p>Source code for the X Recommendation Algorithm</p><p>X's Recommendation Algorithm is a set of services and jobs that are responsible for serving feeds of posts and other content across all X product surfaces (e.g. For You Timeline, Search, Explore, Notifications). For an introduction to how the algorithm works, please refer to our <a href=\"https://blog.x.com/engineering/en_us/topics/open-source/2023/twitter-recommendation-algorithm\">engineering blog</a>.</p><p>Product surfaces at X are built on a shared set of data, models, and software frameworks. The shared components included in this repository are listed below:</p><table><tbody><tr><td>Core service that handles the reading and writing of post data.</td></tr><tr><td>Centralized platform to retrieve explicit (e.g. likes, replies) and implicit (e.g. profile visits, tweet clicks) user signals.</td></tr><tr><td>Community detection and sparse embeddings into those communities.</td></tr><tr><td>Dense knowledge graph embeddings for Users and Posts.</td></tr><tr><td>Model to predict the likelihood of an X User interacting with another User.</td></tr><tr><td>Page-Rank algorithm for calculating X User reputation.</td></tr><tr><td>Serves graph features for a directed pair of users (e.g. how many of User A's following liked posts from User B).</td></tr><tr><td>Compute scores between pairs of entities (Users, Posts, etc.) using embedding similarity.</td></tr><tr><td>High performance, machine learning model serving written in Rust.</td></tr><tr><td>Legacy machine learning framework built on TensorFlow v1.</td></tr></tbody></table><p>The product surfaces currently included in this repository are the For You Timeline and Recommended Notifications.</p><p>The diagram below illustrates how major services and jobs interconnect to construct a For You Timeline.</p><p>The core components of the For You Timeline included in this repository are listed below:</p><table><tbody><tr><td>Find and rank In-Network posts. ~50% of posts come from this candidate source.</td></tr><tr><td>Coordination layer for fetching Out-of-Network tweet candidates from underlying compute services.</td></tr><tr><td>Maintains an in memory User to Post interaction graph, and finds candidates based on traversals of this graph. This is built on the <a href=\"https://github.com/twitter/GraphJet\">GraphJet</a> framework. Several other GraphJet based features and candidate sources are located <a href=\"https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/recos\">here</a>.</td></tr><tr><td>Light Ranker model used by search index (Earlybird) to rank posts.</td></tr><tr><td>Neural network for ranking candidate posts. One of the main signals used to select timeline posts post candidate sourcing.</td></tr><tr><td>Main service used to construct and serve the Home Timeline. Built on <a href=\"https://raw.githubusercontent.com/twitter/the-algorithm/main/product-mixer/README.md\">product-mixer</a>.</td></tr><tr><td>Responsible for filtering X content to support legal compliance, improve product quality, increase user trust, protect revenue through the use of hard-filtering, visible product treatments, and coarse-grained downranking.</td></tr><tr><td>Legacy service which provides relevance-scored posts from the Earlybird Search Index and UTEG service.</td></tr></tbody></table><h3>Recommended Notifications</h3><p>The core components of Recommended Notifications included in this repository are listed below:</p><table><tbody><tr><td>Main recommendation service at X used to surface recommendations to our users via notifications.</td></tr><tr><td>Light Ranker model used by pushservice to rank posts. Bridges candidate generation and heavy ranking by pre-selecting highly-relevant candidates from the initial huge candidate pool.</td></tr><tr><td>Multi-task learning model to predict the probabilities that the target users will open and engage with the sent notifications.</td></tr></tbody></table><p>We include Bazel BUILD files for most components, but not a top-level BUILD or WORKSPACE file. We plan to add a more complete build and test system in the future.</p><p>We invite the community to submit GitHub issues and pull requests for suggestions on improving the recommendation algorithm. We are working on tools to manage these suggestions and sync changes to our internal repository. Any security concerns or issues should be routed to our official <a href=\"https://hackerone.com/x\">bug bounty program</a> through HackerOne. We hope to benefit from the collective intelligence and expertise of the global community in helping us identify issues and suggest improvements, ultimately leading to a better X.</p><p>Read our blog on the open source initiative <a href=\"https://blog.x.com/en_us/topics/company/2023/a-new-era-of-transparency-for-twitter\">here</a>.</p>",
      "contentLength": 3802,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "tambo-ai/tambo",
      "url": "https://github.com/tambo-ai/tambo",
      "date": 1769094695,
      "author": "",
      "guid": 37901,
      "unread": true,
      "content": "<p>Generative UI SDK for React</p><div align=\"center\"><img src=\"https://raw.githubusercontent.com/tambo-ai/tambo/main/assets/octo-white-background-rounded.png\" width=\"150\"><p>Build apps that adapt to your users.</p></div><p>Tambo is a generative UI SDK for React. Register your components, and the AI decides which ones to render based on natural language conversations.</p><p>Most software is built around a one-size-fits-all mental model that doesn't fit every user.</p><p><strong>Users shouldn't have to learn your app.</strong> Generative UI shows the right components based on what someone is trying to do. First-time users and power users see different things.</p><p><strong>Users shouldn't have to click through your workflows.</strong> \"Show me sales from last quarter grouped by region\" should just work. The AI translates what users want into the right interface.</p><pre><code>const components: TamboComponent[] = [{\n  name: \"Graph\",\n  description: \"Displays data as charts\",\n  component: Graph,\n  propsSchema: z.object({ data: z.array(...), type: z.enum([\"line\", \"bar\", \"pie\"]) })\n}];\n</code></pre><pre><code>npx tambo create-app my-tambo-app\ncd my-tambo-app\nnpx tambo init      # choose cloud or self-hosted\nnpm run dev\n</code></pre><p> is a free hosted backend.  runs on your own infrastructure.</p><p>Tambo supports two kinds of components.</p><p> render once in response to a message. Charts, summaries, data visualizations.</p><p> persist and update as users refine requests. Shopping carts, spreadsheets, task boards.</p><p>Tell the AI which components it can use. Zod schemas define the props.</p><pre><code>// Generative: AI creates on-demand\nconst components: TamboComponent[] = [\n  {\n    name: \"Graph\",\n    description: \"Displays data as charts using Recharts library\",\n    component: Graph,\n    propsSchema: z.object({\n      data: z.array(z.object({ name: z.string(), value: z.number() })),\n      type: z.enum([\"line\", \"bar\", \"pie\"]),\n    }),\n  },\n];\n\n// Interactable: persists and updates by ID\nconst InteractableNote = withInteractable(Note, {\n  componentName: \"Note\",\n  description: \"A note supporting title, content, and color modifications\",\n  propsSchema: z.object({\n    title: z.string(),\n    content: z.string(),\n    color: z.enum([\"white\", \"yellow\", \"blue\", \"green\"]).optional(),\n  }),\n});\n</code></pre><p>Wrap your app with .</p><pre><code>&lt;TamboProvider\n  apiKey={process.env.NEXT_PUBLIC_TAMBO_API_KEY!}\n  components={components}\n&gt;\n  &lt;Chat /&gt;\n  &lt;InteractableNote id=\"note-1\" title=\"My Note\" content=\"Start writing...\" /&gt;\n&lt;/TamboProvider&gt;\n</code></pre><p>For apps with signed-in users, pass a per-user  (OAuth access token) to  to enable per-user auth and connect Tambo to your app's end-user identity. See <a href=\"https://docs.tambo.co/concepts/user-authentication\">User Authentication</a> for details.</p><p>Send messages with .  handles streaming, including props for generated components and tool calls.</p><pre><code>const { value, setValue, submit, isPending } = useTamboThreadInput();\n\n&lt;input value={value} onChange={(e) =&gt; setValue(e.target.value)} /&gt;\n&lt;button onClick={() =&gt; submit()} disabled={isPending}&gt;Send&lt;/button&gt;\n</code></pre><pre><code>const { thread } = useTamboThread();\n\n{\n  thread.messages.map((message) =&gt; (\n    &lt;div key={message.id}&gt;\n      {Array.isArray(message.content) ? (\n        message.content.map((part, i) =&gt;\n          part.type === \"text\" ? &lt;p key={i}&gt;{part.text}&lt;/p&gt; : null,\n        )\n      ) : (\n        &lt;p&gt;{String(message.content)}&lt;/p&gt;\n      )}\n      {message.renderedComponent}\n    &lt;/div&gt;\n  ));\n}\n</code></pre><p>Track streaming status if you want progressive loading:</p><pre><code>const { streamStatus, propStatus } = useTamboStreamStatus();\n\nif (!streamStatus.isSuccess) return &lt;Spinner /&gt;;\n{\n  propStatus[\"title\"]?.isSuccess &amp;&amp; &lt;h3&gt;{title}&lt;/h3&gt;;\n}\n</code></pre><p>Connect to Linear, Slack, databases, or your own MCP servers. Tambo supports the full MCP protocol: tools, prompts, elicitations, and sampling.</p><pre><code>import { MCPTransport } from \"@tambo-ai/react/mcp\";\n\nconst mcpServers = [\n  {\n    name: \"filesystem\",\n    url: \"http://localhost:8261/mcp\",\n    transport: MCPTransport.HTTP,\n  },\n];\n\n&lt;TamboProvider components={components} mcpServers={mcpServers}&gt;\n  &lt;App /&gt;\n&lt;/TamboProvider&gt;;\n</code></pre><p>Supports the full MCP protocol: tools, prompts, elicitations, and sampling.</p><p>Sometimes you need functions that run in the browser. DOM manipulation, authenticated fetches, accessing React state. Define them as tools and the AI can call them.</p><pre><code>const tools: TamboTool[] = [\n  {\n    name: \"getWeather\",\n    description: \"Fetches weather for a location\",\n    tool: async (location: string) =&gt;\n      fetch(`/api/weather?q=${encodeURIComponent(location)}`).then((r) =&gt;\n        r.json(),\n      ),\n    toolSchema: z\n      .function()\n      .args(z.string())\n      .returns(\n        z.object({\n          temperature: z.number(),\n          condition: z.string(),\n          location: z.string(),\n        }),\n      ),\n  },\n];\n\n&lt;TamboProvider tools={tools} components={components}&gt;\n  &lt;App /&gt;\n&lt;/TamboProvider&gt;;\n</code></pre><h3>Context, Auth, and Suggestions</h3><p> lets you pass metadata to give the AI better responses. User state, app settings, current page.  passes tokens from your auth provider.  generates prompts users can click based on what they're doing.</p><pre><code>&lt;TamboProvider\n  userToken={userToken}\n  contextHelpers={{\n    selectedItems: () =&gt; ({\n      key: \"selectedItems\",\n      value: selectedItems.map((i) =&gt; i.name).join(\", \"),\n    }),\n    currentPage: () =&gt; ({ key: \"page\", value: window.location.pathname }),\n  }}\n/&gt;\n</code></pre><pre><code>const { suggestions, accept } = useTamboSuggestions({ maxSuggestions: 3 });\n\nsuggestions.map((s) =&gt; (\n  &lt;button key={s.id} onClick={() =&gt; accept(s)}&gt;\n    {s.title}\n  &lt;/button&gt;\n));\n</code></pre><p>OpenAI, Anthropic, Cerebras, Google Gemini, Mistral, and any OpenAI-compatible provider. <a href=\"https://docs.tambo.co/models\">Full list</a>. Missing one? <a href=\"https://github.com/tambo-ai/tambo/issues\">Let us know</a>.</p><table><thead><tr></tr></thead><tbody><tr><td>AI decides which components to render</td><td>Manual tool-to-component mapping</td><td>Via agent frameworks (LangGraph)</td></tr><tr></tr><tr><td><strong>Persistent stateful components</strong></td></tr><tr><td><strong>Client-side tool execution</strong></td></tr><tr></tr><tr></tr><tr><td>Streaming and tool abstractions</td></tr></tbody></table><p>Free forever. MIT licensed. 5-minute Docker setup.</p><pre><code>npx tambo init\n# Select \"Self-hosted\"\n</code></pre><p>Free tier, then pay as you grow.</p><ul><li>: 10,000 messages/month</li><li>: $25/mo for 200k messages + email support</li><li>: Custom volume, SLA, SOC 2, HIPAA</li></ul><p>This Turborepo hosts the React SDK ecosystem and Tambo Cloud platform.</p><p> has the web dashboard (Next.js), the API (NestJS), and MCP services.</p><p> has shared code. Database schema (Drizzle), LLM helpers, pure utilities, and tooling configs.</p><p>The root holds framework packages: , , , , .</p><p>You'll need Node.js 22+, npm 11+, and optionally Docker.</p><pre><code>git clone https://github.com/tambo-ai/tambo.git\ncd tambo\nnpm install\nnpm run dev        # apps/web + apps/api\n</code></pre><pre><code>npm run build        # Build everything\nnpm run lint         # Lint (lint:fix to autofix)\nnpm run check-types  # Type check\nnpm test             # Run tests\n</code></pre><p>Database (requires Docker):</p><pre><code>npm run db:generate  # Generate migrations\nnpm run db:migrate   # Apply migrations\nnpm run db:studio    # Open Drizzle Studio\n</code></pre><p>Unless otherwise noted in a workspace (app or package), code in this repo is licensed under MIT (see the root <a href=\"https://raw.githubusercontent.com/tambo-ai/tambo/main/LICENSE\">LICENSE</a>).</p><p>Some workspaces are licensed under Apache-2.0; see the accompanying  and  files in those workspaces.</p>",
      "contentLength": 6707,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    },
    {
      "title": "microsoft/Data-Science-For-Beginners",
      "url": "https://github.com/microsoft/Data-Science-For-Beginners",
      "date": 1769094695,
      "author": "",
      "guid": 37902,
      "unread": true,
      "content": "<p>10 Weeks, 20 Lessons, Data Science for All!</p><p>Azure Cloud Advocates at Microsoft are pleased to offer a 10-week, 20-lesson curriculum all about Data Science. Each lesson includes pre-lesson and post-lesson quizzes, written instructions to complete the lesson, a solution, and an assignment. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.</p><h4>Supported via GitHub Action (Automated &amp; Always Up-to-Date)</h4><blockquote><p>This repository includes 50+ language translations which significantly increases the download size. To clone without translations, use sparse checkout:</p><pre><code>git clone --filter=blob:none --sparse https://github.com/microsoft/Data-Science-For-Beginners.git\ncd Data-Science-For-Beginners\ngit sparse-checkout set --no-cone '/*' '!translations' '!translated_images'\n</code></pre><p>This gives you everything you need to complete the course with a much faster download.</p></blockquote><p><strong>If you wish to have additional translations languages supported are listed <a href=\"https://github.com/Azure/co-op-translator/raw/main/getting_started/supported-languages.md\">here</a></strong></p><p>We have a Discord learn with AI series ongoing, learn more and join us at <a href=\"https://aka.ms/learnwithai/discord\">Learn with AI Series</a> from 18 - 30 September, 2025. You will get tips and tricks of using GitHub Copilot for Data Science.</p><p>Get started with the following resources:</p><ul><li><a href=\"https://docs.microsoft.com/en-gb/learn/student-hub?WT.mc_id=academic-77958-bethanycheum\">Student Hub page</a> In this page, you will find beginner resources, Student packs and even ways to get a free cert voucher. This is one page you want to bookmark and check from time to time as we switch out content at least monthly.</li></ul><blockquote><p>: New to data science? Start with our <a href=\"https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/examples/README.md\">beginner-friendly examples</a>! These simple, well-commented examples will help you understand the basics before diving into the full curriculum. : to use this curriculum on your own, fork the entire repo and complete the exercises on your own, starting with a pre-lecture quiz. Then read the lecture and complete the rest of the activities. Try to create the projects by comprehending the lessons rather than copying the solution code; however, that code is available in the /solutions folders in each project-oriented lesson. Another idea would be to form a study group with friends and go through the content together. For further study, we recommend <a href=\"https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/qprpajyoy3x0g7?WT.mc_id=academic-77958-bethanycheum\">Microsoft Learn</a>.</p></blockquote><blockquote><p>üé• Click the image above for a video about the project the folks who created it!</p></blockquote><p>We have chosen two pedagogical tenets while building this curriculum: ensuring that it is project-based and that it includes frequent quizzes. By the end of this series, students will have learned basic principles of data science, including ethical concepts, data preparation, different ways of working with data, data visualization, data analysis, real-world use cases of data science, and more.</p><p>In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 10 week cycle.</p><ul><li>Optional supplemental video</li><li>For project-based lessons, step-by-step guides on how to build the project</li></ul><blockquote><p>: All quizzes are contained in the Quiz-App folder, for 40 total quizzes of three questions each. They are linked from within the lessons, but the quiz app can be run locally or deployed to Azure; follow the instruction in the  folder. They are gradually being localized.</p></blockquote><h2>üéì Beginner-Friendly Examples</h2><p> We've created a special <a href=\"https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/examples/README.md\">examples directory</a> with simple, well-commented code to help you get started:</p><ul><li>üåü  - Your first data science program</li><li>üìÇ  - Learn to read and explore datasets</li><li>üìä  - Calculate statistics and find patterns</li><li>üìà  - Create charts and graphs</li><li>üî¨  - Complete workflow from start to finish</li></ul><p>Each example includes detailed comments explaining every step, making it perfect for absolute beginners!</p><table><thead><tr></tr></thead><tbody><tr><td align=\"center\">Learn the basic concepts behind data science and how it‚Äôs related to artificial intelligence, machine learning, and big data.</td></tr><tr><td align=\"center\">Introduction to Statistics &amp; Probability</td><td align=\"center\">The mathematical techniques of probability and statistics to understand data.</td></tr><tr><td align=\"center\">Working with Relational Data</td><td align=\"center\">Introduction to relational data and the basics of exploring and analyzing relational data with the Structured Query Language, also known as SQL (pronounced ‚Äúsee-quell‚Äù).</td></tr><tr><td align=\"center\">Introduction to non-relational data, its various types and the basics of exploring and analyzing document databases.</td></tr><tr><td align=\"center\">Basics of using Python for data exploration with libraries such as Pandas. Foundational understanding of Python programming is recommended.</td></tr><tr><td align=\"center\">Topics on data techniques for cleaning and transforming the data to handle challenges of missing, inaccurate, or incomplete data.</td></tr><tr><td align=\"center\">Visualizing Relationships</td><td align=\"center\">Visualizing connections and correlations between sets of data and their variables.</td></tr><tr><td align=\"center\">Meaningful Visualizations</td><td align=\"center\">Techniques and guidance for making your visualizations valuable for effective problem solving and insights.</td></tr><tr><td align=\"center\">Introduction to the Data Science lifecycle</td><td align=\"center\">Introduction to the data science lifecycle and its first step of acquiring and extracting data.</td></tr><tr><td align=\"center\">This phase of the data science lifecycle focuses on techniques to analyze data.</td></tr><tr><td align=\"center\">This phase of the data science lifecycle focuses on presenting the insights from the data in a way that makes it easier for decision makers to understand.</td></tr></tbody></table><p>Follow these steps to open this sample in a Codespace:</p><ol><li>Click the Code drop-down menu and select the Open with Codespaces option.</li></ol><h2>VSCode Remote - Containers</h2><p>Follow these steps to open this repo in a container using your local machine and VSCode using the VS Code Remote - Containers extension:</p><ol><li>If this is your first time using a development container, please ensure your system meets the pre-reqs (i.e. have Docker installed) in <a href=\"https://code.visualstudio.com/docs/devcontainers/containers#_getting-started\">the getting started documentation</a>.</li></ol><p>To use this repository, you can either open the repository in an isolated Docker volume:</p><p>: Under the hood, this will use the Remote-Containers: <strong>Clone Repository in Container Volume...</strong> command to clone the source code in a Docker volume instead of the local filesystem. <a href=\"https://docs.docker.com/storage/volumes/\">Volumes</a> are the preferred mechanism for persisting container data.</p><p>Or open a locally cloned or downloaded version of the repository:</p><ul><li>Clone this repository to your local filesystem.</li><li>Press F1 and select the <strong>Remote-Containers: Open Folder in Container...</strong> command.</li><li>Select the cloned copy of this folder, wait for the container to start, and try things out.</li></ul><p>You can run this documentation offline by using <a href=\"https://docsify.js.org/#/\">Docsify</a>. Fork this repo, <a href=\"https://docsify.js.org/#/quickstart\">install Docsify</a> on your local machine, then in the root folder of this repo, type . The website will be served on port 3000 on your localhost: .</p><blockquote><p>Note, notebooks will not be rendered via Docsify, so when you need to run a notebook, do that separately in VS Code running a Python kernel.</p></blockquote><p>Our team produces other curricula! Check out:</p><h3>Azure / Edge / MCP / Agents</h3><p>If you get stuck or have any questions about building AI apps. Join fellow learners and experienced developers in discussions about MCP. It's a supportive community where questions are welcome and knowledge is shared freely.</p><p>If you have product feedback or errors while building visit:</p>",
      "contentLength": 6998,
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": "",
      "commentsUrl": null
    }
  ],
  "tags": []
}